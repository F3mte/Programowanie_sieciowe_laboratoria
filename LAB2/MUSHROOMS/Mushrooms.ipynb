{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "le = LabelEncoder()\n",
    "seed = np.random.seed\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>e</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>k</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0        p         x           s         n       t    p               f   \n",
       "1        e         x           s         y       t    a               f   \n",
       "2        e         b           s         w       t    l               f   \n",
       "3        p         x           y         w       t    p               f   \n",
       "4        e         x           s         g       f    n               f   \n",
       "...    ...       ...         ...       ...     ...  ...             ...   \n",
       "8119     e         k           s         n       f    n               a   \n",
       "8120     e         x           s         n       f    n               a   \n",
       "8121     e         f           s         n       f    n               a   \n",
       "8122     p         k           y         n       f    y               f   \n",
       "8123     e         x           s         n       f    n               a   \n",
       "\n",
       "     gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
       "0               c         n          k  ...                        s   \n",
       "1               c         b          k  ...                        s   \n",
       "2               c         b          n  ...                        s   \n",
       "3               c         n          n  ...                        s   \n",
       "4               w         b          k  ...                        s   \n",
       "...           ...       ...        ...  ...                      ...   \n",
       "8119            c         b          y  ...                        s   \n",
       "8120            c         b          y  ...                        s   \n",
       "8121            c         b          n  ...                        s   \n",
       "8122            c         n          b  ...                        k   \n",
       "8123            c         b          y  ...                        s   \n",
       "\n",
       "     stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
       "0                         w                      w         p          w   \n",
       "1                         w                      w         p          w   \n",
       "2                         w                      w         p          w   \n",
       "3                         w                      w         p          w   \n",
       "4                         w                      w         p          w   \n",
       "...                     ...                    ...       ...        ...   \n",
       "8119                      o                      o         p          o   \n",
       "8120                      o                      o         p          n   \n",
       "8121                      o                      o         p          o   \n",
       "8122                      w                      w         p          w   \n",
       "8123                      o                      o         p          o   \n",
       "\n",
       "     ring-number ring-type spore-print-color population habitat  \n",
       "0              o         p                 k          s       u  \n",
       "1              o         p                 n          n       g  \n",
       "2              o         p                 n          n       m  \n",
       "3              o         p                 k          s       u  \n",
       "4              o         e                 n          a       g  \n",
       "...          ...       ...               ...        ...     ...  \n",
       "8119           o         p                 b          c       l  \n",
       "8120           o         p                 b          v       l  \n",
       "8121           o         p                 b          c       l  \n",
       "8122           o         e                 w          v       l  \n",
       "8123           o         p                 o          c       l  \n",
       "\n",
       "[8124 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wczytanie danych mushrooms.csv\n",
    "df = pd.read_csv('mushrooms.csv')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>k</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>k</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0            x           s         n       t    p               f   \n",
       "1            x           s         y       t    a               f   \n",
       "2            b           s         w       t    l               f   \n",
       "3            x           y         w       t    p               f   \n",
       "4            x           s         g       f    n               f   \n",
       "...        ...         ...       ...     ...  ...             ...   \n",
       "8119         k           s         n       f    n               a   \n",
       "8120         x           s         n       f    n               a   \n",
       "8121         f           s         n       f    n               a   \n",
       "8122         k           y         n       f    y               f   \n",
       "8123         x           s         n       f    n               a   \n",
       "\n",
       "     gill-spacing gill-size gill-color stalk-shape  ...  \\\n",
       "0               c         n          k           e  ...   \n",
       "1               c         b          k           e  ...   \n",
       "2               c         b          n           e  ...   \n",
       "3               c         n          n           e  ...   \n",
       "4               w         b          k           t  ...   \n",
       "...           ...       ...        ...         ...  ...   \n",
       "8119            c         b          y           e  ...   \n",
       "8120            c         b          y           e  ...   \n",
       "8121            c         b          n           e  ...   \n",
       "8122            c         n          b           t  ...   \n",
       "8123            c         b          y           e  ...   \n",
       "\n",
       "     stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring  \\\n",
       "0                           s                      w                      w   \n",
       "1                           s                      w                      w   \n",
       "2                           s                      w                      w   \n",
       "3                           s                      w                      w   \n",
       "4                           s                      w                      w   \n",
       "...                       ...                    ...                    ...   \n",
       "8119                        s                      o                      o   \n",
       "8120                        s                      o                      o   \n",
       "8121                        s                      o                      o   \n",
       "8122                        k                      w                      w   \n",
       "8123                        s                      o                      o   \n",
       "\n",
       "     veil-type veil-color ring-number ring-type spore-print-color population  \\\n",
       "0            p          w           o         p                 k          s   \n",
       "1            p          w           o         p                 n          n   \n",
       "2            p          w           o         p                 n          n   \n",
       "3            p          w           o         p                 k          s   \n",
       "4            p          w           o         e                 n          a   \n",
       "...        ...        ...         ...       ...               ...        ...   \n",
       "8119         p          o           o         p                 b          c   \n",
       "8120         p          n           o         p                 b          v   \n",
       "8121         p          o           o         p                 b          c   \n",
       "8122         p          w           o         e                 w          v   \n",
       "8123         p          o           o         p                 o          c   \n",
       "\n",
       "     habitat  \n",
       "0          u  \n",
       "1          g  \n",
       "2          m  \n",
       "3          u  \n",
       "4          g  \n",
       "...      ...  \n",
       "8119       l  \n",
       "8120       l  \n",
       "8121       l  \n",
       "8122       l  \n",
       "8123       l  \n",
       "\n",
       "[8124 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       p\n",
       "1       e\n",
       "2       e\n",
       "3       p\n",
       "4       e\n",
       "       ..\n",
       "8119    e\n",
       "8120    e\n",
       "8121    e\n",
       "8122    p\n",
       "8123    e\n",
       "Name: class, Length: 8124, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wartosci wejsciowe\n",
    "X = df.drop(columns=['class'], axis=1)\n",
    "# Zapisanie klas grzybow (jadalne, niejadalne)\n",
    "y = df['class']\n",
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 117)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8124,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zakodowanie numerow klas wedlug goracej jedynki \n",
    "x = enc.fit_transform(X).toarray()\n",
    "display(x.shape)\n",
    "display(x)\n",
    "# Zmiana nazw klas na liczby\n",
    "y = le.fit_transform(y)\n",
    "display(y.shape)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Podzial na wartosci do trenowania i testowania\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "display(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron wielowarstwowy\n",
    "mlp = MLPClassifier(alpha=0.1, hidden_layer_sizes=60, learning_rate_init=0.1,\n",
    "              max_iter=1000, random_state=1, solver='lbfgs', verbose=10)\n",
    "mlp.out_activation = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametramy ktore posluza nam do znalezienia najlepszego klasyfikatora\n",
    "parametry = {'learning_rate_init' : (0.1, 0.01, 0.001), 'hidden_layer_sizes' : [20, 40, 60, 80, 100], \n",
    "            'solver' : ['adam', 'lbfgs', 'sgd']}\n",
    "# Siatka do przeszukiwania podanych parametrow\n",
    "clf = GridSearchCV(mlp, parametry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.13224266\n",
      "Iteration 2, loss = 0.04641125\n",
      "Iteration 3, loss = 0.02197856\n",
      "Iteration 4, loss = 0.01601985\n",
      "Iteration 5, loss = 0.01322489\n",
      "Iteration 6, loss = 0.01181752\n",
      "Iteration 7, loss = 0.01507871\n",
      "Iteration 8, loss = 0.02219789\n",
      "Iteration 9, loss = 0.01728260\n",
      "Iteration 10, loss = 0.01290325\n",
      "Iteration 11, loss = 0.01337601\n",
      "Iteration 12, loss = 0.01403639\n",
      "Iteration 13, loss = 0.01739473\n",
      "Iteration 14, loss = 0.03058362\n",
      "Iteration 15, loss = 0.01647977\n",
      "Iteration 16, loss = 0.01422199\n",
      "Iteration 17, loss = 0.01303225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.15219207\n",
      "Iteration 2, loss = 0.04488711\n",
      "Iteration 3, loss = 0.02136668\n",
      "Iteration 4, loss = 0.01473008\n",
      "Iteration 5, loss = 0.01463130\n",
      "Iteration 6, loss = 0.01299988\n",
      "Iteration 7, loss = 0.01888597\n",
      "Iteration 8, loss = 0.01296232\n",
      "Iteration 9, loss = 0.01221350\n",
      "Iteration 10, loss = 0.01293839\n",
      "Iteration 11, loss = 0.01565709\n",
      "Iteration 12, loss = 0.01381800\n",
      "Iteration 13, loss = 0.06558603\n",
      "Iteration 14, loss = 0.11846091\n",
      "Iteration 15, loss = 0.04047495\n",
      "Iteration 16, loss = 0.01844038\n",
      "Iteration 17, loss = 0.01483924\n",
      "Iteration 18, loss = 0.01180172\n",
      "Iteration 19, loss = 0.01115514\n",
      "Iteration 20, loss = 0.01098664\n",
      "Iteration 21, loss = 0.01101056\n",
      "Iteration 22, loss = 0.01277643\n",
      "Iteration 23, loss = 0.01132338\n",
      "Iteration 24, loss = 0.01057863\n",
      "Iteration 25, loss = 0.01061193\n",
      "Iteration 26, loss = 0.01415559\n",
      "Iteration 27, loss = 0.01191847\n",
      "Iteration 28, loss = 0.01039923\n",
      "Iteration 29, loss = 0.01040619\n",
      "Iteration 30, loss = 0.01110196\n",
      "Iteration 31, loss = 0.01178847\n",
      "Iteration 32, loss = 0.01288961\n",
      "Iteration 33, loss = 0.01719392\n",
      "Iteration 34, loss = 0.01244005\n",
      "Iteration 35, loss = 0.01435714\n",
      "Iteration 36, loss = 0.05992183\n",
      "Iteration 37, loss = 0.05749403\n",
      "Iteration 38, loss = 0.01848927\n",
      "Iteration 39, loss = 0.01188921\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.15376361\n",
      "Iteration 2, loss = 0.04629139\n",
      "Iteration 3, loss = 0.02923881\n",
      "Iteration 4, loss = 0.02033455\n",
      "Iteration 5, loss = 0.01380117\n",
      "Iteration 6, loss = 0.01330935\n",
      "Iteration 7, loss = 0.01674055\n",
      "Iteration 8, loss = 0.02047531\n",
      "Iteration 9, loss = 0.09092981\n",
      "Iteration 10, loss = 0.08373116\n",
      "Iteration 11, loss = 0.03248726\n",
      "Iteration 12, loss = 0.01706672\n",
      "Iteration 13, loss = 0.01321015\n",
      "Iteration 14, loss = 0.01196977\n",
      "Iteration 15, loss = 0.01060693\n",
      "Iteration 16, loss = 0.01088050\n",
      "Iteration 17, loss = 0.01092623\n",
      "Iteration 18, loss = 0.01398124\n",
      "Iteration 19, loss = 0.01369295\n",
      "Iteration 20, loss = 0.01304881\n",
      "Iteration 21, loss = 0.01527371\n",
      "Iteration 22, loss = 0.01904147\n",
      "Iteration 23, loss = 0.01327654\n",
      "Iteration 24, loss = 0.01335968\n",
      "Iteration 25, loss = 0.01140944\n",
      "Iteration 26, loss = 0.01217848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12771790\n",
      "Iteration 2, loss = 0.04125799\n",
      "Iteration 3, loss = 0.02465270\n",
      "Iteration 4, loss = 0.01677026\n",
      "Iteration 5, loss = 0.01306924\n",
      "Iteration 6, loss = 0.01410902\n",
      "Iteration 7, loss = 0.01521556\n",
      "Iteration 8, loss = 0.03291471\n",
      "Iteration 9, loss = 0.06607743\n",
      "Iteration 10, loss = 0.02848817\n",
      "Iteration 11, loss = 0.03388645\n",
      "Iteration 12, loss = 0.03876798\n",
      "Iteration 13, loss = 0.01622254\n",
      "Iteration 14, loss = 0.01114855\n",
      "Iteration 15, loss = 0.01054346\n",
      "Iteration 16, loss = 0.01085609\n",
      "Iteration 17, loss = 0.01139179\n",
      "Iteration 18, loss = 0.01407542\n",
      "Iteration 19, loss = 0.01078262\n",
      "Iteration 20, loss = 0.01050095\n",
      "Iteration 21, loss = 0.01024715\n",
      "Iteration 22, loss = 0.01093769\n",
      "Iteration 23, loss = 0.04329768\n",
      "Iteration 24, loss = 0.05053460\n",
      "Iteration 25, loss = 0.02204076\n",
      "Iteration 26, loss = 0.01267408\n",
      "Iteration 27, loss = 0.01088726\n",
      "Iteration 28, loss = 0.01052389\n",
      "Iteration 29, loss = 0.01019117\n",
      "Iteration 30, loss = 0.01024902\n",
      "Iteration 31, loss = 0.01027929\n",
      "Iteration 32, loss = 0.01020701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12195411\n",
      "Iteration 2, loss = 0.04331239\n",
      "Iteration 3, loss = 0.01955971\n",
      "Iteration 4, loss = 0.01219975\n",
      "Iteration 5, loss = 0.01295584\n",
      "Iteration 6, loss = 0.02216999\n",
      "Iteration 7, loss = 0.01260667\n",
      "Iteration 8, loss = 0.01381117\n",
      "Iteration 9, loss = 0.01391729\n",
      "Iteration 10, loss = 0.01189725\n",
      "Iteration 11, loss = 0.01393265\n",
      "Iteration 12, loss = 0.01079748\n",
      "Iteration 13, loss = 0.01274224\n",
      "Iteration 14, loss = 0.01513943\n",
      "Iteration 15, loss = 0.01686141\n",
      "Iteration 16, loss = 0.01116846\n",
      "Iteration 17, loss = 0.01576986\n",
      "Iteration 18, loss = 0.03163020\n",
      "Iteration 19, loss = 0.01360123\n",
      "Iteration 20, loss = 0.01272744\n",
      "Iteration 21, loss = 0.01317472\n",
      "Iteration 22, loss = 0.01148743\n",
      "Iteration 23, loss = 0.01550469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18402323\n",
      "Iteration 2, loss = 0.03747285\n",
      "Iteration 3, loss = 0.02715694\n",
      "Iteration 4, loss = 0.02305335\n",
      "Iteration 5, loss = 0.02065817\n",
      "Iteration 6, loss = 0.01912284\n",
      "Iteration 7, loss = 0.01788398\n",
      "Iteration 8, loss = 0.01718143\n",
      "Iteration 9, loss = 0.01658129\n",
      "Iteration 10, loss = 0.01606652\n",
      "Iteration 11, loss = 0.01562106\n",
      "Iteration 12, loss = 0.01531538\n",
      "Iteration 13, loss = 0.01495169\n",
      "Iteration 14, loss = 0.01465817\n",
      "Iteration 15, loss = 0.01441152\n",
      "Iteration 16, loss = 0.01416830\n",
      "Iteration 17, loss = 0.01395683\n",
      "Iteration 18, loss = 0.01376410\n",
      "Iteration 19, loss = 0.01354821\n",
      "Iteration 20, loss = 0.01336419\n",
      "Iteration 21, loss = 0.01318590\n",
      "Iteration 22, loss = 0.01301156\n",
      "Iteration 23, loss = 0.01285593\n",
      "Iteration 24, loss = 0.01269120\n",
      "Iteration 25, loss = 0.01256769\n",
      "Iteration 26, loss = 0.01242334\n",
      "Iteration 27, loss = 0.01227146\n",
      "Iteration 28, loss = 0.01214777\n",
      "Iteration 29, loss = 0.01200751\n",
      "Iteration 30, loss = 0.01189074\n",
      "Iteration 31, loss = 0.01177463\n",
      "Iteration 32, loss = 0.01165535\n",
      "Iteration 33, loss = 0.01155488\n",
      "Iteration 34, loss = 0.01144350\n",
      "Iteration 35, loss = 0.01134362\n",
      "Iteration 36, loss = 0.01125197\n",
      "Iteration 37, loss = 0.01117346\n",
      "Iteration 38, loss = 0.01106708\n",
      "Iteration 39, loss = 0.01099475\n",
      "Iteration 40, loss = 0.01090350\n",
      "Iteration 41, loss = 0.01082925\n",
      "Iteration 42, loss = 0.01073587\n",
      "Iteration 43, loss = 0.01066275\n",
      "Iteration 44, loss = 0.01059405\n",
      "Iteration 45, loss = 0.01052050\n",
      "Iteration 46, loss = 0.01045234\n",
      "Iteration 47, loss = 0.01041074\n",
      "Iteration 48, loss = 0.01032848\n",
      "Iteration 49, loss = 0.01025966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21751896\n",
      "Iteration 2, loss = 0.04195915\n",
      "Iteration 3, loss = 0.02710898\n",
      "Iteration 4, loss = 0.02292901\n",
      "Iteration 5, loss = 0.02066850\n",
      "Iteration 6, loss = 0.01943857\n",
      "Iteration 7, loss = 0.01843848\n",
      "Iteration 8, loss = 0.01786547\n",
      "Iteration 9, loss = 0.01735714\n",
      "Iteration 10, loss = 0.01692490\n",
      "Iteration 11, loss = 0.01659353\n",
      "Iteration 12, loss = 0.01631440\n",
      "Iteration 13, loss = 0.01600268\n",
      "Iteration 14, loss = 0.01575516\n",
      "Iteration 15, loss = 0.01552161\n",
      "Iteration 16, loss = 0.01529871\n",
      "Iteration 17, loss = 0.01507854\n",
      "Iteration 18, loss = 0.01489255\n",
      "Iteration 19, loss = 0.01471518\n",
      "Iteration 20, loss = 0.01453063\n",
      "Iteration 21, loss = 0.01434344\n",
      "Iteration 22, loss = 0.01418490\n",
      "Iteration 23, loss = 0.01401470\n",
      "Iteration 24, loss = 0.01387251\n",
      "Iteration 25, loss = 0.01372283\n",
      "Iteration 26, loss = 0.01358714\n",
      "Iteration 27, loss = 0.01345749\n",
      "Iteration 28, loss = 0.01331651\n",
      "Iteration 29, loss = 0.01319433\n",
      "Iteration 30, loss = 0.01307427\n",
      "Iteration 31, loss = 0.01296330\n",
      "Iteration 32, loss = 0.01284937\n",
      "Iteration 33, loss = 0.01274203\n",
      "Iteration 34, loss = 0.01263618\n",
      "Iteration 35, loss = 0.01253808\n",
      "Iteration 36, loss = 0.01243602\n",
      "Iteration 37, loss = 0.01234655\n",
      "Iteration 38, loss = 0.01226286\n",
      "Iteration 39, loss = 0.01217089\n",
      "Iteration 40, loss = 0.01208491\n",
      "Iteration 41, loss = 0.01200905\n",
      "Iteration 42, loss = 0.01192625\n",
      "Iteration 43, loss = 0.01185257\n",
      "Iteration 44, loss = 0.01178129\n",
      "Iteration 45, loss = 0.01171756\n",
      "Iteration 46, loss = 0.01165381\n",
      "Iteration 47, loss = 0.01159742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19521314\n",
      "Iteration 2, loss = 0.03488336\n",
      "Iteration 3, loss = 0.02772742\n",
      "Iteration 4, loss = 0.02473858\n",
      "Iteration 5, loss = 0.02228026\n",
      "Iteration 6, loss = 0.02040084\n",
      "Iteration 7, loss = 0.01906249\n",
      "Iteration 8, loss = 0.01835211\n",
      "Iteration 9, loss = 0.01752776\n",
      "Iteration 10, loss = 0.01689188\n",
      "Iteration 11, loss = 0.01650165\n",
      "Iteration 12, loss = 0.01605996\n",
      "Iteration 13, loss = 0.01574354\n",
      "Iteration 14, loss = 0.01537049\n",
      "Iteration 15, loss = 0.01507956\n",
      "Iteration 16, loss = 0.01482821\n",
      "Iteration 17, loss = 0.01462837\n",
      "Iteration 18, loss = 0.01437656\n",
      "Iteration 19, loss = 0.01416347\n",
      "Iteration 20, loss = 0.01397010\n",
      "Iteration 21, loss = 0.01379436\n",
      "Iteration 22, loss = 0.01360184\n",
      "Iteration 23, loss = 0.01341715\n",
      "Iteration 24, loss = 0.01324487\n",
      "Iteration 25, loss = 0.01309145\n",
      "Iteration 26, loss = 0.01294408\n",
      "Iteration 27, loss = 0.01281803\n",
      "Iteration 28, loss = 0.01265273\n",
      "Iteration 29, loss = 0.01252294\n",
      "Iteration 30, loss = 0.01240155\n",
      "Iteration 31, loss = 0.01229081\n",
      "Iteration 32, loss = 0.01214560\n",
      "Iteration 33, loss = 0.01207300\n",
      "Iteration 34, loss = 0.01195868\n",
      "Iteration 35, loss = 0.01186033\n",
      "Iteration 36, loss = 0.01175419\n",
      "Iteration 37, loss = 0.01163445\n",
      "Iteration 38, loss = 0.01155046\n",
      "Iteration 39, loss = 0.01146665\n",
      "Iteration 40, loss = 0.01136551\n",
      "Iteration 41, loss = 0.01128287\n",
      "Iteration 42, loss = 0.01120197\n",
      "Iteration 43, loss = 0.01114112\n",
      "Iteration 44, loss = 0.01106269\n",
      "Iteration 45, loss = 0.01098619\n",
      "Iteration 46, loss = 0.01091945\n",
      "Iteration 47, loss = 0.01085116\n",
      "Iteration 48, loss = 0.01082966\n",
      "Iteration 49, loss = 0.01072403\n",
      "Iteration 50, loss = 0.01067080\n",
      "Iteration 51, loss = 0.01060764\n",
      "Iteration 52, loss = 0.01056399\n",
      "Iteration 53, loss = 0.01051454\n",
      "Iteration 54, loss = 0.01044293\n",
      "Iteration 55, loss = 0.01038968\n",
      "Iteration 56, loss = 0.01034572\n",
      "Iteration 57, loss = 0.01029743\n",
      "Iteration 58, loss = 0.01026052\n",
      "Iteration 59, loss = 0.01021273\n",
      "Iteration 60, loss = 0.01016683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22575668\n",
      "Iteration 2, loss = 0.04025342\n",
      "Iteration 3, loss = 0.02658364\n",
      "Iteration 4, loss = 0.02257238\n",
      "Iteration 5, loss = 0.02046131\n",
      "Iteration 6, loss = 0.01920332\n",
      "Iteration 7, loss = 0.01848158\n",
      "Iteration 8, loss = 0.01785900\n",
      "Iteration 9, loss = 0.01738282\n",
      "Iteration 10, loss = 0.01700007\n",
      "Iteration 11, loss = 0.01670260\n",
      "Iteration 12, loss = 0.01637527\n",
      "Iteration 13, loss = 0.01610937\n",
      "Iteration 14, loss = 0.01585491\n",
      "Iteration 15, loss = 0.01562521\n",
      "Iteration 16, loss = 0.01539633\n",
      "Iteration 17, loss = 0.01518606\n",
      "Iteration 18, loss = 0.01498633\n",
      "Iteration 19, loss = 0.01479687\n",
      "Iteration 20, loss = 0.01461133\n",
      "Iteration 21, loss = 0.01444203\n",
      "Iteration 22, loss = 0.01426753\n",
      "Iteration 23, loss = 0.01410864\n",
      "Iteration 24, loss = 0.01395349\n",
      "Iteration 25, loss = 0.01380577\n",
      "Iteration 26, loss = 0.01367515\n",
      "Iteration 27, loss = 0.01352501\n",
      "Iteration 28, loss = 0.01339223\n",
      "Iteration 29, loss = 0.01325935\n",
      "Iteration 30, loss = 0.01314007\n",
      "Iteration 31, loss = 0.01302546\n",
      "Iteration 32, loss = 0.01290474\n",
      "Iteration 33, loss = 0.01280325\n",
      "Iteration 34, loss = 0.01269174\n",
      "Iteration 35, loss = 0.01259212\n",
      "Iteration 36, loss = 0.01250452\n",
      "Iteration 37, loss = 0.01239734\n",
      "Iteration 38, loss = 0.01231168\n",
      "Iteration 39, loss = 0.01222408\n",
      "Iteration 40, loss = 0.01213725\n",
      "Iteration 41, loss = 0.01205651\n",
      "Iteration 42, loss = 0.01197425\n",
      "Iteration 43, loss = 0.01190126\n",
      "Iteration 44, loss = 0.01183069\n",
      "Iteration 45, loss = 0.01175393\n",
      "Iteration 46, loss = 0.01170140\n",
      "Iteration 47, loss = 0.01162178\n",
      "Iteration 48, loss = 0.01155326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18522688\n",
      "Iteration 2, loss = 0.02430211\n",
      "Iteration 3, loss = 0.01985580\n",
      "Iteration 4, loss = 0.01858870\n",
      "Iteration 5, loss = 0.01769131\n",
      "Iteration 6, loss = 0.01709079\n",
      "Iteration 7, loss = 0.01648983\n",
      "Iteration 8, loss = 0.01600370\n",
      "Iteration 9, loss = 0.01559722\n",
      "Iteration 10, loss = 0.01522549\n",
      "Iteration 11, loss = 0.01490574\n",
      "Iteration 12, loss = 0.01457894\n",
      "Iteration 13, loss = 0.01430184\n",
      "Iteration 14, loss = 0.01403791\n",
      "Iteration 15, loss = 0.01375715\n",
      "Iteration 16, loss = 0.01355224\n",
      "Iteration 17, loss = 0.01330494\n",
      "Iteration 18, loss = 0.01307959\n",
      "Iteration 19, loss = 0.01288874\n",
      "Iteration 20, loss = 0.01269212\n",
      "Iteration 21, loss = 0.01250561\n",
      "Iteration 22, loss = 0.01232573\n",
      "Iteration 23, loss = 0.01215297\n",
      "Iteration 24, loss = 0.01200463\n",
      "Iteration 25, loss = 0.01183258\n",
      "Iteration 26, loss = 0.01169532\n",
      "Iteration 27, loss = 0.01154712\n",
      "Iteration 28, loss = 0.01140777\n",
      "Iteration 29, loss = 0.01128760\n",
      "Iteration 30, loss = 0.01115349\n",
      "Iteration 31, loss = 0.01103366\n",
      "Iteration 32, loss = 0.01092444\n",
      "Iteration 33, loss = 0.01080487\n",
      "Iteration 34, loss = 0.01070286\n",
      "Iteration 35, loss = 0.01058828\n",
      "Iteration 36, loss = 0.01048796\n",
      "Iteration 37, loss = 0.01039001\n",
      "Iteration 38, loss = 0.01029459\n",
      "Iteration 39, loss = 0.01020113\n",
      "Iteration 40, loss = 0.01011543\n",
      "Iteration 41, loss = 0.01004392\n",
      "Iteration 42, loss = 0.00995605\n",
      "Iteration 43, loss = 0.00987708\n",
      "Iteration 44, loss = 0.00980060\n",
      "Iteration 45, loss = 0.00972748\n",
      "Iteration 46, loss = 0.00967266\n",
      "Iteration 47, loss = 0.00959832\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16723046\n",
      "Iteration 2, loss = 0.02797034\n",
      "Iteration 3, loss = 0.02013131\n",
      "Iteration 4, loss = 0.01685474\n",
      "Iteration 5, loss = 0.01493296\n",
      "Iteration 6, loss = 0.01358251\n",
      "Iteration 7, loss = 0.01255649\n",
      "Iteration 8, loss = 0.01190402\n",
      "Iteration 9, loss = 0.01134955\n",
      "Iteration 10, loss = 0.01097067\n",
      "Iteration 11, loss = 0.01051648\n",
      "Iteration 12, loss = 0.01027674\n",
      "Iteration 13, loss = 0.01019023\n",
      "Iteration 14, loss = 0.01008729\n",
      "Iteration 15, loss = 0.00979266\n",
      "Iteration 16, loss = 0.00964034\n",
      "Iteration 17, loss = 0.00952236\n",
      "Iteration 18, loss = 0.00938442\n",
      "Iteration 19, loss = 0.00962988\n",
      "Iteration 20, loss = 0.00944057\n",
      "Iteration 21, loss = 0.00934242\n",
      "Iteration 22, loss = 0.00945028\n",
      "Iteration 23, loss = 0.00907525\n",
      "Iteration 24, loss = 0.00909994\n",
      "Iteration 25, loss = 0.00943490\n",
      "Iteration 26, loss = 0.00928769\n",
      "Iteration 27, loss = 0.00923962\n",
      "Iteration 28, loss = 0.00907419\n",
      "Iteration 29, loss = 0.00916620\n",
      "Iteration 30, loss = 0.00911040\n",
      "Iteration 31, loss = 0.00927605\n",
      "Iteration 32, loss = 0.00917284\n",
      "Iteration 33, loss = 0.00891385\n",
      "Iteration 34, loss = 0.00916788\n",
      "Iteration 35, loss = 0.00891481\n",
      "Iteration 36, loss = 0.00895917\n",
      "Iteration 37, loss = 0.00904790\n",
      "Iteration 38, loss = 0.00938408\n",
      "Iteration 39, loss = 0.00940171\n",
      "Iteration 40, loss = 0.00886223\n",
      "Iteration 41, loss = 0.00888392\n",
      "Iteration 42, loss = 0.00895418\n",
      "Iteration 43, loss = 0.00884778\n",
      "Iteration 44, loss = 0.00882639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18393336\n",
      "Iteration 2, loss = 0.03029851\n",
      "Iteration 3, loss = 0.02164090\n",
      "Iteration 4, loss = 0.01827687\n",
      "Iteration 5, loss = 0.01622638\n",
      "Iteration 6, loss = 0.01478198\n",
      "Iteration 7, loss = 0.01366409\n",
      "Iteration 8, loss = 0.01295504\n",
      "Iteration 9, loss = 0.01231209\n",
      "Iteration 10, loss = 0.01185312\n",
      "Iteration 11, loss = 0.01148072\n",
      "Iteration 12, loss = 0.01121809\n",
      "Iteration 13, loss = 0.01090691\n",
      "Iteration 14, loss = 0.01075282\n",
      "Iteration 15, loss = 0.01067374\n",
      "Iteration 16, loss = 0.01064164\n",
      "Iteration 17, loss = 0.01067387\n",
      "Iteration 18, loss = 0.01033172\n",
      "Iteration 19, loss = 0.01036484\n",
      "Iteration 20, loss = 0.01048310\n",
      "Iteration 21, loss = 0.01036647\n",
      "Iteration 22, loss = 0.01037877\n",
      "Iteration 23, loss = 0.01017770\n",
      "Iteration 24, loss = 0.01005060\n",
      "Iteration 25, loss = 0.01031335\n",
      "Iteration 26, loss = 0.01027269\n",
      "Iteration 27, loss = 0.01019545\n",
      "Iteration 28, loss = 0.01033012\n",
      "Iteration 29, loss = 0.00994849\n",
      "Iteration 30, loss = 0.00996816\n",
      "Iteration 31, loss = 0.01013359\n",
      "Iteration 32, loss = 0.01034188\n",
      "Iteration 33, loss = 0.01013112\n",
      "Iteration 34, loss = 0.01009918\n",
      "Iteration 35, loss = 0.00991120\n",
      "Iteration 36, loss = 0.01019598\n",
      "Iteration 37, loss = 0.00996145\n",
      "Iteration 38, loss = 0.00988980\n",
      "Iteration 39, loss = 0.00991890\n",
      "Iteration 40, loss = 0.00990463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17835149\n",
      "Iteration 2, loss = 0.02861550\n",
      "Iteration 3, loss = 0.02067209\n",
      "Iteration 4, loss = 0.01767280\n",
      "Iteration 5, loss = 0.01530664\n",
      "Iteration 6, loss = 0.01395656\n",
      "Iteration 7, loss = 0.01304120\n",
      "Iteration 8, loss = 0.01229943\n",
      "Iteration 9, loss = 0.01176120\n",
      "Iteration 10, loss = 0.01098287\n",
      "Iteration 11, loss = 0.01124259\n",
      "Iteration 12, loss = 0.01061081\n",
      "Iteration 13, loss = 0.01040784\n",
      "Iteration 14, loss = 0.01006670\n",
      "Iteration 15, loss = 0.00992860\n",
      "Iteration 16, loss = 0.00976870\n",
      "Iteration 17, loss = 0.00974797\n",
      "Iteration 18, loss = 0.01002640\n",
      "Iteration 19, loss = 0.00974162\n",
      "Iteration 20, loss = 0.00979786\n",
      "Iteration 21, loss = 0.00996748\n",
      "Iteration 22, loss = 0.00980559\n",
      "Iteration 23, loss = 0.00952063\n",
      "Iteration 24, loss = 0.00942781\n",
      "Iteration 25, loss = 0.00939735\n",
      "Iteration 26, loss = 0.00948885\n",
      "Iteration 27, loss = 0.00977441\n",
      "Iteration 28, loss = 0.01043168\n",
      "Iteration 29, loss = 0.00981798\n",
      "Iteration 30, loss = 0.00948159\n",
      "Iteration 31, loss = 0.00966296\n",
      "Iteration 32, loss = 0.00932938\n",
      "Iteration 33, loss = 0.00937744\n",
      "Iteration 34, loss = 0.00983480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19770947\n",
      "Iteration 2, loss = 0.03119006\n",
      "Iteration 3, loss = 0.02226134\n",
      "Iteration 4, loss = 0.01880220\n",
      "Iteration 5, loss = 0.01660159\n",
      "Iteration 6, loss = 0.01507101\n",
      "Iteration 7, loss = 0.01402170\n",
      "Iteration 8, loss = 0.01315191\n",
      "Iteration 9, loss = 0.01249633\n",
      "Iteration 10, loss = 0.01198333\n",
      "Iteration 11, loss = 0.01172530\n",
      "Iteration 12, loss = 0.01130602\n",
      "Iteration 13, loss = 0.01114560\n",
      "Iteration 14, loss = 0.01088365\n",
      "Iteration 15, loss = 0.01081498\n",
      "Iteration 16, loss = 0.01063108\n",
      "Iteration 17, loss = 0.01057686\n",
      "Iteration 18, loss = 0.01047775\n",
      "Iteration 19, loss = 0.01037796\n",
      "Iteration 20, loss = 0.01032887\n",
      "Iteration 21, loss = 0.01026074\n",
      "Iteration 22, loss = 0.01026394\n",
      "Iteration 23, loss = 0.01014739\n",
      "Iteration 24, loss = 0.01012215\n",
      "Iteration 25, loss = 0.01021485\n",
      "Iteration 26, loss = 0.01027045\n",
      "Iteration 27, loss = 0.01009147\n",
      "Iteration 28, loss = 0.01011630\n",
      "Iteration 29, loss = 0.01001932\n",
      "Iteration 30, loss = 0.01005063\n",
      "Iteration 31, loss = 0.01010000\n",
      "Iteration 32, loss = 0.01006056\n",
      "Iteration 33, loss = 0.01002034\n",
      "Iteration 34, loss = 0.01000820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16911854\n",
      "Iteration 2, loss = 0.02182571\n",
      "Iteration 3, loss = 0.01753954\n",
      "Iteration 4, loss = 0.01527408\n",
      "Iteration 5, loss = 0.01374108\n",
      "Iteration 6, loss = 0.01269982\n",
      "Iteration 7, loss = 0.01161979\n",
      "Iteration 8, loss = 0.01093375\n",
      "Iteration 9, loss = 0.01037866\n",
      "Iteration 10, loss = 0.00994272\n",
      "Iteration 11, loss = 0.00951680\n",
      "Iteration 12, loss = 0.00939250\n",
      "Iteration 13, loss = 0.00900231\n",
      "Iteration 14, loss = 0.00910700\n",
      "Iteration 15, loss = 0.00887319\n",
      "Iteration 16, loss = 0.00849449\n",
      "Iteration 17, loss = 0.00862644\n",
      "Iteration 18, loss = 0.00829856\n",
      "Iteration 19, loss = 0.00853924\n",
      "Iteration 20, loss = 0.00837075\n",
      "Iteration 21, loss = 0.00821315\n",
      "Iteration 22, loss = 0.00826948\n",
      "Iteration 23, loss = 0.00843763\n",
      "Iteration 24, loss = 0.00827252\n",
      "Iteration 25, loss = 0.00877165\n",
      "Iteration 26, loss = 0.00828613\n",
      "Iteration 27, loss = 0.00815413\n",
      "Iteration 28, loss = 0.00816160\n",
      "Iteration 29, loss = 0.00833678\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48605923\n",
      "Iteration 2, loss = 0.17134579\n",
      "Iteration 3, loss = 0.10403147\n",
      "Iteration 4, loss = 0.08045685\n",
      "Iteration 5, loss = 0.06763047\n",
      "Iteration 6, loss = 0.05860441\n",
      "Iteration 7, loss = 0.05208636\n",
      "Iteration 8, loss = 0.04726502\n",
      "Iteration 9, loss = 0.04359618\n",
      "Iteration 10, loss = 0.04056117\n",
      "Iteration 11, loss = 0.03821734\n",
      "Iteration 12, loss = 0.03632728\n",
      "Iteration 13, loss = 0.03462257\n",
      "Iteration 14, loss = 0.03322520\n",
      "Iteration 15, loss = 0.03206385\n",
      "Iteration 16, loss = 0.03096259\n",
      "Iteration 17, loss = 0.03003358\n",
      "Iteration 18, loss = 0.02920233\n",
      "Iteration 19, loss = 0.02840307\n",
      "Iteration 20, loss = 0.02769618\n",
      "Iteration 21, loss = 0.02704865\n",
      "Iteration 22, loss = 0.02648166\n",
      "Iteration 23, loss = 0.02591190\n",
      "Iteration 24, loss = 0.02545039\n",
      "Iteration 25, loss = 0.02495690\n",
      "Iteration 26, loss = 0.02450987\n",
      "Iteration 27, loss = 0.02403357\n",
      "Iteration 28, loss = 0.02363013\n",
      "Iteration 29, loss = 0.02325310\n",
      "Iteration 30, loss = 0.02288920\n",
      "Iteration 31, loss = 0.02257593\n",
      "Iteration 32, loss = 0.02226019\n",
      "Iteration 33, loss = 0.02192567\n",
      "Iteration 34, loss = 0.02163731\n",
      "Iteration 35, loss = 0.02139236\n",
      "Iteration 36, loss = 0.02111865\n",
      "Iteration 37, loss = 0.02087517\n",
      "Iteration 38, loss = 0.02062707\n",
      "Iteration 39, loss = 0.02039967\n",
      "Iteration 40, loss = 0.02018160\n",
      "Iteration 41, loss = 0.01999522\n",
      "Iteration 42, loss = 0.01979791\n",
      "Iteration 43, loss = 0.01960512\n",
      "Iteration 44, loss = 0.01944515\n",
      "Iteration 45, loss = 0.01926035\n",
      "Iteration 46, loss = 0.01909583\n",
      "Iteration 47, loss = 0.01899594\n",
      "Iteration 48, loss = 0.01881512\n",
      "Iteration 49, loss = 0.01865503\n",
      "Iteration 50, loss = 0.01853158\n",
      "Iteration 51, loss = 0.01841683\n",
      "Iteration 52, loss = 0.01827235\n",
      "Iteration 53, loss = 0.01815301\n",
      "Iteration 54, loss = 0.01804483\n",
      "Iteration 55, loss = 0.01794722\n",
      "Iteration 56, loss = 0.01783736\n",
      "Iteration 57, loss = 0.01772513\n",
      "Iteration 58, loss = 0.01763916\n",
      "Iteration 59, loss = 0.01753318\n",
      "Iteration 60, loss = 0.01744418\n",
      "Iteration 61, loss = 0.01735740\n",
      "Iteration 62, loss = 0.01728035\n",
      "Iteration 63, loss = 0.01718328\n",
      "Iteration 64, loss = 0.01710512\n",
      "Iteration 65, loss = 0.01703107\n",
      "Iteration 66, loss = 0.01695728\n",
      "Iteration 67, loss = 0.01687886\n",
      "Iteration 68, loss = 0.01683681\n",
      "Iteration 69, loss = 0.01673958\n",
      "Iteration 70, loss = 0.01667003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51198591\n",
      "Iteration 2, loss = 0.24385429\n",
      "Iteration 3, loss = 0.15282514\n",
      "Iteration 4, loss = 0.11482816\n",
      "Iteration 5, loss = 0.09407925\n",
      "Iteration 6, loss = 0.07989243\n",
      "Iteration 7, loss = 0.06934111\n",
      "Iteration 8, loss = 0.06132645\n",
      "Iteration 9, loss = 0.05511209\n",
      "Iteration 10, loss = 0.05003403\n",
      "Iteration 11, loss = 0.04613714\n",
      "Iteration 12, loss = 0.04294763\n",
      "Iteration 13, loss = 0.04027289\n",
      "Iteration 14, loss = 0.03805145\n",
      "Iteration 15, loss = 0.03617725\n",
      "Iteration 16, loss = 0.03458345\n",
      "Iteration 17, loss = 0.03309300\n",
      "Iteration 18, loss = 0.03188027\n",
      "Iteration 19, loss = 0.03078900\n",
      "Iteration 20, loss = 0.02977644\n",
      "Iteration 21, loss = 0.02887630\n",
      "Iteration 22, loss = 0.02807484\n",
      "Iteration 23, loss = 0.02733544\n",
      "Iteration 24, loss = 0.02672491\n",
      "Iteration 25, loss = 0.02609504\n",
      "Iteration 26, loss = 0.02555245\n",
      "Iteration 27, loss = 0.02500441\n",
      "Iteration 28, loss = 0.02452635\n",
      "Iteration 29, loss = 0.02408735\n",
      "Iteration 30, loss = 0.02367947\n",
      "Iteration 31, loss = 0.02333644\n",
      "Iteration 32, loss = 0.02297586\n",
      "Iteration 33, loss = 0.02262266\n",
      "Iteration 34, loss = 0.02229981\n",
      "Iteration 35, loss = 0.02201610\n",
      "Iteration 36, loss = 0.02174335\n",
      "Iteration 37, loss = 0.02149052\n",
      "Iteration 38, loss = 0.02125625\n",
      "Iteration 39, loss = 0.02101228\n",
      "Iteration 40, loss = 0.02080714\n",
      "Iteration 41, loss = 0.02060531\n",
      "Iteration 42, loss = 0.02042040\n",
      "Iteration 43, loss = 0.02023960\n",
      "Iteration 44, loss = 0.02005916\n",
      "Iteration 45, loss = 0.01991466\n",
      "Iteration 46, loss = 0.01975926\n",
      "Iteration 47, loss = 0.01965085\n",
      "Iteration 48, loss = 0.01948633\n",
      "Iteration 49, loss = 0.01934464\n",
      "Iteration 50, loss = 0.01921852\n",
      "Iteration 51, loss = 0.01910399\n",
      "Iteration 52, loss = 0.01898531\n",
      "Iteration 53, loss = 0.01888888\n",
      "Iteration 54, loss = 0.01877561\n",
      "Iteration 55, loss = 0.01868075\n",
      "Iteration 56, loss = 0.01857912\n",
      "Iteration 57, loss = 0.01849051\n",
      "Iteration 58, loss = 0.01841004\n",
      "Iteration 59, loss = 0.01831364\n",
      "Iteration 60, loss = 0.01823119\n",
      "Iteration 61, loss = 0.01815783\n",
      "Iteration 62, loss = 0.01808149\n",
      "Iteration 63, loss = 0.01800387\n",
      "Iteration 64, loss = 0.01793416\n",
      "Iteration 65, loss = 0.01786832\n",
      "Iteration 66, loss = 0.01779705\n",
      "Iteration 67, loss = 0.01773388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51451638\n",
      "Iteration 2, loss = 0.21206017\n",
      "Iteration 3, loss = 0.12312436\n",
      "Iteration 4, loss = 0.08609351\n",
      "Iteration 5, loss = 0.06777665\n",
      "Iteration 6, loss = 0.05740285\n",
      "Iteration 7, loss = 0.05057943\n",
      "Iteration 8, loss = 0.04600863\n",
      "Iteration 9, loss = 0.04247933\n",
      "Iteration 10, loss = 0.03977942\n",
      "Iteration 11, loss = 0.03771276\n",
      "Iteration 12, loss = 0.03595876\n",
      "Iteration 13, loss = 0.03455340\n",
      "Iteration 14, loss = 0.03328776\n",
      "Iteration 15, loss = 0.03222718\n",
      "Iteration 16, loss = 0.03128588\n",
      "Iteration 17, loss = 0.03046312\n",
      "Iteration 18, loss = 0.02971903\n",
      "Iteration 19, loss = 0.02903362\n",
      "Iteration 20, loss = 0.02842250\n",
      "Iteration 21, loss = 0.02784924\n",
      "Iteration 22, loss = 0.02731243\n",
      "Iteration 23, loss = 0.02678432\n",
      "Iteration 24, loss = 0.02632041\n",
      "Iteration 25, loss = 0.02589531\n",
      "Iteration 26, loss = 0.02545550\n",
      "Iteration 27, loss = 0.02510887\n",
      "Iteration 28, loss = 0.02469458\n",
      "Iteration 29, loss = 0.02433932\n",
      "Iteration 30, loss = 0.02401660\n",
      "Iteration 31, loss = 0.02369610\n",
      "Iteration 32, loss = 0.02338371\n",
      "Iteration 33, loss = 0.02308327\n",
      "Iteration 34, loss = 0.02278866\n",
      "Iteration 35, loss = 0.02251239\n",
      "Iteration 36, loss = 0.02224838\n",
      "Iteration 37, loss = 0.02199181\n",
      "Iteration 38, loss = 0.02175278\n",
      "Iteration 39, loss = 0.02152388\n",
      "Iteration 40, loss = 0.02127790\n",
      "Iteration 41, loss = 0.02107058\n",
      "Iteration 42, loss = 0.02084957\n",
      "Iteration 43, loss = 0.02063836\n",
      "Iteration 44, loss = 0.02045978\n",
      "Iteration 45, loss = 0.02025759\n",
      "Iteration 46, loss = 0.02009280\n",
      "Iteration 47, loss = 0.01993224\n",
      "Iteration 48, loss = 0.01977851\n",
      "Iteration 49, loss = 0.01961366\n",
      "Iteration 50, loss = 0.01944427\n",
      "Iteration 51, loss = 0.01931429\n",
      "Iteration 52, loss = 0.01919125\n",
      "Iteration 53, loss = 0.01902680\n",
      "Iteration 54, loss = 0.01890465\n",
      "Iteration 55, loss = 0.01878824\n",
      "Iteration 56, loss = 0.01867362\n",
      "Iteration 57, loss = 0.01855731\n",
      "Iteration 58, loss = 0.01845172\n",
      "Iteration 59, loss = 0.01834850\n",
      "Iteration 60, loss = 0.01824474\n",
      "Iteration 61, loss = 0.01816234\n",
      "Iteration 62, loss = 0.01804197\n",
      "Iteration 63, loss = 0.01796006\n",
      "Iteration 64, loss = 0.01786228\n",
      "Iteration 65, loss = 0.01779564\n",
      "Iteration 66, loss = 0.01770245\n",
      "Iteration 67, loss = 0.01762348\n",
      "Iteration 68, loss = 0.01756375\n",
      "Iteration 69, loss = 0.01747524\n",
      "Iteration 70, loss = 0.01740135\n",
      "Iteration 71, loss = 0.01733177\n",
      "Iteration 72, loss = 0.01727026\n",
      "Iteration 73, loss = 0.01719402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55165432\n",
      "Iteration 2, loss = 0.25526125\n",
      "Iteration 3, loss = 0.15507114\n",
      "Iteration 4, loss = 0.11370012\n",
      "Iteration 5, loss = 0.09165827\n",
      "Iteration 6, loss = 0.07715903\n",
      "Iteration 7, loss = 0.06679919\n",
      "Iteration 8, loss = 0.05921225\n",
      "Iteration 9, loss = 0.05340365\n",
      "Iteration 10, loss = 0.04868687\n",
      "Iteration 11, loss = 0.04499758\n",
      "Iteration 12, loss = 0.04194702\n",
      "Iteration 13, loss = 0.03938262\n",
      "Iteration 14, loss = 0.03721276\n",
      "Iteration 15, loss = 0.03540948\n",
      "Iteration 16, loss = 0.03376436\n",
      "Iteration 17, loss = 0.03241297\n",
      "Iteration 18, loss = 0.03118007\n",
      "Iteration 19, loss = 0.03010876\n",
      "Iteration 20, loss = 0.02914000\n",
      "Iteration 21, loss = 0.02831083\n",
      "Iteration 22, loss = 0.02749907\n",
      "Iteration 23, loss = 0.02678615\n",
      "Iteration 24, loss = 0.02614868\n",
      "Iteration 25, loss = 0.02557595\n",
      "Iteration 26, loss = 0.02508061\n",
      "Iteration 27, loss = 0.02456160\n",
      "Iteration 28, loss = 0.02408047\n",
      "Iteration 29, loss = 0.02367342\n",
      "Iteration 30, loss = 0.02328346\n",
      "Iteration 31, loss = 0.02292375\n",
      "Iteration 32, loss = 0.02258708\n",
      "Iteration 33, loss = 0.02229005\n",
      "Iteration 34, loss = 0.02199459\n",
      "Iteration 35, loss = 0.02172108\n",
      "Iteration 36, loss = 0.02149635\n",
      "Iteration 37, loss = 0.02125191\n",
      "Iteration 38, loss = 0.02101497\n",
      "Iteration 39, loss = 0.02081261\n",
      "Iteration 40, loss = 0.02061256\n",
      "Iteration 41, loss = 0.02044335\n",
      "Iteration 42, loss = 0.02025010\n",
      "Iteration 43, loss = 0.02009108\n",
      "Iteration 44, loss = 0.01992942\n",
      "Iteration 45, loss = 0.01978409\n",
      "Iteration 46, loss = 0.01965633\n",
      "Iteration 47, loss = 0.01951212\n",
      "Iteration 48, loss = 0.01937609\n",
      "Iteration 49, loss = 0.01927186\n",
      "Iteration 50, loss = 0.01914407\n",
      "Iteration 51, loss = 0.01902828\n",
      "Iteration 52, loss = 0.01892400\n",
      "Iteration 53, loss = 0.01881834\n",
      "Iteration 54, loss = 0.01872216\n",
      "Iteration 55, loss = 0.01863245\n",
      "Iteration 56, loss = 0.01854362\n",
      "Iteration 57, loss = 0.01846029\n",
      "Iteration 58, loss = 0.01837246\n",
      "Iteration 59, loss = 0.01829439\n",
      "Iteration 60, loss = 0.01822156\n",
      "Iteration 61, loss = 0.01813956\n",
      "Iteration 62, loss = 0.01807016\n",
      "Iteration 63, loss = 0.01799595\n",
      "Iteration 64, loss = 0.01793759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52223432\n",
      "Iteration 2, loss = 0.20304219\n",
      "Iteration 3, loss = 0.11225256\n",
      "Iteration 4, loss = 0.07875964\n",
      "Iteration 5, loss = 0.06088164\n",
      "Iteration 6, loss = 0.05029128\n",
      "Iteration 7, loss = 0.04327721\n",
      "Iteration 8, loss = 0.03827596\n",
      "Iteration 9, loss = 0.03460312\n",
      "Iteration 10, loss = 0.03184443\n",
      "Iteration 11, loss = 0.02968790\n",
      "Iteration 12, loss = 0.02797029\n",
      "Iteration 13, loss = 0.02656627\n",
      "Iteration 14, loss = 0.02544642\n",
      "Iteration 15, loss = 0.02446796\n",
      "Iteration 16, loss = 0.02364982\n",
      "Iteration 17, loss = 0.02295515\n",
      "Iteration 18, loss = 0.02235034\n",
      "Iteration 19, loss = 0.02183439\n",
      "Iteration 20, loss = 0.02137113\n",
      "Iteration 21, loss = 0.02096463\n",
      "Iteration 22, loss = 0.02060297\n",
      "Iteration 23, loss = 0.02027963\n",
      "Iteration 24, loss = 0.01999206\n",
      "Iteration 25, loss = 0.01971835\n",
      "Iteration 26, loss = 0.01947998\n",
      "Iteration 27, loss = 0.01926213\n",
      "Iteration 28, loss = 0.01906059\n",
      "Iteration 29, loss = 0.01887467\n",
      "Iteration 30, loss = 0.01870167\n",
      "Iteration 31, loss = 0.01853993\n",
      "Iteration 32, loss = 0.01838586\n",
      "Iteration 33, loss = 0.01824805\n",
      "Iteration 34, loss = 0.01812087\n",
      "Iteration 35, loss = 0.01798358\n",
      "Iteration 36, loss = 0.01786750\n",
      "Iteration 37, loss = 0.01775375\n",
      "Iteration 38, loss = 0.01765053\n",
      "Iteration 39, loss = 0.01755392\n",
      "Iteration 40, loss = 0.01744673\n",
      "Iteration 41, loss = 0.01736184\n",
      "Iteration 42, loss = 0.01726917\n",
      "Iteration 43, loss = 0.01718095\n",
      "Iteration 44, loss = 0.01709882\n",
      "Iteration 45, loss = 0.01703030\n",
      "Iteration 46, loss = 0.01693834\n",
      "Iteration 47, loss = 0.01686791\n",
      "Iteration 48, loss = 0.01679864\n",
      "Iteration 49, loss = 0.01672313\n",
      "Iteration 50, loss = 0.01666902\n",
      "Iteration 51, loss = 0.01660674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56508734\n",
      "Iteration 2, loss = 0.30551232\n",
      "Iteration 3, loss = 0.16627918\n",
      "Iteration 4, loss = 0.10526084\n",
      "Iteration 5, loss = 0.07440923\n",
      "Iteration 6, loss = 0.05636005\n",
      "Iteration 7, loss = 0.04586486\n",
      "Iteration 8, loss = 0.03935377\n",
      "Iteration 9, loss = 0.03482951\n",
      "Iteration 10, loss = 0.03151435\n",
      "Iteration 11, loss = 0.02903469\n",
      "Iteration 12, loss = 0.02712768\n",
      "Iteration 13, loss = 0.02544682\n",
      "Iteration 14, loss = 0.02408848\n",
      "Iteration 15, loss = 0.02297806\n",
      "Iteration 16, loss = 0.02199077\n",
      "Iteration 17, loss = 0.02114005\n",
      "Iteration 18, loss = 0.02040408\n",
      "Iteration 19, loss = 0.01972357\n",
      "Iteration 20, loss = 0.01914802\n",
      "Iteration 21, loss = 0.01860819\n",
      "Iteration 22, loss = 0.01813076\n",
      "Iteration 23, loss = 0.01769139\n",
      "Iteration 24, loss = 0.01729682\n",
      "Iteration 25, loss = 0.01697544\n",
      "Iteration 26, loss = 0.01661123\n",
      "Iteration 27, loss = 0.01625332\n",
      "Iteration 28, loss = 0.01595995\n",
      "Iteration 29, loss = 0.01566736\n",
      "Iteration 30, loss = 0.01540532\n",
      "Iteration 31, loss = 0.01516209\n",
      "Iteration 32, loss = 0.01491697\n",
      "Iteration 33, loss = 0.01469952\n",
      "Iteration 34, loss = 0.01446937\n",
      "Iteration 35, loss = 0.01428215\n",
      "Iteration 36, loss = 0.01408862\n",
      "Iteration 37, loss = 0.01389423\n",
      "Iteration 38, loss = 0.01370765\n",
      "Iteration 39, loss = 0.01355310\n",
      "Iteration 40, loss = 0.01337653\n",
      "Iteration 41, loss = 0.01322981\n",
      "Iteration 42, loss = 0.01305879\n",
      "Iteration 43, loss = 0.01291760\n",
      "Iteration 44, loss = 0.01278082\n",
      "Iteration 45, loss = 0.01264060\n",
      "Iteration 46, loss = 0.01251286\n",
      "Iteration 47, loss = 0.01242983\n",
      "Iteration 48, loss = 0.01227207\n",
      "Iteration 49, loss = 0.01214725\n",
      "Iteration 50, loss = 0.01204522\n",
      "Iteration 51, loss = 0.01195191\n",
      "Iteration 52, loss = 0.01183089\n",
      "Iteration 53, loss = 0.01173294\n",
      "Iteration 54, loss = 0.01163930\n",
      "Iteration 55, loss = 0.01154467\n",
      "Iteration 56, loss = 0.01146687\n",
      "Iteration 57, loss = 0.01138000\n",
      "Iteration 58, loss = 0.01129132\n",
      "Iteration 59, loss = 0.01120597\n",
      "Iteration 60, loss = 0.01112330\n",
      "Iteration 61, loss = 0.01103608\n",
      "Iteration 62, loss = 0.01102010\n",
      "Iteration 63, loss = 0.01089361\n",
      "Iteration 64, loss = 0.01081976\n",
      "Iteration 65, loss = 0.01075784\n",
      "Iteration 66, loss = 0.01067066\n",
      "Iteration 67, loss = 0.01061944\n",
      "Iteration 68, loss = 0.01056582\n",
      "Iteration 69, loss = 0.01051838\n",
      "Iteration 70, loss = 0.01043637\n",
      "Iteration 71, loss = 0.01039444\n",
      "Iteration 72, loss = 0.01033442\n",
      "Iteration 73, loss = 0.01027634\n",
      "Iteration 74, loss = 0.01023770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56069741\n",
      "Iteration 2, loss = 0.33396545\n",
      "Iteration 3, loss = 0.20451423\n",
      "Iteration 4, loss = 0.13738208\n",
      "Iteration 5, loss = 0.09921916\n",
      "Iteration 6, loss = 0.07518219\n",
      "Iteration 7, loss = 0.05967187\n",
      "Iteration 8, loss = 0.04945063\n",
      "Iteration 9, loss = 0.04246778\n",
      "Iteration 10, loss = 0.03742688\n",
      "Iteration 11, loss = 0.03380699\n",
      "Iteration 12, loss = 0.03107084\n",
      "Iteration 13, loss = 0.02890134\n",
      "Iteration 14, loss = 0.02717200\n",
      "Iteration 15, loss = 0.02576963\n",
      "Iteration 16, loss = 0.02459307\n",
      "Iteration 17, loss = 0.02354839\n",
      "Iteration 18, loss = 0.02269168\n",
      "Iteration 19, loss = 0.02196323\n",
      "Iteration 20, loss = 0.02127661\n",
      "Iteration 21, loss = 0.02066691\n",
      "Iteration 22, loss = 0.02012583\n",
      "Iteration 23, loss = 0.01964164\n",
      "Iteration 24, loss = 0.01920768\n",
      "Iteration 25, loss = 0.01879979\n",
      "Iteration 26, loss = 0.01841982\n",
      "Iteration 27, loss = 0.01805094\n",
      "Iteration 28, loss = 0.01771893\n",
      "Iteration 29, loss = 0.01740317\n",
      "Iteration 30, loss = 0.01711106\n",
      "Iteration 31, loss = 0.01684643\n",
      "Iteration 32, loss = 0.01657911\n",
      "Iteration 33, loss = 0.01632298\n",
      "Iteration 34, loss = 0.01607666\n",
      "Iteration 35, loss = 0.01584953\n",
      "Iteration 36, loss = 0.01562345\n",
      "Iteration 37, loss = 0.01541835\n",
      "Iteration 38, loss = 0.01522175\n",
      "Iteration 39, loss = 0.01502319\n",
      "Iteration 40, loss = 0.01483882\n",
      "Iteration 41, loss = 0.01466381\n",
      "Iteration 42, loss = 0.01449005\n",
      "Iteration 43, loss = 0.01432582\n",
      "Iteration 44, loss = 0.01416676\n",
      "Iteration 45, loss = 0.01402483\n",
      "Iteration 46, loss = 0.01388314\n",
      "Iteration 47, loss = 0.01376553\n",
      "Iteration 48, loss = 0.01360670\n",
      "Iteration 49, loss = 0.01348179\n",
      "Iteration 50, loss = 0.01334589\n",
      "Iteration 51, loss = 0.01322811\n",
      "Iteration 52, loss = 0.01311328\n",
      "Iteration 53, loss = 0.01302091\n",
      "Iteration 54, loss = 0.01289400\n",
      "Iteration 55, loss = 0.01279891\n",
      "Iteration 56, loss = 0.01269767\n",
      "Iteration 57, loss = 0.01260295\n",
      "Iteration 58, loss = 0.01251304\n",
      "Iteration 59, loss = 0.01241941\n",
      "Iteration 60, loss = 0.01232373\n",
      "Iteration 61, loss = 0.01224608\n",
      "Iteration 62, loss = 0.01216292\n",
      "Iteration 63, loss = 0.01208013\n",
      "Iteration 64, loss = 0.01200191\n",
      "Iteration 65, loss = 0.01193528\n",
      "Iteration 66, loss = 0.01185147\n",
      "Iteration 67, loss = 0.01179137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57092760\n",
      "Iteration 2, loss = 0.32495621\n",
      "Iteration 3, loss = 0.19230667\n",
      "Iteration 4, loss = 0.12652587\n",
      "Iteration 5, loss = 0.08994496\n",
      "Iteration 6, loss = 0.06822083\n",
      "Iteration 7, loss = 0.05430060\n",
      "Iteration 8, loss = 0.04533582\n",
      "Iteration 9, loss = 0.03913674\n",
      "Iteration 10, loss = 0.03471104\n",
      "Iteration 11, loss = 0.03157359\n",
      "Iteration 12, loss = 0.02901388\n",
      "Iteration 13, loss = 0.02707902\n",
      "Iteration 14, loss = 0.02537617\n",
      "Iteration 15, loss = 0.02402142\n",
      "Iteration 16, loss = 0.02288949\n",
      "Iteration 17, loss = 0.02195577\n",
      "Iteration 18, loss = 0.02110641\n",
      "Iteration 19, loss = 0.02038662\n",
      "Iteration 20, loss = 0.01975663\n",
      "Iteration 21, loss = 0.01917523\n",
      "Iteration 22, loss = 0.01863441\n",
      "Iteration 23, loss = 0.01815373\n",
      "Iteration 24, loss = 0.01772339\n",
      "Iteration 25, loss = 0.01733807\n",
      "Iteration 26, loss = 0.01695208\n",
      "Iteration 27, loss = 0.01665597\n",
      "Iteration 28, loss = 0.01629717\n",
      "Iteration 29, loss = 0.01600666\n",
      "Iteration 30, loss = 0.01574473\n",
      "Iteration 31, loss = 0.01548333\n",
      "Iteration 32, loss = 0.01522886\n",
      "Iteration 33, loss = 0.01502305\n",
      "Iteration 34, loss = 0.01478934\n",
      "Iteration 35, loss = 0.01459814\n",
      "Iteration 36, loss = 0.01438406\n",
      "Iteration 37, loss = 0.01417860\n",
      "Iteration 38, loss = 0.01402550\n",
      "Iteration 39, loss = 0.01384340\n",
      "Iteration 40, loss = 0.01364961\n",
      "Iteration 41, loss = 0.01348940\n",
      "Iteration 42, loss = 0.01334709\n",
      "Iteration 43, loss = 0.01319859\n",
      "Iteration 44, loss = 0.01306459\n",
      "Iteration 45, loss = 0.01291850\n",
      "Iteration 46, loss = 0.01280086\n",
      "Iteration 47, loss = 0.01266768\n",
      "Iteration 48, loss = 0.01258975\n",
      "Iteration 49, loss = 0.01242640\n",
      "Iteration 50, loss = 0.01231565\n",
      "Iteration 51, loss = 0.01221736\n",
      "Iteration 52, loss = 0.01213489\n",
      "Iteration 53, loss = 0.01201556\n",
      "Iteration 54, loss = 0.01190443\n",
      "Iteration 55, loss = 0.01180679\n",
      "Iteration 56, loss = 0.01171686\n",
      "Iteration 57, loss = 0.01162378\n",
      "Iteration 58, loss = 0.01155142\n",
      "Iteration 59, loss = 0.01146289\n",
      "Iteration 60, loss = 0.01138006\n",
      "Iteration 61, loss = 0.01132352\n",
      "Iteration 62, loss = 0.01122623\n",
      "Iteration 63, loss = 0.01115867\n",
      "Iteration 64, loss = 0.01108418\n",
      "Iteration 65, loss = 0.01101873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60282190\n",
      "Iteration 2, loss = 0.36239188\n",
      "Iteration 3, loss = 0.22629632\n",
      "Iteration 4, loss = 0.15130426\n",
      "Iteration 5, loss = 0.10753783\n",
      "Iteration 6, loss = 0.08090370\n",
      "Iteration 7, loss = 0.06335308\n",
      "Iteration 8, loss = 0.05168646\n",
      "Iteration 9, loss = 0.04395116\n",
      "Iteration 10, loss = 0.03843620\n",
      "Iteration 11, loss = 0.03446010\n",
      "Iteration 12, loss = 0.03147539\n",
      "Iteration 13, loss = 0.02922000\n",
      "Iteration 14, loss = 0.02740219\n",
      "Iteration 15, loss = 0.02596061\n",
      "Iteration 16, loss = 0.02473610\n",
      "Iteration 17, loss = 0.02372167\n",
      "Iteration 18, loss = 0.02284507\n",
      "Iteration 19, loss = 0.02207389\n",
      "Iteration 20, loss = 0.02140077\n",
      "Iteration 21, loss = 0.02081506\n",
      "Iteration 22, loss = 0.02025205\n",
      "Iteration 23, loss = 0.01975914\n",
      "Iteration 24, loss = 0.01930196\n",
      "Iteration 25, loss = 0.01889425\n",
      "Iteration 26, loss = 0.01853729\n",
      "Iteration 27, loss = 0.01817096\n",
      "Iteration 28, loss = 0.01782104\n",
      "Iteration 29, loss = 0.01750705\n",
      "Iteration 30, loss = 0.01721903\n",
      "Iteration 31, loss = 0.01693889\n",
      "Iteration 32, loss = 0.01667297\n",
      "Iteration 33, loss = 0.01642859\n",
      "Iteration 34, loss = 0.01619147\n",
      "Iteration 35, loss = 0.01596505\n",
      "Iteration 36, loss = 0.01577403\n",
      "Iteration 37, loss = 0.01554276\n",
      "Iteration 38, loss = 0.01535681\n",
      "Iteration 39, loss = 0.01516767\n",
      "Iteration 40, loss = 0.01498539\n",
      "Iteration 41, loss = 0.01481328\n",
      "Iteration 42, loss = 0.01464073\n",
      "Iteration 43, loss = 0.01448257\n",
      "Iteration 44, loss = 0.01432924\n",
      "Iteration 45, loss = 0.01417660\n",
      "Iteration 46, loss = 0.01405430\n",
      "Iteration 47, loss = 0.01389749\n",
      "Iteration 48, loss = 0.01375717\n",
      "Iteration 49, loss = 0.01364624\n",
      "Iteration 50, loss = 0.01352810\n",
      "Iteration 51, loss = 0.01339593\n",
      "Iteration 52, loss = 0.01327611\n",
      "Iteration 53, loss = 0.01316620\n",
      "Iteration 54, loss = 0.01305425\n",
      "Iteration 55, loss = 0.01296361\n",
      "Iteration 56, loss = 0.01285347\n",
      "Iteration 57, loss = 0.01275901\n",
      "Iteration 58, loss = 0.01267330\n",
      "Iteration 59, loss = 0.01257359\n",
      "Iteration 60, loss = 0.01250438\n",
      "Iteration 61, loss = 0.01240574\n",
      "Iteration 62, loss = 0.01231485\n",
      "Iteration 63, loss = 0.01223513\n",
      "Iteration 64, loss = 0.01216324\n",
      "Iteration 65, loss = 0.01209370\n",
      "Iteration 66, loss = 0.01201106\n",
      "Iteration 67, loss = 0.01195012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59472657\n",
      "Iteration 2, loss = 0.33838493\n",
      "Iteration 3, loss = 0.19432853\n",
      "Iteration 4, loss = 0.12315928\n",
      "Iteration 5, loss = 0.08386815\n",
      "Iteration 6, loss = 0.06054141\n",
      "Iteration 7, loss = 0.04630616\n",
      "Iteration 8, loss = 0.03749674\n",
      "Iteration 9, loss = 0.03195323\n",
      "Iteration 10, loss = 0.02822572\n",
      "Iteration 11, loss = 0.02559743\n",
      "Iteration 12, loss = 0.02361536\n",
      "Iteration 13, loss = 0.02210744\n",
      "Iteration 14, loss = 0.02094121\n",
      "Iteration 15, loss = 0.01995210\n",
      "Iteration 16, loss = 0.01915705\n",
      "Iteration 17, loss = 0.01846834\n",
      "Iteration 18, loss = 0.01788633\n",
      "Iteration 19, loss = 0.01738363\n",
      "Iteration 20, loss = 0.01692192\n",
      "Iteration 21, loss = 0.01650535\n",
      "Iteration 22, loss = 0.01613469\n",
      "Iteration 23, loss = 0.01580088\n",
      "Iteration 24, loss = 0.01548952\n",
      "Iteration 25, loss = 0.01518276\n",
      "Iteration 26, loss = 0.01492125\n",
      "Iteration 27, loss = 0.01466814\n",
      "Iteration 28, loss = 0.01442827\n",
      "Iteration 29, loss = 0.01420695\n",
      "Iteration 30, loss = 0.01398897\n",
      "Iteration 31, loss = 0.01378754\n",
      "Iteration 32, loss = 0.01359034\n",
      "Iteration 33, loss = 0.01340567\n",
      "Iteration 34, loss = 0.01323086\n",
      "Iteration 35, loss = 0.01304735\n",
      "Iteration 36, loss = 0.01288773\n",
      "Iteration 37, loss = 0.01272513\n",
      "Iteration 38, loss = 0.01257374\n",
      "Iteration 39, loss = 0.01242200\n",
      "Iteration 40, loss = 0.01227712\n",
      "Iteration 41, loss = 0.01215216\n",
      "Iteration 42, loss = 0.01201227\n",
      "Iteration 43, loss = 0.01188151\n",
      "Iteration 44, loss = 0.01175517\n",
      "Iteration 45, loss = 0.01164296\n",
      "Iteration 46, loss = 0.01153462\n",
      "Iteration 47, loss = 0.01141096\n",
      "Iteration 48, loss = 0.01130374\n",
      "Iteration 49, loss = 0.01118490\n",
      "Iteration 50, loss = 0.01109015\n",
      "Iteration 51, loss = 0.01100296\n",
      "Iteration 52, loss = 0.01088903\n",
      "Iteration 53, loss = 0.01079443\n",
      "Iteration 54, loss = 0.01071351\n",
      "Iteration 55, loss = 0.01060812\n",
      "Iteration 56, loss = 0.01053589\n",
      "Iteration 57, loss = 0.01045186\n",
      "Iteration 58, loss = 0.01036527\n",
      "Iteration 59, loss = 0.01028641\n",
      "Iteration 60, loss = 0.01021102\n",
      "Iteration 61, loss = 0.01012345\n",
      "Iteration 62, loss = 0.01005791\n",
      "Iteration 63, loss = 0.00997700\n",
      "Iteration 64, loss = 0.00990772\n",
      "Iteration 65, loss = 0.00984413\n",
      "Iteration 66, loss = 0.00978429\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69257291\n",
      "Iteration 2, loss = 0.58982753\n",
      "Iteration 3, loss = 0.51333649\n",
      "Iteration 4, loss = 0.44664468\n",
      "Iteration 5, loss = 0.38758064\n",
      "Iteration 6, loss = 0.33740419\n",
      "Iteration 7, loss = 0.29612719\n",
      "Iteration 8, loss = 0.26251507\n",
      "Iteration 9, loss = 0.23553431\n",
      "Iteration 10, loss = 0.21365507\n",
      "Iteration 11, loss = 0.19576854\n",
      "Iteration 12, loss = 0.18092000\n",
      "Iteration 13, loss = 0.16841793\n",
      "Iteration 14, loss = 0.15776910\n",
      "Iteration 15, loss = 0.14859865\n",
      "Iteration 16, loss = 0.14064553\n",
      "Iteration 17, loss = 0.13367335\n",
      "Iteration 18, loss = 0.12748808\n",
      "Iteration 19, loss = 0.12200335\n",
      "Iteration 20, loss = 0.11710329\n",
      "Iteration 21, loss = 0.11268705\n",
      "Iteration 22, loss = 0.10874648\n",
      "Iteration 23, loss = 0.10511083\n",
      "Iteration 24, loss = 0.10178120\n",
      "Iteration 25, loss = 0.09877151\n",
      "Iteration 26, loss = 0.09596665\n",
      "Iteration 27, loss = 0.09339532\n",
      "Iteration 28, loss = 0.09098208\n",
      "Iteration 29, loss = 0.08874283\n",
      "Iteration 30, loss = 0.08664554\n",
      "Iteration 31, loss = 0.08469684\n",
      "Iteration 32, loss = 0.08284958\n",
      "Iteration 33, loss = 0.08109207\n",
      "Iteration 34, loss = 0.07943932\n",
      "Iteration 35, loss = 0.07787184\n",
      "Iteration 36, loss = 0.07636880\n",
      "Iteration 37, loss = 0.07493870\n",
      "Iteration 38, loss = 0.07357200\n",
      "Iteration 39, loss = 0.07226589\n",
      "Iteration 40, loss = 0.07102041\n",
      "Iteration 41, loss = 0.06982658\n",
      "Iteration 42, loss = 0.06867429\n",
      "Iteration 43, loss = 0.06756798\n",
      "Iteration 44, loss = 0.06650517\n",
      "Iteration 45, loss = 0.06548608\n",
      "Iteration 46, loss = 0.06449705\n",
      "Iteration 47, loss = 0.06355731\n",
      "Iteration 48, loss = 0.06263344\n",
      "Iteration 49, loss = 0.06173118\n",
      "Iteration 50, loss = 0.06087130\n",
      "Iteration 51, loss = 0.06004005\n",
      "Iteration 52, loss = 0.05923146\n",
      "Iteration 53, loss = 0.05845246\n",
      "Iteration 54, loss = 0.05770029\n",
      "Iteration 55, loss = 0.05696986\n",
      "Iteration 56, loss = 0.05627312\n",
      "Iteration 57, loss = 0.05556069\n",
      "Iteration 58, loss = 0.05490310\n",
      "Iteration 59, loss = 0.05425623\n",
      "Iteration 60, loss = 0.05362931\n",
      "Iteration 61, loss = 0.05302289\n",
      "Iteration 62, loss = 0.05243973\n",
      "Iteration 63, loss = 0.05185256\n",
      "Iteration 64, loss = 0.05129956\n",
      "Iteration 65, loss = 0.05076659\n",
      "Iteration 66, loss = 0.05024508\n",
      "Iteration 67, loss = 0.04973291\n",
      "Iteration 68, loss = 0.04924828\n",
      "Iteration 69, loss = 0.04876827\n",
      "Iteration 70, loss = 0.04829773\n",
      "Iteration 71, loss = 0.04783990\n",
      "Iteration 72, loss = 0.04740113\n",
      "Iteration 73, loss = 0.04696950\n",
      "Iteration 74, loss = 0.04655617\n",
      "Iteration 75, loss = 0.04613304\n",
      "Iteration 76, loss = 0.04573466\n",
      "Iteration 77, loss = 0.04535639\n",
      "Iteration 78, loss = 0.04497578\n",
      "Iteration 79, loss = 0.04460223\n",
      "Iteration 80, loss = 0.04423556\n",
      "Iteration 81, loss = 0.04389246\n",
      "Iteration 82, loss = 0.04354060\n",
      "Iteration 83, loss = 0.04323384\n",
      "Iteration 84, loss = 0.04287987\n",
      "Iteration 85, loss = 0.04256484\n",
      "Iteration 86, loss = 0.04227253\n",
      "Iteration 87, loss = 0.04194743\n",
      "Iteration 88, loss = 0.04165674\n",
      "Iteration 89, loss = 0.04136720\n",
      "Iteration 90, loss = 0.04108172\n",
      "Iteration 91, loss = 0.04080737\n",
      "Iteration 92, loss = 0.04053068\n",
      "Iteration 93, loss = 0.04025864\n",
      "Iteration 94, loss = 0.03999987\n",
      "Iteration 95, loss = 0.03974070\n",
      "Iteration 96, loss = 0.03949397\n",
      "Iteration 97, loss = 0.03925192\n",
      "Iteration 98, loss = 0.03901039\n",
      "Iteration 99, loss = 0.03878873\n",
      "Iteration 100, loss = 0.03855752\n",
      "Iteration 101, loss = 0.03832242\n",
      "Iteration 102, loss = 0.03810160\n",
      "Iteration 103, loss = 0.03788544\n",
      "Iteration 104, loss = 0.03767418\n",
      "Iteration 105, loss = 0.03746825\n",
      "Iteration 106, loss = 0.03726515\n",
      "Iteration 107, loss = 0.03706416\n",
      "Iteration 108, loss = 0.03686558\n",
      "Iteration 109, loss = 0.03667496\n",
      "Iteration 110, loss = 0.03648813\n",
      "Iteration 111, loss = 0.03630599\n",
      "Iteration 112, loss = 0.03611814\n",
      "Iteration 113, loss = 0.03593549\n",
      "Iteration 114, loss = 0.03575976\n",
      "Iteration 115, loss = 0.03559305\n",
      "Iteration 116, loss = 0.03542153\n",
      "Iteration 117, loss = 0.03525427\n",
      "Iteration 118, loss = 0.03509340\n",
      "Iteration 119, loss = 0.03492496\n",
      "Iteration 120, loss = 0.03477425\n",
      "Iteration 121, loss = 0.03461394\n",
      "Iteration 122, loss = 0.03446002\n",
      "Iteration 123, loss = 0.03431187\n",
      "Iteration 124, loss = 0.03415686\n",
      "Iteration 125, loss = 0.03400545\n",
      "Iteration 126, loss = 0.03386581\n",
      "Iteration 127, loss = 0.03371631\n",
      "Iteration 128, loss = 0.03357750\n",
      "Iteration 129, loss = 0.03343799\n",
      "Iteration 130, loss = 0.03331057\n",
      "Iteration 131, loss = 0.03316722\n",
      "Iteration 132, loss = 0.03303476\n",
      "Iteration 133, loss = 0.03290459\n",
      "Iteration 134, loss = 0.03277937\n",
      "Iteration 135, loss = 0.03265248\n",
      "Iteration 136, loss = 0.03252258\n",
      "Iteration 137, loss = 0.03240902\n",
      "Iteration 138, loss = 0.03227600\n",
      "Iteration 139, loss = 0.03216377\n",
      "Iteration 140, loss = 0.03205133\n",
      "Iteration 141, loss = 0.03192185\n",
      "Iteration 142, loss = 0.03180906\n",
      "Iteration 143, loss = 0.03169050\n",
      "Iteration 144, loss = 0.03158160\n",
      "Iteration 145, loss = 0.03146868\n",
      "Iteration 146, loss = 0.03136013\n",
      "Iteration 147, loss = 0.03125021\n",
      "Iteration 148, loss = 0.03115485\n",
      "Iteration 149, loss = 0.03103642\n",
      "Iteration 150, loss = 0.03093771\n",
      "Iteration 151, loss = 0.03083623\n",
      "Iteration 152, loss = 0.03072772\n",
      "Iteration 153, loss = 0.03062826\n",
      "Iteration 154, loss = 0.03052758\n",
      "Iteration 155, loss = 0.03042916\n",
      "Iteration 156, loss = 0.03033370\n",
      "Iteration 157, loss = 0.03023843\n",
      "Iteration 158, loss = 0.03014243\n",
      "Iteration 159, loss = 0.03004593\n",
      "Iteration 160, loss = 0.02995676\n",
      "Iteration 161, loss = 0.02986657\n",
      "Iteration 162, loss = 0.02977299\n",
      "Iteration 163, loss = 0.02968516\n",
      "Iteration 164, loss = 0.02959570\n",
      "Iteration 165, loss = 0.02950595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67702653\n",
      "Iteration 2, loss = 0.59409000\n",
      "Iteration 3, loss = 0.53340175\n",
      "Iteration 4, loss = 0.48146500\n",
      "Iteration 5, loss = 0.43590259\n",
      "Iteration 6, loss = 0.39686552\n",
      "Iteration 7, loss = 0.36363448\n",
      "Iteration 8, loss = 0.33508327\n",
      "Iteration 9, loss = 0.31041607\n",
      "Iteration 10, loss = 0.28889164\n",
      "Iteration 11, loss = 0.26998091\n",
      "Iteration 12, loss = 0.25328763\n",
      "Iteration 13, loss = 0.23856255\n",
      "Iteration 14, loss = 0.22547314\n",
      "Iteration 15, loss = 0.21377295\n",
      "Iteration 16, loss = 0.20325201\n",
      "Iteration 17, loss = 0.19378188\n",
      "Iteration 18, loss = 0.18517722\n",
      "Iteration 19, loss = 0.17740952\n",
      "Iteration 20, loss = 0.17033513\n",
      "Iteration 21, loss = 0.16382541\n",
      "Iteration 22, loss = 0.15793308\n",
      "Iteration 23, loss = 0.15245978\n",
      "Iteration 24, loss = 0.14740207\n",
      "Iteration 25, loss = 0.14277638\n",
      "Iteration 26, loss = 0.13842614\n",
      "Iteration 27, loss = 0.13440444\n",
      "Iteration 28, loss = 0.13064997\n",
      "Iteration 29, loss = 0.12714772\n",
      "Iteration 30, loss = 0.12384832\n",
      "Iteration 31, loss = 0.12076851\n",
      "Iteration 32, loss = 0.11784297\n",
      "Iteration 33, loss = 0.11508333\n",
      "Iteration 34, loss = 0.11248439\n",
      "Iteration 35, loss = 0.11001467\n",
      "Iteration 36, loss = 0.10766774\n",
      "Iteration 37, loss = 0.10542233\n",
      "Iteration 38, loss = 0.10328298\n",
      "Iteration 39, loss = 0.10123202\n",
      "Iteration 40, loss = 0.09928803\n",
      "Iteration 41, loss = 0.09742056\n",
      "Iteration 42, loss = 0.09562003\n",
      "Iteration 43, loss = 0.09387848\n",
      "Iteration 44, loss = 0.09220997\n",
      "Iteration 45, loss = 0.09061316\n",
      "Iteration 46, loss = 0.08906040\n",
      "Iteration 47, loss = 0.08756610\n",
      "Iteration 48, loss = 0.08613083\n",
      "Iteration 49, loss = 0.08472904\n",
      "Iteration 50, loss = 0.08335110\n",
      "Iteration 51, loss = 0.08203250\n",
      "Iteration 52, loss = 0.08075602\n",
      "Iteration 53, loss = 0.07953013\n",
      "Iteration 54, loss = 0.07833348\n",
      "Iteration 55, loss = 0.07715472\n",
      "Iteration 56, loss = 0.07601341\n",
      "Iteration 57, loss = 0.07491311\n",
      "Iteration 58, loss = 0.07384031\n",
      "Iteration 59, loss = 0.07279924\n",
      "Iteration 60, loss = 0.07178954\n",
      "Iteration 61, loss = 0.07080832\n",
      "Iteration 62, loss = 0.06985405\n",
      "Iteration 63, loss = 0.06891383\n",
      "Iteration 64, loss = 0.06800846\n",
      "Iteration 65, loss = 0.06712959\n",
      "Iteration 66, loss = 0.06628191\n",
      "Iteration 67, loss = 0.06543742\n",
      "Iteration 68, loss = 0.06464639\n",
      "Iteration 69, loss = 0.06385686\n",
      "Iteration 70, loss = 0.06308522\n",
      "Iteration 71, loss = 0.06232407\n",
      "Iteration 72, loss = 0.06159823\n",
      "Iteration 73, loss = 0.06088172\n",
      "Iteration 74, loss = 0.06018796\n",
      "Iteration 75, loss = 0.05949876\n",
      "Iteration 76, loss = 0.05884675\n",
      "Iteration 77, loss = 0.05821207\n",
      "Iteration 78, loss = 0.05757797\n",
      "Iteration 79, loss = 0.05696575\n",
      "Iteration 80, loss = 0.05635170\n",
      "Iteration 81, loss = 0.05578929\n",
      "Iteration 82, loss = 0.05519866\n",
      "Iteration 83, loss = 0.05466943\n",
      "Iteration 84, loss = 0.05409913\n",
      "Iteration 85, loss = 0.05358109\n",
      "Iteration 86, loss = 0.05308235\n",
      "Iteration 87, loss = 0.05255077\n",
      "Iteration 88, loss = 0.05206380\n",
      "Iteration 89, loss = 0.05158520\n",
      "Iteration 90, loss = 0.05111373\n",
      "Iteration 91, loss = 0.05066534\n",
      "Iteration 92, loss = 0.05020588\n",
      "Iteration 93, loss = 0.04975428\n",
      "Iteration 94, loss = 0.04933022\n",
      "Iteration 95, loss = 0.04889670\n",
      "Iteration 96, loss = 0.04848922\n",
      "Iteration 97, loss = 0.04808556\n",
      "Iteration 98, loss = 0.04769079\n",
      "Iteration 99, loss = 0.04731001\n",
      "Iteration 100, loss = 0.04693348\n",
      "Iteration 101, loss = 0.04655785\n",
      "Iteration 102, loss = 0.04619406\n",
      "Iteration 103, loss = 0.04584444\n",
      "Iteration 104, loss = 0.04549875\n",
      "Iteration 105, loss = 0.04515665\n",
      "Iteration 106, loss = 0.04482220\n",
      "Iteration 107, loss = 0.04449574\n",
      "Iteration 108, loss = 0.04417020\n",
      "Iteration 109, loss = 0.04385883\n",
      "Iteration 110, loss = 0.04355233\n",
      "Iteration 111, loss = 0.04326312\n",
      "Iteration 112, loss = 0.04295481\n",
      "Iteration 113, loss = 0.04266017\n",
      "Iteration 114, loss = 0.04237688\n",
      "Iteration 115, loss = 0.04209818\n",
      "Iteration 116, loss = 0.04183042\n",
      "Iteration 117, loss = 0.04155099\n",
      "Iteration 118, loss = 0.04129319\n",
      "Iteration 119, loss = 0.04102223\n",
      "Iteration 120, loss = 0.04077067\n",
      "Iteration 121, loss = 0.04051744\n",
      "Iteration 122, loss = 0.04027046\n",
      "Iteration 123, loss = 0.04002905\n",
      "Iteration 124, loss = 0.03979057\n",
      "Iteration 125, loss = 0.03954401\n",
      "Iteration 126, loss = 0.03931672\n",
      "Iteration 127, loss = 0.03908439\n",
      "Iteration 128, loss = 0.03886414\n",
      "Iteration 129, loss = 0.03864577\n",
      "Iteration 130, loss = 0.03843882\n",
      "Iteration 131, loss = 0.03821256\n",
      "Iteration 132, loss = 0.03800396\n",
      "Iteration 133, loss = 0.03780294\n",
      "Iteration 134, loss = 0.03759555\n",
      "Iteration 135, loss = 0.03740253\n",
      "Iteration 136, loss = 0.03719637\n",
      "Iteration 137, loss = 0.03701477\n",
      "Iteration 138, loss = 0.03680560\n",
      "Iteration 139, loss = 0.03662300\n",
      "Iteration 140, loss = 0.03644888\n",
      "Iteration 141, loss = 0.03625128\n",
      "Iteration 142, loss = 0.03607273\n",
      "Iteration 143, loss = 0.03589828\n",
      "Iteration 144, loss = 0.03572135\n",
      "Iteration 145, loss = 0.03554762\n",
      "Iteration 146, loss = 0.03537308\n",
      "Iteration 147, loss = 0.03520767\n",
      "Iteration 148, loss = 0.03504408\n",
      "Iteration 149, loss = 0.03487542\n",
      "Iteration 150, loss = 0.03471969\n",
      "Iteration 151, loss = 0.03456509\n",
      "Iteration 152, loss = 0.03440596\n",
      "Iteration 153, loss = 0.03424710\n",
      "Iteration 154, loss = 0.03409774\n",
      "Iteration 155, loss = 0.03394919\n",
      "Iteration 156, loss = 0.03380101\n",
      "Iteration 157, loss = 0.03365355\n",
      "Iteration 158, loss = 0.03351257\n",
      "Iteration 159, loss = 0.03336726\n",
      "Iteration 160, loss = 0.03323179\n",
      "Iteration 161, loss = 0.03309424\n",
      "Iteration 162, loss = 0.03295890\n",
      "Iteration 163, loss = 0.03282488\n",
      "Iteration 164, loss = 0.03269166\n",
      "Iteration 165, loss = 0.03255934\n",
      "Iteration 166, loss = 0.03243224\n",
      "Iteration 167, loss = 0.03230428\n",
      "Iteration 168, loss = 0.03217504\n",
      "Iteration 169, loss = 0.03205377\n",
      "Iteration 170, loss = 0.03193094\n",
      "Iteration 171, loss = 0.03181110\n",
      "Iteration 172, loss = 0.03168991\n",
      "Iteration 173, loss = 0.03157802\n",
      "Iteration 174, loss = 0.03146247\n",
      "Iteration 175, loss = 0.03133441\n",
      "Iteration 176, loss = 0.03122044\n",
      "Iteration 177, loss = 0.03110667\n",
      "Iteration 178, loss = 0.03099971\n",
      "Iteration 179, loss = 0.03088661\n",
      "Iteration 180, loss = 0.03077984\n",
      "Iteration 181, loss = 0.03067327\n",
      "Iteration 182, loss = 0.03056438\n",
      "Iteration 183, loss = 0.03045859\n",
      "Iteration 184, loss = 0.03036130\n",
      "Iteration 185, loss = 0.03025465\n",
      "Iteration 186, loss = 0.03015315\n",
      "Iteration 187, loss = 0.03005271\n",
      "Iteration 188, loss = 0.02995004\n",
      "Iteration 189, loss = 0.02986598\n",
      "Iteration 190, loss = 0.02975794\n",
      "Iteration 191, loss = 0.02966394\n",
      "Iteration 192, loss = 0.02956767\n",
      "Iteration 193, loss = 0.02947377\n",
      "Iteration 194, loss = 0.02937745\n",
      "Iteration 195, loss = 0.02928723\n",
      "Iteration 196, loss = 0.02919498\n",
      "Iteration 197, loss = 0.02911106\n",
      "Iteration 198, loss = 0.02901265\n",
      "Iteration 199, loss = 0.02892493\n",
      "Iteration 200, loss = 0.02884055\n",
      "Iteration 201, loss = 0.02874963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69959051\n",
      "Iteration 2, loss = 0.60486258\n",
      "Iteration 3, loss = 0.53881446\n",
      "Iteration 4, loss = 0.48166273\n",
      "Iteration 5, loss = 0.42965595\n",
      "Iteration 6, loss = 0.38316024\n",
      "Iteration 7, loss = 0.34296549\n",
      "Iteration 8, loss = 0.30867362\n",
      "Iteration 9, loss = 0.28004012\n",
      "Iteration 10, loss = 0.25617549\n",
      "Iteration 11, loss = 0.23611009\n",
      "Iteration 12, loss = 0.21911187\n",
      "Iteration 13, loss = 0.20441165\n",
      "Iteration 14, loss = 0.19167056\n",
      "Iteration 15, loss = 0.18037719\n",
      "Iteration 16, loss = 0.17026591\n",
      "Iteration 17, loss = 0.16124064\n",
      "Iteration 18, loss = 0.15306778\n",
      "Iteration 19, loss = 0.14563163\n",
      "Iteration 20, loss = 0.13887718\n",
      "Iteration 21, loss = 0.13269681\n",
      "Iteration 22, loss = 0.12706842\n",
      "Iteration 23, loss = 0.12180234\n",
      "Iteration 24, loss = 0.11703628\n",
      "Iteration 25, loss = 0.11261062\n",
      "Iteration 26, loss = 0.10848668\n",
      "Iteration 27, loss = 0.10468866\n",
      "Iteration 28, loss = 0.10117011\n",
      "Iteration 29, loss = 0.09788522\n",
      "Iteration 30, loss = 0.09483116\n",
      "Iteration 31, loss = 0.09198315\n",
      "Iteration 32, loss = 0.08932738\n",
      "Iteration 33, loss = 0.08681917\n",
      "Iteration 34, loss = 0.08451799\n",
      "Iteration 35, loss = 0.08231668\n",
      "Iteration 36, loss = 0.08027282\n",
      "Iteration 37, loss = 0.07833230\n",
      "Iteration 38, loss = 0.07651366\n",
      "Iteration 39, loss = 0.07478665\n",
      "Iteration 40, loss = 0.07317302\n",
      "Iteration 41, loss = 0.07162198\n",
      "Iteration 42, loss = 0.07016567\n",
      "Iteration 43, loss = 0.06877299\n",
      "Iteration 44, loss = 0.06747206\n",
      "Iteration 45, loss = 0.06620973\n",
      "Iteration 46, loss = 0.06502614\n",
      "Iteration 47, loss = 0.06390148\n",
      "Iteration 48, loss = 0.06282539\n",
      "Iteration 49, loss = 0.06178513\n",
      "Iteration 50, loss = 0.06080381\n",
      "Iteration 51, loss = 0.05986679\n",
      "Iteration 52, loss = 0.05896394\n",
      "Iteration 53, loss = 0.05809381\n",
      "Iteration 54, loss = 0.05726758\n",
      "Iteration 55, loss = 0.05647805\n",
      "Iteration 56, loss = 0.05571002\n",
      "Iteration 57, loss = 0.05497877\n",
      "Iteration 58, loss = 0.05427030\n",
      "Iteration 59, loss = 0.05359823\n",
      "Iteration 60, loss = 0.05294608\n",
      "Iteration 61, loss = 0.05231781\n",
      "Iteration 62, loss = 0.05170768\n",
      "Iteration 63, loss = 0.05112488\n",
      "Iteration 64, loss = 0.05055004\n",
      "Iteration 65, loss = 0.05001399\n",
      "Iteration 66, loss = 0.04947874\n",
      "Iteration 67, loss = 0.04897008\n",
      "Iteration 68, loss = 0.04848133\n",
      "Iteration 69, loss = 0.04800144\n",
      "Iteration 70, loss = 0.04754050\n",
      "Iteration 71, loss = 0.04709363\n",
      "Iteration 72, loss = 0.04666438\n",
      "Iteration 73, loss = 0.04623408\n",
      "Iteration 74, loss = 0.04583606\n",
      "Iteration 75, loss = 0.04543108\n",
      "Iteration 76, loss = 0.04504559\n",
      "Iteration 77, loss = 0.04469042\n",
      "Iteration 78, loss = 0.04430999\n",
      "Iteration 79, loss = 0.04396239\n",
      "Iteration 80, loss = 0.04361804\n",
      "Iteration 81, loss = 0.04327732\n",
      "Iteration 82, loss = 0.04295203\n",
      "Iteration 83, loss = 0.04263633\n",
      "Iteration 84, loss = 0.04232799\n",
      "Iteration 85, loss = 0.04203307\n",
      "Iteration 86, loss = 0.04173949\n",
      "Iteration 87, loss = 0.04145310\n",
      "Iteration 88, loss = 0.04117402\n",
      "Iteration 89, loss = 0.04090331\n",
      "Iteration 90, loss = 0.04063966\n",
      "Iteration 91, loss = 0.04037966\n",
      "Iteration 92, loss = 0.04012835\n",
      "Iteration 93, loss = 0.03988069\n",
      "Iteration 94, loss = 0.03964050\n",
      "Iteration 95, loss = 0.03940856\n",
      "Iteration 96, loss = 0.03917805\n",
      "Iteration 97, loss = 0.03895812\n",
      "Iteration 98, loss = 0.03873717\n",
      "Iteration 99, loss = 0.03852475\n",
      "Iteration 100, loss = 0.03831269\n",
      "Iteration 101, loss = 0.03811032\n",
      "Iteration 102, loss = 0.03790503\n",
      "Iteration 103, loss = 0.03771175\n",
      "Iteration 104, loss = 0.03751759\n",
      "Iteration 105, loss = 0.03732989\n",
      "Iteration 106, loss = 0.03714327\n",
      "Iteration 107, loss = 0.03695978\n",
      "Iteration 108, loss = 0.03678647\n",
      "Iteration 109, loss = 0.03660919\n",
      "Iteration 110, loss = 0.03643921\n",
      "Iteration 111, loss = 0.03627346\n",
      "Iteration 112, loss = 0.03610497\n",
      "Iteration 113, loss = 0.03594887\n",
      "Iteration 114, loss = 0.03578599\n",
      "Iteration 115, loss = 0.03562853\n",
      "Iteration 116, loss = 0.03548090\n",
      "Iteration 117, loss = 0.03532662\n",
      "Iteration 118, loss = 0.03517527\n",
      "Iteration 119, loss = 0.03502902\n",
      "Iteration 120, loss = 0.03489670\n",
      "Iteration 121, loss = 0.03474940\n",
      "Iteration 122, loss = 0.03460925\n",
      "Iteration 123, loss = 0.03447435\n",
      "Iteration 124, loss = 0.03433686\n",
      "Iteration 125, loss = 0.03420454\n",
      "Iteration 126, loss = 0.03408238\n",
      "Iteration 127, loss = 0.03394815\n",
      "Iteration 128, loss = 0.03381810\n",
      "Iteration 129, loss = 0.03369211\n",
      "Iteration 130, loss = 0.03357528\n",
      "Iteration 131, loss = 0.03344796\n",
      "Iteration 132, loss = 0.03332603\n",
      "Iteration 133, loss = 0.03321044\n",
      "Iteration 134, loss = 0.03309738\n",
      "Iteration 135, loss = 0.03297730\n",
      "Iteration 136, loss = 0.03286764\n",
      "Iteration 137, loss = 0.03275242\n",
      "Iteration 138, loss = 0.03264804\n",
      "Iteration 139, loss = 0.03253351\n",
      "Iteration 140, loss = 0.03242552\n",
      "Iteration 141, loss = 0.03232996\n",
      "Iteration 142, loss = 0.03221846\n",
      "Iteration 143, loss = 0.03211908\n",
      "Iteration 144, loss = 0.03201102\n",
      "Iteration 145, loss = 0.03190959\n",
      "Iteration 146, loss = 0.03181145\n",
      "Iteration 147, loss = 0.03171779\n",
      "Iteration 148, loss = 0.03161797\n",
      "Iteration 149, loss = 0.03152099\n",
      "Iteration 150, loss = 0.03142864\n",
      "Iteration 151, loss = 0.03133592\n",
      "Iteration 152, loss = 0.03124569\n",
      "Iteration 153, loss = 0.03115534\n",
      "Iteration 154, loss = 0.03106412\n",
      "Iteration 155, loss = 0.03098070\n",
      "Iteration 156, loss = 0.03088880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72810993\n",
      "Iteration 2, loss = 0.63186096\n",
      "Iteration 3, loss = 0.57073932\n",
      "Iteration 4, loss = 0.51933259\n",
      "Iteration 5, loss = 0.47179736\n",
      "Iteration 6, loss = 0.42760676\n",
      "Iteration 7, loss = 0.38795166\n",
      "Iteration 8, loss = 0.35340591\n",
      "Iteration 9, loss = 0.32395828\n",
      "Iteration 10, loss = 0.29899709\n",
      "Iteration 11, loss = 0.27779981\n",
      "Iteration 12, loss = 0.25950070\n",
      "Iteration 13, loss = 0.24357603\n",
      "Iteration 14, loss = 0.22958003\n",
      "Iteration 15, loss = 0.21714413\n",
      "Iteration 16, loss = 0.20592712\n",
      "Iteration 17, loss = 0.19591183\n",
      "Iteration 18, loss = 0.18684245\n",
      "Iteration 19, loss = 0.17861778\n",
      "Iteration 20, loss = 0.17115346\n",
      "Iteration 21, loss = 0.16430098\n",
      "Iteration 22, loss = 0.15803879\n",
      "Iteration 23, loss = 0.15226044\n",
      "Iteration 24, loss = 0.14696787\n",
      "Iteration 25, loss = 0.14206924\n",
      "Iteration 26, loss = 0.13755959\n",
      "Iteration 27, loss = 0.13329011\n",
      "Iteration 28, loss = 0.12934284\n",
      "Iteration 29, loss = 0.12564564\n",
      "Iteration 30, loss = 0.12218479\n",
      "Iteration 31, loss = 0.11893533\n",
      "Iteration 32, loss = 0.11590044\n",
      "Iteration 33, loss = 0.11299941\n",
      "Iteration 34, loss = 0.11031243\n",
      "Iteration 35, loss = 0.10771323\n",
      "Iteration 36, loss = 0.10529103\n",
      "Iteration 37, loss = 0.10296219\n",
      "Iteration 38, loss = 0.10073724\n",
      "Iteration 39, loss = 0.09864338\n",
      "Iteration 40, loss = 0.09662339\n",
      "Iteration 41, loss = 0.09470150\n",
      "Iteration 42, loss = 0.09283899\n",
      "Iteration 43, loss = 0.09106091\n",
      "Iteration 44, loss = 0.08939638\n",
      "Iteration 45, loss = 0.08773942\n",
      "Iteration 46, loss = 0.08618048\n",
      "Iteration 47, loss = 0.08466979\n",
      "Iteration 48, loss = 0.08321603\n",
      "Iteration 49, loss = 0.08183417\n",
      "Iteration 50, loss = 0.08045175\n",
      "Iteration 51, loss = 0.07914074\n",
      "Iteration 52, loss = 0.07787467\n",
      "Iteration 53, loss = 0.07664657\n",
      "Iteration 54, loss = 0.07547228\n",
      "Iteration 55, loss = 0.07434281\n",
      "Iteration 56, loss = 0.07321937\n",
      "Iteration 57, loss = 0.07216793\n",
      "Iteration 58, loss = 0.07113694\n",
      "Iteration 59, loss = 0.07010813\n",
      "Iteration 60, loss = 0.06914594\n",
      "Iteration 61, loss = 0.06819522\n",
      "Iteration 62, loss = 0.06725647\n",
      "Iteration 63, loss = 0.06638326\n",
      "Iteration 64, loss = 0.06551576\n",
      "Iteration 65, loss = 0.06466037\n",
      "Iteration 66, loss = 0.06383268\n",
      "Iteration 67, loss = 0.06303781\n",
      "Iteration 68, loss = 0.06227512\n",
      "Iteration 69, loss = 0.06151226\n",
      "Iteration 70, loss = 0.06078119\n",
      "Iteration 71, loss = 0.06006920\n",
      "Iteration 72, loss = 0.05938223\n",
      "Iteration 73, loss = 0.05870352\n",
      "Iteration 74, loss = 0.05805118\n",
      "Iteration 75, loss = 0.05741323\n",
      "Iteration 76, loss = 0.05678763\n",
      "Iteration 77, loss = 0.05619231\n",
      "Iteration 78, loss = 0.05558449\n",
      "Iteration 79, loss = 0.05502180\n",
      "Iteration 80, loss = 0.05446527\n",
      "Iteration 81, loss = 0.05390871\n",
      "Iteration 82, loss = 0.05337189\n",
      "Iteration 83, loss = 0.05284984\n",
      "Iteration 84, loss = 0.05234267\n",
      "Iteration 85, loss = 0.05185802\n",
      "Iteration 86, loss = 0.05137063\n",
      "Iteration 87, loss = 0.05089135\n",
      "Iteration 88, loss = 0.05042778\n",
      "Iteration 89, loss = 0.04997539\n",
      "Iteration 90, loss = 0.04954814\n",
      "Iteration 91, loss = 0.04911180\n",
      "Iteration 92, loss = 0.04868579\n",
      "Iteration 93, loss = 0.04826650\n",
      "Iteration 94, loss = 0.04786159\n",
      "Iteration 95, loss = 0.04745854\n",
      "Iteration 96, loss = 0.04707265\n",
      "Iteration 97, loss = 0.04669416\n",
      "Iteration 98, loss = 0.04633725\n",
      "Iteration 99, loss = 0.04597473\n",
      "Iteration 100, loss = 0.04560276\n",
      "Iteration 101, loss = 0.04526454\n",
      "Iteration 102, loss = 0.04491933\n",
      "Iteration 103, loss = 0.04458458\n",
      "Iteration 104, loss = 0.04425267\n",
      "Iteration 105, loss = 0.04393562\n",
      "Iteration 106, loss = 0.04365951\n",
      "Iteration 107, loss = 0.04331861\n",
      "Iteration 108, loss = 0.04301364\n",
      "Iteration 109, loss = 0.04271570\n",
      "Iteration 110, loss = 0.04242442\n",
      "Iteration 111, loss = 0.04213505\n",
      "Iteration 112, loss = 0.04185501\n",
      "Iteration 113, loss = 0.04158183\n",
      "Iteration 114, loss = 0.04130550\n",
      "Iteration 115, loss = 0.04104174\n",
      "Iteration 116, loss = 0.04078700\n",
      "Iteration 117, loss = 0.04052597\n",
      "Iteration 118, loss = 0.04028221\n",
      "Iteration 119, loss = 0.04002681\n",
      "Iteration 120, loss = 0.03977968\n",
      "Iteration 121, loss = 0.03954607\n",
      "Iteration 122, loss = 0.03930026\n",
      "Iteration 123, loss = 0.03907221\n",
      "Iteration 124, loss = 0.03884260\n",
      "Iteration 125, loss = 0.03861131\n",
      "Iteration 126, loss = 0.03839169\n",
      "Iteration 127, loss = 0.03817244\n",
      "Iteration 128, loss = 0.03796051\n",
      "Iteration 129, loss = 0.03775355\n",
      "Iteration 130, loss = 0.03754191\n",
      "Iteration 131, loss = 0.03735144\n",
      "Iteration 132, loss = 0.03715568\n",
      "Iteration 133, loss = 0.03694297\n",
      "Iteration 134, loss = 0.03675703\n",
      "Iteration 135, loss = 0.03655788\n",
      "Iteration 136, loss = 0.03636850\n",
      "Iteration 137, loss = 0.03618617\n",
      "Iteration 138, loss = 0.03600528\n",
      "Iteration 139, loss = 0.03581951\n",
      "Iteration 140, loss = 0.03564305\n",
      "Iteration 141, loss = 0.03546901\n",
      "Iteration 142, loss = 0.03530393\n",
      "Iteration 143, loss = 0.03512715\n",
      "Iteration 144, loss = 0.03496026\n",
      "Iteration 145, loss = 0.03479451\n",
      "Iteration 146, loss = 0.03463588\n",
      "Iteration 147, loss = 0.03447112\n",
      "Iteration 148, loss = 0.03431981\n",
      "Iteration 149, loss = 0.03415572\n",
      "Iteration 150, loss = 0.03400459\n",
      "Iteration 151, loss = 0.03385364\n",
      "Iteration 152, loss = 0.03370458\n",
      "Iteration 153, loss = 0.03355899\n",
      "Iteration 154, loss = 0.03340952\n",
      "Iteration 155, loss = 0.03326903\n",
      "Iteration 156, loss = 0.03312649\n",
      "Iteration 157, loss = 0.03299277\n",
      "Iteration 158, loss = 0.03285153\n",
      "Iteration 159, loss = 0.03271683\n",
      "Iteration 160, loss = 0.03258114\n",
      "Iteration 161, loss = 0.03244640\n",
      "Iteration 162, loss = 0.03231386\n",
      "Iteration 163, loss = 0.03218965\n",
      "Iteration 164, loss = 0.03205586\n",
      "Iteration 165, loss = 0.03193028\n",
      "Iteration 166, loss = 0.03180694\n",
      "Iteration 167, loss = 0.03168358\n",
      "Iteration 168, loss = 0.03156199\n",
      "Iteration 169, loss = 0.03145000\n",
      "Iteration 170, loss = 0.03132607\n",
      "Iteration 171, loss = 0.03121152\n",
      "Iteration 172, loss = 0.03109772\n",
      "Iteration 173, loss = 0.03098039\n",
      "Iteration 174, loss = 0.03087420\n",
      "Iteration 175, loss = 0.03076062\n",
      "Iteration 176, loss = 0.03065310\n",
      "Iteration 177, loss = 0.03053523\n",
      "Iteration 178, loss = 0.03044552\n",
      "Iteration 179, loss = 0.03032300\n",
      "Iteration 180, loss = 0.03021871\n",
      "Iteration 181, loss = 0.03011798\n",
      "Iteration 182, loss = 0.03001413\n",
      "Iteration 183, loss = 0.02991176\n",
      "Iteration 184, loss = 0.02981485\n",
      "Iteration 185, loss = 0.02971682\n",
      "Iteration 186, loss = 0.02962118\n",
      "Iteration 187, loss = 0.02952086\n",
      "Iteration 188, loss = 0.02943456\n",
      "Iteration 189, loss = 0.02932713\n",
      "Iteration 190, loss = 0.02923108\n",
      "Iteration 191, loss = 0.02914391\n",
      "Iteration 192, loss = 0.02905499\n",
      "Iteration 193, loss = 0.02896134\n",
      "Iteration 194, loss = 0.02887105\n",
      "Iteration 195, loss = 0.02877906\n",
      "Iteration 196, loss = 0.02869345\n",
      "Iteration 197, loss = 0.02860653\n",
      "Iteration 198, loss = 0.02852238\n",
      "Iteration 199, loss = 0.02843579\n",
      "Iteration 200, loss = 0.02835384\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72040410\n",
      "Iteration 2, loss = 0.61693252\n",
      "Iteration 3, loss = 0.54594246\n",
      "Iteration 4, loss = 0.48438756\n",
      "Iteration 5, loss = 0.42883938\n",
      "Iteration 6, loss = 0.37970077\n",
      "Iteration 7, loss = 0.33751311\n",
      "Iteration 8, loss = 0.30206509\n",
      "Iteration 9, loss = 0.27257675\n",
      "Iteration 10, loss = 0.24807771\n",
      "Iteration 11, loss = 0.22759236\n",
      "Iteration 12, loss = 0.21016604\n",
      "Iteration 13, loss = 0.19524332\n",
      "Iteration 14, loss = 0.18223845\n",
      "Iteration 15, loss = 0.17089158\n",
      "Iteration 16, loss = 0.16079031\n",
      "Iteration 17, loss = 0.15182019\n",
      "Iteration 18, loss = 0.14376794\n",
      "Iteration 19, loss = 0.13653026\n",
      "Iteration 20, loss = 0.13000821\n",
      "Iteration 21, loss = 0.12403413\n",
      "Iteration 22, loss = 0.11860953\n",
      "Iteration 23, loss = 0.11364076\n",
      "Iteration 24, loss = 0.10906017\n",
      "Iteration 25, loss = 0.10483671\n",
      "Iteration 26, loss = 0.10093987\n",
      "Iteration 27, loss = 0.09732093\n",
      "Iteration 28, loss = 0.09397447\n",
      "Iteration 29, loss = 0.09084568\n",
      "Iteration 30, loss = 0.08792832\n",
      "Iteration 31, loss = 0.08519550\n",
      "Iteration 32, loss = 0.08263329\n",
      "Iteration 33, loss = 0.08021868\n",
      "Iteration 34, loss = 0.07798853\n",
      "Iteration 35, loss = 0.07581195\n",
      "Iteration 36, loss = 0.07379886\n",
      "Iteration 37, loss = 0.07188054\n",
      "Iteration 38, loss = 0.07006130\n",
      "Iteration 39, loss = 0.06836335\n",
      "Iteration 40, loss = 0.06673037\n",
      "Iteration 41, loss = 0.06519251\n",
      "Iteration 42, loss = 0.06370903\n",
      "Iteration 43, loss = 0.06230438\n",
      "Iteration 44, loss = 0.06096883\n",
      "Iteration 45, loss = 0.05969485\n",
      "Iteration 46, loss = 0.05849646\n",
      "Iteration 47, loss = 0.05732220\n",
      "Iteration 48, loss = 0.05621356\n",
      "Iteration 49, loss = 0.05514984\n",
      "Iteration 50, loss = 0.05414532\n",
      "Iteration 51, loss = 0.05316954\n",
      "Iteration 52, loss = 0.05221909\n",
      "Iteration 53, loss = 0.05132862\n",
      "Iteration 54, loss = 0.05046394\n",
      "Iteration 55, loss = 0.04963090\n",
      "Iteration 56, loss = 0.04882979\n",
      "Iteration 57, loss = 0.04805723\n",
      "Iteration 58, loss = 0.04732345\n",
      "Iteration 59, loss = 0.04660331\n",
      "Iteration 60, loss = 0.04592171\n",
      "Iteration 61, loss = 0.04527188\n",
      "Iteration 62, loss = 0.04462625\n",
      "Iteration 63, loss = 0.04400763\n",
      "Iteration 64, loss = 0.04341305\n",
      "Iteration 65, loss = 0.04283204\n",
      "Iteration 66, loss = 0.04227187\n",
      "Iteration 67, loss = 0.04173624\n",
      "Iteration 68, loss = 0.04121123\n",
      "Iteration 69, loss = 0.04070622\n",
      "Iteration 70, loss = 0.04021668\n",
      "Iteration 71, loss = 0.03974686\n",
      "Iteration 72, loss = 0.03929068\n",
      "Iteration 73, loss = 0.03884768\n",
      "Iteration 74, loss = 0.03842022\n",
      "Iteration 75, loss = 0.03799979\n",
      "Iteration 76, loss = 0.03759643\n",
      "Iteration 77, loss = 0.03720860\n",
      "Iteration 78, loss = 0.03682485\n",
      "Iteration 79, loss = 0.03646072\n",
      "Iteration 80, loss = 0.03609007\n",
      "Iteration 81, loss = 0.03574273\n",
      "Iteration 82, loss = 0.03540067\n",
      "Iteration 83, loss = 0.03507058\n",
      "Iteration 84, loss = 0.03476229\n",
      "Iteration 85, loss = 0.03443739\n",
      "Iteration 86, loss = 0.03413165\n",
      "Iteration 87, loss = 0.03383412\n",
      "Iteration 88, loss = 0.03354562\n",
      "Iteration 89, loss = 0.03326524\n",
      "Iteration 90, loss = 0.03299083\n",
      "Iteration 91, loss = 0.03271812\n",
      "Iteration 92, loss = 0.03246531\n",
      "Iteration 93, loss = 0.03220826\n",
      "Iteration 94, loss = 0.03195579\n",
      "Iteration 95, loss = 0.03171623\n",
      "Iteration 96, loss = 0.03148248\n",
      "Iteration 97, loss = 0.03124937\n",
      "Iteration 98, loss = 0.03102446\n",
      "Iteration 99, loss = 0.03080322\n",
      "Iteration 100, loss = 0.03058944\n",
      "Iteration 101, loss = 0.03037678\n",
      "Iteration 102, loss = 0.03017456\n",
      "Iteration 103, loss = 0.02997168\n",
      "Iteration 104, loss = 0.02977680\n",
      "Iteration 105, loss = 0.02958546\n",
      "Iteration 106, loss = 0.02940221\n",
      "Iteration 107, loss = 0.02921755\n",
      "Iteration 108, loss = 0.02904418\n",
      "Iteration 109, loss = 0.02886480\n",
      "Iteration 110, loss = 0.02869478\n",
      "Iteration 111, loss = 0.02852898\n",
      "Iteration 112, loss = 0.02836211\n",
      "Iteration 113, loss = 0.02820191\n",
      "Iteration 114, loss = 0.02804823\n",
      "Iteration 115, loss = 0.02788859\n",
      "Iteration 116, loss = 0.02773959\n",
      "Iteration 117, loss = 0.02759050\n",
      "Iteration 118, loss = 0.02744516\n",
      "Iteration 119, loss = 0.02730075\n",
      "Iteration 120, loss = 0.02716090\n",
      "Iteration 121, loss = 0.02702383\n",
      "Iteration 122, loss = 0.02689147\n",
      "Iteration 123, loss = 0.02675905\n",
      "Iteration 124, loss = 0.02663672\n",
      "Iteration 125, loss = 0.02650380\n",
      "Iteration 126, loss = 0.02638001\n",
      "Iteration 127, loss = 0.02625909\n",
      "Iteration 128, loss = 0.02613831\n",
      "Iteration 129, loss = 0.02602161\n",
      "Iteration 130, loss = 0.02590856\n",
      "Iteration 131, loss = 0.02579501\n",
      "Iteration 132, loss = 0.02568474\n",
      "Iteration 133, loss = 0.02557266\n",
      "Iteration 134, loss = 0.02546634\n",
      "Iteration 135, loss = 0.02536116\n",
      "Iteration 136, loss = 0.02525685\n",
      "Iteration 137, loss = 0.02515577\n",
      "Iteration 138, loss = 0.02505866\n",
      "Iteration 139, loss = 0.02495876\n",
      "Iteration 140, loss = 0.02486111\n",
      "Iteration 141, loss = 0.02476612\n",
      "Iteration 142, loss = 0.02467374\n",
      "Iteration 143, loss = 0.02458489\n",
      "Iteration 144, loss = 0.02449230\n",
      "Iteration 145, loss = 0.02440416\n",
      "Iteration 146, loss = 0.02431961\n",
      "Iteration 147, loss = 0.02423177\n",
      "Iteration 148, loss = 0.02414843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.14547184\n",
      "Iteration 2, loss = 0.04199899\n",
      "Iteration 3, loss = 0.01903803\n",
      "Iteration 4, loss = 0.01463524\n",
      "Iteration 5, loss = 0.01496507\n",
      "Iteration 6, loss = 0.01773753\n",
      "Iteration 7, loss = 0.02400923\n",
      "Iteration 8, loss = 0.06111175\n",
      "Iteration 9, loss = 0.03902353\n",
      "Iteration 10, loss = 0.01269330\n",
      "Iteration 11, loss = 0.01165280\n",
      "Iteration 12, loss = 0.01098620\n",
      "Iteration 13, loss = 0.01089645\n",
      "Iteration 14, loss = 0.01253943\n",
      "Iteration 15, loss = 0.01362062\n",
      "Iteration 16, loss = 0.01179005\n",
      "Iteration 17, loss = 0.02792787\n",
      "Iteration 18, loss = 0.05015571\n",
      "Iteration 19, loss = 0.02116935\n",
      "Iteration 20, loss = 0.01443977\n",
      "Iteration 21, loss = 0.01305306\n",
      "Iteration 22, loss = 0.03271507\n",
      "Iteration 23, loss = 0.03592713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20119521\n",
      "Iteration 2, loss = 0.05533366\n",
      "Iteration 3, loss = 0.02593231\n",
      "Iteration 4, loss = 0.02226834\n",
      "Iteration 5, loss = 0.02148633\n",
      "Iteration 6, loss = 0.01393834\n",
      "Iteration 7, loss = 0.01306498\n",
      "Iteration 8, loss = 0.01104268\n",
      "Iteration 9, loss = 0.01331986\n",
      "Iteration 10, loss = 0.01205074\n",
      "Iteration 11, loss = 0.04217651\n",
      "Iteration 12, loss = 0.03261164\n",
      "Iteration 13, loss = 0.01310107\n",
      "Iteration 14, loss = 0.01256942\n",
      "Iteration 15, loss = 0.01078707\n",
      "Iteration 16, loss = 0.01247361\n",
      "Iteration 17, loss = 0.01264766\n",
      "Iteration 18, loss = 0.01206227\n",
      "Iteration 19, loss = 0.01098618\n",
      "Iteration 20, loss = 0.01103605\n",
      "Iteration 21, loss = 0.01175961\n",
      "Iteration 22, loss = 0.01193294\n",
      "Iteration 23, loss = 0.03429221\n",
      "Iteration 24, loss = 0.15251479\n",
      "Iteration 25, loss = 0.04654708\n",
      "Iteration 26, loss = 0.02109062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16575342\n",
      "Iteration 2, loss = 0.05226469\n",
      "Iteration 3, loss = 0.02610934\n",
      "Iteration 4, loss = 0.03525095\n",
      "Iteration 5, loss = 0.02199284\n",
      "Iteration 6, loss = 0.01453280\n",
      "Iteration 7, loss = 0.01583116\n",
      "Iteration 8, loss = 0.01407796\n",
      "Iteration 9, loss = 0.01484512\n",
      "Iteration 10, loss = 0.01377835\n",
      "Iteration 11, loss = 0.01356932\n",
      "Iteration 12, loss = 0.01252097\n",
      "Iteration 13, loss = 0.01154987\n",
      "Iteration 14, loss = 0.01573458\n",
      "Iteration 15, loss = 0.02390314\n",
      "Iteration 16, loss = 0.02555920\n",
      "Iteration 17, loss = 0.01387817\n",
      "Iteration 18, loss = 0.01306255\n",
      "Iteration 19, loss = 0.01252813\n",
      "Iteration 20, loss = 0.01202231\n",
      "Iteration 21, loss = 0.01189683\n",
      "Iteration 22, loss = 0.01162403\n",
      "Iteration 23, loss = 0.17543523\n",
      "Iteration 24, loss = 0.12327811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18461116\n",
      "Iteration 2, loss = 0.05038337\n",
      "Iteration 3, loss = 0.02909844\n",
      "Iteration 4, loss = 0.02441453\n",
      "Iteration 5, loss = 0.01620134\n",
      "Iteration 6, loss = 0.01296740\n",
      "Iteration 7, loss = 0.01406381\n",
      "Iteration 8, loss = 0.01416332\n",
      "Iteration 9, loss = 0.01646848\n",
      "Iteration 10, loss = 0.10723952\n",
      "Iteration 11, loss = 0.04713107\n",
      "Iteration 12, loss = 0.01865821\n",
      "Iteration 13, loss = 0.01332954\n",
      "Iteration 14, loss = 0.01168101\n",
      "Iteration 15, loss = 0.01091576\n",
      "Iteration 16, loss = 0.01084380\n",
      "Iteration 17, loss = 0.01080599\n",
      "Iteration 18, loss = 0.01049890\n",
      "Iteration 19, loss = 0.01056711\n",
      "Iteration 20, loss = 0.01059096\n",
      "Iteration 21, loss = 0.01091920\n",
      "Iteration 22, loss = 0.01093791\n",
      "Iteration 23, loss = 0.01145284\n",
      "Iteration 24, loss = 0.01645884\n",
      "Iteration 25, loss = 0.07112806\n",
      "Iteration 26, loss = 0.06198922\n",
      "Iteration 27, loss = 0.02349714\n",
      "Iteration 28, loss = 0.01420913\n",
      "Iteration 29, loss = 0.01196203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23456605\n",
      "Iteration 2, loss = 0.07872645\n",
      "Iteration 3, loss = 0.03515601\n",
      "Iteration 4, loss = 0.02120512\n",
      "Iteration 5, loss = 0.01998726\n",
      "Iteration 6, loss = 0.01720809\n",
      "Iteration 7, loss = 0.01513728\n",
      "Iteration 8, loss = 0.01475148\n",
      "Iteration 9, loss = 0.01420034\n",
      "Iteration 10, loss = 0.02307971\n",
      "Iteration 11, loss = 0.01292095\n",
      "Iteration 12, loss = 0.01328761\n",
      "Iteration 13, loss = 0.01339244\n",
      "Iteration 14, loss = 0.01405214\n",
      "Iteration 15, loss = 0.01658779\n",
      "Iteration 16, loss = 0.01917653\n",
      "Iteration 17, loss = 0.01337909\n",
      "Iteration 18, loss = 0.01579259\n",
      "Iteration 19, loss = 0.04098927\n",
      "Iteration 20, loss = 0.03255571\n",
      "Iteration 21, loss = 0.01590827\n",
      "Iteration 22, loss = 0.02224608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17298647\n",
      "Iteration 2, loss = 0.04192827\n",
      "Iteration 3, loss = 0.03206423\n",
      "Iteration 4, loss = 0.02792911\n",
      "Iteration 5, loss = 0.02546551\n",
      "Iteration 6, loss = 0.02385819\n",
      "Iteration 7, loss = 0.02271418\n",
      "Iteration 8, loss = 0.02185209\n",
      "Iteration 9, loss = 0.02117813\n",
      "Iteration 10, loss = 0.02052521\n",
      "Iteration 11, loss = 0.02002054\n",
      "Iteration 12, loss = 0.01952958\n",
      "Iteration 13, loss = 0.01908209\n",
      "Iteration 14, loss = 0.01867493\n",
      "Iteration 15, loss = 0.01832398\n",
      "Iteration 16, loss = 0.01791893\n",
      "Iteration 17, loss = 0.01754885\n",
      "Iteration 18, loss = 0.01729537\n",
      "Iteration 19, loss = 0.01693873\n",
      "Iteration 20, loss = 0.01663962\n",
      "Iteration 21, loss = 0.01637046\n",
      "Iteration 22, loss = 0.01607838\n",
      "Iteration 23, loss = 0.01582829\n",
      "Iteration 24, loss = 0.01558837\n",
      "Iteration 25, loss = 0.01532806\n",
      "Iteration 26, loss = 0.01508471\n",
      "Iteration 27, loss = 0.01485452\n",
      "Iteration 28, loss = 0.01464199\n",
      "Iteration 29, loss = 0.01444145\n",
      "Iteration 30, loss = 0.01424675\n",
      "Iteration 31, loss = 0.01403925\n",
      "Iteration 32, loss = 0.01385432\n",
      "Iteration 33, loss = 0.01367274\n",
      "Iteration 34, loss = 0.01350632\n",
      "Iteration 35, loss = 0.01332102\n",
      "Iteration 36, loss = 0.01319737\n",
      "Iteration 37, loss = 0.01303393\n",
      "Iteration 38, loss = 0.01284663\n",
      "Iteration 39, loss = 0.01269990\n",
      "Iteration 40, loss = 0.01258672\n",
      "Iteration 41, loss = 0.01242831\n",
      "Iteration 42, loss = 0.01230437\n",
      "Iteration 43, loss = 0.01216474\n",
      "Iteration 44, loss = 0.01204819\n",
      "Iteration 45, loss = 0.01192011\n",
      "Iteration 46, loss = 0.01181610\n",
      "Iteration 47, loss = 0.01171007\n",
      "Iteration 48, loss = 0.01159608\n",
      "Iteration 49, loss = 0.01151326\n",
      "Iteration 50, loss = 0.01141457\n",
      "Iteration 51, loss = 0.01131696\n",
      "Iteration 52, loss = 0.01120178\n",
      "Iteration 53, loss = 0.01112209\n",
      "Iteration 54, loss = 0.01104778\n",
      "Iteration 55, loss = 0.01094250\n",
      "Iteration 56, loss = 0.01086904\n",
      "Iteration 57, loss = 0.01079784\n",
      "Iteration 58, loss = 0.01070678\n",
      "Iteration 59, loss = 0.01063854\n",
      "Iteration 60, loss = 0.01055826\n",
      "Iteration 61, loss = 0.01050616\n",
      "Iteration 62, loss = 0.01043794\n",
      "Iteration 63, loss = 0.01038159\n",
      "Iteration 64, loss = 0.01031438\n",
      "Iteration 65, loss = 0.01025689\n",
      "Iteration 66, loss = 0.01020471\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20900694\n",
      "Iteration 2, loss = 0.04662989\n",
      "Iteration 3, loss = 0.03231653\n",
      "Iteration 4, loss = 0.02788247\n",
      "Iteration 5, loss = 0.02558696\n",
      "Iteration 6, loss = 0.02417178\n",
      "Iteration 7, loss = 0.02328240\n",
      "Iteration 8, loss = 0.02257119\n",
      "Iteration 9, loss = 0.02194026\n",
      "Iteration 10, loss = 0.02139358\n",
      "Iteration 11, loss = 0.02091362\n",
      "Iteration 12, loss = 0.02047570\n",
      "Iteration 13, loss = 0.02006697\n",
      "Iteration 14, loss = 0.01967921\n",
      "Iteration 15, loss = 0.01930009\n",
      "Iteration 16, loss = 0.01896527\n",
      "Iteration 17, loss = 0.01864796\n",
      "Iteration 18, loss = 0.01831484\n",
      "Iteration 19, loss = 0.01800995\n",
      "Iteration 20, loss = 0.01771506\n",
      "Iteration 21, loss = 0.01744554\n",
      "Iteration 22, loss = 0.01716596\n",
      "Iteration 23, loss = 0.01691674\n",
      "Iteration 24, loss = 0.01666472\n",
      "Iteration 25, loss = 0.01641449\n",
      "Iteration 26, loss = 0.01618487\n",
      "Iteration 27, loss = 0.01595999\n",
      "Iteration 28, loss = 0.01574842\n",
      "Iteration 29, loss = 0.01555715\n",
      "Iteration 30, loss = 0.01535159\n",
      "Iteration 31, loss = 0.01515406\n",
      "Iteration 32, loss = 0.01495935\n",
      "Iteration 33, loss = 0.01477637\n",
      "Iteration 34, loss = 0.01460416\n",
      "Iteration 35, loss = 0.01443388\n",
      "Iteration 36, loss = 0.01427469\n",
      "Iteration 37, loss = 0.01412465\n",
      "Iteration 38, loss = 0.01396835\n",
      "Iteration 39, loss = 0.01381717\n",
      "Iteration 40, loss = 0.01369735\n",
      "Iteration 41, loss = 0.01354549\n",
      "Iteration 42, loss = 0.01342302\n",
      "Iteration 43, loss = 0.01328873\n",
      "Iteration 44, loss = 0.01317914\n",
      "Iteration 45, loss = 0.01305307\n",
      "Iteration 46, loss = 0.01293778\n",
      "Iteration 47, loss = 0.01282818\n",
      "Iteration 48, loss = 0.01272026\n",
      "Iteration 49, loss = 0.01262595\n",
      "Iteration 50, loss = 0.01252474\n",
      "Iteration 51, loss = 0.01243630\n",
      "Iteration 52, loss = 0.01233490\n",
      "Iteration 53, loss = 0.01226169\n",
      "Iteration 54, loss = 0.01217869\n",
      "Iteration 55, loss = 0.01207509\n",
      "Iteration 56, loss = 0.01199146\n",
      "Iteration 57, loss = 0.01192434\n",
      "Iteration 58, loss = 0.01183860\n",
      "Iteration 59, loss = 0.01176572\n",
      "Iteration 60, loss = 0.01169139\n",
      "Iteration 61, loss = 0.01163171\n",
      "Iteration 62, loss = 0.01157316\n",
      "Iteration 63, loss = 0.01149826\n",
      "Iteration 64, loss = 0.01144466\n",
      "Iteration 65, loss = 0.01138240\n",
      "Iteration 66, loss = 0.01132557\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18361563\n",
      "Iteration 2, loss = 0.04069106\n",
      "Iteration 3, loss = 0.03362220\n",
      "Iteration 4, loss = 0.03029658\n",
      "Iteration 5, loss = 0.02724147\n",
      "Iteration 6, loss = 0.02516020\n",
      "Iteration 7, loss = 0.02374164\n",
      "Iteration 8, loss = 0.02279396\n",
      "Iteration 9, loss = 0.02197361\n",
      "Iteration 10, loss = 0.02132463\n",
      "Iteration 11, loss = 0.02072104\n",
      "Iteration 12, loss = 0.02019193\n",
      "Iteration 13, loss = 0.01979490\n",
      "Iteration 14, loss = 0.01930960\n",
      "Iteration 15, loss = 0.01888931\n",
      "Iteration 16, loss = 0.01851451\n",
      "Iteration 17, loss = 0.01813373\n",
      "Iteration 18, loss = 0.01781974\n",
      "Iteration 19, loss = 0.01749920\n",
      "Iteration 20, loss = 0.01715663\n",
      "Iteration 21, loss = 0.01686153\n",
      "Iteration 22, loss = 0.01661579\n",
      "Iteration 23, loss = 0.01631455\n",
      "Iteration 24, loss = 0.01606825\n",
      "Iteration 25, loss = 0.01579897\n",
      "Iteration 26, loss = 0.01554195\n",
      "Iteration 27, loss = 0.01530561\n",
      "Iteration 28, loss = 0.01509284\n",
      "Iteration 29, loss = 0.01485327\n",
      "Iteration 30, loss = 0.01468569\n",
      "Iteration 31, loss = 0.01446176\n",
      "Iteration 32, loss = 0.01428879\n",
      "Iteration 33, loss = 0.01411043\n",
      "Iteration 34, loss = 0.01391204\n",
      "Iteration 35, loss = 0.01373376\n",
      "Iteration 36, loss = 0.01357342\n",
      "Iteration 37, loss = 0.01341786\n",
      "Iteration 38, loss = 0.01326048\n",
      "Iteration 39, loss = 0.01310904\n",
      "Iteration 40, loss = 0.01299867\n",
      "Iteration 41, loss = 0.01282691\n",
      "Iteration 42, loss = 0.01270075\n",
      "Iteration 43, loss = 0.01255827\n",
      "Iteration 44, loss = 0.01243933\n",
      "Iteration 45, loss = 0.01234113\n",
      "Iteration 46, loss = 0.01220134\n",
      "Iteration 47, loss = 0.01209573\n",
      "Iteration 48, loss = 0.01201870\n",
      "Iteration 49, loss = 0.01190084\n",
      "Iteration 50, loss = 0.01178569\n",
      "Iteration 51, loss = 0.01169201\n",
      "Iteration 52, loss = 0.01160765\n",
      "Iteration 53, loss = 0.01152278\n",
      "Iteration 54, loss = 0.01141845\n",
      "Iteration 55, loss = 0.01134676\n",
      "Iteration 56, loss = 0.01124909\n",
      "Iteration 57, loss = 0.01117647\n",
      "Iteration 58, loss = 0.01111697\n",
      "Iteration 59, loss = 0.01102536\n",
      "Iteration 60, loss = 0.01095818\n",
      "Iteration 61, loss = 0.01089674\n",
      "Iteration 62, loss = 0.01086201\n",
      "Iteration 63, loss = 0.01075950\n",
      "Iteration 64, loss = 0.01069451\n",
      "Iteration 65, loss = 0.01063877\n",
      "Iteration 66, loss = 0.01060167\n",
      "Iteration 67, loss = 0.01052669\n",
      "Iteration 68, loss = 0.01048045\n",
      "Iteration 69, loss = 0.01041418\n",
      "Iteration 70, loss = 0.01039577\n",
      "Iteration 71, loss = 0.01032727\n",
      "Iteration 72, loss = 0.01028059\n",
      "Iteration 73, loss = 0.01024104\n",
      "Iteration 74, loss = 0.01018893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21113421\n",
      "Iteration 2, loss = 0.04551131\n",
      "Iteration 3, loss = 0.03192843\n",
      "Iteration 4, loss = 0.02747674\n",
      "Iteration 5, loss = 0.02552183\n",
      "Iteration 6, loss = 0.02418771\n",
      "Iteration 7, loss = 0.02335080\n",
      "Iteration 8, loss = 0.02262841\n",
      "Iteration 9, loss = 0.02200944\n",
      "Iteration 10, loss = 0.02151400\n",
      "Iteration 11, loss = 0.02102242\n",
      "Iteration 12, loss = 0.02060860\n",
      "Iteration 13, loss = 0.02018463\n",
      "Iteration 14, loss = 0.01981184\n",
      "Iteration 15, loss = 0.01944033\n",
      "Iteration 16, loss = 0.01908954\n",
      "Iteration 17, loss = 0.01875558\n",
      "Iteration 18, loss = 0.01843328\n",
      "Iteration 19, loss = 0.01812482\n",
      "Iteration 20, loss = 0.01785263\n",
      "Iteration 21, loss = 0.01755326\n",
      "Iteration 22, loss = 0.01728673\n",
      "Iteration 23, loss = 0.01701976\n",
      "Iteration 24, loss = 0.01677228\n",
      "Iteration 25, loss = 0.01652953\n",
      "Iteration 26, loss = 0.01629694\n",
      "Iteration 27, loss = 0.01606158\n",
      "Iteration 28, loss = 0.01584874\n",
      "Iteration 29, loss = 0.01564844\n",
      "Iteration 30, loss = 0.01544557\n",
      "Iteration 31, loss = 0.01524080\n",
      "Iteration 32, loss = 0.01505336\n",
      "Iteration 33, loss = 0.01486983\n",
      "Iteration 34, loss = 0.01470704\n",
      "Iteration 35, loss = 0.01452099\n",
      "Iteration 36, loss = 0.01435560\n",
      "Iteration 37, loss = 0.01419691\n",
      "Iteration 38, loss = 0.01405538\n",
      "Iteration 39, loss = 0.01389926\n",
      "Iteration 40, loss = 0.01376785\n",
      "Iteration 41, loss = 0.01361913\n",
      "Iteration 42, loss = 0.01348955\n",
      "Iteration 43, loss = 0.01336625\n",
      "Iteration 44, loss = 0.01324314\n",
      "Iteration 45, loss = 0.01311772\n",
      "Iteration 46, loss = 0.01300185\n",
      "Iteration 47, loss = 0.01289282\n",
      "Iteration 48, loss = 0.01278139\n",
      "Iteration 49, loss = 0.01268948\n",
      "Iteration 50, loss = 0.01258297\n",
      "Iteration 51, loss = 0.01248625\n",
      "Iteration 52, loss = 0.01239096\n",
      "Iteration 53, loss = 0.01230196\n",
      "Iteration 54, loss = 0.01221950\n",
      "Iteration 55, loss = 0.01214403\n",
      "Iteration 56, loss = 0.01205000\n",
      "Iteration 57, loss = 0.01197788\n",
      "Iteration 58, loss = 0.01189725\n",
      "Iteration 59, loss = 0.01182847\n",
      "Iteration 60, loss = 0.01175478\n",
      "Iteration 61, loss = 0.01168288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16219907\n",
      "Iteration 2, loss = 0.02945505\n",
      "Iteration 3, loss = 0.02543203\n",
      "Iteration 4, loss = 0.02400096\n",
      "Iteration 5, loss = 0.02295424\n",
      "Iteration 6, loss = 0.02218536\n",
      "Iteration 7, loss = 0.02145134\n",
      "Iteration 8, loss = 0.02081136\n",
      "Iteration 9, loss = 0.02028922\n",
      "Iteration 10, loss = 0.01973925\n",
      "Iteration 11, loss = 0.01921987\n",
      "Iteration 12, loss = 0.01878481\n",
      "Iteration 13, loss = 0.01832705\n",
      "Iteration 14, loss = 0.01794125\n",
      "Iteration 15, loss = 0.01757810\n",
      "Iteration 16, loss = 0.01719805\n",
      "Iteration 17, loss = 0.01682425\n",
      "Iteration 18, loss = 0.01648868\n",
      "Iteration 19, loss = 0.01617198\n",
      "Iteration 20, loss = 0.01589079\n",
      "Iteration 21, loss = 0.01558465\n",
      "Iteration 22, loss = 0.01530636\n",
      "Iteration 23, loss = 0.01503324\n",
      "Iteration 24, loss = 0.01478071\n",
      "Iteration 25, loss = 0.01453214\n",
      "Iteration 26, loss = 0.01428733\n",
      "Iteration 27, loss = 0.01405791\n",
      "Iteration 28, loss = 0.01383738\n",
      "Iteration 29, loss = 0.01364168\n",
      "Iteration 30, loss = 0.01344899\n",
      "Iteration 31, loss = 0.01322825\n",
      "Iteration 32, loss = 0.01305073\n",
      "Iteration 33, loss = 0.01286691\n",
      "Iteration 34, loss = 0.01268049\n",
      "Iteration 35, loss = 0.01251218\n",
      "Iteration 36, loss = 0.01234391\n",
      "Iteration 37, loss = 0.01217878\n",
      "Iteration 38, loss = 0.01202404\n",
      "Iteration 39, loss = 0.01188523\n",
      "Iteration 40, loss = 0.01173247\n",
      "Iteration 41, loss = 0.01160841\n",
      "Iteration 42, loss = 0.01147466\n",
      "Iteration 43, loss = 0.01134666\n",
      "Iteration 44, loss = 0.01120449\n",
      "Iteration 45, loss = 0.01109827\n",
      "Iteration 46, loss = 0.01097470\n",
      "Iteration 47, loss = 0.01085795\n",
      "Iteration 48, loss = 0.01075792\n",
      "Iteration 49, loss = 0.01066513\n",
      "Iteration 50, loss = 0.01054888\n",
      "Iteration 51, loss = 0.01045212\n",
      "Iteration 52, loss = 0.01035643\n",
      "Iteration 53, loss = 0.01027285\n",
      "Iteration 54, loss = 0.01018052\n",
      "Iteration 55, loss = 0.01010022\n",
      "Iteration 56, loss = 0.01001198\n",
      "Iteration 57, loss = 0.00994468\n",
      "Iteration 58, loss = 0.00987412\n",
      "Iteration 59, loss = 0.00978999\n",
      "Iteration 60, loss = 0.00971451\n",
      "Iteration 61, loss = 0.00965270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12370261\n",
      "Iteration 2, loss = 0.02954055\n",
      "Iteration 3, loss = 0.02089739\n",
      "Iteration 4, loss = 0.01694863\n",
      "Iteration 5, loss = 0.01475522\n",
      "Iteration 6, loss = 0.01304725\n",
      "Iteration 7, loss = 0.01206471\n",
      "Iteration 8, loss = 0.01134664\n",
      "Iteration 9, loss = 0.01055924\n",
      "Iteration 10, loss = 0.01025774\n",
      "Iteration 11, loss = 0.01000808\n",
      "Iteration 12, loss = 0.00987752\n",
      "Iteration 13, loss = 0.00944953\n",
      "Iteration 14, loss = 0.00937324\n",
      "Iteration 15, loss = 0.00921321\n",
      "Iteration 16, loss = 0.00916760\n",
      "Iteration 17, loss = 0.00910510\n",
      "Iteration 18, loss = 0.00905568\n",
      "Iteration 19, loss = 0.00930638\n",
      "Iteration 20, loss = 0.00908256\n",
      "Iteration 21, loss = 0.00896626\n",
      "Iteration 22, loss = 0.00894756\n",
      "Iteration 23, loss = 0.00931863\n",
      "Iteration 24, loss = 0.01086065\n",
      "Iteration 25, loss = 0.01057660\n",
      "Iteration 26, loss = 0.00940724\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.14551826\n",
      "Iteration 2, loss = 0.02953510\n",
      "Iteration 3, loss = 0.02204197\n",
      "Iteration 4, loss = 0.01803392\n",
      "Iteration 5, loss = 0.01553798\n",
      "Iteration 6, loss = 0.01387217\n",
      "Iteration 7, loss = 0.01307579\n",
      "Iteration 8, loss = 0.01248416\n",
      "Iteration 9, loss = 0.01171360\n",
      "Iteration 10, loss = 0.01108932\n",
      "Iteration 11, loss = 0.01097138\n",
      "Iteration 12, loss = 0.01090516\n",
      "Iteration 13, loss = 0.01050955\n",
      "Iteration 14, loss = 0.01036750\n",
      "Iteration 15, loss = 0.01026760\n",
      "Iteration 16, loss = 0.01029371\n",
      "Iteration 17, loss = 0.01006846\n",
      "Iteration 18, loss = 0.01008960\n",
      "Iteration 19, loss = 0.01035398\n",
      "Iteration 20, loss = 0.01051939\n",
      "Iteration 21, loss = 0.01018503\n",
      "Iteration 22, loss = 0.00995964\n",
      "Iteration 23, loss = 0.01041513\n",
      "Iteration 24, loss = 0.01041402\n",
      "Iteration 25, loss = 0.00991712\n",
      "Iteration 26, loss = 0.01020885\n",
      "Iteration 27, loss = 0.01014401\n",
      "Iteration 28, loss = 0.01001656\n",
      "Iteration 29, loss = 0.00994427\n",
      "Iteration 30, loss = 0.01041322\n",
      "Iteration 31, loss = 0.01087077\n",
      "Iteration 32, loss = 0.01066161\n",
      "Iteration 33, loss = 0.01023450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13892665\n",
      "Iteration 2, loss = 0.02928918\n",
      "Iteration 3, loss = 0.02158183\n",
      "Iteration 4, loss = 0.01829137\n",
      "Iteration 5, loss = 0.01534948\n",
      "Iteration 6, loss = 0.01359211\n",
      "Iteration 7, loss = 0.01236129\n",
      "Iteration 8, loss = 0.01172359\n",
      "Iteration 9, loss = 0.01131180\n",
      "Iteration 10, loss = 0.01077954\n",
      "Iteration 11, loss = 0.01038743\n",
      "Iteration 12, loss = 0.01015242\n",
      "Iteration 13, loss = 0.01035942\n",
      "Iteration 14, loss = 0.01074391\n",
      "Iteration 15, loss = 0.00996749\n",
      "Iteration 16, loss = 0.00960252\n",
      "Iteration 17, loss = 0.01097320\n",
      "Iteration 18, loss = 0.01209766\n",
      "Iteration 19, loss = 0.00987934\n",
      "Iteration 20, loss = 0.00952730\n",
      "Iteration 21, loss = 0.01009038\n",
      "Iteration 22, loss = 0.00939087\n",
      "Iteration 23, loss = 0.00975783\n",
      "Iteration 24, loss = 0.00957474\n",
      "Iteration 25, loss = 0.00958763\n",
      "Iteration 26, loss = 0.00948344\n",
      "Iteration 27, loss = 0.00960120\n",
      "Iteration 28, loss = 0.01033587\n",
      "Iteration 29, loss = 0.01123290\n",
      "Iteration 30, loss = 0.01131145\n",
      "Iteration 31, loss = 0.01000880\n",
      "Iteration 32, loss = 0.00960287\n",
      "Iteration 33, loss = 0.01009808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.14874902\n",
      "Iteration 2, loss = 0.03067333\n",
      "Iteration 3, loss = 0.02233386\n",
      "Iteration 4, loss = 0.01824388\n",
      "Iteration 5, loss = 0.01587927\n",
      "Iteration 6, loss = 0.01412982\n",
      "Iteration 7, loss = 0.01302668\n",
      "Iteration 8, loss = 0.01225756\n",
      "Iteration 9, loss = 0.01164014\n",
      "Iteration 10, loss = 0.01130872\n",
      "Iteration 11, loss = 0.01083643\n",
      "Iteration 12, loss = 0.01078373\n",
      "Iteration 13, loss = 0.01062632\n",
      "Iteration 14, loss = 0.01042874\n",
      "Iteration 15, loss = 0.01034616\n",
      "Iteration 16, loss = 0.01053016\n",
      "Iteration 17, loss = 0.01020731\n",
      "Iteration 18, loss = 0.01016376\n",
      "Iteration 19, loss = 0.01009827\n",
      "Iteration 20, loss = 0.01049051\n",
      "Iteration 21, loss = 0.01010755\n",
      "Iteration 22, loss = 0.01038285\n",
      "Iteration 23, loss = 0.01026495\n",
      "Iteration 24, loss = 0.01012141\n",
      "Iteration 25, loss = 0.01002098\n",
      "Iteration 26, loss = 0.01023978\n",
      "Iteration 27, loss = 0.01087973\n",
      "Iteration 28, loss = 0.01078700\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11575593\n",
      "Iteration 2, loss = 0.02323895\n",
      "Iteration 3, loss = 0.01769519\n",
      "Iteration 4, loss = 0.01491508\n",
      "Iteration 5, loss = 0.01269412\n",
      "Iteration 6, loss = 0.01189061\n",
      "Iteration 7, loss = 0.01085583\n",
      "Iteration 8, loss = 0.01031048\n",
      "Iteration 9, loss = 0.00981261\n",
      "Iteration 10, loss = 0.00948573\n",
      "Iteration 11, loss = 0.00878385\n",
      "Iteration 12, loss = 0.00947189\n",
      "Iteration 13, loss = 0.00866423\n",
      "Iteration 14, loss = 0.00869888\n",
      "Iteration 15, loss = 0.00936283\n",
      "Iteration 16, loss = 0.00954209\n",
      "Iteration 17, loss = 0.00855004\n",
      "Iteration 18, loss = 0.00894280\n",
      "Iteration 19, loss = 0.00833943\n",
      "Iteration 20, loss = 0.00876223\n",
      "Iteration 21, loss = 0.00905709\n",
      "Iteration 22, loss = 0.00873509\n",
      "Iteration 23, loss = 0.00846903\n",
      "Iteration 24, loss = 0.00892657\n",
      "Iteration 25, loss = 0.00840520\n",
      "Iteration 26, loss = 0.00872818\n",
      "Iteration 27, loss = 0.00840823\n",
      "Iteration 28, loss = 0.00840520\n",
      "Iteration 29, loss = 0.00776601\n",
      "Iteration 30, loss = 0.00931633\n",
      "Iteration 31, loss = 0.00884314\n",
      "Iteration 32, loss = 0.00865141\n",
      "Iteration 33, loss = 0.00874045\n",
      "Iteration 34, loss = 0.00829876\n",
      "Iteration 35, loss = 0.00837948\n",
      "Iteration 36, loss = 0.00820795\n",
      "Iteration 37, loss = 0.00849777\n",
      "Iteration 38, loss = 0.00866222\n",
      "Iteration 39, loss = 0.00810694\n",
      "Iteration 40, loss = 0.00813492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44778715\n",
      "Iteration 2, loss = 0.16518095\n",
      "Iteration 3, loss = 0.10798959\n",
      "Iteration 4, loss = 0.08613093\n",
      "Iteration 5, loss = 0.07333931\n",
      "Iteration 6, loss = 0.06454076\n",
      "Iteration 7, loss = 0.05811064\n",
      "Iteration 8, loss = 0.05340683\n",
      "Iteration 9, loss = 0.04973479\n",
      "Iteration 10, loss = 0.04670664\n",
      "Iteration 11, loss = 0.04438562\n",
      "Iteration 12, loss = 0.04231636\n",
      "Iteration 13, loss = 0.04061412\n",
      "Iteration 14, loss = 0.03915538\n",
      "Iteration 15, loss = 0.03788802\n",
      "Iteration 16, loss = 0.03675604\n",
      "Iteration 17, loss = 0.03576345\n",
      "Iteration 18, loss = 0.03481857\n",
      "Iteration 19, loss = 0.03398822\n",
      "Iteration 20, loss = 0.03323561\n",
      "Iteration 21, loss = 0.03255333\n",
      "Iteration 22, loss = 0.03191662\n",
      "Iteration 23, loss = 0.03133766\n",
      "Iteration 24, loss = 0.03083188\n",
      "Iteration 25, loss = 0.03031870\n",
      "Iteration 26, loss = 0.02983772\n",
      "Iteration 27, loss = 0.02938999\n",
      "Iteration 28, loss = 0.02900052\n",
      "Iteration 29, loss = 0.02860426\n",
      "Iteration 30, loss = 0.02825536\n",
      "Iteration 31, loss = 0.02789121\n",
      "Iteration 32, loss = 0.02760336\n",
      "Iteration 33, loss = 0.02725159\n",
      "Iteration 34, loss = 0.02697105\n",
      "Iteration 35, loss = 0.02670368\n",
      "Iteration 36, loss = 0.02647313\n",
      "Iteration 37, loss = 0.02616556\n",
      "Iteration 38, loss = 0.02591933\n",
      "Iteration 39, loss = 0.02569402\n",
      "Iteration 40, loss = 0.02548678\n",
      "Iteration 41, loss = 0.02527485\n",
      "Iteration 42, loss = 0.02507960\n",
      "Iteration 43, loss = 0.02488348\n",
      "Iteration 44, loss = 0.02470100\n",
      "Iteration 45, loss = 0.02452378\n",
      "Iteration 46, loss = 0.02436159\n",
      "Iteration 47, loss = 0.02419844\n",
      "Iteration 48, loss = 0.02403466\n",
      "Iteration 49, loss = 0.02392131\n",
      "Iteration 50, loss = 0.02374723\n",
      "Iteration 51, loss = 0.02360780\n",
      "Iteration 52, loss = 0.02346576\n",
      "Iteration 53, loss = 0.02333871\n",
      "Iteration 54, loss = 0.02321480\n",
      "Iteration 55, loss = 0.02308811\n",
      "Iteration 56, loss = 0.02297402\n",
      "Iteration 57, loss = 0.02285988\n",
      "Iteration 58, loss = 0.02274232\n",
      "Iteration 59, loss = 0.02264062\n",
      "Iteration 60, loss = 0.02253370\n",
      "Iteration 61, loss = 0.02243291\n",
      "Iteration 62, loss = 0.02233250\n",
      "Iteration 63, loss = 0.02224260\n",
      "Iteration 64, loss = 0.02214074\n",
      "Iteration 65, loss = 0.02205317\n",
      "Iteration 66, loss = 0.02195964\n",
      "Iteration 67, loss = 0.02187885\n",
      "Iteration 68, loss = 0.02179199\n",
      "Iteration 69, loss = 0.02170538\n",
      "Iteration 70, loss = 0.02162482\n",
      "Iteration 71, loss = 0.02154729\n",
      "Iteration 72, loss = 0.02145765\n",
      "Iteration 73, loss = 0.02138694\n",
      "Iteration 74, loss = 0.02131612\n",
      "Iteration 75, loss = 0.02123739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48490588\n",
      "Iteration 2, loss = 0.21921955\n",
      "Iteration 3, loss = 0.14750598\n",
      "Iteration 4, loss = 0.11668803\n",
      "Iteration 5, loss = 0.09770089\n",
      "Iteration 6, loss = 0.08413109\n",
      "Iteration 7, loss = 0.07386066\n",
      "Iteration 8, loss = 0.06605645\n",
      "Iteration 9, loss = 0.05976491\n",
      "Iteration 10, loss = 0.05490950\n",
      "Iteration 11, loss = 0.05103672\n",
      "Iteration 12, loss = 0.04785695\n",
      "Iteration 13, loss = 0.04520773\n",
      "Iteration 14, loss = 0.04302280\n",
      "Iteration 15, loss = 0.04111744\n",
      "Iteration 16, loss = 0.03955000\n",
      "Iteration 17, loss = 0.03818347\n",
      "Iteration 18, loss = 0.03684990\n",
      "Iteration 19, loss = 0.03573721\n",
      "Iteration 20, loss = 0.03475100\n",
      "Iteration 21, loss = 0.03389683\n",
      "Iteration 22, loss = 0.03309420\n",
      "Iteration 23, loss = 0.03237498\n",
      "Iteration 24, loss = 0.03171447\n",
      "Iteration 25, loss = 0.03109403\n",
      "Iteration 26, loss = 0.03055341\n",
      "Iteration 27, loss = 0.03004517\n",
      "Iteration 28, loss = 0.02958057\n",
      "Iteration 29, loss = 0.02915004\n",
      "Iteration 30, loss = 0.02876917\n",
      "Iteration 31, loss = 0.02837819\n",
      "Iteration 32, loss = 0.02803221\n",
      "Iteration 33, loss = 0.02769565\n",
      "Iteration 34, loss = 0.02738330\n",
      "Iteration 35, loss = 0.02710422\n",
      "Iteration 36, loss = 0.02684627\n",
      "Iteration 37, loss = 0.02658494\n",
      "Iteration 38, loss = 0.02634729\n",
      "Iteration 39, loss = 0.02611785\n",
      "Iteration 40, loss = 0.02591173\n",
      "Iteration 41, loss = 0.02571101\n",
      "Iteration 42, loss = 0.02552190\n",
      "Iteration 43, loss = 0.02533802\n",
      "Iteration 44, loss = 0.02518084\n",
      "Iteration 45, loss = 0.02499893\n",
      "Iteration 46, loss = 0.02484773\n",
      "Iteration 47, loss = 0.02469078\n",
      "Iteration 48, loss = 0.02454655\n",
      "Iteration 49, loss = 0.02441245\n",
      "Iteration 50, loss = 0.02427420\n",
      "Iteration 51, loss = 0.02416022\n",
      "Iteration 52, loss = 0.02402295\n",
      "Iteration 53, loss = 0.02393445\n",
      "Iteration 54, loss = 0.02381056\n",
      "Iteration 55, loss = 0.02368933\n",
      "Iteration 56, loss = 0.02357991\n",
      "Iteration 57, loss = 0.02348495\n",
      "Iteration 58, loss = 0.02337702\n",
      "Iteration 59, loss = 0.02328494\n",
      "Iteration 60, loss = 0.02318849\n",
      "Iteration 61, loss = 0.02310669\n",
      "Iteration 62, loss = 0.02300587\n",
      "Iteration 63, loss = 0.02292501\n",
      "Iteration 64, loss = 0.02284158\n",
      "Iteration 65, loss = 0.02276128\n",
      "Iteration 66, loss = 0.02267965\n",
      "Iteration 67, loss = 0.02260665\n",
      "Iteration 68, loss = 0.02253099\n",
      "Iteration 69, loss = 0.02245528\n",
      "Iteration 70, loss = 0.02238675\n",
      "Iteration 71, loss = 0.02230942\n",
      "Iteration 72, loss = 0.02224722\n",
      "Iteration 73, loss = 0.02217480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47334967\n",
      "Iteration 2, loss = 0.18162190\n",
      "Iteration 3, loss = 0.11464795\n",
      "Iteration 4, loss = 0.08752924\n",
      "Iteration 5, loss = 0.07228296\n",
      "Iteration 6, loss = 0.06287301\n",
      "Iteration 7, loss = 0.05647184\n",
      "Iteration 8, loss = 0.05195871\n",
      "Iteration 9, loss = 0.04857806\n",
      "Iteration 10, loss = 0.04600809\n",
      "Iteration 11, loss = 0.04379953\n",
      "Iteration 12, loss = 0.04207312\n",
      "Iteration 13, loss = 0.04059962\n",
      "Iteration 14, loss = 0.03941880\n",
      "Iteration 15, loss = 0.03822642\n",
      "Iteration 16, loss = 0.03726918\n",
      "Iteration 17, loss = 0.03640893\n",
      "Iteration 18, loss = 0.03564879\n",
      "Iteration 19, loss = 0.03486602\n",
      "Iteration 20, loss = 0.03420367\n",
      "Iteration 21, loss = 0.03362026\n",
      "Iteration 22, loss = 0.03308147\n",
      "Iteration 23, loss = 0.03251519\n",
      "Iteration 24, loss = 0.03200247\n",
      "Iteration 25, loss = 0.03155361\n",
      "Iteration 26, loss = 0.03108692\n",
      "Iteration 27, loss = 0.03067019\n",
      "Iteration 28, loss = 0.03030833\n",
      "Iteration 29, loss = 0.02986367\n",
      "Iteration 30, loss = 0.02953223\n",
      "Iteration 31, loss = 0.02916212\n",
      "Iteration 32, loss = 0.02882867\n",
      "Iteration 33, loss = 0.02852784\n",
      "Iteration 34, loss = 0.02821934\n",
      "Iteration 35, loss = 0.02791659\n",
      "Iteration 36, loss = 0.02761442\n",
      "Iteration 37, loss = 0.02733175\n",
      "Iteration 38, loss = 0.02707305\n",
      "Iteration 39, loss = 0.02686251\n",
      "Iteration 40, loss = 0.02660774\n",
      "Iteration 41, loss = 0.02634889\n",
      "Iteration 42, loss = 0.02613314\n",
      "Iteration 43, loss = 0.02590770\n",
      "Iteration 44, loss = 0.02571497\n",
      "Iteration 45, loss = 0.02551678\n",
      "Iteration 46, loss = 0.02531134\n",
      "Iteration 47, loss = 0.02517863\n",
      "Iteration 48, loss = 0.02499605\n",
      "Iteration 49, loss = 0.02480968\n",
      "Iteration 50, loss = 0.02464028\n",
      "Iteration 51, loss = 0.02448698\n",
      "Iteration 52, loss = 0.02434785\n",
      "Iteration 53, loss = 0.02418732\n",
      "Iteration 54, loss = 0.02404063\n",
      "Iteration 55, loss = 0.02392142\n",
      "Iteration 56, loss = 0.02378647\n",
      "Iteration 57, loss = 0.02366231\n",
      "Iteration 58, loss = 0.02353127\n",
      "Iteration 59, loss = 0.02341350\n",
      "Iteration 60, loss = 0.02330853\n",
      "Iteration 61, loss = 0.02319495\n",
      "Iteration 62, loss = 0.02309505\n",
      "Iteration 63, loss = 0.02298524\n",
      "Iteration 64, loss = 0.02288036\n",
      "Iteration 65, loss = 0.02278712\n",
      "Iteration 66, loss = 0.02268809\n",
      "Iteration 67, loss = 0.02259991\n",
      "Iteration 68, loss = 0.02250270\n",
      "Iteration 69, loss = 0.02240859\n",
      "Iteration 70, loss = 0.02233349\n",
      "Iteration 71, loss = 0.02223933\n",
      "Iteration 72, loss = 0.02216044\n",
      "Iteration 73, loss = 0.02207640\n",
      "Iteration 74, loss = 0.02199599\n",
      "Iteration 75, loss = 0.02191725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49917894\n",
      "Iteration 2, loss = 0.22508781\n",
      "Iteration 3, loss = 0.14811014\n",
      "Iteration 4, loss = 0.11459755\n",
      "Iteration 5, loss = 0.09515493\n",
      "Iteration 6, loss = 0.08175738\n",
      "Iteration 7, loss = 0.07192150\n",
      "Iteration 8, loss = 0.06447637\n",
      "Iteration 9, loss = 0.05871185\n",
      "Iteration 10, loss = 0.05416961\n",
      "Iteration 11, loss = 0.05042204\n",
      "Iteration 12, loss = 0.04748273\n",
      "Iteration 13, loss = 0.04490116\n",
      "Iteration 14, loss = 0.04280401\n",
      "Iteration 15, loss = 0.04093211\n",
      "Iteration 16, loss = 0.03932236\n",
      "Iteration 17, loss = 0.03790881\n",
      "Iteration 18, loss = 0.03668728\n",
      "Iteration 19, loss = 0.03561232\n",
      "Iteration 20, loss = 0.03464387\n",
      "Iteration 21, loss = 0.03373812\n",
      "Iteration 22, loss = 0.03297928\n",
      "Iteration 23, loss = 0.03225390\n",
      "Iteration 24, loss = 0.03159679\n",
      "Iteration 25, loss = 0.03102799\n",
      "Iteration 26, loss = 0.03045751\n",
      "Iteration 27, loss = 0.02995332\n",
      "Iteration 28, loss = 0.02948253\n",
      "Iteration 29, loss = 0.02909383\n",
      "Iteration 30, loss = 0.02866565\n",
      "Iteration 31, loss = 0.02829062\n",
      "Iteration 32, loss = 0.02794586\n",
      "Iteration 33, loss = 0.02763611\n",
      "Iteration 34, loss = 0.02734738\n",
      "Iteration 35, loss = 0.02705791\n",
      "Iteration 36, loss = 0.02679329\n",
      "Iteration 37, loss = 0.02654592\n",
      "Iteration 38, loss = 0.02632251\n",
      "Iteration 39, loss = 0.02609499\n",
      "Iteration 40, loss = 0.02589337\n",
      "Iteration 41, loss = 0.02569077\n",
      "Iteration 42, loss = 0.02549791\n",
      "Iteration 43, loss = 0.02534202\n",
      "Iteration 44, loss = 0.02515286\n",
      "Iteration 45, loss = 0.02500399\n",
      "Iteration 46, loss = 0.02483877\n",
      "Iteration 47, loss = 0.02469434\n",
      "Iteration 48, loss = 0.02455400\n",
      "Iteration 49, loss = 0.02442650\n",
      "Iteration 50, loss = 0.02429367\n",
      "Iteration 51, loss = 0.02416745\n",
      "Iteration 52, loss = 0.02405103\n",
      "Iteration 53, loss = 0.02393229\n",
      "Iteration 54, loss = 0.02383117\n",
      "Iteration 55, loss = 0.02372114\n",
      "Iteration 56, loss = 0.02361833\n",
      "Iteration 57, loss = 0.02351249\n",
      "Iteration 58, loss = 0.02341849\n",
      "Iteration 59, loss = 0.02332323\n",
      "Iteration 60, loss = 0.02323303\n",
      "Iteration 61, loss = 0.02314263\n",
      "Iteration 62, loss = 0.02305984\n",
      "Iteration 63, loss = 0.02297600\n",
      "Iteration 64, loss = 0.02289216\n",
      "Iteration 65, loss = 0.02281932\n",
      "Iteration 66, loss = 0.02273766\n",
      "Iteration 67, loss = 0.02266425\n",
      "Iteration 68, loss = 0.02259517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44615383\n",
      "Iteration 2, loss = 0.16924803\n",
      "Iteration 3, loss = 0.10282737\n",
      "Iteration 4, loss = 0.07605924\n",
      "Iteration 5, loss = 0.06201951\n",
      "Iteration 6, loss = 0.05292173\n",
      "Iteration 7, loss = 0.04686959\n",
      "Iteration 8, loss = 0.04251394\n",
      "Iteration 9, loss = 0.03936957\n",
      "Iteration 10, loss = 0.03684614\n",
      "Iteration 11, loss = 0.03496162\n",
      "Iteration 12, loss = 0.03336046\n",
      "Iteration 13, loss = 0.03203780\n",
      "Iteration 14, loss = 0.03097095\n",
      "Iteration 15, loss = 0.03008009\n",
      "Iteration 16, loss = 0.02929896\n",
      "Iteration 17, loss = 0.02860967\n",
      "Iteration 18, loss = 0.02802750\n",
      "Iteration 19, loss = 0.02752126\n",
      "Iteration 20, loss = 0.02706431\n",
      "Iteration 21, loss = 0.02665755\n",
      "Iteration 22, loss = 0.02629676\n",
      "Iteration 23, loss = 0.02595823\n",
      "Iteration 24, loss = 0.02566333\n",
      "Iteration 25, loss = 0.02539135\n",
      "Iteration 26, loss = 0.02513083\n",
      "Iteration 27, loss = 0.02489078\n",
      "Iteration 28, loss = 0.02467261\n",
      "Iteration 29, loss = 0.02448659\n",
      "Iteration 30, loss = 0.02428672\n",
      "Iteration 31, loss = 0.02411100\n",
      "Iteration 32, loss = 0.02394621\n",
      "Iteration 33, loss = 0.02378369\n",
      "Iteration 34, loss = 0.02364229\n",
      "Iteration 35, loss = 0.02348438\n",
      "Iteration 36, loss = 0.02335668\n",
      "Iteration 37, loss = 0.02321807\n",
      "Iteration 38, loss = 0.02309634\n",
      "Iteration 39, loss = 0.02298012\n",
      "Iteration 40, loss = 0.02285949\n",
      "Iteration 41, loss = 0.02275391\n",
      "Iteration 42, loss = 0.02265372\n",
      "Iteration 43, loss = 0.02254124\n",
      "Iteration 44, loss = 0.02243256\n",
      "Iteration 45, loss = 0.02234142\n",
      "Iteration 46, loss = 0.02223976\n",
      "Iteration 47, loss = 0.02214490\n",
      "Iteration 48, loss = 0.02205702\n",
      "Iteration 49, loss = 0.02197775\n",
      "Iteration 50, loss = 0.02188254\n",
      "Iteration 51, loss = 0.02180065\n",
      "Iteration 52, loss = 0.02172100\n",
      "Iteration 53, loss = 0.02164023\n",
      "Iteration 54, loss = 0.02156045\n",
      "Iteration 55, loss = 0.02149060\n",
      "Iteration 56, loss = 0.02141634\n",
      "Iteration 57, loss = 0.02133793\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46847266\n",
      "Iteration 2, loss = 0.19674511\n",
      "Iteration 3, loss = 0.10761184\n",
      "Iteration 4, loss = 0.07274186\n",
      "Iteration 5, loss = 0.05504872\n",
      "Iteration 6, loss = 0.04519160\n",
      "Iteration 7, loss = 0.03900407\n",
      "Iteration 8, loss = 0.03483374\n",
      "Iteration 9, loss = 0.03167243\n",
      "Iteration 10, loss = 0.02912754\n",
      "Iteration 11, loss = 0.02724574\n",
      "Iteration 12, loss = 0.02557088\n",
      "Iteration 13, loss = 0.02421080\n",
      "Iteration 14, loss = 0.02306862\n",
      "Iteration 15, loss = 0.02209476\n",
      "Iteration 16, loss = 0.02115406\n",
      "Iteration 17, loss = 0.02037595\n",
      "Iteration 18, loss = 0.01970390\n",
      "Iteration 19, loss = 0.01902398\n",
      "Iteration 20, loss = 0.01844943\n",
      "Iteration 21, loss = 0.01793060\n",
      "Iteration 22, loss = 0.01743686\n",
      "Iteration 23, loss = 0.01699136\n",
      "Iteration 24, loss = 0.01658381\n",
      "Iteration 25, loss = 0.01620334\n",
      "Iteration 26, loss = 0.01582812\n",
      "Iteration 27, loss = 0.01546528\n",
      "Iteration 28, loss = 0.01516058\n",
      "Iteration 29, loss = 0.01485652\n",
      "Iteration 30, loss = 0.01457556\n",
      "Iteration 31, loss = 0.01429709\n",
      "Iteration 32, loss = 0.01403339\n",
      "Iteration 33, loss = 0.01378905\n",
      "Iteration 34, loss = 0.01355718\n",
      "Iteration 35, loss = 0.01333459\n",
      "Iteration 36, loss = 0.01318733\n",
      "Iteration 37, loss = 0.01295651\n",
      "Iteration 38, loss = 0.01272712\n",
      "Iteration 39, loss = 0.01254272\n",
      "Iteration 40, loss = 0.01240562\n",
      "Iteration 41, loss = 0.01222040\n",
      "Iteration 42, loss = 0.01207391\n",
      "Iteration 43, loss = 0.01189715\n",
      "Iteration 44, loss = 0.01176131\n",
      "Iteration 45, loss = 0.01160569\n",
      "Iteration 46, loss = 0.01148570\n",
      "Iteration 47, loss = 0.01136036\n",
      "Iteration 48, loss = 0.01122736\n",
      "Iteration 49, loss = 0.01114613\n",
      "Iteration 50, loss = 0.01103633\n",
      "Iteration 51, loss = 0.01092040\n",
      "Iteration 52, loss = 0.01079345\n",
      "Iteration 53, loss = 0.01071049\n",
      "Iteration 54, loss = 0.01063880\n",
      "Iteration 55, loss = 0.01051174\n",
      "Iteration 56, loss = 0.01044165\n",
      "Iteration 57, loss = 0.01037307\n",
      "Iteration 58, loss = 0.01026208\n",
      "Iteration 59, loss = 0.01019022\n",
      "Iteration 60, loss = 0.01011288\n",
      "Iteration 61, loss = 0.01006870\n",
      "Iteration 62, loss = 0.01000278\n",
      "Iteration 63, loss = 0.00995355\n",
      "Iteration 64, loss = 0.00987093\n",
      "Iteration 65, loss = 0.00982328\n",
      "Iteration 66, loss = 0.00977163\n",
      "Iteration 67, loss = 0.00968652\n",
      "Iteration 68, loss = 0.00966626\n",
      "Iteration 69, loss = 0.00961208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49459837\n",
      "Iteration 2, loss = 0.24185197\n",
      "Iteration 3, loss = 0.14251588\n",
      "Iteration 4, loss = 0.09699221\n",
      "Iteration 5, loss = 0.07172258\n",
      "Iteration 6, loss = 0.05666407\n",
      "Iteration 7, loss = 0.04726204\n",
      "Iteration 8, loss = 0.04109256\n",
      "Iteration 9, loss = 0.03657375\n",
      "Iteration 10, loss = 0.03327495\n",
      "Iteration 11, loss = 0.03073138\n",
      "Iteration 12, loss = 0.02866595\n",
      "Iteration 13, loss = 0.02696329\n",
      "Iteration 14, loss = 0.02559554\n",
      "Iteration 15, loss = 0.02438738\n",
      "Iteration 16, loss = 0.02343006\n",
      "Iteration 17, loss = 0.02257714\n",
      "Iteration 18, loss = 0.02172199\n",
      "Iteration 19, loss = 0.02104236\n",
      "Iteration 20, loss = 0.02040465\n",
      "Iteration 21, loss = 0.01983018\n",
      "Iteration 22, loss = 0.01928548\n",
      "Iteration 23, loss = 0.01881080\n",
      "Iteration 24, loss = 0.01833727\n",
      "Iteration 25, loss = 0.01790380\n",
      "Iteration 26, loss = 0.01750165\n",
      "Iteration 27, loss = 0.01711971\n",
      "Iteration 28, loss = 0.01677482\n",
      "Iteration 29, loss = 0.01646133\n",
      "Iteration 30, loss = 0.01614452\n",
      "Iteration 31, loss = 0.01583572\n",
      "Iteration 32, loss = 0.01553896\n",
      "Iteration 33, loss = 0.01526279\n",
      "Iteration 34, loss = 0.01501156\n",
      "Iteration 35, loss = 0.01476359\n",
      "Iteration 36, loss = 0.01454004\n",
      "Iteration 37, loss = 0.01432212\n",
      "Iteration 38, loss = 0.01410674\n",
      "Iteration 39, loss = 0.01389999\n",
      "Iteration 40, loss = 0.01375414\n",
      "Iteration 41, loss = 0.01354180\n",
      "Iteration 42, loss = 0.01338308\n",
      "Iteration 43, loss = 0.01320737\n",
      "Iteration 44, loss = 0.01306864\n",
      "Iteration 45, loss = 0.01291180\n",
      "Iteration 46, loss = 0.01275969\n",
      "Iteration 47, loss = 0.01262489\n",
      "Iteration 48, loss = 0.01249026\n",
      "Iteration 49, loss = 0.01238221\n",
      "Iteration 50, loss = 0.01226428\n",
      "Iteration 51, loss = 0.01217061\n",
      "Iteration 52, loss = 0.01204493\n",
      "Iteration 53, loss = 0.01196892\n",
      "Iteration 54, loss = 0.01187179\n",
      "Iteration 55, loss = 0.01174230\n",
      "Iteration 56, loss = 0.01163820\n",
      "Iteration 57, loss = 0.01157530\n",
      "Iteration 58, loss = 0.01147345\n",
      "Iteration 59, loss = 0.01138707\n",
      "Iteration 60, loss = 0.01130833\n",
      "Iteration 61, loss = 0.01125293\n",
      "Iteration 62, loss = 0.01119474\n",
      "Iteration 63, loss = 0.01110602\n",
      "Iteration 64, loss = 0.01105707\n",
      "Iteration 65, loss = 0.01098327\n",
      "Iteration 66, loss = 0.01093229\n",
      "Iteration 67, loss = 0.01086728\n",
      "Iteration 68, loss = 0.01083117\n",
      "Iteration 69, loss = 0.01076646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48904707\n",
      "Iteration 2, loss = 0.22285564\n",
      "Iteration 3, loss = 0.12430652\n",
      "Iteration 4, loss = 0.08238374\n",
      "Iteration 5, loss = 0.06144473\n",
      "Iteration 6, loss = 0.04988907\n",
      "Iteration 7, loss = 0.04250195\n",
      "Iteration 8, loss = 0.03746004\n",
      "Iteration 9, loss = 0.03381290\n",
      "Iteration 10, loss = 0.03104959\n",
      "Iteration 11, loss = 0.02876089\n",
      "Iteration 12, loss = 0.02687426\n",
      "Iteration 13, loss = 0.02534378\n",
      "Iteration 14, loss = 0.02404724\n",
      "Iteration 15, loss = 0.02286946\n",
      "Iteration 16, loss = 0.02199458\n",
      "Iteration 17, loss = 0.02111897\n",
      "Iteration 18, loss = 0.02041794\n",
      "Iteration 19, loss = 0.01971036\n",
      "Iteration 20, loss = 0.01909645\n",
      "Iteration 21, loss = 0.01856482\n",
      "Iteration 22, loss = 0.01809981\n",
      "Iteration 23, loss = 0.01759176\n",
      "Iteration 24, loss = 0.01713818\n",
      "Iteration 25, loss = 0.01674319\n",
      "Iteration 26, loss = 0.01634163\n",
      "Iteration 27, loss = 0.01597266\n",
      "Iteration 28, loss = 0.01567729\n",
      "Iteration 29, loss = 0.01531757\n",
      "Iteration 30, loss = 0.01506261\n",
      "Iteration 31, loss = 0.01474263\n",
      "Iteration 32, loss = 0.01450293\n",
      "Iteration 33, loss = 0.01427482\n",
      "Iteration 34, loss = 0.01400856\n",
      "Iteration 35, loss = 0.01377809\n",
      "Iteration 36, loss = 0.01354995\n",
      "Iteration 37, loss = 0.01335106\n",
      "Iteration 38, loss = 0.01315031\n",
      "Iteration 39, loss = 0.01297208\n",
      "Iteration 40, loss = 0.01283549\n",
      "Iteration 41, loss = 0.01261891\n",
      "Iteration 42, loss = 0.01246823\n",
      "Iteration 43, loss = 0.01228139\n",
      "Iteration 44, loss = 0.01213951\n",
      "Iteration 45, loss = 0.01202070\n",
      "Iteration 46, loss = 0.01184501\n",
      "Iteration 47, loss = 0.01172817\n",
      "Iteration 48, loss = 0.01164482\n",
      "Iteration 49, loss = 0.01150237\n",
      "Iteration 50, loss = 0.01136960\n",
      "Iteration 51, loss = 0.01126404\n",
      "Iteration 52, loss = 0.01116967\n",
      "Iteration 53, loss = 0.01107693\n",
      "Iteration 54, loss = 0.01096177\n",
      "Iteration 55, loss = 0.01088239\n",
      "Iteration 56, loss = 0.01077734\n",
      "Iteration 57, loss = 0.01070368\n",
      "Iteration 58, loss = 0.01065317\n",
      "Iteration 59, loss = 0.01054126\n",
      "Iteration 60, loss = 0.01047361\n",
      "Iteration 61, loss = 0.01042776\n",
      "Iteration 62, loss = 0.01040232\n",
      "Iteration 63, loss = 0.01027890\n",
      "Iteration 64, loss = 0.01021714\n",
      "Iteration 65, loss = 0.01016583\n",
      "Iteration 66, loss = 0.01013144\n",
      "Iteration 67, loss = 0.01005610\n",
      "Iteration 68, loss = 0.01001645\n",
      "Iteration 69, loss = 0.00993928\n",
      "Iteration 70, loss = 0.00995195\n",
      "Iteration 71, loss = 0.00988396\n",
      "Iteration 72, loss = 0.00982480\n",
      "Iteration 73, loss = 0.00979899\n",
      "Iteration 74, loss = 0.00975601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50312093\n",
      "Iteration 2, loss = 0.24981741\n",
      "Iteration 3, loss = 0.14535541\n",
      "Iteration 4, loss = 0.09685735\n",
      "Iteration 5, loss = 0.07190033\n",
      "Iteration 6, loss = 0.05737079\n",
      "Iteration 7, loss = 0.04830439\n",
      "Iteration 8, loss = 0.04210306\n",
      "Iteration 9, loss = 0.03772877\n",
      "Iteration 10, loss = 0.03447335\n",
      "Iteration 11, loss = 0.03184048\n",
      "Iteration 12, loss = 0.02989169\n",
      "Iteration 13, loss = 0.02814739\n",
      "Iteration 14, loss = 0.02680051\n",
      "Iteration 15, loss = 0.02556482\n",
      "Iteration 16, loss = 0.02450029\n",
      "Iteration 17, loss = 0.02356256\n",
      "Iteration 18, loss = 0.02273609\n",
      "Iteration 19, loss = 0.02200031\n",
      "Iteration 20, loss = 0.02136624\n",
      "Iteration 21, loss = 0.02069753\n",
      "Iteration 22, loss = 0.02013960\n",
      "Iteration 23, loss = 0.01962153\n",
      "Iteration 24, loss = 0.01913561\n",
      "Iteration 25, loss = 0.01869611\n",
      "Iteration 26, loss = 0.01826117\n",
      "Iteration 27, loss = 0.01785916\n",
      "Iteration 28, loss = 0.01748453\n",
      "Iteration 29, loss = 0.01716332\n",
      "Iteration 30, loss = 0.01682536\n",
      "Iteration 31, loss = 0.01649226\n",
      "Iteration 32, loss = 0.01619743\n",
      "Iteration 33, loss = 0.01591463\n",
      "Iteration 34, loss = 0.01565410\n",
      "Iteration 35, loss = 0.01538112\n",
      "Iteration 36, loss = 0.01513570\n",
      "Iteration 37, loss = 0.01490680\n",
      "Iteration 38, loss = 0.01469778\n",
      "Iteration 39, loss = 0.01446187\n",
      "Iteration 40, loss = 0.01427277\n",
      "Iteration 41, loss = 0.01405140\n",
      "Iteration 42, loss = 0.01386732\n",
      "Iteration 43, loss = 0.01370363\n",
      "Iteration 44, loss = 0.01351713\n",
      "Iteration 45, loss = 0.01334727\n",
      "Iteration 46, loss = 0.01318450\n",
      "Iteration 47, loss = 0.01303562\n",
      "Iteration 48, loss = 0.01288847\n",
      "Iteration 49, loss = 0.01277439\n",
      "Iteration 50, loss = 0.01263060\n",
      "Iteration 51, loss = 0.01250440\n",
      "Iteration 52, loss = 0.01238373\n",
      "Iteration 53, loss = 0.01226461\n",
      "Iteration 54, loss = 0.01216918\n",
      "Iteration 55, loss = 0.01208167\n",
      "Iteration 56, loss = 0.01195208\n",
      "Iteration 57, loss = 0.01187095\n",
      "Iteration 58, loss = 0.01176557\n",
      "Iteration 59, loss = 0.01168455\n",
      "Iteration 60, loss = 0.01159560\n",
      "Iteration 61, loss = 0.01150863\n",
      "Iteration 62, loss = 0.01144180\n",
      "Iteration 63, loss = 0.01136452\n",
      "Iteration 64, loss = 0.01129992\n",
      "Iteration 65, loss = 0.01122269\n",
      "Iteration 66, loss = 0.01117863\n",
      "Iteration 67, loss = 0.01110967\n",
      "Iteration 68, loss = 0.01103824\n",
      "Iteration 69, loss = 0.01098804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46365751\n",
      "Iteration 2, loss = 0.19761464\n",
      "Iteration 3, loss = 0.10034323\n",
      "Iteration 4, loss = 0.06304888\n",
      "Iteration 5, loss = 0.04636564\n",
      "Iteration 6, loss = 0.03726816\n",
      "Iteration 7, loss = 0.03182442\n",
      "Iteration 8, loss = 0.02827607\n",
      "Iteration 9, loss = 0.02587682\n",
      "Iteration 10, loss = 0.02404137\n",
      "Iteration 11, loss = 0.02267811\n",
      "Iteration 12, loss = 0.02151540\n",
      "Iteration 13, loss = 0.02053139\n",
      "Iteration 14, loss = 0.01972959\n",
      "Iteration 15, loss = 0.01906783\n",
      "Iteration 16, loss = 0.01839157\n",
      "Iteration 17, loss = 0.01780127\n",
      "Iteration 18, loss = 0.01729060\n",
      "Iteration 19, loss = 0.01680748\n",
      "Iteration 20, loss = 0.01637727\n",
      "Iteration 21, loss = 0.01595082\n",
      "Iteration 22, loss = 0.01556818\n",
      "Iteration 23, loss = 0.01519807\n",
      "Iteration 24, loss = 0.01485980\n",
      "Iteration 25, loss = 0.01454556\n",
      "Iteration 26, loss = 0.01422399\n",
      "Iteration 27, loss = 0.01393081\n",
      "Iteration 28, loss = 0.01365692\n",
      "Iteration 29, loss = 0.01341699\n",
      "Iteration 30, loss = 0.01317741\n",
      "Iteration 31, loss = 0.01292537\n",
      "Iteration 32, loss = 0.01270849\n",
      "Iteration 33, loss = 0.01250026\n",
      "Iteration 34, loss = 0.01228343\n",
      "Iteration 35, loss = 0.01207468\n",
      "Iteration 36, loss = 0.01189671\n",
      "Iteration 37, loss = 0.01170321\n",
      "Iteration 38, loss = 0.01152841\n",
      "Iteration 39, loss = 0.01137990\n",
      "Iteration 40, loss = 0.01121219\n",
      "Iteration 41, loss = 0.01108732\n",
      "Iteration 42, loss = 0.01093742\n",
      "Iteration 43, loss = 0.01079323\n",
      "Iteration 44, loss = 0.01063296\n",
      "Iteration 45, loss = 0.01053110\n",
      "Iteration 46, loss = 0.01039992\n",
      "Iteration 47, loss = 0.01027335\n",
      "Iteration 48, loss = 0.01017709\n",
      "Iteration 49, loss = 0.01008857\n",
      "Iteration 50, loss = 0.00995716\n",
      "Iteration 51, loss = 0.00986468\n",
      "Iteration 52, loss = 0.00976378\n",
      "Iteration 53, loss = 0.00969510\n",
      "Iteration 54, loss = 0.00959061\n",
      "Iteration 55, loss = 0.00951594\n",
      "Iteration 56, loss = 0.00943000\n",
      "Iteration 57, loss = 0.00937295\n",
      "Iteration 58, loss = 0.00931758\n",
      "Iteration 59, loss = 0.00922121\n",
      "Iteration 60, loss = 0.00914361\n",
      "Iteration 61, loss = 0.00909652\n",
      "Iteration 62, loss = 0.00903997\n",
      "Iteration 63, loss = 0.00898432\n",
      "Iteration 64, loss = 0.00893436\n",
      "Iteration 65, loss = 0.00887463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65784449\n",
      "Iteration 2, loss = 0.55874523\n",
      "Iteration 3, loss = 0.47408031\n",
      "Iteration 4, loss = 0.40638709\n",
      "Iteration 5, loss = 0.35272934\n",
      "Iteration 6, loss = 0.31001370\n",
      "Iteration 7, loss = 0.27584150\n",
      "Iteration 8, loss = 0.24840178\n",
      "Iteration 9, loss = 0.22616164\n",
      "Iteration 10, loss = 0.20790105\n",
      "Iteration 11, loss = 0.19266183\n",
      "Iteration 12, loss = 0.17989501\n",
      "Iteration 13, loss = 0.16893894\n",
      "Iteration 14, loss = 0.15958118\n",
      "Iteration 15, loss = 0.15136921\n",
      "Iteration 16, loss = 0.14420701\n",
      "Iteration 17, loss = 0.13785151\n",
      "Iteration 18, loss = 0.13225555\n",
      "Iteration 19, loss = 0.12716260\n",
      "Iteration 20, loss = 0.12262179\n",
      "Iteration 21, loss = 0.11849137\n",
      "Iteration 22, loss = 0.11473694\n",
      "Iteration 23, loss = 0.11128997\n",
      "Iteration 24, loss = 0.10813718\n",
      "Iteration 25, loss = 0.10520791\n",
      "Iteration 26, loss = 0.10249456\n",
      "Iteration 27, loss = 0.09997668\n",
      "Iteration 28, loss = 0.09761070\n",
      "Iteration 29, loss = 0.09542693\n",
      "Iteration 30, loss = 0.09336207\n",
      "Iteration 31, loss = 0.09143045\n",
      "Iteration 32, loss = 0.08958144\n",
      "Iteration 33, loss = 0.08784886\n",
      "Iteration 34, loss = 0.08619579\n",
      "Iteration 35, loss = 0.08463242\n",
      "Iteration 36, loss = 0.08313906\n",
      "Iteration 37, loss = 0.08169783\n",
      "Iteration 38, loss = 0.08033548\n",
      "Iteration 39, loss = 0.07902540\n",
      "Iteration 40, loss = 0.07777029\n",
      "Iteration 41, loss = 0.07657029\n",
      "Iteration 42, loss = 0.07542933\n",
      "Iteration 43, loss = 0.07430424\n",
      "Iteration 44, loss = 0.07325440\n",
      "Iteration 45, loss = 0.07220005\n",
      "Iteration 46, loss = 0.07122801\n",
      "Iteration 47, loss = 0.07026500\n",
      "Iteration 48, loss = 0.06933018\n",
      "Iteration 49, loss = 0.06844068\n",
      "Iteration 50, loss = 0.06758602\n",
      "Iteration 51, loss = 0.06674756\n",
      "Iteration 52, loss = 0.06595354\n",
      "Iteration 53, loss = 0.06516481\n",
      "Iteration 54, loss = 0.06441442\n",
      "Iteration 55, loss = 0.06367641\n",
      "Iteration 56, loss = 0.06296403\n",
      "Iteration 57, loss = 0.06227833\n",
      "Iteration 58, loss = 0.06161563\n",
      "Iteration 59, loss = 0.06097213\n",
      "Iteration 60, loss = 0.06033934\n",
      "Iteration 61, loss = 0.05973439\n",
      "Iteration 62, loss = 0.05913789\n",
      "Iteration 63, loss = 0.05856865\n",
      "Iteration 64, loss = 0.05801494\n",
      "Iteration 65, loss = 0.05746993\n",
      "Iteration 66, loss = 0.05694960\n",
      "Iteration 67, loss = 0.05644500\n",
      "Iteration 68, loss = 0.05594048\n",
      "Iteration 69, loss = 0.05545653\n",
      "Iteration 70, loss = 0.05499865\n",
      "Iteration 71, loss = 0.05452873\n",
      "Iteration 72, loss = 0.05408066\n",
      "Iteration 73, loss = 0.05364938\n",
      "Iteration 74, loss = 0.05322291\n",
      "Iteration 75, loss = 0.05281010\n",
      "Iteration 76, loss = 0.05240846\n",
      "Iteration 77, loss = 0.05201767\n",
      "Iteration 78, loss = 0.05164039\n",
      "Iteration 79, loss = 0.05125205\n",
      "Iteration 80, loss = 0.05089326\n",
      "Iteration 81, loss = 0.05053285\n",
      "Iteration 82, loss = 0.05019150\n",
      "Iteration 83, loss = 0.04984676\n",
      "Iteration 84, loss = 0.04950920\n",
      "Iteration 85, loss = 0.04919830\n",
      "Iteration 86, loss = 0.04886667\n",
      "Iteration 87, loss = 0.04855451\n",
      "Iteration 88, loss = 0.04825782\n",
      "Iteration 89, loss = 0.04794248\n",
      "Iteration 90, loss = 0.04766191\n",
      "Iteration 91, loss = 0.04736965\n",
      "Iteration 92, loss = 0.04708676\n",
      "Iteration 93, loss = 0.04681175\n",
      "Iteration 94, loss = 0.04654420\n",
      "Iteration 95, loss = 0.04628204\n",
      "Iteration 96, loss = 0.04602127\n",
      "Iteration 97, loss = 0.04577071\n",
      "Iteration 98, loss = 0.04552054\n",
      "Iteration 99, loss = 0.04528520\n",
      "Iteration 100, loss = 0.04503671\n",
      "Iteration 101, loss = 0.04480387\n",
      "Iteration 102, loss = 0.04457414\n",
      "Iteration 103, loss = 0.04434942\n",
      "Iteration 104, loss = 0.04412679\n",
      "Iteration 105, loss = 0.04390668\n",
      "Iteration 106, loss = 0.04369713\n",
      "Iteration 107, loss = 0.04348898\n",
      "Iteration 108, loss = 0.04328198\n",
      "Iteration 109, loss = 0.04308327\n",
      "Iteration 110, loss = 0.04288104\n",
      "Iteration 111, loss = 0.04268944\n",
      "Iteration 112, loss = 0.04248764\n",
      "Iteration 113, loss = 0.04230117\n",
      "Iteration 114, loss = 0.04211654\n",
      "Iteration 115, loss = 0.04193313\n",
      "Iteration 116, loss = 0.04176171\n",
      "Iteration 117, loss = 0.04157592\n",
      "Iteration 118, loss = 0.04140105\n",
      "Iteration 119, loss = 0.04122658\n",
      "Iteration 120, loss = 0.04106130\n",
      "Iteration 121, loss = 0.04089665\n",
      "Iteration 122, loss = 0.04072759\n",
      "Iteration 123, loss = 0.04056779\n",
      "Iteration 124, loss = 0.04040846\n",
      "Iteration 125, loss = 0.04026491\n",
      "Iteration 126, loss = 0.04009715\n",
      "Iteration 127, loss = 0.03995054\n",
      "Iteration 128, loss = 0.03980220\n",
      "Iteration 129, loss = 0.03965155\n",
      "Iteration 130, loss = 0.03950659\n",
      "Iteration 131, loss = 0.03936151\n",
      "Iteration 132, loss = 0.03922102\n",
      "Iteration 133, loss = 0.03908211\n",
      "Iteration 134, loss = 0.03895093\n",
      "Iteration 135, loss = 0.03880746\n",
      "Iteration 136, loss = 0.03867860\n",
      "Iteration 137, loss = 0.03854332\n",
      "Iteration 138, loss = 0.03841086\n",
      "Iteration 139, loss = 0.03828123\n",
      "Iteration 140, loss = 0.03816295\n",
      "Iteration 141, loss = 0.03803914\n",
      "Iteration 142, loss = 0.03790803\n",
      "Iteration 143, loss = 0.03778357\n",
      "Iteration 144, loss = 0.03766550\n",
      "Iteration 145, loss = 0.03754726\n",
      "Iteration 146, loss = 0.03742820\n",
      "Iteration 147, loss = 0.03730877\n",
      "Iteration 148, loss = 0.03720637\n",
      "Iteration 149, loss = 0.03707882\n",
      "Iteration 150, loss = 0.03697280\n",
      "Iteration 151, loss = 0.03685623\n",
      "Iteration 152, loss = 0.03674715\n",
      "Iteration 153, loss = 0.03665054\n",
      "Iteration 154, loss = 0.03652990\n",
      "Iteration 155, loss = 0.03642242\n",
      "Iteration 156, loss = 0.03632280\n",
      "Iteration 157, loss = 0.03621352\n",
      "Iteration 158, loss = 0.03611477\n",
      "Iteration 159, loss = 0.03601703\n",
      "Iteration 160, loss = 0.03591266\n",
      "Iteration 161, loss = 0.03581314\n",
      "Iteration 162, loss = 0.03571571\n",
      "Iteration 163, loss = 0.03561496\n",
      "Iteration 164, loss = 0.03552319\n",
      "Iteration 165, loss = 0.03542803\n",
      "Iteration 166, loss = 0.03533019\n",
      "Iteration 167, loss = 0.03523792\n",
      "Iteration 168, loss = 0.03514650\n",
      "Iteration 169, loss = 0.03505379\n",
      "Iteration 170, loss = 0.03496632\n",
      "Iteration 171, loss = 0.03487704\n",
      "Iteration 172, loss = 0.03479314\n",
      "Iteration 173, loss = 0.03470056\n",
      "Iteration 174, loss = 0.03461082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66845893\n",
      "Iteration 2, loss = 0.58181802\n",
      "Iteration 3, loss = 0.50860634\n",
      "Iteration 4, loss = 0.44952120\n",
      "Iteration 5, loss = 0.40195512\n",
      "Iteration 6, loss = 0.36305396\n",
      "Iteration 7, loss = 0.33114601\n",
      "Iteration 8, loss = 0.30462911\n",
      "Iteration 9, loss = 0.28227945\n",
      "Iteration 10, loss = 0.26331084\n",
      "Iteration 11, loss = 0.24699747\n",
      "Iteration 12, loss = 0.23293350\n",
      "Iteration 13, loss = 0.22050327\n",
      "Iteration 14, loss = 0.20962618\n",
      "Iteration 15, loss = 0.19991897\n",
      "Iteration 16, loss = 0.19129167\n",
      "Iteration 17, loss = 0.18348309\n",
      "Iteration 18, loss = 0.17646347\n",
      "Iteration 19, loss = 0.17004793\n",
      "Iteration 20, loss = 0.16421547\n",
      "Iteration 21, loss = 0.15885560\n",
      "Iteration 22, loss = 0.15393601\n",
      "Iteration 23, loss = 0.14936562\n",
      "Iteration 24, loss = 0.14517096\n",
      "Iteration 25, loss = 0.14125378\n",
      "Iteration 26, loss = 0.13759147\n",
      "Iteration 27, loss = 0.13416807\n",
      "Iteration 28, loss = 0.13094443\n",
      "Iteration 29, loss = 0.12793263\n",
      "Iteration 30, loss = 0.12509285\n",
      "Iteration 31, loss = 0.12240877\n",
      "Iteration 32, loss = 0.11981276\n",
      "Iteration 33, loss = 0.11738332\n",
      "Iteration 34, loss = 0.11501808\n",
      "Iteration 35, loss = 0.11277524\n",
      "Iteration 36, loss = 0.11064917\n",
      "Iteration 37, loss = 0.10859830\n",
      "Iteration 38, loss = 0.10661777\n",
      "Iteration 39, loss = 0.10473382\n",
      "Iteration 40, loss = 0.10289344\n",
      "Iteration 41, loss = 0.10114289\n",
      "Iteration 42, loss = 0.09946157\n",
      "Iteration 43, loss = 0.09781598\n",
      "Iteration 44, loss = 0.09624999\n",
      "Iteration 45, loss = 0.09469706\n",
      "Iteration 46, loss = 0.09322083\n",
      "Iteration 47, loss = 0.09179552\n",
      "Iteration 48, loss = 0.09038282\n",
      "Iteration 49, loss = 0.08903577\n",
      "Iteration 50, loss = 0.08772270\n",
      "Iteration 51, loss = 0.08645566\n",
      "Iteration 52, loss = 0.08521306\n",
      "Iteration 53, loss = 0.08401987\n",
      "Iteration 54, loss = 0.08285292\n",
      "Iteration 55, loss = 0.08171768\n",
      "Iteration 56, loss = 0.08061226\n",
      "Iteration 57, loss = 0.07953530\n",
      "Iteration 58, loss = 0.07849815\n",
      "Iteration 59, loss = 0.07748364\n",
      "Iteration 60, loss = 0.07649342\n",
      "Iteration 61, loss = 0.07554811\n",
      "Iteration 62, loss = 0.07458180\n",
      "Iteration 63, loss = 0.07367491\n",
      "Iteration 64, loss = 0.07280137\n",
      "Iteration 65, loss = 0.07193070\n",
      "Iteration 66, loss = 0.07109748\n",
      "Iteration 67, loss = 0.07027882\n",
      "Iteration 68, loss = 0.06948006\n",
      "Iteration 69, loss = 0.06869968\n",
      "Iteration 70, loss = 0.06796156\n",
      "Iteration 71, loss = 0.06720389\n",
      "Iteration 72, loss = 0.06649555\n",
      "Iteration 73, loss = 0.06578598\n",
      "Iteration 74, loss = 0.06510655\n",
      "Iteration 75, loss = 0.06444014\n",
      "Iteration 76, loss = 0.06379501\n",
      "Iteration 77, loss = 0.06316286\n",
      "Iteration 78, loss = 0.06254999\n",
      "Iteration 79, loss = 0.06193607\n",
      "Iteration 80, loss = 0.06134549\n",
      "Iteration 81, loss = 0.06077620\n",
      "Iteration 82, loss = 0.06021847\n",
      "Iteration 83, loss = 0.05966298\n",
      "Iteration 84, loss = 0.05912944\n",
      "Iteration 85, loss = 0.05861321\n",
      "Iteration 86, loss = 0.05809958\n",
      "Iteration 87, loss = 0.05759666\n",
      "Iteration 88, loss = 0.05712000\n",
      "Iteration 89, loss = 0.05662210\n",
      "Iteration 90, loss = 0.05616586\n",
      "Iteration 91, loss = 0.05570656\n",
      "Iteration 92, loss = 0.05525742\n",
      "Iteration 93, loss = 0.05482466\n",
      "Iteration 94, loss = 0.05439565\n",
      "Iteration 95, loss = 0.05398326\n",
      "Iteration 96, loss = 0.05356316\n",
      "Iteration 97, loss = 0.05317139\n",
      "Iteration 98, loss = 0.05277772\n",
      "Iteration 99, loss = 0.05240001\n",
      "Iteration 100, loss = 0.05201831\n",
      "Iteration 101, loss = 0.05165097\n",
      "Iteration 102, loss = 0.05129065\n",
      "Iteration 103, loss = 0.05093923\n",
      "Iteration 104, loss = 0.05059461\n",
      "Iteration 105, loss = 0.05026109\n",
      "Iteration 106, loss = 0.04992753\n",
      "Iteration 107, loss = 0.04960011\n",
      "Iteration 108, loss = 0.04927874\n",
      "Iteration 109, loss = 0.04896959\n",
      "Iteration 110, loss = 0.04866316\n",
      "Iteration 111, loss = 0.04835738\n",
      "Iteration 112, loss = 0.04806770\n",
      "Iteration 113, loss = 0.04778061\n",
      "Iteration 114, loss = 0.04748886\n",
      "Iteration 115, loss = 0.04721206\n",
      "Iteration 116, loss = 0.04694433\n",
      "Iteration 117, loss = 0.04666539\n",
      "Iteration 118, loss = 0.04640395\n",
      "Iteration 119, loss = 0.04614394\n",
      "Iteration 120, loss = 0.04589852\n",
      "Iteration 121, loss = 0.04565547\n",
      "Iteration 122, loss = 0.04539874\n",
      "Iteration 123, loss = 0.04515339\n",
      "Iteration 124, loss = 0.04491592\n",
      "Iteration 125, loss = 0.04468603\n",
      "Iteration 126, loss = 0.04445717\n",
      "Iteration 127, loss = 0.04422498\n",
      "Iteration 128, loss = 0.04400908\n",
      "Iteration 129, loss = 0.04379067\n",
      "Iteration 130, loss = 0.04357236\n",
      "Iteration 131, loss = 0.04336065\n",
      "Iteration 132, loss = 0.04315679\n",
      "Iteration 133, loss = 0.04295548\n",
      "Iteration 134, loss = 0.04275877\n",
      "Iteration 135, loss = 0.04255219\n",
      "Iteration 136, loss = 0.04235736\n",
      "Iteration 137, loss = 0.04217188\n",
      "Iteration 138, loss = 0.04197834\n",
      "Iteration 139, loss = 0.04178723\n",
      "Iteration 140, loss = 0.04161407\n",
      "Iteration 141, loss = 0.04143592\n",
      "Iteration 142, loss = 0.04124709\n",
      "Iteration 143, loss = 0.04107542\n",
      "Iteration 144, loss = 0.04090954\n",
      "Iteration 145, loss = 0.04074585\n",
      "Iteration 146, loss = 0.04055874\n",
      "Iteration 147, loss = 0.04039363\n",
      "Iteration 148, loss = 0.04024878\n",
      "Iteration 149, loss = 0.04006966\n",
      "Iteration 150, loss = 0.03992114\n",
      "Iteration 151, loss = 0.03975791\n",
      "Iteration 152, loss = 0.03960061\n",
      "Iteration 153, loss = 0.03945422\n",
      "Iteration 154, loss = 0.03930032\n",
      "Iteration 155, loss = 0.03915026\n",
      "Iteration 156, loss = 0.03901152\n",
      "Iteration 157, loss = 0.03885851\n",
      "Iteration 158, loss = 0.03871806\n",
      "Iteration 159, loss = 0.03858269\n",
      "Iteration 160, loss = 0.03844854\n",
      "Iteration 161, loss = 0.03830364\n",
      "Iteration 162, loss = 0.03816877\n",
      "Iteration 163, loss = 0.03803161\n",
      "Iteration 164, loss = 0.03790764\n",
      "Iteration 165, loss = 0.03777156\n",
      "Iteration 166, loss = 0.03765001\n",
      "Iteration 167, loss = 0.03752123\n",
      "Iteration 168, loss = 0.03739644\n",
      "Iteration 169, loss = 0.03727197\n",
      "Iteration 170, loss = 0.03714737\n",
      "Iteration 171, loss = 0.03702954\n",
      "Iteration 172, loss = 0.03691694\n",
      "Iteration 173, loss = 0.03679404\n",
      "Iteration 174, loss = 0.03667594\n",
      "Iteration 175, loss = 0.03655974\n",
      "Iteration 176, loss = 0.03645218\n",
      "Iteration 177, loss = 0.03633962\n",
      "Iteration 178, loss = 0.03622784\n",
      "Iteration 179, loss = 0.03612210\n",
      "Iteration 180, loss = 0.03601437\n",
      "Iteration 181, loss = 0.03589989\n",
      "Iteration 182, loss = 0.03579391\n",
      "Iteration 183, loss = 0.03569005\n",
      "Iteration 184, loss = 0.03558767\n",
      "Iteration 185, loss = 0.03548841\n",
      "Iteration 186, loss = 0.03538798\n",
      "Iteration 187, loss = 0.03528752\n",
      "Iteration 188, loss = 0.03518750\n",
      "Iteration 189, loss = 0.03508887\n",
      "Iteration 190, loss = 0.03499311\n",
      "Iteration 191, loss = 0.03489617\n",
      "Iteration 192, loss = 0.03480196\n",
      "Iteration 193, loss = 0.03470999\n",
      "Iteration 194, loss = 0.03461615\n",
      "Iteration 195, loss = 0.03452869\n",
      "Iteration 196, loss = 0.03443664\n",
      "Iteration 197, loss = 0.03434756\n",
      "Iteration 198, loss = 0.03426257\n",
      "Iteration 199, loss = 0.03417028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67463615\n",
      "Iteration 2, loss = 0.58284194\n",
      "Iteration 3, loss = 0.50269695\n",
      "Iteration 4, loss = 0.43575854\n",
      "Iteration 5, loss = 0.38092595\n",
      "Iteration 6, loss = 0.33606492\n",
      "Iteration 7, loss = 0.29984293\n",
      "Iteration 8, loss = 0.27050128\n",
      "Iteration 9, loss = 0.24659440\n",
      "Iteration 10, loss = 0.22685683\n",
      "Iteration 11, loss = 0.21033879\n",
      "Iteration 12, loss = 0.19638919\n",
      "Iteration 13, loss = 0.18433681\n",
      "Iteration 14, loss = 0.17385429\n",
      "Iteration 15, loss = 0.16467115\n",
      "Iteration 16, loss = 0.15653896\n",
      "Iteration 17, loss = 0.14924879\n",
      "Iteration 18, loss = 0.14274066\n",
      "Iteration 19, loss = 0.13682259\n",
      "Iteration 20, loss = 0.13145590\n",
      "Iteration 21, loss = 0.12656340\n",
      "Iteration 22, loss = 0.12208854\n",
      "Iteration 23, loss = 0.11793664\n",
      "Iteration 24, loss = 0.11412014\n",
      "Iteration 25, loss = 0.11057984\n",
      "Iteration 26, loss = 0.10728408\n",
      "Iteration 27, loss = 0.10422252\n",
      "Iteration 28, loss = 0.10138444\n",
      "Iteration 29, loss = 0.09869187\n",
      "Iteration 30, loss = 0.09618433\n",
      "Iteration 31, loss = 0.09384075\n",
      "Iteration 32, loss = 0.09160509\n",
      "Iteration 33, loss = 0.08953466\n",
      "Iteration 34, loss = 0.08755506\n",
      "Iteration 35, loss = 0.08566273\n",
      "Iteration 36, loss = 0.08390597\n",
      "Iteration 37, loss = 0.08220063\n",
      "Iteration 38, loss = 0.08059393\n",
      "Iteration 39, loss = 0.07907656\n",
      "Iteration 40, loss = 0.07762791\n",
      "Iteration 41, loss = 0.07625392\n",
      "Iteration 42, loss = 0.07494806\n",
      "Iteration 43, loss = 0.07368130\n",
      "Iteration 44, loss = 0.07249699\n",
      "Iteration 45, loss = 0.07135366\n",
      "Iteration 46, loss = 0.07024921\n",
      "Iteration 47, loss = 0.06922915\n",
      "Iteration 48, loss = 0.06820383\n",
      "Iteration 49, loss = 0.06726882\n",
      "Iteration 50, loss = 0.06633116\n",
      "Iteration 51, loss = 0.06545501\n",
      "Iteration 52, loss = 0.06460076\n",
      "Iteration 53, loss = 0.06379975\n",
      "Iteration 54, loss = 0.06300387\n",
      "Iteration 55, loss = 0.06225836\n",
      "Iteration 56, loss = 0.06151959\n",
      "Iteration 57, loss = 0.06082450\n",
      "Iteration 58, loss = 0.06015105\n",
      "Iteration 59, loss = 0.05949279\n",
      "Iteration 60, loss = 0.05886404\n",
      "Iteration 61, loss = 0.05827323\n",
      "Iteration 62, loss = 0.05767903\n",
      "Iteration 63, loss = 0.05710924\n",
      "Iteration 64, loss = 0.05656298\n",
      "Iteration 65, loss = 0.05602920\n",
      "Iteration 66, loss = 0.05551587\n",
      "Iteration 67, loss = 0.05502294\n",
      "Iteration 68, loss = 0.05454148\n",
      "Iteration 69, loss = 0.05407387\n",
      "Iteration 70, loss = 0.05362824\n",
      "Iteration 71, loss = 0.05318966\n",
      "Iteration 72, loss = 0.05276843\n",
      "Iteration 73, loss = 0.05235154\n",
      "Iteration 74, loss = 0.05195013\n",
      "Iteration 75, loss = 0.05156663\n",
      "Iteration 76, loss = 0.05119312\n",
      "Iteration 77, loss = 0.05082552\n",
      "Iteration 78, loss = 0.05046535\n",
      "Iteration 79, loss = 0.05012318\n",
      "Iteration 80, loss = 0.04978146\n",
      "Iteration 81, loss = 0.04945087\n",
      "Iteration 82, loss = 0.04912705\n",
      "Iteration 83, loss = 0.04881817\n",
      "Iteration 84, loss = 0.04851566\n",
      "Iteration 85, loss = 0.04821073\n",
      "Iteration 86, loss = 0.04792636\n",
      "Iteration 87, loss = 0.04763966\n",
      "Iteration 88, loss = 0.04737078\n",
      "Iteration 89, loss = 0.04709617\n",
      "Iteration 90, loss = 0.04682822\n",
      "Iteration 91, loss = 0.04656833\n",
      "Iteration 92, loss = 0.04631912\n",
      "Iteration 93, loss = 0.04607322\n",
      "Iteration 94, loss = 0.04582913\n",
      "Iteration 95, loss = 0.04559751\n",
      "Iteration 96, loss = 0.04536171\n",
      "Iteration 97, loss = 0.04513712\n",
      "Iteration 98, loss = 0.04491699\n",
      "Iteration 99, loss = 0.04470226\n",
      "Iteration 100, loss = 0.04448745\n",
      "Iteration 101, loss = 0.04427988\n",
      "Iteration 102, loss = 0.04407610\n",
      "Iteration 103, loss = 0.04387843\n",
      "Iteration 104, loss = 0.04368462\n",
      "Iteration 105, loss = 0.04349204\n",
      "Iteration 106, loss = 0.04330182\n",
      "Iteration 107, loss = 0.04312326\n",
      "Iteration 108, loss = 0.04294682\n",
      "Iteration 109, loss = 0.04275722\n",
      "Iteration 110, loss = 0.04258871\n",
      "Iteration 111, loss = 0.04241473\n",
      "Iteration 112, loss = 0.04224764\n",
      "Iteration 113, loss = 0.04208207\n",
      "Iteration 114, loss = 0.04191899\n",
      "Iteration 115, loss = 0.04175773\n",
      "Iteration 116, loss = 0.04160103\n",
      "Iteration 117, loss = 0.04144627\n",
      "Iteration 118, loss = 0.04129393\n",
      "Iteration 119, loss = 0.04114380\n",
      "Iteration 120, loss = 0.04099770\n",
      "Iteration 121, loss = 0.04085736\n",
      "Iteration 122, loss = 0.04070818\n",
      "Iteration 123, loss = 0.04057003\n",
      "Iteration 124, loss = 0.04043143\n",
      "Iteration 125, loss = 0.04029244\n",
      "Iteration 126, loss = 0.04016165\n",
      "Iteration 127, loss = 0.04002876\n",
      "Iteration 128, loss = 0.03989842\n",
      "Iteration 129, loss = 0.03976372\n",
      "Iteration 130, loss = 0.03964012\n",
      "Iteration 131, loss = 0.03951529\n",
      "Iteration 132, loss = 0.03939236\n",
      "Iteration 133, loss = 0.03927506\n",
      "Iteration 134, loss = 0.03915354\n",
      "Iteration 135, loss = 0.03903427\n",
      "Iteration 136, loss = 0.03891679\n",
      "Iteration 137, loss = 0.03881158\n",
      "Iteration 138, loss = 0.03868940\n",
      "Iteration 139, loss = 0.03857975\n",
      "Iteration 140, loss = 0.03846816\n",
      "Iteration 141, loss = 0.03836492\n",
      "Iteration 142, loss = 0.03825358\n",
      "Iteration 143, loss = 0.03814812\n",
      "Iteration 144, loss = 0.03804688\n",
      "Iteration 145, loss = 0.03793899\n",
      "Iteration 146, loss = 0.03783597\n",
      "Iteration 147, loss = 0.03773588\n",
      "Iteration 148, loss = 0.03763676\n",
      "Iteration 149, loss = 0.03753942\n",
      "Iteration 150, loss = 0.03743747\n",
      "Iteration 151, loss = 0.03733884\n",
      "Iteration 152, loss = 0.03724587\n",
      "Iteration 153, loss = 0.03715086\n",
      "Iteration 154, loss = 0.03705745\n",
      "Iteration 155, loss = 0.03696540\n",
      "Iteration 156, loss = 0.03687623\n",
      "Iteration 157, loss = 0.03678527\n",
      "Iteration 158, loss = 0.03669518\n",
      "Iteration 159, loss = 0.03660716\n",
      "Iteration 160, loss = 0.03651643\n",
      "Iteration 161, loss = 0.03643058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67721297\n",
      "Iteration 2, loss = 0.59471997\n",
      "Iteration 3, loss = 0.52425718\n",
      "Iteration 4, loss = 0.46567453\n",
      "Iteration 5, loss = 0.41737690\n",
      "Iteration 6, loss = 0.37714988\n",
      "Iteration 7, loss = 0.34371646\n",
      "Iteration 8, loss = 0.31558570\n",
      "Iteration 9, loss = 0.29185734\n",
      "Iteration 10, loss = 0.27163085\n",
      "Iteration 11, loss = 0.25414931\n",
      "Iteration 12, loss = 0.23898950\n",
      "Iteration 13, loss = 0.22564626\n",
      "Iteration 14, loss = 0.21391933\n",
      "Iteration 15, loss = 0.20346989\n",
      "Iteration 16, loss = 0.19419275\n",
      "Iteration 17, loss = 0.18582959\n",
      "Iteration 18, loss = 0.17831895\n",
      "Iteration 19, loss = 0.17151300\n",
      "Iteration 20, loss = 0.16523019\n",
      "Iteration 21, loss = 0.15956579\n",
      "Iteration 22, loss = 0.15431192\n",
      "Iteration 23, loss = 0.14948315\n",
      "Iteration 24, loss = 0.14502262\n",
      "Iteration 25, loss = 0.14093408\n",
      "Iteration 26, loss = 0.13699114\n",
      "Iteration 27, loss = 0.13339255\n",
      "Iteration 28, loss = 0.13001985\n",
      "Iteration 29, loss = 0.12683343\n",
      "Iteration 30, loss = 0.12384303\n",
      "Iteration 31, loss = 0.12099951\n",
      "Iteration 32, loss = 0.11832235\n",
      "Iteration 33, loss = 0.11577720\n",
      "Iteration 34, loss = 0.11334857\n",
      "Iteration 35, loss = 0.11104105\n",
      "Iteration 36, loss = 0.10883130\n",
      "Iteration 37, loss = 0.10671255\n",
      "Iteration 38, loss = 0.10469204\n",
      "Iteration 39, loss = 0.10276768\n",
      "Iteration 40, loss = 0.10089766\n",
      "Iteration 41, loss = 0.09910576\n",
      "Iteration 42, loss = 0.09739060\n",
      "Iteration 43, loss = 0.09571649\n",
      "Iteration 44, loss = 0.09411114\n",
      "Iteration 45, loss = 0.09256895\n",
      "Iteration 46, loss = 0.09107513\n",
      "Iteration 47, loss = 0.08964974\n",
      "Iteration 48, loss = 0.08826973\n",
      "Iteration 49, loss = 0.08694417\n",
      "Iteration 50, loss = 0.08563158\n",
      "Iteration 51, loss = 0.08438575\n",
      "Iteration 52, loss = 0.08315966\n",
      "Iteration 53, loss = 0.08198657\n",
      "Iteration 54, loss = 0.08084185\n",
      "Iteration 55, loss = 0.07973744\n",
      "Iteration 56, loss = 0.07866661\n",
      "Iteration 57, loss = 0.07762775\n",
      "Iteration 58, loss = 0.07660777\n",
      "Iteration 59, loss = 0.07563194\n",
      "Iteration 60, loss = 0.07466854\n",
      "Iteration 61, loss = 0.07374363\n",
      "Iteration 62, loss = 0.07284198\n",
      "Iteration 63, loss = 0.07197379\n",
      "Iteration 64, loss = 0.07113082\n",
      "Iteration 65, loss = 0.07032820\n",
      "Iteration 66, loss = 0.06950191\n",
      "Iteration 67, loss = 0.06873843\n",
      "Iteration 68, loss = 0.06798043\n",
      "Iteration 69, loss = 0.06724058\n",
      "Iteration 70, loss = 0.06652736\n",
      "Iteration 71, loss = 0.06581542\n",
      "Iteration 72, loss = 0.06512666\n",
      "Iteration 73, loss = 0.06446496\n",
      "Iteration 74, loss = 0.06382884\n",
      "Iteration 75, loss = 0.06318853\n",
      "Iteration 76, loss = 0.06257600\n",
      "Iteration 77, loss = 0.06197444\n",
      "Iteration 78, loss = 0.06138523\n",
      "Iteration 79, loss = 0.06081876\n",
      "Iteration 80, loss = 0.06025808\n",
      "Iteration 81, loss = 0.05971335\n",
      "Iteration 82, loss = 0.05918996\n",
      "Iteration 83, loss = 0.05869286\n",
      "Iteration 84, loss = 0.05816897\n",
      "Iteration 85, loss = 0.05769122\n",
      "Iteration 86, loss = 0.05718989\n",
      "Iteration 87, loss = 0.05671388\n",
      "Iteration 88, loss = 0.05625575\n",
      "Iteration 89, loss = 0.05579879\n",
      "Iteration 90, loss = 0.05537752\n",
      "Iteration 91, loss = 0.05492669\n",
      "Iteration 92, loss = 0.05451134\n",
      "Iteration 93, loss = 0.05409548\n",
      "Iteration 94, loss = 0.05369760\n",
      "Iteration 95, loss = 0.05331435\n",
      "Iteration 96, loss = 0.05291185\n",
      "Iteration 97, loss = 0.05253648\n",
      "Iteration 98, loss = 0.05216479\n",
      "Iteration 99, loss = 0.05180255\n",
      "Iteration 100, loss = 0.05145229\n",
      "Iteration 101, loss = 0.05110977\n",
      "Iteration 102, loss = 0.05075349\n",
      "Iteration 103, loss = 0.05042146\n",
      "Iteration 104, loss = 0.05009168\n",
      "Iteration 105, loss = 0.04976921\n",
      "Iteration 106, loss = 0.04945222\n",
      "Iteration 107, loss = 0.04915195\n",
      "Iteration 108, loss = 0.04883513\n",
      "Iteration 109, loss = 0.04854287\n",
      "Iteration 110, loss = 0.04825061\n",
      "Iteration 111, loss = 0.04795878\n",
      "Iteration 112, loss = 0.04767620\n",
      "Iteration 113, loss = 0.04740316\n",
      "Iteration 114, loss = 0.04712389\n",
      "Iteration 115, loss = 0.04685817\n",
      "Iteration 116, loss = 0.04659385\n",
      "Iteration 117, loss = 0.04634282\n",
      "Iteration 118, loss = 0.04607693\n",
      "Iteration 119, loss = 0.04583624\n",
      "Iteration 120, loss = 0.04558109\n",
      "Iteration 121, loss = 0.04534536\n",
      "Iteration 122, loss = 0.04510418\n",
      "Iteration 123, loss = 0.04486892\n",
      "Iteration 124, loss = 0.04464475\n",
      "Iteration 125, loss = 0.04442146\n",
      "Iteration 126, loss = 0.04419931\n",
      "Iteration 127, loss = 0.04397073\n",
      "Iteration 128, loss = 0.04375903\n",
      "Iteration 129, loss = 0.04354530\n",
      "Iteration 130, loss = 0.04333923\n",
      "Iteration 131, loss = 0.04312877\n",
      "Iteration 132, loss = 0.04292834\n",
      "Iteration 133, loss = 0.04272829\n",
      "Iteration 134, loss = 0.04252561\n",
      "Iteration 135, loss = 0.04233039\n",
      "Iteration 136, loss = 0.04213933\n",
      "Iteration 137, loss = 0.04195447\n",
      "Iteration 138, loss = 0.04177528\n",
      "Iteration 139, loss = 0.04158730\n",
      "Iteration 140, loss = 0.04140757\n",
      "Iteration 141, loss = 0.04123417\n",
      "Iteration 142, loss = 0.04105323\n",
      "Iteration 143, loss = 0.04088019\n",
      "Iteration 144, loss = 0.04071393\n",
      "Iteration 145, loss = 0.04054909\n",
      "Iteration 146, loss = 0.04037756\n",
      "Iteration 147, loss = 0.04021633\n",
      "Iteration 148, loss = 0.04005375\n",
      "Iteration 149, loss = 0.03990118\n",
      "Iteration 150, loss = 0.03974078\n",
      "Iteration 151, loss = 0.03958597\n",
      "Iteration 152, loss = 0.03943284\n",
      "Iteration 153, loss = 0.03928618\n",
      "Iteration 154, loss = 0.03913758\n",
      "Iteration 155, loss = 0.03899182\n",
      "Iteration 156, loss = 0.03884535\n",
      "Iteration 157, loss = 0.03870908\n",
      "Iteration 158, loss = 0.03856956\n",
      "Iteration 159, loss = 0.03842917\n",
      "Iteration 160, loss = 0.03829092\n",
      "Iteration 161, loss = 0.03815914\n",
      "Iteration 162, loss = 0.03802513\n",
      "Iteration 163, loss = 0.03789318\n",
      "Iteration 164, loss = 0.03776329\n",
      "Iteration 165, loss = 0.03763453\n",
      "Iteration 166, loss = 0.03751098\n",
      "Iteration 167, loss = 0.03738665\n",
      "Iteration 168, loss = 0.03725628\n",
      "Iteration 169, loss = 0.03713776\n",
      "Iteration 170, loss = 0.03702107\n",
      "Iteration 171, loss = 0.03689718\n",
      "Iteration 172, loss = 0.03677923\n",
      "Iteration 173, loss = 0.03666885\n",
      "Iteration 174, loss = 0.03655049\n",
      "Iteration 175, loss = 0.03643088\n",
      "Iteration 176, loss = 0.03632594\n",
      "Iteration 177, loss = 0.03621062\n",
      "Iteration 178, loss = 0.03609900\n",
      "Iteration 179, loss = 0.03598987\n",
      "Iteration 180, loss = 0.03588165\n",
      "Iteration 181, loss = 0.03577639\n",
      "Iteration 182, loss = 0.03566823\n",
      "Iteration 183, loss = 0.03556631\n",
      "Iteration 184, loss = 0.03546474\n",
      "Iteration 185, loss = 0.03536386\n",
      "Iteration 186, loss = 0.03526569\n",
      "Iteration 187, loss = 0.03517071\n",
      "Iteration 188, loss = 0.03506963\n",
      "Iteration 189, loss = 0.03497162\n",
      "Iteration 190, loss = 0.03487764\n",
      "Iteration 191, loss = 0.03477981\n",
      "Iteration 192, loss = 0.03469203\n",
      "Iteration 193, loss = 0.03459349\n",
      "Iteration 194, loss = 0.03450588\n",
      "Iteration 195, loss = 0.03440898\n",
      "Iteration 196, loss = 0.03432156\n",
      "Iteration 197, loss = 0.03423306\n",
      "Iteration 198, loss = 0.03414299\n",
      "Iteration 199, loss = 0.03405806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64937606\n",
      "Iteration 2, loss = 0.55142305\n",
      "Iteration 3, loss = 0.46933943\n",
      "Iteration 4, loss = 0.40534858\n",
      "Iteration 5, loss = 0.35462955\n",
      "Iteration 6, loss = 0.31423346\n",
      "Iteration 7, loss = 0.28163036\n",
      "Iteration 8, loss = 0.25508474\n",
      "Iteration 9, loss = 0.23311682\n",
      "Iteration 10, loss = 0.21473282\n",
      "Iteration 11, loss = 0.19912697\n",
      "Iteration 12, loss = 0.18559944\n",
      "Iteration 13, loss = 0.17383213\n",
      "Iteration 14, loss = 0.16353314\n",
      "Iteration 15, loss = 0.15441614\n",
      "Iteration 16, loss = 0.14628127\n",
      "Iteration 17, loss = 0.13901314\n",
      "Iteration 18, loss = 0.13241193\n",
      "Iteration 19, loss = 0.12649673\n",
      "Iteration 20, loss = 0.12109095\n",
      "Iteration 21, loss = 0.11617323\n",
      "Iteration 22, loss = 0.11167925\n",
      "Iteration 23, loss = 0.10754326\n",
      "Iteration 24, loss = 0.10376695\n",
      "Iteration 25, loss = 0.10018087\n",
      "Iteration 26, loss = 0.09694966\n",
      "Iteration 27, loss = 0.09388882\n",
      "Iteration 28, loss = 0.09105688\n",
      "Iteration 29, loss = 0.08841828\n",
      "Iteration 30, loss = 0.08592667\n",
      "Iteration 31, loss = 0.08361009\n",
      "Iteration 32, loss = 0.08144415\n",
      "Iteration 33, loss = 0.07939906\n",
      "Iteration 34, loss = 0.07747264\n",
      "Iteration 35, loss = 0.07563209\n",
      "Iteration 36, loss = 0.07391012\n",
      "Iteration 37, loss = 0.07226150\n",
      "Iteration 38, loss = 0.07072120\n",
      "Iteration 39, loss = 0.06926883\n",
      "Iteration 40, loss = 0.06785351\n",
      "Iteration 41, loss = 0.06652939\n",
      "Iteration 42, loss = 0.06527293\n",
      "Iteration 43, loss = 0.06405236\n",
      "Iteration 44, loss = 0.06289988\n",
      "Iteration 45, loss = 0.06180285\n",
      "Iteration 46, loss = 0.06074105\n",
      "Iteration 47, loss = 0.05973023\n",
      "Iteration 48, loss = 0.05878079\n",
      "Iteration 49, loss = 0.05784257\n",
      "Iteration 50, loss = 0.05695565\n",
      "Iteration 51, loss = 0.05610762\n",
      "Iteration 52, loss = 0.05529393\n",
      "Iteration 53, loss = 0.05449366\n",
      "Iteration 54, loss = 0.05373617\n",
      "Iteration 55, loss = 0.05301499\n",
      "Iteration 56, loss = 0.05231508\n",
      "Iteration 57, loss = 0.05163659\n",
      "Iteration 58, loss = 0.05098869\n",
      "Iteration 59, loss = 0.05035404\n",
      "Iteration 60, loss = 0.04975961\n",
      "Iteration 61, loss = 0.04916806\n",
      "Iteration 62, loss = 0.04860642\n",
      "Iteration 63, loss = 0.04805751\n",
      "Iteration 64, loss = 0.04752239\n",
      "Iteration 65, loss = 0.04701088\n",
      "Iteration 66, loss = 0.04651881\n",
      "Iteration 67, loss = 0.04603118\n",
      "Iteration 68, loss = 0.04557058\n",
      "Iteration 69, loss = 0.04511662\n",
      "Iteration 70, loss = 0.04467871\n",
      "Iteration 71, loss = 0.04426147\n",
      "Iteration 72, loss = 0.04384809\n",
      "Iteration 73, loss = 0.04346139\n",
      "Iteration 74, loss = 0.04307930\n",
      "Iteration 75, loss = 0.04269331\n",
      "Iteration 76, loss = 0.04232747\n",
      "Iteration 77, loss = 0.04197701\n",
      "Iteration 78, loss = 0.04163041\n",
      "Iteration 79, loss = 0.04129747\n",
      "Iteration 80, loss = 0.04097504\n",
      "Iteration 81, loss = 0.04065531\n",
      "Iteration 82, loss = 0.04034887\n",
      "Iteration 83, loss = 0.04004165\n",
      "Iteration 84, loss = 0.03975164\n",
      "Iteration 85, loss = 0.03946634\n",
      "Iteration 86, loss = 0.03918918\n",
      "Iteration 87, loss = 0.03891453\n",
      "Iteration 88, loss = 0.03865259\n",
      "Iteration 89, loss = 0.03839299\n",
      "Iteration 90, loss = 0.03814086\n",
      "Iteration 91, loss = 0.03789528\n",
      "Iteration 92, loss = 0.03765876\n",
      "Iteration 93, loss = 0.03742469\n",
      "Iteration 94, loss = 0.03719861\n",
      "Iteration 95, loss = 0.03697731\n",
      "Iteration 96, loss = 0.03676142\n",
      "Iteration 97, loss = 0.03654860\n",
      "Iteration 98, loss = 0.03634120\n",
      "Iteration 99, loss = 0.03613655\n",
      "Iteration 100, loss = 0.03593726\n",
      "Iteration 101, loss = 0.03574348\n",
      "Iteration 102, loss = 0.03555032\n",
      "Iteration 103, loss = 0.03536482\n",
      "Iteration 104, loss = 0.03518461\n",
      "Iteration 105, loss = 0.03500634\n",
      "Iteration 106, loss = 0.03483060\n",
      "Iteration 107, loss = 0.03466145\n",
      "Iteration 108, loss = 0.03449525\n",
      "Iteration 109, loss = 0.03433097\n",
      "Iteration 110, loss = 0.03416926\n",
      "Iteration 111, loss = 0.03401647\n",
      "Iteration 112, loss = 0.03386037\n",
      "Iteration 113, loss = 0.03370605\n",
      "Iteration 114, loss = 0.03355654\n",
      "Iteration 115, loss = 0.03341418\n",
      "Iteration 116, loss = 0.03327012\n",
      "Iteration 117, loss = 0.03313210\n",
      "Iteration 118, loss = 0.03299490\n",
      "Iteration 119, loss = 0.03285956\n",
      "Iteration 120, loss = 0.03272848\n",
      "Iteration 121, loss = 0.03260319\n",
      "Iteration 122, loss = 0.03247635\n",
      "Iteration 123, loss = 0.03234948\n",
      "Iteration 124, loss = 0.03222786\n",
      "Iteration 125, loss = 0.03211116\n",
      "Iteration 126, loss = 0.03199032\n",
      "Iteration 127, loss = 0.03187550\n",
      "Iteration 128, loss = 0.03176534\n",
      "Iteration 129, loss = 0.03165055\n",
      "Iteration 130, loss = 0.03153702\n",
      "Iteration 131, loss = 0.03142978\n",
      "Iteration 132, loss = 0.03132392\n",
      "Iteration 133, loss = 0.03122254\n",
      "Iteration 134, loss = 0.03111633\n",
      "Iteration 135, loss = 0.03101576\n",
      "Iteration 136, loss = 0.03091786\n",
      "Iteration 137, loss = 0.03082013\n",
      "Iteration 138, loss = 0.03072431\n",
      "Iteration 139, loss = 0.03063152\n",
      "Iteration 140, loss = 0.03054079\n",
      "Iteration 141, loss = 0.03044767\n",
      "Iteration 142, loss = 0.03035825\n",
      "Iteration 143, loss = 0.03026964\n",
      "Iteration 144, loss = 0.03018231\n",
      "Iteration 145, loss = 0.03009771\n",
      "Iteration 146, loss = 0.03001275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22509318\n",
      "Iteration 2, loss = 0.07365742\n",
      "Iteration 3, loss = 0.02736806\n",
      "Iteration 4, loss = 0.01527586\n",
      "Iteration 5, loss = 0.10433395\n",
      "Iteration 6, loss = 0.19146198\n",
      "Iteration 7, loss = 0.08263982\n",
      "Iteration 8, loss = 0.04394645\n",
      "Iteration 9, loss = 0.03018065\n",
      "Iteration 10, loss = 0.02024215\n",
      "Iteration 11, loss = 0.01617686\n",
      "Iteration 12, loss = 0.01360527\n",
      "Iteration 13, loss = 0.01499640\n",
      "Iteration 14, loss = 0.02194460\n",
      "Iteration 15, loss = 0.01520201\n",
      "Iteration 16, loss = 0.01319896\n",
      "Iteration 17, loss = 0.01151813\n",
      "Iteration 18, loss = 0.01151781\n",
      "Iteration 19, loss = 0.01444124\n",
      "Iteration 20, loss = 0.02560445\n",
      "Iteration 21, loss = 0.01762906\n",
      "Iteration 22, loss = 0.01286547\n",
      "Iteration 23, loss = 0.01251502\n",
      "Iteration 24, loss = 0.01267270\n",
      "Iteration 25, loss = 0.02725392\n",
      "Iteration 26, loss = 0.02983726\n",
      "Iteration 27, loss = 0.01456418\n",
      "Iteration 28, loss = 0.01265477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25967436\n",
      "Iteration 2, loss = 0.07571842\n",
      "Iteration 3, loss = 0.02765683\n",
      "Iteration 4, loss = 0.01780245\n",
      "Iteration 5, loss = 0.01367423\n",
      "Iteration 6, loss = 0.01294513\n",
      "Iteration 7, loss = 0.01229477\n",
      "Iteration 8, loss = 0.01719080\n",
      "Iteration 9, loss = 0.06789011\n",
      "Iteration 10, loss = 0.02345758\n",
      "Iteration 11, loss = 0.01297843\n",
      "Iteration 12, loss = 0.01228050\n",
      "Iteration 13, loss = 0.01196759\n",
      "Iteration 14, loss = 0.01228970\n",
      "Iteration 15, loss = 0.01256398\n",
      "Iteration 16, loss = 0.01694027\n",
      "Iteration 17, loss = 0.03221304\n",
      "Iteration 18, loss = 0.03579937\n",
      "Iteration 19, loss = 0.01685133\n",
      "Iteration 20, loss = 0.01145719\n",
      "Iteration 21, loss = 0.01265571\n",
      "Iteration 22, loss = 0.01707975\n",
      "Iteration 23, loss = 0.04505362\n",
      "Iteration 24, loss = 0.06949637\n",
      "Iteration 25, loss = 0.02011391\n",
      "Iteration 26, loss = 0.01220934\n",
      "Iteration 27, loss = 0.01056788\n",
      "Iteration 28, loss = 0.01042754\n",
      "Iteration 29, loss = 0.01048004\n",
      "Iteration 30, loss = 0.01024983\n",
      "Iteration 31, loss = 0.01093661\n",
      "Iteration 32, loss = 0.01091474\n",
      "Iteration 33, loss = 0.02067833\n",
      "Iteration 34, loss = 0.02657242\n",
      "Iteration 35, loss = 0.01827542\n",
      "Iteration 36, loss = 0.01281225\n",
      "Iteration 37, loss = 0.01220362\n",
      "Iteration 38, loss = 0.01282878\n",
      "Iteration 39, loss = 0.01148221\n",
      "Iteration 40, loss = 0.01124420\n",
      "Iteration 41, loss = 0.01072542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28543294\n",
      "Iteration 2, loss = 0.08993758\n",
      "Iteration 3, loss = 0.03430781\n",
      "Iteration 4, loss = 0.02125439\n",
      "Iteration 5, loss = 0.01986559\n",
      "Iteration 6, loss = 0.01762179\n",
      "Iteration 7, loss = 0.02168435\n",
      "Iteration 8, loss = 0.02582265\n",
      "Iteration 9, loss = 0.01967911\n",
      "Iteration 10, loss = 0.01474379\n",
      "Iteration 11, loss = 0.03463284\n",
      "Iteration 12, loss = 0.02626089\n",
      "Iteration 13, loss = 0.01273436\n",
      "Iteration 14, loss = 0.01486486\n",
      "Iteration 15, loss = 0.01901616\n",
      "Iteration 16, loss = 0.02192652\n",
      "Iteration 17, loss = 0.02024861\n",
      "Iteration 18, loss = 0.01537351\n",
      "Iteration 19, loss = 0.01312778\n",
      "Iteration 20, loss = 0.01224868\n",
      "Iteration 21, loss = 0.01355793\n",
      "Iteration 22, loss = 0.01129402\n",
      "Iteration 23, loss = 0.01193526\n",
      "Iteration 24, loss = 0.01451887\n",
      "Iteration 25, loss = 0.01140801\n",
      "Iteration 26, loss = 0.02494801\n",
      "Iteration 27, loss = 0.03102181\n",
      "Iteration 28, loss = 0.02530536\n",
      "Iteration 29, loss = 0.03340028\n",
      "Iteration 30, loss = 0.02835198\n",
      "Iteration 31, loss = 0.01405409\n",
      "Iteration 32, loss = 0.01131349\n",
      "Iteration 33, loss = 0.01351918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30159676\n",
      "Iteration 2, loss = 0.09027430\n",
      "Iteration 3, loss = 0.03647046\n",
      "Iteration 4, loss = 0.02207455\n",
      "Iteration 5, loss = 0.01517173\n",
      "Iteration 6, loss = 0.01321676\n",
      "Iteration 7, loss = 0.01278383\n",
      "Iteration 8, loss = 0.01202925\n",
      "Iteration 9, loss = 0.01225135\n",
      "Iteration 10, loss = 0.01258994\n",
      "Iteration 11, loss = 0.01625199\n",
      "Iteration 12, loss = 0.04468491\n",
      "Iteration 13, loss = 0.02085621\n",
      "Iteration 14, loss = 0.01202764\n",
      "Iteration 15, loss = 0.01097112\n",
      "Iteration 16, loss = 0.03056410\n",
      "Iteration 17, loss = 0.06802411\n",
      "Iteration 18, loss = 0.02265177\n",
      "Iteration 19, loss = 0.01278149\n",
      "Iteration 20, loss = 0.01114790\n",
      "Iteration 21, loss = 0.01066866\n",
      "Iteration 22, loss = 0.01090385\n",
      "Iteration 23, loss = 0.01112866\n",
      "Iteration 24, loss = 0.01177325\n",
      "Iteration 25, loss = 0.01154400\n",
      "Iteration 26, loss = 0.01059712\n",
      "Iteration 27, loss = 0.01181936\n",
      "Iteration 28, loss = 0.01172795\n",
      "Iteration 29, loss = 0.01147335\n",
      "Iteration 30, loss = 0.34286376\n",
      "Iteration 31, loss = 0.32433260\n",
      "Iteration 32, loss = 0.15820015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20778598\n",
      "Iteration 2, loss = 0.06147192\n",
      "Iteration 3, loss = 0.02293519\n",
      "Iteration 4, loss = 0.01639686\n",
      "Iteration 5, loss = 0.02623662\n",
      "Iteration 6, loss = 0.02965082\n",
      "Iteration 7, loss = 0.10518707\n",
      "Iteration 8, loss = 0.07668878\n",
      "Iteration 9, loss = 0.02836116\n",
      "Iteration 10, loss = 0.01649419\n",
      "Iteration 11, loss = 0.01296493\n",
      "Iteration 12, loss = 0.01483322\n",
      "Iteration 13, loss = 0.01740175\n",
      "Iteration 14, loss = 0.01317961\n",
      "Iteration 15, loss = 0.00981495\n",
      "Iteration 16, loss = 0.01083264\n",
      "Iteration 17, loss = 0.01195988\n",
      "Iteration 18, loss = 0.01230747\n",
      "Iteration 19, loss = 0.01112855\n",
      "Iteration 20, loss = 0.01590990\n",
      "Iteration 21, loss = 0.00939439\n",
      "Iteration 22, loss = 0.01095542\n",
      "Iteration 23, loss = 0.01112641\n",
      "Iteration 24, loss = 0.01068795\n",
      "Iteration 25, loss = 0.00974859\n",
      "Iteration 26, loss = 0.01057809\n",
      "Iteration 27, loss = 0.01049880\n",
      "Iteration 28, loss = 0.01077548\n",
      "Iteration 29, loss = 0.01616728\n",
      "Iteration 30, loss = 0.01090771\n",
      "Iteration 31, loss = 0.01127214\n",
      "Iteration 32, loss = 0.01069920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17444945\n",
      "Iteration 2, loss = 0.04430166\n",
      "Iteration 3, loss = 0.03553965\n",
      "Iteration 4, loss = 0.03164654\n",
      "Iteration 5, loss = 0.02929140\n",
      "Iteration 6, loss = 0.02782316\n",
      "Iteration 7, loss = 0.02652138\n",
      "Iteration 8, loss = 0.02570140\n",
      "Iteration 9, loss = 0.02484938\n",
      "Iteration 10, loss = 0.02416115\n",
      "Iteration 11, loss = 0.02349583\n",
      "Iteration 12, loss = 0.02293327\n",
      "Iteration 13, loss = 0.02240396\n",
      "Iteration 14, loss = 0.02189852\n",
      "Iteration 15, loss = 0.02140521\n",
      "Iteration 16, loss = 0.02094495\n",
      "Iteration 17, loss = 0.02049984\n",
      "Iteration 18, loss = 0.02007016\n",
      "Iteration 19, loss = 0.01966405\n",
      "Iteration 20, loss = 0.01928530\n",
      "Iteration 21, loss = 0.01890552\n",
      "Iteration 22, loss = 0.01854411\n",
      "Iteration 23, loss = 0.01819226\n",
      "Iteration 24, loss = 0.01784673\n",
      "Iteration 25, loss = 0.01756255\n",
      "Iteration 26, loss = 0.01723915\n",
      "Iteration 27, loss = 0.01693832\n",
      "Iteration 28, loss = 0.01664317\n",
      "Iteration 29, loss = 0.01635595\n",
      "Iteration 30, loss = 0.01610318\n",
      "Iteration 31, loss = 0.01583308\n",
      "Iteration 32, loss = 0.01560502\n",
      "Iteration 33, loss = 0.01534090\n",
      "Iteration 34, loss = 0.01511957\n",
      "Iteration 35, loss = 0.01488129\n",
      "Iteration 36, loss = 0.01467469\n",
      "Iteration 37, loss = 0.01446736\n",
      "Iteration 38, loss = 0.01424934\n",
      "Iteration 39, loss = 0.01406503\n",
      "Iteration 40, loss = 0.01387919\n",
      "Iteration 41, loss = 0.01369686\n",
      "Iteration 42, loss = 0.01351332\n",
      "Iteration 43, loss = 0.01334530\n",
      "Iteration 44, loss = 0.01318743\n",
      "Iteration 45, loss = 0.01302333\n",
      "Iteration 46, loss = 0.01287481\n",
      "Iteration 47, loss = 0.01273034\n",
      "Iteration 48, loss = 0.01257754\n",
      "Iteration 49, loss = 0.01245524\n",
      "Iteration 50, loss = 0.01232923\n",
      "Iteration 51, loss = 0.01219183\n",
      "Iteration 52, loss = 0.01206054\n",
      "Iteration 53, loss = 0.01194247\n",
      "Iteration 54, loss = 0.01182287\n",
      "Iteration 55, loss = 0.01171675\n",
      "Iteration 56, loss = 0.01161711\n",
      "Iteration 57, loss = 0.01150051\n",
      "Iteration 58, loss = 0.01139612\n",
      "Iteration 59, loss = 0.01130196\n",
      "Iteration 60, loss = 0.01121172\n",
      "Iteration 61, loss = 0.01112186\n",
      "Iteration 62, loss = 0.01103390\n",
      "Iteration 63, loss = 0.01095732\n",
      "Iteration 64, loss = 0.01086087\n",
      "Iteration 65, loss = 0.01078843\n",
      "Iteration 66, loss = 0.01071221\n",
      "Iteration 67, loss = 0.01063251\n",
      "Iteration 68, loss = 0.01057584\n",
      "Iteration 69, loss = 0.01049384\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20710904\n",
      "Iteration 2, loss = 0.04785620\n",
      "Iteration 3, loss = 0.03554100\n",
      "Iteration 4, loss = 0.03165587\n",
      "Iteration 5, loss = 0.02954461\n",
      "Iteration 6, loss = 0.02823241\n",
      "Iteration 7, loss = 0.02718018\n",
      "Iteration 8, loss = 0.02641651\n",
      "Iteration 9, loss = 0.02568632\n",
      "Iteration 10, loss = 0.02503133\n",
      "Iteration 11, loss = 0.02442509\n",
      "Iteration 12, loss = 0.02387847\n",
      "Iteration 13, loss = 0.02335507\n",
      "Iteration 14, loss = 0.02286466\n",
      "Iteration 15, loss = 0.02240795\n",
      "Iteration 16, loss = 0.02196522\n",
      "Iteration 17, loss = 0.02153792\n",
      "Iteration 18, loss = 0.02111517\n",
      "Iteration 19, loss = 0.02071416\n",
      "Iteration 20, loss = 0.02032634\n",
      "Iteration 21, loss = 0.01996809\n",
      "Iteration 22, loss = 0.01959956\n",
      "Iteration 23, loss = 0.01925874\n",
      "Iteration 24, loss = 0.01892393\n",
      "Iteration 25, loss = 0.01863183\n",
      "Iteration 26, loss = 0.01830950\n",
      "Iteration 27, loss = 0.01800816\n",
      "Iteration 28, loss = 0.01772178\n",
      "Iteration 29, loss = 0.01744334\n",
      "Iteration 30, loss = 0.01718870\n",
      "Iteration 31, loss = 0.01691921\n",
      "Iteration 32, loss = 0.01668316\n",
      "Iteration 33, loss = 0.01643639\n",
      "Iteration 34, loss = 0.01620666\n",
      "Iteration 35, loss = 0.01597889\n",
      "Iteration 36, loss = 0.01576259\n",
      "Iteration 37, loss = 0.01556064\n",
      "Iteration 38, loss = 0.01535649\n",
      "Iteration 39, loss = 0.01516535\n",
      "Iteration 40, loss = 0.01497292\n",
      "Iteration 41, loss = 0.01479351\n",
      "Iteration 42, loss = 0.01461751\n",
      "Iteration 43, loss = 0.01445032\n",
      "Iteration 44, loss = 0.01429718\n",
      "Iteration 45, loss = 0.01413343\n",
      "Iteration 46, loss = 0.01398371\n",
      "Iteration 47, loss = 0.01383287\n",
      "Iteration 48, loss = 0.01369170\n",
      "Iteration 49, loss = 0.01356023\n",
      "Iteration 50, loss = 0.01343138\n",
      "Iteration 51, loss = 0.01329266\n",
      "Iteration 52, loss = 0.01317280\n",
      "Iteration 53, loss = 0.01304525\n",
      "Iteration 54, loss = 0.01294157\n",
      "Iteration 55, loss = 0.01283113\n",
      "Iteration 56, loss = 0.01271768\n",
      "Iteration 57, loss = 0.01260960\n",
      "Iteration 58, loss = 0.01251991\n",
      "Iteration 59, loss = 0.01241680\n",
      "Iteration 60, loss = 0.01232306\n",
      "Iteration 61, loss = 0.01223399\n",
      "Iteration 62, loss = 0.01214582\n",
      "Iteration 63, loss = 0.01207187\n",
      "Iteration 64, loss = 0.01197153\n",
      "Iteration 65, loss = 0.01190693\n",
      "Iteration 66, loss = 0.01182164\n",
      "Iteration 67, loss = 0.01175417\n",
      "Iteration 68, loss = 0.01168178\n",
      "Iteration 69, loss = 0.01160762\n",
      "Iteration 70, loss = 0.01154774\n",
      "Iteration 71, loss = 0.01148471\n",
      "Iteration 72, loss = 0.01141367\n",
      "Iteration 73, loss = 0.01135786\n",
      "Iteration 74, loss = 0.01129939\n",
      "Iteration 75, loss = 0.01124789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17787666\n",
      "Iteration 2, loss = 0.04346487\n",
      "Iteration 3, loss = 0.03714754\n",
      "Iteration 4, loss = 0.03321908\n",
      "Iteration 5, loss = 0.03058013\n",
      "Iteration 6, loss = 0.02870988\n",
      "Iteration 7, loss = 0.02743605\n",
      "Iteration 8, loss = 0.02640725\n",
      "Iteration 9, loss = 0.02556981\n",
      "Iteration 10, loss = 0.02481341\n",
      "Iteration 11, loss = 0.02410182\n",
      "Iteration 12, loss = 0.02347508\n",
      "Iteration 13, loss = 0.02295417\n",
      "Iteration 14, loss = 0.02240254\n",
      "Iteration 15, loss = 0.02190250\n",
      "Iteration 16, loss = 0.02142186\n",
      "Iteration 17, loss = 0.02108314\n",
      "Iteration 18, loss = 0.02054769\n",
      "Iteration 19, loss = 0.02011802\n",
      "Iteration 20, loss = 0.01975823\n",
      "Iteration 21, loss = 0.01940930\n",
      "Iteration 22, loss = 0.01897849\n",
      "Iteration 23, loss = 0.01862210\n",
      "Iteration 24, loss = 0.01831279\n",
      "Iteration 25, loss = 0.01799334\n",
      "Iteration 26, loss = 0.01770542\n",
      "Iteration 27, loss = 0.01735594\n",
      "Iteration 28, loss = 0.01715955\n",
      "Iteration 29, loss = 0.01679801\n",
      "Iteration 30, loss = 0.01652032\n",
      "Iteration 31, loss = 0.01624912\n",
      "Iteration 32, loss = 0.01600902\n",
      "Iteration 33, loss = 0.01578103\n",
      "Iteration 34, loss = 0.01554650\n",
      "Iteration 35, loss = 0.01532423\n",
      "Iteration 36, loss = 0.01508650\n",
      "Iteration 37, loss = 0.01488215\n",
      "Iteration 38, loss = 0.01468022\n",
      "Iteration 39, loss = 0.01449731\n",
      "Iteration 40, loss = 0.01429826\n",
      "Iteration 41, loss = 0.01411209\n",
      "Iteration 42, loss = 0.01394609\n",
      "Iteration 43, loss = 0.01378034\n",
      "Iteration 44, loss = 0.01359575\n",
      "Iteration 45, loss = 0.01344561\n",
      "Iteration 46, loss = 0.01330927\n",
      "Iteration 47, loss = 0.01315098\n",
      "Iteration 48, loss = 0.01301566\n",
      "Iteration 49, loss = 0.01286666\n",
      "Iteration 50, loss = 0.01273452\n",
      "Iteration 51, loss = 0.01260019\n",
      "Iteration 52, loss = 0.01247717\n",
      "Iteration 53, loss = 0.01236487\n",
      "Iteration 54, loss = 0.01223540\n",
      "Iteration 55, loss = 0.01217796\n",
      "Iteration 56, loss = 0.01204631\n",
      "Iteration 57, loss = 0.01191115\n",
      "Iteration 58, loss = 0.01181248\n",
      "Iteration 59, loss = 0.01171371\n",
      "Iteration 60, loss = 0.01161809\n",
      "Iteration 61, loss = 0.01154991\n",
      "Iteration 62, loss = 0.01144642\n",
      "Iteration 63, loss = 0.01136812\n",
      "Iteration 64, loss = 0.01128309\n",
      "Iteration 65, loss = 0.01118761\n",
      "Iteration 66, loss = 0.01113618\n",
      "Iteration 67, loss = 0.01107358\n",
      "Iteration 68, loss = 0.01096973\n",
      "Iteration 69, loss = 0.01090886\n",
      "Iteration 70, loss = 0.01084656\n",
      "Iteration 71, loss = 0.01077116\n",
      "Iteration 72, loss = 0.01072056\n",
      "Iteration 73, loss = 0.01066035\n",
      "Iteration 74, loss = 0.01060218\n",
      "Iteration 75, loss = 0.01057800\n",
      "Iteration 76, loss = 0.01048934\n",
      "Iteration 77, loss = 0.01043779\n",
      "Iteration 78, loss = 0.01037084\n",
      "Iteration 79, loss = 0.01034138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20003787\n",
      "Iteration 2, loss = 0.04570848\n",
      "Iteration 3, loss = 0.03534470\n",
      "Iteration 4, loss = 0.03153514\n",
      "Iteration 5, loss = 0.02958337\n",
      "Iteration 6, loss = 0.02835731\n",
      "Iteration 7, loss = 0.02729484\n",
      "Iteration 8, loss = 0.02652323\n",
      "Iteration 9, loss = 0.02583570\n",
      "Iteration 10, loss = 0.02517560\n",
      "Iteration 11, loss = 0.02459276\n",
      "Iteration 12, loss = 0.02404331\n",
      "Iteration 13, loss = 0.02357154\n",
      "Iteration 14, loss = 0.02302676\n",
      "Iteration 15, loss = 0.02256820\n",
      "Iteration 16, loss = 0.02210583\n",
      "Iteration 17, loss = 0.02172026\n",
      "Iteration 18, loss = 0.02125902\n",
      "Iteration 19, loss = 0.02086556\n",
      "Iteration 20, loss = 0.02047929\n",
      "Iteration 21, loss = 0.02010244\n",
      "Iteration 22, loss = 0.01974639\n",
      "Iteration 23, loss = 0.01939356\n",
      "Iteration 24, loss = 0.01906322\n",
      "Iteration 25, loss = 0.01874061\n",
      "Iteration 26, loss = 0.01842960\n",
      "Iteration 27, loss = 0.01814279\n",
      "Iteration 28, loss = 0.01785731\n",
      "Iteration 29, loss = 0.01757493\n",
      "Iteration 30, loss = 0.01729584\n",
      "Iteration 31, loss = 0.01703855\n",
      "Iteration 32, loss = 0.01678690\n",
      "Iteration 33, loss = 0.01654631\n",
      "Iteration 34, loss = 0.01631635\n",
      "Iteration 35, loss = 0.01609399\n",
      "Iteration 36, loss = 0.01587280\n",
      "Iteration 37, loss = 0.01566461\n",
      "Iteration 38, loss = 0.01546351\n",
      "Iteration 39, loss = 0.01526907\n",
      "Iteration 40, loss = 0.01507805\n",
      "Iteration 41, loss = 0.01489865\n",
      "Iteration 42, loss = 0.01472047\n",
      "Iteration 43, loss = 0.01455670\n",
      "Iteration 44, loss = 0.01438771\n",
      "Iteration 45, loss = 0.01422621\n",
      "Iteration 46, loss = 0.01407598\n",
      "Iteration 47, loss = 0.01393622\n",
      "Iteration 48, loss = 0.01379517\n",
      "Iteration 49, loss = 0.01364504\n",
      "Iteration 50, loss = 0.01350865\n",
      "Iteration 51, loss = 0.01339424\n",
      "Iteration 52, loss = 0.01326493\n",
      "Iteration 53, loss = 0.01313711\n",
      "Iteration 54, loss = 0.01302607\n",
      "Iteration 55, loss = 0.01290755\n",
      "Iteration 56, loss = 0.01280592\n",
      "Iteration 57, loss = 0.01269739\n",
      "Iteration 58, loss = 0.01259676\n",
      "Iteration 59, loss = 0.01250705\n",
      "Iteration 60, loss = 0.01241672\n",
      "Iteration 61, loss = 0.01231217\n",
      "Iteration 62, loss = 0.01222982\n",
      "Iteration 63, loss = 0.01214103\n",
      "Iteration 64, loss = 0.01206066\n",
      "Iteration 65, loss = 0.01197736\n",
      "Iteration 66, loss = 0.01190837\n",
      "Iteration 67, loss = 0.01183614\n",
      "Iteration 68, loss = 0.01175951\n",
      "Iteration 69, loss = 0.01169588\n",
      "Iteration 70, loss = 0.01162026\n",
      "Iteration 71, loss = 0.01156146\n",
      "Iteration 72, loss = 0.01149684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16320723\n",
      "Iteration 2, loss = 0.03274048\n",
      "Iteration 3, loss = 0.02930452\n",
      "Iteration 4, loss = 0.02800142\n",
      "Iteration 5, loss = 0.02696341\n",
      "Iteration 6, loss = 0.02595466\n",
      "Iteration 7, loss = 0.02533789\n",
      "Iteration 8, loss = 0.02448162\n",
      "Iteration 9, loss = 0.02380686\n",
      "Iteration 10, loss = 0.02321521\n",
      "Iteration 11, loss = 0.02262920\n",
      "Iteration 12, loss = 0.02208423\n",
      "Iteration 13, loss = 0.02156386\n",
      "Iteration 14, loss = 0.02106546\n",
      "Iteration 15, loss = 0.02058685\n",
      "Iteration 16, loss = 0.02013470\n",
      "Iteration 17, loss = 0.01967491\n",
      "Iteration 18, loss = 0.01928587\n",
      "Iteration 19, loss = 0.01886537\n",
      "Iteration 20, loss = 0.01849118\n",
      "Iteration 21, loss = 0.01809560\n",
      "Iteration 22, loss = 0.01773922\n",
      "Iteration 23, loss = 0.01739111\n",
      "Iteration 24, loss = 0.01706779\n",
      "Iteration 25, loss = 0.01674307\n",
      "Iteration 26, loss = 0.01642390\n",
      "Iteration 27, loss = 0.01612986\n",
      "Iteration 28, loss = 0.01583897\n",
      "Iteration 29, loss = 0.01557676\n",
      "Iteration 30, loss = 0.01529706\n",
      "Iteration 31, loss = 0.01503088\n",
      "Iteration 32, loss = 0.01477980\n",
      "Iteration 33, loss = 0.01453814\n",
      "Iteration 34, loss = 0.01430884\n",
      "Iteration 35, loss = 0.01407759\n",
      "Iteration 36, loss = 0.01387432\n",
      "Iteration 37, loss = 0.01367458\n",
      "Iteration 38, loss = 0.01344518\n",
      "Iteration 39, loss = 0.01325729\n",
      "Iteration 40, loss = 0.01306170\n",
      "Iteration 41, loss = 0.01289068\n",
      "Iteration 42, loss = 0.01270393\n",
      "Iteration 43, loss = 0.01254053\n",
      "Iteration 44, loss = 0.01237427\n",
      "Iteration 45, loss = 0.01221690\n",
      "Iteration 46, loss = 0.01206128\n",
      "Iteration 47, loss = 0.01192321\n",
      "Iteration 48, loss = 0.01177763\n",
      "Iteration 49, loss = 0.01162476\n",
      "Iteration 50, loss = 0.01150989\n",
      "Iteration 51, loss = 0.01136764\n",
      "Iteration 52, loss = 0.01124653\n",
      "Iteration 53, loss = 0.01113258\n",
      "Iteration 54, loss = 0.01100875\n",
      "Iteration 55, loss = 0.01091200\n",
      "Iteration 56, loss = 0.01079620\n",
      "Iteration 57, loss = 0.01068782\n",
      "Iteration 58, loss = 0.01058143\n",
      "Iteration 59, loss = 0.01048562\n",
      "Iteration 60, loss = 0.01039683\n",
      "Iteration 61, loss = 0.01030637\n",
      "Iteration 62, loss = 0.01020611\n",
      "Iteration 63, loss = 0.01012604\n",
      "Iteration 64, loss = 0.01005449\n",
      "Iteration 65, loss = 0.00997198\n",
      "Iteration 66, loss = 0.00989886\n",
      "Iteration 67, loss = 0.00982044\n",
      "Iteration 68, loss = 0.00973494\n",
      "Iteration 69, loss = 0.00968106\n",
      "Iteration 70, loss = 0.00961678\n",
      "Iteration 71, loss = 0.00954073\n",
      "Iteration 72, loss = 0.00948828\n",
      "Iteration 73, loss = 0.00942692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11476789\n",
      "Iteration 2, loss = 0.02859182\n",
      "Iteration 3, loss = 0.02080126\n",
      "Iteration 4, loss = 0.01676280\n",
      "Iteration 5, loss = 0.01416894\n",
      "Iteration 6, loss = 0.01300087\n",
      "Iteration 7, loss = 0.01169307\n",
      "Iteration 8, loss = 0.01092818\n",
      "Iteration 9, loss = 0.01035255\n",
      "Iteration 10, loss = 0.01004726\n",
      "Iteration 11, loss = 0.00960209\n",
      "Iteration 12, loss = 0.00937807\n",
      "Iteration 13, loss = 0.00964103\n",
      "Iteration 14, loss = 0.00918928\n",
      "Iteration 15, loss = 0.00931251\n",
      "Iteration 16, loss = 0.00933311\n",
      "Iteration 17, loss = 0.00927570\n",
      "Iteration 18, loss = 0.00925757\n",
      "Iteration 19, loss = 0.00919097\n",
      "Iteration 20, loss = 0.00906891\n",
      "Iteration 21, loss = 0.00912515\n",
      "Iteration 22, loss = 0.00925987\n",
      "Iteration 23, loss = 0.00940923\n",
      "Iteration 24, loss = 0.00962614\n",
      "Iteration 25, loss = 0.00932951\n",
      "Iteration 26, loss = 0.00893999\n",
      "Iteration 27, loss = 0.00891075\n",
      "Iteration 28, loss = 0.01107975\n",
      "Iteration 29, loss = 0.01184359\n",
      "Iteration 30, loss = 0.00963924\n",
      "Iteration 31, loss = 0.00906432\n",
      "Iteration 32, loss = 0.00886444\n",
      "Iteration 33, loss = 0.00888151\n",
      "Iteration 34, loss = 0.00937159\n",
      "Iteration 35, loss = 0.00954804\n",
      "Iteration 36, loss = 0.00967583\n",
      "Iteration 37, loss = 0.00932003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13891565\n",
      "Iteration 2, loss = 0.03250427\n",
      "Iteration 3, loss = 0.02372231\n",
      "Iteration 4, loss = 0.01932167\n",
      "Iteration 5, loss = 0.01659381\n",
      "Iteration 6, loss = 0.01492385\n",
      "Iteration 7, loss = 0.01358293\n",
      "Iteration 8, loss = 0.01273136\n",
      "Iteration 9, loss = 0.01205807\n",
      "Iteration 10, loss = 0.01144870\n",
      "Iteration 11, loss = 0.01110922\n",
      "Iteration 12, loss = 0.01083074\n",
      "Iteration 13, loss = 0.01067931\n",
      "Iteration 14, loss = 0.01047089\n",
      "Iteration 15, loss = 0.01036553\n",
      "Iteration 16, loss = 0.01029036\n",
      "Iteration 17, loss = 0.01056583\n",
      "Iteration 18, loss = 0.01022971\n",
      "Iteration 19, loss = 0.01013143\n",
      "Iteration 20, loss = 0.01004565\n",
      "Iteration 21, loss = 0.01040546\n",
      "Iteration 22, loss = 0.01002536\n",
      "Iteration 23, loss = 0.01016143\n",
      "Iteration 24, loss = 0.01002075\n",
      "Iteration 25, loss = 0.01085343\n",
      "Iteration 26, loss = 0.01009495\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12344095\n",
      "Iteration 2, loss = 0.03023661\n",
      "Iteration 3, loss = 0.02206666\n",
      "Iteration 4, loss = 0.01788174\n",
      "Iteration 5, loss = 0.01558386\n",
      "Iteration 6, loss = 0.01379769\n",
      "Iteration 7, loss = 0.01259410\n",
      "Iteration 8, loss = 0.01172480\n",
      "Iteration 9, loss = 0.01159340\n",
      "Iteration 10, loss = 0.01084766\n",
      "Iteration 11, loss = 0.01029245\n",
      "Iteration 12, loss = 0.01023300\n",
      "Iteration 13, loss = 0.01029596\n",
      "Iteration 14, loss = 0.01028163\n",
      "Iteration 15, loss = 0.01019102\n",
      "Iteration 16, loss = 0.00977823\n",
      "Iteration 17, loss = 0.01014663\n",
      "Iteration 18, loss = 0.00993985\n",
      "Iteration 19, loss = 0.00992357\n",
      "Iteration 20, loss = 0.01083258\n",
      "Iteration 21, loss = 0.01154342\n",
      "Iteration 22, loss = 0.00998411\n",
      "Iteration 23, loss = 0.00958782\n",
      "Iteration 24, loss = 0.00997555\n",
      "Iteration 25, loss = 0.00942582\n",
      "Iteration 26, loss = 0.01134896\n",
      "Iteration 27, loss = 0.01173937\n",
      "Iteration 28, loss = 0.01144868\n",
      "Iteration 29, loss = 0.00956384\n",
      "Iteration 30, loss = 0.00954058\n",
      "Iteration 31, loss = 0.00982260\n",
      "Iteration 32, loss = 0.00972048\n",
      "Iteration 33, loss = 0.01044620\n",
      "Iteration 34, loss = 0.01072406\n",
      "Iteration 35, loss = 0.01067702\n",
      "Iteration 36, loss = 0.00978208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12771692\n",
      "Iteration 2, loss = 0.02984967\n",
      "Iteration 3, loss = 0.02207976\n",
      "Iteration 4, loss = 0.01785450\n",
      "Iteration 5, loss = 0.01551744\n",
      "Iteration 6, loss = 0.01377098\n",
      "Iteration 7, loss = 0.01291408\n",
      "Iteration 8, loss = 0.01194860\n",
      "Iteration 9, loss = 0.01137142\n",
      "Iteration 10, loss = 0.01105085\n",
      "Iteration 11, loss = 0.01070508\n",
      "Iteration 12, loss = 0.01058296\n",
      "Iteration 13, loss = 0.01058739\n",
      "Iteration 14, loss = 0.01060566\n",
      "Iteration 15, loss = 0.01025889\n",
      "Iteration 16, loss = 0.01022015\n",
      "Iteration 17, loss = 0.01029470\n",
      "Iteration 18, loss = 0.01173786\n",
      "Iteration 19, loss = 0.01065366\n",
      "Iteration 20, loss = 0.01041680\n",
      "Iteration 21, loss = 0.00998219\n",
      "Iteration 22, loss = 0.01023002\n",
      "Iteration 23, loss = 0.01045312\n",
      "Iteration 24, loss = 0.01013289\n",
      "Iteration 25, loss = 0.00994511\n",
      "Iteration 26, loss = 0.01002566\n",
      "Iteration 27, loss = 0.01104775\n",
      "Iteration 28, loss = 0.01126320\n",
      "Iteration 29, loss = 0.01025492\n",
      "Iteration 30, loss = 0.01007751\n",
      "Iteration 31, loss = 0.01043006\n",
      "Iteration 32, loss = 0.01054723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.10009850\n",
      "Iteration 2, loss = 0.02439017\n",
      "Iteration 3, loss = 0.01849049\n",
      "Iteration 4, loss = 0.01526190\n",
      "Iteration 5, loss = 0.01341174\n",
      "Iteration 6, loss = 0.01259413\n",
      "Iteration 7, loss = 0.01120403\n",
      "Iteration 8, loss = 0.01028818\n",
      "Iteration 9, loss = 0.00987362\n",
      "Iteration 10, loss = 0.00943228\n",
      "Iteration 11, loss = 0.00932763\n",
      "Iteration 12, loss = 0.00956996\n",
      "Iteration 13, loss = 0.01222148\n",
      "Iteration 14, loss = 0.00915097\n",
      "Iteration 15, loss = 0.00852592\n",
      "Iteration 16, loss = 0.00868061\n",
      "Iteration 17, loss = 0.00855314\n",
      "Iteration 18, loss = 0.00854250\n",
      "Iteration 19, loss = 0.00868899\n",
      "Iteration 20, loss = 0.00964313\n",
      "Iteration 21, loss = 0.00948134\n",
      "Iteration 22, loss = 0.00854233\n",
      "Iteration 23, loss = 0.00831263\n",
      "Iteration 24, loss = 0.00936139\n",
      "Iteration 25, loss = 0.00896693\n",
      "Iteration 26, loss = 0.00876020\n",
      "Iteration 27, loss = 0.00858288\n",
      "Iteration 28, loss = 0.00880102\n",
      "Iteration 29, loss = 0.00878700\n",
      "Iteration 30, loss = 0.00874435\n",
      "Iteration 31, loss = 0.00904608\n",
      "Iteration 32, loss = 0.00836645\n",
      "Iteration 33, loss = 0.00858118\n",
      "Iteration 34, loss = 0.00813963\n",
      "Iteration 35, loss = 0.00850538\n",
      "Iteration 36, loss = 0.00889301\n",
      "Iteration 37, loss = 0.00859489\n",
      "Iteration 38, loss = 0.00922083\n",
      "Iteration 39, loss = 0.00918626\n",
      "Iteration 40, loss = 0.00833453\n",
      "Iteration 41, loss = 0.00878793\n",
      "Iteration 42, loss = 0.00828297\n",
      "Iteration 43, loss = 0.00905803\n",
      "Iteration 44, loss = 0.00907197\n",
      "Iteration 45, loss = 0.00843174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45348614\n",
      "Iteration 2, loss = 0.16341740\n",
      "Iteration 3, loss = 0.10614463\n",
      "Iteration 4, loss = 0.08487604\n",
      "Iteration 5, loss = 0.07319577\n",
      "Iteration 6, loss = 0.06505250\n",
      "Iteration 7, loss = 0.05927463\n",
      "Iteration 8, loss = 0.05500204\n",
      "Iteration 9, loss = 0.05170400\n",
      "Iteration 10, loss = 0.04911637\n",
      "Iteration 11, loss = 0.04686475\n",
      "Iteration 12, loss = 0.04506681\n",
      "Iteration 13, loss = 0.04352532\n",
      "Iteration 14, loss = 0.04218405\n",
      "Iteration 15, loss = 0.04100844\n",
      "Iteration 16, loss = 0.03996643\n",
      "Iteration 17, loss = 0.03901250\n",
      "Iteration 18, loss = 0.03813263\n",
      "Iteration 19, loss = 0.03736996\n",
      "Iteration 20, loss = 0.03668958\n",
      "Iteration 21, loss = 0.03601714\n",
      "Iteration 22, loss = 0.03538516\n",
      "Iteration 23, loss = 0.03483230\n",
      "Iteration 24, loss = 0.03433060\n",
      "Iteration 25, loss = 0.03386782\n",
      "Iteration 26, loss = 0.03337562\n",
      "Iteration 27, loss = 0.03291989\n",
      "Iteration 28, loss = 0.03256430\n",
      "Iteration 29, loss = 0.03213171\n",
      "Iteration 30, loss = 0.03178617\n",
      "Iteration 31, loss = 0.03142790\n",
      "Iteration 32, loss = 0.03117418\n",
      "Iteration 33, loss = 0.03083344\n",
      "Iteration 34, loss = 0.03056184\n",
      "Iteration 35, loss = 0.03025571\n",
      "Iteration 36, loss = 0.03002057\n",
      "Iteration 37, loss = 0.02978488\n",
      "Iteration 38, loss = 0.02953396\n",
      "Iteration 39, loss = 0.02932434\n",
      "Iteration 40, loss = 0.02913329\n",
      "Iteration 41, loss = 0.02895308\n",
      "Iteration 42, loss = 0.02870175\n",
      "Iteration 43, loss = 0.02851483\n",
      "Iteration 44, loss = 0.02833979\n",
      "Iteration 45, loss = 0.02816948\n",
      "Iteration 46, loss = 0.02801585\n",
      "Iteration 47, loss = 0.02785213\n",
      "Iteration 48, loss = 0.02769346\n",
      "Iteration 49, loss = 0.02755891\n",
      "Iteration 50, loss = 0.02741091\n",
      "Iteration 51, loss = 0.02726853\n",
      "Iteration 52, loss = 0.02713052\n",
      "Iteration 53, loss = 0.02700428\n",
      "Iteration 54, loss = 0.02687605\n",
      "Iteration 55, loss = 0.02675711\n",
      "Iteration 56, loss = 0.02664516\n",
      "Iteration 57, loss = 0.02651624\n",
      "Iteration 58, loss = 0.02641059\n",
      "Iteration 59, loss = 0.02629471\n",
      "Iteration 60, loss = 0.02619566\n",
      "Iteration 61, loss = 0.02610254\n",
      "Iteration 62, loss = 0.02598844\n",
      "Iteration 63, loss = 0.02589376\n",
      "Iteration 64, loss = 0.02578845\n",
      "Iteration 65, loss = 0.02569648\n",
      "Iteration 66, loss = 0.02560216\n",
      "Iteration 67, loss = 0.02550758\n",
      "Iteration 68, loss = 0.02542713\n",
      "Iteration 69, loss = 0.02534248\n",
      "Iteration 70, loss = 0.02525276\n",
      "Iteration 71, loss = 0.02516001\n",
      "Iteration 72, loss = 0.02508082\n",
      "Iteration 73, loss = 0.02500281\n",
      "Iteration 74, loss = 0.02491604\n",
      "Iteration 75, loss = 0.02483583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48490824\n",
      "Iteration 2, loss = 0.21600099\n",
      "Iteration 3, loss = 0.14157776\n",
      "Iteration 4, loss = 0.11060822\n",
      "Iteration 5, loss = 0.09310129\n",
      "Iteration 6, loss = 0.08060421\n",
      "Iteration 7, loss = 0.07167160\n",
      "Iteration 8, loss = 0.06493109\n",
      "Iteration 9, loss = 0.05969057\n",
      "Iteration 10, loss = 0.05573518\n",
      "Iteration 11, loss = 0.05237299\n",
      "Iteration 12, loss = 0.04975596\n",
      "Iteration 13, loss = 0.04744999\n",
      "Iteration 14, loss = 0.04556146\n",
      "Iteration 15, loss = 0.04394455\n",
      "Iteration 16, loss = 0.04251559\n",
      "Iteration 17, loss = 0.04127678\n",
      "Iteration 18, loss = 0.04012471\n",
      "Iteration 19, loss = 0.03914107\n",
      "Iteration 20, loss = 0.03825701\n",
      "Iteration 21, loss = 0.03745525\n",
      "Iteration 22, loss = 0.03671564\n",
      "Iteration 23, loss = 0.03605337\n",
      "Iteration 24, loss = 0.03548693\n",
      "Iteration 25, loss = 0.03495706\n",
      "Iteration 26, loss = 0.03436745\n",
      "Iteration 27, loss = 0.03388264\n",
      "Iteration 28, loss = 0.03345345\n",
      "Iteration 29, loss = 0.03303350\n",
      "Iteration 30, loss = 0.03268057\n",
      "Iteration 31, loss = 0.03229837\n",
      "Iteration 32, loss = 0.03198702\n",
      "Iteration 33, loss = 0.03166982\n",
      "Iteration 34, loss = 0.03138966\n",
      "Iteration 35, loss = 0.03109559\n",
      "Iteration 36, loss = 0.03083107\n",
      "Iteration 37, loss = 0.03059528\n",
      "Iteration 38, loss = 0.03035793\n",
      "Iteration 39, loss = 0.03014857\n",
      "Iteration 40, loss = 0.02992931\n",
      "Iteration 41, loss = 0.02976107\n",
      "Iteration 42, loss = 0.02953829\n",
      "Iteration 43, loss = 0.02935687\n",
      "Iteration 44, loss = 0.02918592\n",
      "Iteration 45, loss = 0.02901818\n",
      "Iteration 46, loss = 0.02886505\n",
      "Iteration 47, loss = 0.02870321\n",
      "Iteration 48, loss = 0.02855299\n",
      "Iteration 49, loss = 0.02841775\n",
      "Iteration 50, loss = 0.02827699\n",
      "Iteration 51, loss = 0.02814409\n",
      "Iteration 52, loss = 0.02801347\n",
      "Iteration 53, loss = 0.02789300\n",
      "Iteration 54, loss = 0.02777683\n",
      "Iteration 55, loss = 0.02765337\n",
      "Iteration 56, loss = 0.02753970\n",
      "Iteration 57, loss = 0.02742854\n",
      "Iteration 58, loss = 0.02733127\n",
      "Iteration 59, loss = 0.02722013\n",
      "Iteration 60, loss = 0.02712130\n",
      "Iteration 61, loss = 0.02702784\n",
      "Iteration 62, loss = 0.02692648\n",
      "Iteration 63, loss = 0.02683711\n",
      "Iteration 64, loss = 0.02674175\n",
      "Iteration 65, loss = 0.02665056\n",
      "Iteration 66, loss = 0.02656253\n",
      "Iteration 67, loss = 0.02648035\n",
      "Iteration 68, loss = 0.02639117\n",
      "Iteration 69, loss = 0.02631630\n",
      "Iteration 70, loss = 0.02622935\n",
      "Iteration 71, loss = 0.02614820\n",
      "Iteration 72, loss = 0.02606673\n",
      "Iteration 73, loss = 0.02599154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45973771\n",
      "Iteration 2, loss = 0.17369513\n",
      "Iteration 3, loss = 0.11082746\n",
      "Iteration 4, loss = 0.08587683\n",
      "Iteration 5, loss = 0.07231912\n",
      "Iteration 6, loss = 0.06396808\n",
      "Iteration 7, loss = 0.05824283\n",
      "Iteration 8, loss = 0.05437368\n",
      "Iteration 9, loss = 0.05121191\n",
      "Iteration 10, loss = 0.04884043\n",
      "Iteration 11, loss = 0.04690907\n",
      "Iteration 12, loss = 0.04531511\n",
      "Iteration 13, loss = 0.04399097\n",
      "Iteration 14, loss = 0.04281568\n",
      "Iteration 15, loss = 0.04178945\n",
      "Iteration 16, loss = 0.04084126\n",
      "Iteration 17, loss = 0.04003981\n",
      "Iteration 18, loss = 0.03925462\n",
      "Iteration 19, loss = 0.03857337\n",
      "Iteration 20, loss = 0.03797576\n",
      "Iteration 21, loss = 0.03736269\n",
      "Iteration 22, loss = 0.03676512\n",
      "Iteration 23, loss = 0.03623318\n",
      "Iteration 24, loss = 0.03573965\n",
      "Iteration 25, loss = 0.03529658\n",
      "Iteration 26, loss = 0.03483582\n",
      "Iteration 27, loss = 0.03444383\n",
      "Iteration 28, loss = 0.03400966\n",
      "Iteration 29, loss = 0.03360145\n",
      "Iteration 30, loss = 0.03324803\n",
      "Iteration 31, loss = 0.03290524\n",
      "Iteration 32, loss = 0.03254554\n",
      "Iteration 33, loss = 0.03222325\n",
      "Iteration 34, loss = 0.03192912\n",
      "Iteration 35, loss = 0.03163185\n",
      "Iteration 36, loss = 0.03134406\n",
      "Iteration 37, loss = 0.03110512\n",
      "Iteration 38, loss = 0.03083962\n",
      "Iteration 39, loss = 0.03060151\n",
      "Iteration 40, loss = 0.03036662\n",
      "Iteration 41, loss = 0.03013071\n",
      "Iteration 42, loss = 0.02993301\n",
      "Iteration 43, loss = 0.02976812\n",
      "Iteration 44, loss = 0.02951006\n",
      "Iteration 45, loss = 0.02932035\n",
      "Iteration 46, loss = 0.02911598\n",
      "Iteration 47, loss = 0.02895149\n",
      "Iteration 48, loss = 0.02877521\n",
      "Iteration 49, loss = 0.02859702\n",
      "Iteration 50, loss = 0.02843631\n",
      "Iteration 51, loss = 0.02827541\n",
      "Iteration 52, loss = 0.02814360\n",
      "Iteration 53, loss = 0.02799079\n",
      "Iteration 54, loss = 0.02785848\n",
      "Iteration 55, loss = 0.02771894\n",
      "Iteration 56, loss = 0.02758360\n",
      "Iteration 57, loss = 0.02743863\n",
      "Iteration 58, loss = 0.02730922\n",
      "Iteration 59, loss = 0.02719423\n",
      "Iteration 60, loss = 0.02708828\n",
      "Iteration 61, loss = 0.02695918\n",
      "Iteration 62, loss = 0.02684546\n",
      "Iteration 63, loss = 0.02673738\n",
      "Iteration 64, loss = 0.02663149\n",
      "Iteration 65, loss = 0.02654173\n",
      "Iteration 66, loss = 0.02644534\n",
      "Iteration 67, loss = 0.02632482\n",
      "Iteration 68, loss = 0.02621630\n",
      "Iteration 69, loss = 0.02612320\n",
      "Iteration 70, loss = 0.02603561\n",
      "Iteration 71, loss = 0.02593603\n",
      "Iteration 72, loss = 0.02584901\n",
      "Iteration 73, loss = 0.02576492\n",
      "Iteration 74, loss = 0.02567825\n",
      "Iteration 75, loss = 0.02561516\n",
      "Iteration 76, loss = 0.02550468\n",
      "Iteration 77, loss = 0.02542425\n",
      "Iteration 78, loss = 0.02534502\n",
      "Iteration 79, loss = 0.02526558\n",
      "Iteration 80, loss = 0.02518575\n",
      "Iteration 81, loss = 0.02510839\n",
      "Iteration 82, loss = 0.02502700\n",
      "Iteration 83, loss = 0.02495484\n",
      "Iteration 84, loss = 0.02487668\n",
      "Iteration 85, loss = 0.02480368\n",
      "Iteration 86, loss = 0.02472944\n",
      "Iteration 87, loss = 0.02466181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49777662\n",
      "Iteration 2, loss = 0.21266442\n",
      "Iteration 3, loss = 0.13624041\n",
      "Iteration 4, loss = 0.10617752\n",
      "Iteration 5, loss = 0.08909184\n",
      "Iteration 6, loss = 0.07767530\n",
      "Iteration 7, loss = 0.06936328\n",
      "Iteration 8, loss = 0.06325943\n",
      "Iteration 9, loss = 0.05854593\n",
      "Iteration 10, loss = 0.05477552\n",
      "Iteration 11, loss = 0.05171252\n",
      "Iteration 12, loss = 0.04923216\n",
      "Iteration 13, loss = 0.04714934\n",
      "Iteration 14, loss = 0.04521116\n",
      "Iteration 15, loss = 0.04362452\n",
      "Iteration 16, loss = 0.04218652\n",
      "Iteration 17, loss = 0.04100729\n",
      "Iteration 18, loss = 0.03983227\n",
      "Iteration 19, loss = 0.03884715\n",
      "Iteration 20, loss = 0.03798332\n",
      "Iteration 21, loss = 0.03718636\n",
      "Iteration 22, loss = 0.03650878\n",
      "Iteration 23, loss = 0.03581420\n",
      "Iteration 24, loss = 0.03522029\n",
      "Iteration 25, loss = 0.03466154\n",
      "Iteration 26, loss = 0.03416137\n",
      "Iteration 27, loss = 0.03369916\n",
      "Iteration 28, loss = 0.03327056\n",
      "Iteration 29, loss = 0.03285194\n",
      "Iteration 30, loss = 0.03248192\n",
      "Iteration 31, loss = 0.03214640\n",
      "Iteration 32, loss = 0.03182280\n",
      "Iteration 33, loss = 0.03151242\n",
      "Iteration 34, loss = 0.03123305\n",
      "Iteration 35, loss = 0.03096948\n",
      "Iteration 36, loss = 0.03072055\n",
      "Iteration 37, loss = 0.03048458\n",
      "Iteration 38, loss = 0.03026068\n",
      "Iteration 39, loss = 0.03004989\n",
      "Iteration 40, loss = 0.02985196\n",
      "Iteration 41, loss = 0.02967227\n",
      "Iteration 42, loss = 0.02947904\n",
      "Iteration 43, loss = 0.02932274\n",
      "Iteration 44, loss = 0.02913613\n",
      "Iteration 45, loss = 0.02897877\n",
      "Iteration 46, loss = 0.02882598\n",
      "Iteration 47, loss = 0.02867978\n",
      "Iteration 48, loss = 0.02854254\n",
      "Iteration 49, loss = 0.02839629\n",
      "Iteration 50, loss = 0.02826476\n",
      "Iteration 51, loss = 0.02814941\n",
      "Iteration 52, loss = 0.02801394\n",
      "Iteration 53, loss = 0.02789166\n",
      "Iteration 54, loss = 0.02777780\n",
      "Iteration 55, loss = 0.02766233\n",
      "Iteration 56, loss = 0.02755753\n",
      "Iteration 57, loss = 0.02744957\n",
      "Iteration 58, loss = 0.02735074\n",
      "Iteration 59, loss = 0.02725364\n",
      "Iteration 60, loss = 0.02714961\n",
      "Iteration 61, loss = 0.02705841\n",
      "Iteration 62, loss = 0.02695851\n",
      "Iteration 63, loss = 0.02686745\n",
      "Iteration 64, loss = 0.02677990\n",
      "Iteration 65, loss = 0.02668680\n",
      "Iteration 66, loss = 0.02660640\n",
      "Iteration 67, loss = 0.02651993\n",
      "Iteration 68, loss = 0.02643711\n",
      "Iteration 69, loss = 0.02635868\n",
      "Iteration 70, loss = 0.02627549\n",
      "Iteration 71, loss = 0.02619525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45792600\n",
      "Iteration 2, loss = 0.16255525\n",
      "Iteration 3, loss = 0.09704393\n",
      "Iteration 4, loss = 0.07277319\n",
      "Iteration 5, loss = 0.06033903\n",
      "Iteration 6, loss = 0.05270200\n",
      "Iteration 7, loss = 0.04765962\n",
      "Iteration 8, loss = 0.04391349\n",
      "Iteration 9, loss = 0.04116803\n",
      "Iteration 10, loss = 0.03908365\n",
      "Iteration 11, loss = 0.03742292\n",
      "Iteration 12, loss = 0.03609773\n",
      "Iteration 13, loss = 0.03501455\n",
      "Iteration 14, loss = 0.03410620\n",
      "Iteration 15, loss = 0.03333848\n",
      "Iteration 16, loss = 0.03265307\n",
      "Iteration 17, loss = 0.03207595\n",
      "Iteration 18, loss = 0.03158610\n",
      "Iteration 19, loss = 0.03111781\n",
      "Iteration 20, loss = 0.03073724\n",
      "Iteration 21, loss = 0.03037307\n",
      "Iteration 22, loss = 0.03003705\n",
      "Iteration 23, loss = 0.02974079\n",
      "Iteration 24, loss = 0.02947996\n",
      "Iteration 25, loss = 0.02922440\n",
      "Iteration 26, loss = 0.02899178\n",
      "Iteration 27, loss = 0.02877858\n",
      "Iteration 28, loss = 0.02857867\n",
      "Iteration 29, loss = 0.02840185\n",
      "Iteration 30, loss = 0.02820919\n",
      "Iteration 31, loss = 0.02803672\n",
      "Iteration 32, loss = 0.02787416\n",
      "Iteration 33, loss = 0.02771892\n",
      "Iteration 34, loss = 0.02757366\n",
      "Iteration 35, loss = 0.02743136\n",
      "Iteration 36, loss = 0.02730488\n",
      "Iteration 37, loss = 0.02717809\n",
      "Iteration 38, loss = 0.02704026\n",
      "Iteration 39, loss = 0.02692957\n",
      "Iteration 40, loss = 0.02679690\n",
      "Iteration 41, loss = 0.02668578\n",
      "Iteration 42, loss = 0.02656813\n",
      "Iteration 43, loss = 0.02646988\n",
      "Iteration 44, loss = 0.02635827\n",
      "Iteration 45, loss = 0.02625408\n",
      "Iteration 46, loss = 0.02616217\n",
      "Iteration 47, loss = 0.02605664\n",
      "Iteration 48, loss = 0.02596442\n",
      "Iteration 49, loss = 0.02586920\n",
      "Iteration 50, loss = 0.02577854\n",
      "Iteration 51, loss = 0.02568173\n",
      "Iteration 52, loss = 0.02559455\n",
      "Iteration 53, loss = 0.02551258\n",
      "Iteration 54, loss = 0.02542478\n",
      "Iteration 55, loss = 0.02533673\n",
      "Iteration 56, loss = 0.02525899\n",
      "Iteration 57, loss = 0.02517067\n",
      "Iteration 58, loss = 0.02509095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43771148\n",
      "Iteration 2, loss = 0.16093050\n",
      "Iteration 3, loss = 0.08815451\n",
      "Iteration 4, loss = 0.06060946\n",
      "Iteration 5, loss = 0.04762593\n",
      "Iteration 6, loss = 0.04025184\n",
      "Iteration 7, loss = 0.03552602\n",
      "Iteration 8, loss = 0.03210989\n",
      "Iteration 9, loss = 0.02939698\n",
      "Iteration 10, loss = 0.02735680\n",
      "Iteration 11, loss = 0.02562577\n",
      "Iteration 12, loss = 0.02427156\n",
      "Iteration 13, loss = 0.02313114\n",
      "Iteration 14, loss = 0.02213363\n",
      "Iteration 15, loss = 0.02125381\n",
      "Iteration 16, loss = 0.02047237\n",
      "Iteration 17, loss = 0.01975681\n",
      "Iteration 18, loss = 0.01912076\n",
      "Iteration 19, loss = 0.01853810\n",
      "Iteration 20, loss = 0.01801100\n",
      "Iteration 21, loss = 0.01750015\n",
      "Iteration 22, loss = 0.01703451\n",
      "Iteration 23, loss = 0.01658364\n",
      "Iteration 24, loss = 0.01616358\n",
      "Iteration 25, loss = 0.01585296\n",
      "Iteration 26, loss = 0.01546966\n",
      "Iteration 27, loss = 0.01511439\n",
      "Iteration 28, loss = 0.01479819\n",
      "Iteration 29, loss = 0.01446019\n",
      "Iteration 30, loss = 0.01420673\n",
      "Iteration 31, loss = 0.01391394\n",
      "Iteration 32, loss = 0.01370220\n",
      "Iteration 33, loss = 0.01341844\n",
      "Iteration 34, loss = 0.01321342\n",
      "Iteration 35, loss = 0.01296466\n",
      "Iteration 36, loss = 0.01278417\n",
      "Iteration 37, loss = 0.01258125\n",
      "Iteration 38, loss = 0.01236184\n",
      "Iteration 39, loss = 0.01220606\n",
      "Iteration 40, loss = 0.01203747\n",
      "Iteration 41, loss = 0.01189352\n",
      "Iteration 42, loss = 0.01170427\n",
      "Iteration 43, loss = 0.01156608\n",
      "Iteration 44, loss = 0.01143808\n",
      "Iteration 45, loss = 0.01129498\n",
      "Iteration 46, loss = 0.01117749\n",
      "Iteration 47, loss = 0.01107235\n",
      "Iteration 48, loss = 0.01092523\n",
      "Iteration 49, loss = 0.01084786\n",
      "Iteration 50, loss = 0.01077650\n",
      "Iteration 51, loss = 0.01065382\n",
      "Iteration 52, loss = 0.01055422\n",
      "Iteration 53, loss = 0.01045355\n",
      "Iteration 54, loss = 0.01036592\n",
      "Iteration 55, loss = 0.01028045\n",
      "Iteration 56, loss = 0.01020838\n",
      "Iteration 57, loss = 0.01012989\n",
      "Iteration 58, loss = 0.01003980\n",
      "Iteration 59, loss = 0.00997607\n",
      "Iteration 60, loss = 0.00992016\n",
      "Iteration 61, loss = 0.00985958\n",
      "Iteration 62, loss = 0.00979537\n",
      "Iteration 63, loss = 0.00977334\n",
      "Iteration 64, loss = 0.00967487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45847191\n",
      "Iteration 2, loss = 0.19291636\n",
      "Iteration 3, loss = 0.11265165\n",
      "Iteration 4, loss = 0.07775661\n",
      "Iteration 5, loss = 0.05932664\n",
      "Iteration 6, loss = 0.04835430\n",
      "Iteration 7, loss = 0.04150510\n",
      "Iteration 8, loss = 0.03689667\n",
      "Iteration 9, loss = 0.03342795\n",
      "Iteration 10, loss = 0.03087040\n",
      "Iteration 11, loss = 0.02882002\n",
      "Iteration 12, loss = 0.02720222\n",
      "Iteration 13, loss = 0.02585098\n",
      "Iteration 14, loss = 0.02472192\n",
      "Iteration 15, loss = 0.02376433\n",
      "Iteration 16, loss = 0.02289978\n",
      "Iteration 17, loss = 0.02211742\n",
      "Iteration 18, loss = 0.02139494\n",
      "Iteration 19, loss = 0.02074142\n",
      "Iteration 20, loss = 0.02013390\n",
      "Iteration 21, loss = 0.01958368\n",
      "Iteration 22, loss = 0.01905845\n",
      "Iteration 23, loss = 0.01857391\n",
      "Iteration 24, loss = 0.01811707\n",
      "Iteration 25, loss = 0.01773035\n",
      "Iteration 26, loss = 0.01729870\n",
      "Iteration 27, loss = 0.01690293\n",
      "Iteration 28, loss = 0.01654474\n",
      "Iteration 29, loss = 0.01620275\n",
      "Iteration 30, loss = 0.01590046\n",
      "Iteration 31, loss = 0.01557449\n",
      "Iteration 32, loss = 0.01531240\n",
      "Iteration 33, loss = 0.01502527\n",
      "Iteration 34, loss = 0.01477970\n",
      "Iteration 35, loss = 0.01451524\n",
      "Iteration 36, loss = 0.01428212\n",
      "Iteration 37, loss = 0.01406999\n",
      "Iteration 38, loss = 0.01385877\n",
      "Iteration 39, loss = 0.01366422\n",
      "Iteration 40, loss = 0.01346398\n",
      "Iteration 41, loss = 0.01328830\n",
      "Iteration 42, loss = 0.01312133\n",
      "Iteration 43, loss = 0.01294310\n",
      "Iteration 44, loss = 0.01281756\n",
      "Iteration 45, loss = 0.01263760\n",
      "Iteration 46, loss = 0.01250766\n",
      "Iteration 47, loss = 0.01236764\n",
      "Iteration 48, loss = 0.01223798\n",
      "Iteration 49, loss = 0.01211976\n",
      "Iteration 50, loss = 0.01202975\n",
      "Iteration 51, loss = 0.01188674\n",
      "Iteration 52, loss = 0.01178113\n",
      "Iteration 53, loss = 0.01167014\n",
      "Iteration 54, loss = 0.01158990\n",
      "Iteration 55, loss = 0.01150508\n",
      "Iteration 56, loss = 0.01141221\n",
      "Iteration 57, loss = 0.01131504\n",
      "Iteration 58, loss = 0.01127279\n",
      "Iteration 59, loss = 0.01116392\n",
      "Iteration 60, loss = 0.01109959\n",
      "Iteration 61, loss = 0.01103452\n",
      "Iteration 62, loss = 0.01097438\n",
      "Iteration 63, loss = 0.01093681\n",
      "Iteration 64, loss = 0.01083506\n",
      "Iteration 65, loss = 0.01081013\n",
      "Iteration 66, loss = 0.01073475\n",
      "Iteration 67, loss = 0.01068468\n",
      "Iteration 68, loss = 0.01065873\n",
      "Iteration 69, loss = 0.01059631\n",
      "Iteration 70, loss = 0.01055737\n",
      "Iteration 71, loss = 0.01053644\n",
      "Iteration 72, loss = 0.01047277\n",
      "Iteration 73, loss = 0.01043505\n",
      "Iteration 74, loss = 0.01040424\n",
      "Iteration 75, loss = 0.01036957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43394389\n",
      "Iteration 2, loss = 0.16342505\n",
      "Iteration 3, loss = 0.09445276\n",
      "Iteration 4, loss = 0.06643647\n",
      "Iteration 5, loss = 0.05160378\n",
      "Iteration 6, loss = 0.04307603\n",
      "Iteration 7, loss = 0.03756252\n",
      "Iteration 8, loss = 0.03398250\n",
      "Iteration 9, loss = 0.03099350\n",
      "Iteration 10, loss = 0.02878758\n",
      "Iteration 11, loss = 0.02706651\n",
      "Iteration 12, loss = 0.02556863\n",
      "Iteration 13, loss = 0.02437096\n",
      "Iteration 14, loss = 0.02329498\n",
      "Iteration 15, loss = 0.02237585\n",
      "Iteration 16, loss = 0.02152789\n",
      "Iteration 17, loss = 0.02083870\n",
      "Iteration 18, loss = 0.02008160\n",
      "Iteration 19, loss = 0.01945056\n",
      "Iteration 20, loss = 0.01893803\n",
      "Iteration 21, loss = 0.01844196\n",
      "Iteration 22, loss = 0.01784904\n",
      "Iteration 23, loss = 0.01740712\n",
      "Iteration 24, loss = 0.01698109\n",
      "Iteration 25, loss = 0.01662318\n",
      "Iteration 26, loss = 0.01623752\n",
      "Iteration 27, loss = 0.01585363\n",
      "Iteration 28, loss = 0.01558934\n",
      "Iteration 29, loss = 0.01517928\n",
      "Iteration 30, loss = 0.01487149\n",
      "Iteration 31, loss = 0.01457296\n",
      "Iteration 32, loss = 0.01430154\n",
      "Iteration 33, loss = 0.01406622\n",
      "Iteration 34, loss = 0.01382287\n",
      "Iteration 35, loss = 0.01360910\n",
      "Iteration 36, loss = 0.01335083\n",
      "Iteration 37, loss = 0.01315866\n",
      "Iteration 38, loss = 0.01296872\n",
      "Iteration 39, loss = 0.01278292\n",
      "Iteration 40, loss = 0.01260151\n",
      "Iteration 41, loss = 0.01242524\n",
      "Iteration 42, loss = 0.01227435\n",
      "Iteration 43, loss = 0.01212649\n",
      "Iteration 44, loss = 0.01194587\n",
      "Iteration 45, loss = 0.01181657\n",
      "Iteration 46, loss = 0.01171167\n",
      "Iteration 47, loss = 0.01156994\n",
      "Iteration 48, loss = 0.01145286\n",
      "Iteration 49, loss = 0.01132697\n",
      "Iteration 50, loss = 0.01120714\n",
      "Iteration 51, loss = 0.01110333\n",
      "Iteration 52, loss = 0.01100938\n",
      "Iteration 53, loss = 0.01091326\n",
      "Iteration 54, loss = 0.01083025\n",
      "Iteration 55, loss = 0.01080940\n",
      "Iteration 56, loss = 0.01069885\n",
      "Iteration 57, loss = 0.01056192\n",
      "Iteration 58, loss = 0.01048912\n",
      "Iteration 59, loss = 0.01041617\n",
      "Iteration 60, loss = 0.01035089\n",
      "Iteration 61, loss = 0.01030987\n",
      "Iteration 62, loss = 0.01022965\n",
      "Iteration 63, loss = 0.01018060\n",
      "Iteration 64, loss = 0.01011546\n",
      "Iteration 65, loss = 0.01003741\n",
      "Iteration 66, loss = 0.01003266\n",
      "Iteration 67, loss = 0.01001071\n",
      "Iteration 68, loss = 0.00991468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45706611\n",
      "Iteration 2, loss = 0.19479444\n",
      "Iteration 3, loss = 0.11089500\n",
      "Iteration 4, loss = 0.07496105\n",
      "Iteration 5, loss = 0.05642502\n",
      "Iteration 6, loss = 0.04595676\n",
      "Iteration 7, loss = 0.03916300\n",
      "Iteration 8, loss = 0.03482198\n",
      "Iteration 9, loss = 0.03178452\n",
      "Iteration 10, loss = 0.02950276\n",
      "Iteration 11, loss = 0.02775903\n",
      "Iteration 12, loss = 0.02633602\n",
      "Iteration 13, loss = 0.02519179\n",
      "Iteration 14, loss = 0.02408277\n",
      "Iteration 15, loss = 0.02319611\n",
      "Iteration 16, loss = 0.02234519\n",
      "Iteration 17, loss = 0.02168094\n",
      "Iteration 18, loss = 0.02091332\n",
      "Iteration 19, loss = 0.02029562\n",
      "Iteration 20, loss = 0.01972458\n",
      "Iteration 21, loss = 0.01917605\n",
      "Iteration 22, loss = 0.01868082\n",
      "Iteration 23, loss = 0.01819466\n",
      "Iteration 24, loss = 0.01774210\n",
      "Iteration 25, loss = 0.01732721\n",
      "Iteration 26, loss = 0.01692649\n",
      "Iteration 27, loss = 0.01657419\n",
      "Iteration 28, loss = 0.01623086\n",
      "Iteration 29, loss = 0.01589150\n",
      "Iteration 30, loss = 0.01556642\n",
      "Iteration 31, loss = 0.01526848\n",
      "Iteration 32, loss = 0.01498811\n",
      "Iteration 33, loss = 0.01472670\n",
      "Iteration 34, loss = 0.01448595\n",
      "Iteration 35, loss = 0.01425199\n",
      "Iteration 36, loss = 0.01402649\n",
      "Iteration 37, loss = 0.01381770\n",
      "Iteration 38, loss = 0.01361994\n",
      "Iteration 39, loss = 0.01343413\n",
      "Iteration 40, loss = 0.01325564\n",
      "Iteration 41, loss = 0.01308677\n",
      "Iteration 42, loss = 0.01292152\n",
      "Iteration 43, loss = 0.01278310\n",
      "Iteration 44, loss = 0.01262866\n",
      "Iteration 45, loss = 0.01248360\n",
      "Iteration 46, loss = 0.01234877\n",
      "Iteration 47, loss = 0.01225024\n",
      "Iteration 48, loss = 0.01213165\n",
      "Iteration 49, loss = 0.01200062\n",
      "Iteration 50, loss = 0.01188342\n",
      "Iteration 51, loss = 0.01181305\n",
      "Iteration 52, loss = 0.01169989\n",
      "Iteration 53, loss = 0.01159604\n",
      "Iteration 54, loss = 0.01152875\n",
      "Iteration 55, loss = 0.01142140\n",
      "Iteration 56, loss = 0.01136302\n",
      "Iteration 57, loss = 0.01127692\n",
      "Iteration 58, loss = 0.01120528\n",
      "Iteration 59, loss = 0.01115113\n",
      "Iteration 60, loss = 0.01109182\n",
      "Iteration 61, loss = 0.01101864\n",
      "Iteration 62, loss = 0.01095951\n",
      "Iteration 63, loss = 0.01089110\n",
      "Iteration 64, loss = 0.01086378\n",
      "Iteration 65, loss = 0.01078381\n",
      "Iteration 66, loss = 0.01075309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42610603\n",
      "Iteration 2, loss = 0.14574880\n",
      "Iteration 3, loss = 0.07443964\n",
      "Iteration 4, loss = 0.05021305\n",
      "Iteration 5, loss = 0.03929921\n",
      "Iteration 6, loss = 0.03324855\n",
      "Iteration 7, loss = 0.02947021\n",
      "Iteration 8, loss = 0.02669987\n",
      "Iteration 9, loss = 0.02472933\n",
      "Iteration 10, loss = 0.02329291\n",
      "Iteration 11, loss = 0.02210086\n",
      "Iteration 12, loss = 0.02112886\n",
      "Iteration 13, loss = 0.02033711\n",
      "Iteration 14, loss = 0.01958123\n",
      "Iteration 15, loss = 0.01891420\n",
      "Iteration 16, loss = 0.01829438\n",
      "Iteration 17, loss = 0.01771307\n",
      "Iteration 18, loss = 0.01722988\n",
      "Iteration 19, loss = 0.01673191\n",
      "Iteration 20, loss = 0.01631264\n",
      "Iteration 21, loss = 0.01583591\n",
      "Iteration 22, loss = 0.01544194\n",
      "Iteration 23, loss = 0.01506078\n",
      "Iteration 24, loss = 0.01472949\n",
      "Iteration 25, loss = 0.01438741\n",
      "Iteration 26, loss = 0.01406122\n",
      "Iteration 27, loss = 0.01377146\n",
      "Iteration 28, loss = 0.01348655\n",
      "Iteration 29, loss = 0.01323654\n",
      "Iteration 30, loss = 0.01296570\n",
      "Iteration 31, loss = 0.01271027\n",
      "Iteration 32, loss = 0.01248543\n",
      "Iteration 33, loss = 0.01225760\n",
      "Iteration 34, loss = 0.01205315\n",
      "Iteration 35, loss = 0.01184569\n",
      "Iteration 36, loss = 0.01167654\n",
      "Iteration 37, loss = 0.01152422\n",
      "Iteration 38, loss = 0.01129553\n",
      "Iteration 39, loss = 0.01113765\n",
      "Iteration 40, loss = 0.01096532\n",
      "Iteration 41, loss = 0.01083442\n",
      "Iteration 42, loss = 0.01067531\n",
      "Iteration 43, loss = 0.01054707\n",
      "Iteration 44, loss = 0.01042210\n",
      "Iteration 45, loss = 0.01030215\n",
      "Iteration 46, loss = 0.01016256\n",
      "Iteration 47, loss = 0.01009221\n",
      "Iteration 48, loss = 0.00995826\n",
      "Iteration 49, loss = 0.00982316\n",
      "Iteration 50, loss = 0.00978025\n",
      "Iteration 51, loss = 0.00964148\n",
      "Iteration 52, loss = 0.00955726\n",
      "Iteration 53, loss = 0.00948480\n",
      "Iteration 54, loss = 0.00939770\n",
      "Iteration 55, loss = 0.00935026\n",
      "Iteration 56, loss = 0.00924347\n",
      "Iteration 57, loss = 0.00918027\n",
      "Iteration 58, loss = 0.00908662\n",
      "Iteration 59, loss = 0.00902084\n",
      "Iteration 60, loss = 0.00897668\n",
      "Iteration 61, loss = 0.00892041\n",
      "Iteration 62, loss = 0.00882251\n",
      "Iteration 63, loss = 0.00878277\n",
      "Iteration 64, loss = 0.00875749\n",
      "Iteration 65, loss = 0.00871833\n",
      "Iteration 66, loss = 0.00866706\n",
      "Iteration 67, loss = 0.00860366\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69876682\n",
      "Iteration 2, loss = 0.56542679\n",
      "Iteration 3, loss = 0.46942420\n",
      "Iteration 4, loss = 0.39892399\n",
      "Iteration 5, loss = 0.34508278\n",
      "Iteration 6, loss = 0.30343334\n",
      "Iteration 7, loss = 0.27057396\n",
      "Iteration 8, loss = 0.24431575\n",
      "Iteration 9, loss = 0.22285246\n",
      "Iteration 10, loss = 0.20515594\n",
      "Iteration 11, loss = 0.19025206\n",
      "Iteration 12, loss = 0.17768606\n",
      "Iteration 13, loss = 0.16679641\n",
      "Iteration 14, loss = 0.15743317\n",
      "Iteration 15, loss = 0.14925424\n",
      "Iteration 16, loss = 0.14210537\n",
      "Iteration 17, loss = 0.13579485\n",
      "Iteration 18, loss = 0.13019473\n",
      "Iteration 19, loss = 0.12517059\n",
      "Iteration 20, loss = 0.12067898\n",
      "Iteration 21, loss = 0.11661720\n",
      "Iteration 22, loss = 0.11291810\n",
      "Iteration 23, loss = 0.10953535\n",
      "Iteration 24, loss = 0.10646727\n",
      "Iteration 25, loss = 0.10359144\n",
      "Iteration 26, loss = 0.10097843\n",
      "Iteration 27, loss = 0.09852380\n",
      "Iteration 28, loss = 0.09626192\n",
      "Iteration 29, loss = 0.09413378\n",
      "Iteration 30, loss = 0.09213576\n",
      "Iteration 31, loss = 0.09025776\n",
      "Iteration 32, loss = 0.08849738\n",
      "Iteration 33, loss = 0.08680768\n",
      "Iteration 34, loss = 0.08523719\n",
      "Iteration 35, loss = 0.08372180\n",
      "Iteration 36, loss = 0.08228706\n",
      "Iteration 37, loss = 0.08093301\n",
      "Iteration 38, loss = 0.07963295\n",
      "Iteration 39, loss = 0.07840194\n",
      "Iteration 40, loss = 0.07723675\n",
      "Iteration 41, loss = 0.07609651\n",
      "Iteration 42, loss = 0.07500341\n",
      "Iteration 43, loss = 0.07395721\n",
      "Iteration 44, loss = 0.07294982\n",
      "Iteration 45, loss = 0.07198636\n",
      "Iteration 46, loss = 0.07106861\n",
      "Iteration 47, loss = 0.07017627\n",
      "Iteration 48, loss = 0.06930881\n",
      "Iteration 49, loss = 0.06849486\n",
      "Iteration 50, loss = 0.06770923\n",
      "Iteration 51, loss = 0.06692006\n",
      "Iteration 52, loss = 0.06617632\n",
      "Iteration 53, loss = 0.06548079\n",
      "Iteration 54, loss = 0.06477629\n",
      "Iteration 55, loss = 0.06412259\n",
      "Iteration 56, loss = 0.06347436\n",
      "Iteration 57, loss = 0.06283984\n",
      "Iteration 58, loss = 0.06225219\n",
      "Iteration 59, loss = 0.06166407\n",
      "Iteration 60, loss = 0.06109960\n",
      "Iteration 61, loss = 0.06056874\n",
      "Iteration 62, loss = 0.06001839\n",
      "Iteration 63, loss = 0.05950237\n",
      "Iteration 64, loss = 0.05901151\n",
      "Iteration 65, loss = 0.05851985\n",
      "Iteration 66, loss = 0.05804624\n",
      "Iteration 67, loss = 0.05759003\n",
      "Iteration 68, loss = 0.05714617\n",
      "Iteration 69, loss = 0.05672084\n",
      "Iteration 70, loss = 0.05629782\n",
      "Iteration 71, loss = 0.05588228\n",
      "Iteration 72, loss = 0.05548944\n",
      "Iteration 73, loss = 0.05510047\n",
      "Iteration 74, loss = 0.05471607\n",
      "Iteration 75, loss = 0.05435451\n",
      "Iteration 76, loss = 0.05399756\n",
      "Iteration 77, loss = 0.05364185\n",
      "Iteration 78, loss = 0.05329530\n",
      "Iteration 79, loss = 0.05297022\n",
      "Iteration 80, loss = 0.05264314\n",
      "Iteration 81, loss = 0.05232092\n",
      "Iteration 82, loss = 0.05200936\n",
      "Iteration 83, loss = 0.05170969\n",
      "Iteration 84, loss = 0.05141450\n",
      "Iteration 85, loss = 0.05112560\n",
      "Iteration 86, loss = 0.05084006\n",
      "Iteration 87, loss = 0.05055672\n",
      "Iteration 88, loss = 0.05029303\n",
      "Iteration 89, loss = 0.05002377\n",
      "Iteration 90, loss = 0.04976263\n",
      "Iteration 91, loss = 0.04951263\n",
      "Iteration 92, loss = 0.04925475\n",
      "Iteration 93, loss = 0.04901184\n",
      "Iteration 94, loss = 0.04877745\n",
      "Iteration 95, loss = 0.04853934\n",
      "Iteration 96, loss = 0.04831065\n",
      "Iteration 97, loss = 0.04808776\n",
      "Iteration 98, loss = 0.04787464\n",
      "Iteration 99, loss = 0.04764214\n",
      "Iteration 100, loss = 0.04743476\n",
      "Iteration 101, loss = 0.04722167\n",
      "Iteration 102, loss = 0.04701836\n",
      "Iteration 103, loss = 0.04681891\n",
      "Iteration 104, loss = 0.04662095\n",
      "Iteration 105, loss = 0.04642657\n",
      "Iteration 106, loss = 0.04622746\n",
      "Iteration 107, loss = 0.04603743\n",
      "Iteration 108, loss = 0.04585848\n",
      "Iteration 109, loss = 0.04567204\n",
      "Iteration 110, loss = 0.04549353\n",
      "Iteration 111, loss = 0.04531597\n",
      "Iteration 112, loss = 0.04514283\n",
      "Iteration 113, loss = 0.04497348\n",
      "Iteration 114, loss = 0.04480331\n",
      "Iteration 115, loss = 0.04464638\n",
      "Iteration 116, loss = 0.04447201\n",
      "Iteration 117, loss = 0.04431230\n",
      "Iteration 118, loss = 0.04415403\n",
      "Iteration 119, loss = 0.04400492\n",
      "Iteration 120, loss = 0.04384407\n",
      "Iteration 121, loss = 0.04369452\n",
      "Iteration 122, loss = 0.04354490\n",
      "Iteration 123, loss = 0.04339676\n",
      "Iteration 124, loss = 0.04325639\n",
      "Iteration 125, loss = 0.04311986\n",
      "Iteration 126, loss = 0.04296886\n",
      "Iteration 127, loss = 0.04283072\n",
      "Iteration 128, loss = 0.04269520\n",
      "Iteration 129, loss = 0.04256437\n",
      "Iteration 130, loss = 0.04242267\n",
      "Iteration 131, loss = 0.04229084\n",
      "Iteration 132, loss = 0.04216314\n",
      "Iteration 133, loss = 0.04204003\n",
      "Iteration 134, loss = 0.04191139\n",
      "Iteration 135, loss = 0.04179097\n",
      "Iteration 136, loss = 0.04166645\n",
      "Iteration 137, loss = 0.04154096\n",
      "Iteration 138, loss = 0.04142420\n",
      "Iteration 139, loss = 0.04130034\n",
      "Iteration 140, loss = 0.04118748\n",
      "Iteration 141, loss = 0.04106766\n",
      "Iteration 142, loss = 0.04095524\n",
      "Iteration 143, loss = 0.04084511\n",
      "Iteration 144, loss = 0.04073142\n",
      "Iteration 145, loss = 0.04061962\n",
      "Iteration 146, loss = 0.04052330\n",
      "Iteration 147, loss = 0.04040292\n",
      "Iteration 148, loss = 0.04029855\n",
      "Iteration 149, loss = 0.04020269\n",
      "Iteration 150, loss = 0.04008967\n",
      "Iteration 151, loss = 0.03999038\n",
      "Iteration 152, loss = 0.03988493\n",
      "Iteration 153, loss = 0.03978334\n",
      "Iteration 154, loss = 0.03968258\n",
      "Iteration 155, loss = 0.03958520\n",
      "Iteration 156, loss = 0.03948922\n",
      "Iteration 157, loss = 0.03939135\n",
      "Iteration 158, loss = 0.03929901\n",
      "Iteration 159, loss = 0.03920410\n",
      "Iteration 160, loss = 0.03911097\n",
      "Iteration 161, loss = 0.03901610\n",
      "Iteration 162, loss = 0.03892652\n",
      "Iteration 163, loss = 0.03883381\n",
      "Iteration 164, loss = 0.03874240\n",
      "Iteration 165, loss = 0.03865218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70122983\n",
      "Iteration 2, loss = 0.58053982\n",
      "Iteration 3, loss = 0.49635160\n",
      "Iteration 4, loss = 0.43597556\n",
      "Iteration 5, loss = 0.38973685\n",
      "Iteration 6, loss = 0.35310480\n",
      "Iteration 7, loss = 0.32301857\n",
      "Iteration 8, loss = 0.29779904\n",
      "Iteration 9, loss = 0.27630193\n",
      "Iteration 10, loss = 0.25781169\n",
      "Iteration 11, loss = 0.24158400\n",
      "Iteration 12, loss = 0.22745881\n",
      "Iteration 13, loss = 0.21491507\n",
      "Iteration 14, loss = 0.20384518\n",
      "Iteration 15, loss = 0.19397503\n",
      "Iteration 16, loss = 0.18516397\n",
      "Iteration 17, loss = 0.17726399\n",
      "Iteration 18, loss = 0.17011662\n",
      "Iteration 19, loss = 0.16367452\n",
      "Iteration 20, loss = 0.15777937\n",
      "Iteration 21, loss = 0.15237470\n",
      "Iteration 22, loss = 0.14742518\n",
      "Iteration 23, loss = 0.14288249\n",
      "Iteration 24, loss = 0.13870886\n",
      "Iteration 25, loss = 0.13481341\n",
      "Iteration 26, loss = 0.13119964\n",
      "Iteration 27, loss = 0.12784802\n",
      "Iteration 28, loss = 0.12471956\n",
      "Iteration 29, loss = 0.12176373\n",
      "Iteration 30, loss = 0.11899696\n",
      "Iteration 31, loss = 0.11635980\n",
      "Iteration 32, loss = 0.11389632\n",
      "Iteration 33, loss = 0.11152317\n",
      "Iteration 34, loss = 0.10930621\n",
      "Iteration 35, loss = 0.10716259\n",
      "Iteration 36, loss = 0.10513701\n",
      "Iteration 37, loss = 0.10320109\n",
      "Iteration 38, loss = 0.10134406\n",
      "Iteration 39, loss = 0.09959269\n",
      "Iteration 40, loss = 0.09788328\n",
      "Iteration 41, loss = 0.09625182\n",
      "Iteration 42, loss = 0.09468519\n",
      "Iteration 43, loss = 0.09316928\n",
      "Iteration 44, loss = 0.09171138\n",
      "Iteration 45, loss = 0.09030798\n",
      "Iteration 46, loss = 0.08895475\n",
      "Iteration 47, loss = 0.08763937\n",
      "Iteration 48, loss = 0.08636442\n",
      "Iteration 49, loss = 0.08514861\n",
      "Iteration 50, loss = 0.08399882\n",
      "Iteration 51, loss = 0.08281796\n",
      "Iteration 52, loss = 0.08171172\n",
      "Iteration 53, loss = 0.08064927\n",
      "Iteration 54, loss = 0.07960633\n",
      "Iteration 55, loss = 0.07861369\n",
      "Iteration 56, loss = 0.07764450\n",
      "Iteration 57, loss = 0.07668499\n",
      "Iteration 58, loss = 0.07580114\n",
      "Iteration 59, loss = 0.07489623\n",
      "Iteration 60, loss = 0.07403651\n",
      "Iteration 61, loss = 0.07321475\n",
      "Iteration 62, loss = 0.07238879\n",
      "Iteration 63, loss = 0.07161446\n",
      "Iteration 64, loss = 0.07086716\n",
      "Iteration 65, loss = 0.07010174\n",
      "Iteration 66, loss = 0.06938036\n",
      "Iteration 67, loss = 0.06868812\n",
      "Iteration 68, loss = 0.06799629\n",
      "Iteration 69, loss = 0.06733761\n",
      "Iteration 70, loss = 0.06668976\n",
      "Iteration 71, loss = 0.06606584\n",
      "Iteration 72, loss = 0.06544851\n",
      "Iteration 73, loss = 0.06485272\n",
      "Iteration 74, loss = 0.06426507\n",
      "Iteration 75, loss = 0.06370894\n",
      "Iteration 76, loss = 0.06315408\n",
      "Iteration 77, loss = 0.06261355\n",
      "Iteration 78, loss = 0.06208801\n",
      "Iteration 79, loss = 0.06159177\n",
      "Iteration 80, loss = 0.06109535\n",
      "Iteration 81, loss = 0.06060925\n",
      "Iteration 82, loss = 0.06012937\n",
      "Iteration 83, loss = 0.05967213\n",
      "Iteration 84, loss = 0.05921998\n",
      "Iteration 85, loss = 0.05878278\n",
      "Iteration 86, loss = 0.05834790\n",
      "Iteration 87, loss = 0.05794170\n",
      "Iteration 88, loss = 0.05752533\n",
      "Iteration 89, loss = 0.05712148\n",
      "Iteration 90, loss = 0.05674485\n",
      "Iteration 91, loss = 0.05635669\n",
      "Iteration 92, loss = 0.05596938\n",
      "Iteration 93, loss = 0.05560082\n",
      "Iteration 94, loss = 0.05524741\n",
      "Iteration 95, loss = 0.05489391\n",
      "Iteration 96, loss = 0.05455906\n",
      "Iteration 97, loss = 0.05421650\n",
      "Iteration 98, loss = 0.05388897\n",
      "Iteration 99, loss = 0.05355911\n",
      "Iteration 100, loss = 0.05324524\n",
      "Iteration 101, loss = 0.05293527\n",
      "Iteration 102, loss = 0.05263213\n",
      "Iteration 103, loss = 0.05233687\n",
      "Iteration 104, loss = 0.05204737\n",
      "Iteration 105, loss = 0.05176627\n",
      "Iteration 106, loss = 0.05147820\n",
      "Iteration 107, loss = 0.05119502\n",
      "Iteration 108, loss = 0.05092851\n",
      "Iteration 109, loss = 0.05066085\n",
      "Iteration 110, loss = 0.05039930\n",
      "Iteration 111, loss = 0.05014055\n",
      "Iteration 112, loss = 0.04989046\n",
      "Iteration 113, loss = 0.04964480\n",
      "Iteration 114, loss = 0.04940325\n",
      "Iteration 115, loss = 0.04917453\n",
      "Iteration 116, loss = 0.04892705\n",
      "Iteration 117, loss = 0.04869181\n",
      "Iteration 118, loss = 0.04846796\n",
      "Iteration 119, loss = 0.04824588\n",
      "Iteration 120, loss = 0.04802466\n",
      "Iteration 121, loss = 0.04780635\n",
      "Iteration 122, loss = 0.04759573\n",
      "Iteration 123, loss = 0.04738749\n",
      "Iteration 124, loss = 0.04718761\n",
      "Iteration 125, loss = 0.04699023\n",
      "Iteration 126, loss = 0.04678056\n",
      "Iteration 127, loss = 0.04658832\n",
      "Iteration 128, loss = 0.04639760\n",
      "Iteration 129, loss = 0.04621333\n",
      "Iteration 130, loss = 0.04601670\n",
      "Iteration 131, loss = 0.04583063\n",
      "Iteration 132, loss = 0.04565556\n",
      "Iteration 133, loss = 0.04547808\n",
      "Iteration 134, loss = 0.04530098\n",
      "Iteration 135, loss = 0.04513211\n",
      "Iteration 136, loss = 0.04496087\n",
      "Iteration 137, loss = 0.04478676\n",
      "Iteration 138, loss = 0.04462774\n",
      "Iteration 139, loss = 0.04446017\n",
      "Iteration 140, loss = 0.04429989\n",
      "Iteration 141, loss = 0.04414342\n",
      "Iteration 142, loss = 0.04398134\n",
      "Iteration 143, loss = 0.04383138\n",
      "Iteration 144, loss = 0.04367746\n",
      "Iteration 145, loss = 0.04352800\n",
      "Iteration 146, loss = 0.04338307\n",
      "Iteration 147, loss = 0.04323366\n",
      "Iteration 148, loss = 0.04309225\n",
      "Iteration 149, loss = 0.04295646\n",
      "Iteration 150, loss = 0.04280891\n",
      "Iteration 151, loss = 0.04268168\n",
      "Iteration 152, loss = 0.04253303\n",
      "Iteration 153, loss = 0.04240144\n",
      "Iteration 154, loss = 0.04226559\n",
      "Iteration 155, loss = 0.04213684\n",
      "Iteration 156, loss = 0.04200959\n",
      "Iteration 157, loss = 0.04187840\n",
      "Iteration 158, loss = 0.04175855\n",
      "Iteration 159, loss = 0.04162775\n",
      "Iteration 160, loss = 0.04150522\n",
      "Iteration 161, loss = 0.04138114\n",
      "Iteration 162, loss = 0.04126573\n",
      "Iteration 163, loss = 0.04114038\n",
      "Iteration 164, loss = 0.04102291\n",
      "Iteration 165, loss = 0.04090989\n",
      "Iteration 166, loss = 0.04079321\n",
      "Iteration 167, loss = 0.04068526\n",
      "Iteration 168, loss = 0.04057641\n",
      "Iteration 169, loss = 0.04045831\n",
      "Iteration 170, loss = 0.04034758\n",
      "Iteration 171, loss = 0.04024029\n",
      "Iteration 172, loss = 0.04013535\n",
      "Iteration 173, loss = 0.04003045\n",
      "Iteration 174, loss = 0.03992079\n",
      "Iteration 175, loss = 0.03982196\n",
      "Iteration 176, loss = 0.03971481\n",
      "Iteration 177, loss = 0.03961689\n",
      "Iteration 178, loss = 0.03951715\n",
      "Iteration 179, loss = 0.03942144\n",
      "Iteration 180, loss = 0.03932423\n",
      "Iteration 181, loss = 0.03922553\n",
      "Iteration 182, loss = 0.03912867\n",
      "Iteration 183, loss = 0.03903703\n",
      "Iteration 184, loss = 0.03894030\n",
      "Iteration 185, loss = 0.03884965\n",
      "Iteration 186, loss = 0.03875636\n",
      "Iteration 187, loss = 0.03867353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69218678\n",
      "Iteration 2, loss = 0.57043512\n",
      "Iteration 3, loss = 0.47876653\n",
      "Iteration 4, loss = 0.40923947\n",
      "Iteration 5, loss = 0.35538622\n",
      "Iteration 6, loss = 0.31325346\n",
      "Iteration 7, loss = 0.28010561\n",
      "Iteration 8, loss = 0.25370151\n",
      "Iteration 9, loss = 0.23226040\n",
      "Iteration 10, loss = 0.21455477\n",
      "Iteration 11, loss = 0.19963261\n",
      "Iteration 12, loss = 0.18688501\n",
      "Iteration 13, loss = 0.17587312\n",
      "Iteration 14, loss = 0.16623757\n",
      "Iteration 15, loss = 0.15771819\n",
      "Iteration 16, loss = 0.15016298\n",
      "Iteration 17, loss = 0.14342570\n",
      "Iteration 18, loss = 0.13732830\n",
      "Iteration 19, loss = 0.13184362\n",
      "Iteration 20, loss = 0.12685406\n",
      "Iteration 21, loss = 0.12229785\n",
      "Iteration 22, loss = 0.11812458\n",
      "Iteration 23, loss = 0.11428405\n",
      "Iteration 24, loss = 0.11074398\n",
      "Iteration 25, loss = 0.10750010\n",
      "Iteration 26, loss = 0.10445175\n",
      "Iteration 27, loss = 0.10162358\n",
      "Iteration 28, loss = 0.09898231\n",
      "Iteration 29, loss = 0.09653900\n",
      "Iteration 30, loss = 0.09423610\n",
      "Iteration 31, loss = 0.09207173\n",
      "Iteration 32, loss = 0.09005176\n",
      "Iteration 33, loss = 0.08815261\n",
      "Iteration 34, loss = 0.08634887\n",
      "Iteration 35, loss = 0.08463798\n",
      "Iteration 36, loss = 0.08303488\n",
      "Iteration 37, loss = 0.08153118\n",
      "Iteration 38, loss = 0.08006561\n",
      "Iteration 39, loss = 0.07869851\n",
      "Iteration 40, loss = 0.07739470\n",
      "Iteration 41, loss = 0.07615088\n",
      "Iteration 42, loss = 0.07497211\n",
      "Iteration 43, loss = 0.07384293\n",
      "Iteration 44, loss = 0.07276225\n",
      "Iteration 45, loss = 0.07175008\n",
      "Iteration 46, loss = 0.07074725\n",
      "Iteration 47, loss = 0.06981658\n",
      "Iteration 48, loss = 0.06890666\n",
      "Iteration 49, loss = 0.06805236\n",
      "Iteration 50, loss = 0.06721741\n",
      "Iteration 51, loss = 0.06642190\n",
      "Iteration 52, loss = 0.06566686\n",
      "Iteration 53, loss = 0.06492423\n",
      "Iteration 54, loss = 0.06421975\n",
      "Iteration 55, loss = 0.06354816\n",
      "Iteration 56, loss = 0.06290484\n",
      "Iteration 57, loss = 0.06227954\n",
      "Iteration 58, loss = 0.06167353\n",
      "Iteration 59, loss = 0.06108480\n",
      "Iteration 60, loss = 0.06052924\n",
      "Iteration 61, loss = 0.05998188\n",
      "Iteration 62, loss = 0.05945209\n",
      "Iteration 63, loss = 0.05894949\n",
      "Iteration 64, loss = 0.05847014\n",
      "Iteration 65, loss = 0.05800590\n",
      "Iteration 66, loss = 0.05754734\n",
      "Iteration 67, loss = 0.05710167\n",
      "Iteration 68, loss = 0.05665855\n",
      "Iteration 69, loss = 0.05624595\n",
      "Iteration 70, loss = 0.05584496\n",
      "Iteration 71, loss = 0.05545228\n",
      "Iteration 72, loss = 0.05506952\n",
      "Iteration 73, loss = 0.05470330\n",
      "Iteration 74, loss = 0.05434371\n",
      "Iteration 75, loss = 0.05400466\n",
      "Iteration 76, loss = 0.05366104\n",
      "Iteration 77, loss = 0.05332832\n",
      "Iteration 78, loss = 0.05300672\n",
      "Iteration 79, loss = 0.05269344\n",
      "Iteration 80, loss = 0.05239056\n",
      "Iteration 81, loss = 0.05209432\n",
      "Iteration 82, loss = 0.05180619\n",
      "Iteration 83, loss = 0.05152614\n",
      "Iteration 84, loss = 0.05124444\n",
      "Iteration 85, loss = 0.05098636\n",
      "Iteration 86, loss = 0.05072059\n",
      "Iteration 87, loss = 0.05046431\n",
      "Iteration 88, loss = 0.05021104\n",
      "Iteration 89, loss = 0.04996661\n",
      "Iteration 90, loss = 0.04972947\n",
      "Iteration 91, loss = 0.04949600\n",
      "Iteration 92, loss = 0.04926681\n",
      "Iteration 93, loss = 0.04904028\n",
      "Iteration 94, loss = 0.04882605\n",
      "Iteration 95, loss = 0.04860913\n",
      "Iteration 96, loss = 0.04839985\n",
      "Iteration 97, loss = 0.04819438\n",
      "Iteration 98, loss = 0.04799694\n",
      "Iteration 99, loss = 0.04779977\n",
      "Iteration 100, loss = 0.04760791\n",
      "Iteration 101, loss = 0.04741524\n",
      "Iteration 102, loss = 0.04722966\n",
      "Iteration 103, loss = 0.04705059\n",
      "Iteration 104, loss = 0.04686801\n",
      "Iteration 105, loss = 0.04670026\n",
      "Iteration 106, loss = 0.04652091\n",
      "Iteration 107, loss = 0.04635370\n",
      "Iteration 108, loss = 0.04618412\n",
      "Iteration 109, loss = 0.04602152\n",
      "Iteration 110, loss = 0.04586440\n",
      "Iteration 111, loss = 0.04570389\n",
      "Iteration 112, loss = 0.04554625\n",
      "Iteration 113, loss = 0.04539700\n",
      "Iteration 114, loss = 0.04524895\n",
      "Iteration 115, loss = 0.04509812\n",
      "Iteration 116, loss = 0.04495314\n",
      "Iteration 117, loss = 0.04480860\n",
      "Iteration 118, loss = 0.04466802\n",
      "Iteration 119, loss = 0.04452723\n",
      "Iteration 120, loss = 0.04439386\n",
      "Iteration 121, loss = 0.04425615\n",
      "Iteration 122, loss = 0.04412239\n",
      "Iteration 123, loss = 0.04399342\n",
      "Iteration 124, loss = 0.04386613\n",
      "Iteration 125, loss = 0.04373979\n",
      "Iteration 126, loss = 0.04361359\n",
      "Iteration 127, loss = 0.04349280\n",
      "Iteration 128, loss = 0.04337622\n",
      "Iteration 129, loss = 0.04326669\n",
      "Iteration 130, loss = 0.04313270\n",
      "Iteration 131, loss = 0.04300939\n",
      "Iteration 132, loss = 0.04289458\n",
      "Iteration 133, loss = 0.04278227\n",
      "Iteration 134, loss = 0.04266478\n",
      "Iteration 135, loss = 0.04255625\n",
      "Iteration 136, loss = 0.04245031\n",
      "Iteration 137, loss = 0.04233693\n",
      "Iteration 138, loss = 0.04222994\n",
      "Iteration 139, loss = 0.04212373\n",
      "Iteration 140, loss = 0.04201828\n",
      "Iteration 141, loss = 0.04191760\n",
      "Iteration 142, loss = 0.04181167\n",
      "Iteration 143, loss = 0.04171159\n",
      "Iteration 144, loss = 0.04161081\n",
      "Iteration 145, loss = 0.04151368\n",
      "Iteration 146, loss = 0.04141443\n",
      "Iteration 147, loss = 0.04131784\n",
      "Iteration 148, loss = 0.04122373\n",
      "Iteration 149, loss = 0.04113108\n",
      "Iteration 150, loss = 0.04103517\n",
      "Iteration 151, loss = 0.04094469\n",
      "Iteration 152, loss = 0.04085584\n",
      "Iteration 153, loss = 0.04076670\n",
      "Iteration 154, loss = 0.04067453\n",
      "Iteration 155, loss = 0.04058683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69706829\n",
      "Iteration 2, loss = 0.59940584\n",
      "Iteration 3, loss = 0.52158121\n",
      "Iteration 4, loss = 0.45928985\n",
      "Iteration 5, loss = 0.40831515\n",
      "Iteration 6, loss = 0.36658212\n",
      "Iteration 7, loss = 0.33223454\n",
      "Iteration 8, loss = 0.30382196\n",
      "Iteration 9, loss = 0.27991970\n",
      "Iteration 10, loss = 0.25964302\n",
      "Iteration 11, loss = 0.24217226\n",
      "Iteration 12, loss = 0.22705793\n",
      "Iteration 13, loss = 0.21385177\n",
      "Iteration 14, loss = 0.20218275\n",
      "Iteration 15, loss = 0.19188828\n",
      "Iteration 16, loss = 0.18273299\n",
      "Iteration 17, loss = 0.17455222\n",
      "Iteration 18, loss = 0.16717475\n",
      "Iteration 19, loss = 0.16055886\n",
      "Iteration 20, loss = 0.15452284\n",
      "Iteration 21, loss = 0.14906523\n",
      "Iteration 22, loss = 0.14409891\n",
      "Iteration 23, loss = 0.13953573\n",
      "Iteration 24, loss = 0.13529826\n",
      "Iteration 25, loss = 0.13143731\n",
      "Iteration 26, loss = 0.12784176\n",
      "Iteration 27, loss = 0.12449144\n",
      "Iteration 28, loss = 0.12137113\n",
      "Iteration 29, loss = 0.11842835\n",
      "Iteration 30, loss = 0.11565000\n",
      "Iteration 31, loss = 0.11305770\n",
      "Iteration 32, loss = 0.11059843\n",
      "Iteration 33, loss = 0.10826487\n",
      "Iteration 34, loss = 0.10606509\n",
      "Iteration 35, loss = 0.10398030\n",
      "Iteration 36, loss = 0.10200362\n",
      "Iteration 37, loss = 0.10009106\n",
      "Iteration 38, loss = 0.09829686\n",
      "Iteration 39, loss = 0.09656522\n",
      "Iteration 40, loss = 0.09492118\n",
      "Iteration 41, loss = 0.09332920\n",
      "Iteration 42, loss = 0.09180900\n",
      "Iteration 43, loss = 0.09035760\n",
      "Iteration 44, loss = 0.08895470\n",
      "Iteration 45, loss = 0.08760373\n",
      "Iteration 46, loss = 0.08630097\n",
      "Iteration 47, loss = 0.08505948\n",
      "Iteration 48, loss = 0.08385266\n",
      "Iteration 49, loss = 0.08268475\n",
      "Iteration 50, loss = 0.08156314\n",
      "Iteration 51, loss = 0.08047564\n",
      "Iteration 52, loss = 0.07944072\n",
      "Iteration 53, loss = 0.07843150\n",
      "Iteration 54, loss = 0.07745092\n",
      "Iteration 55, loss = 0.07649897\n",
      "Iteration 56, loss = 0.07560627\n",
      "Iteration 57, loss = 0.07470594\n",
      "Iteration 58, loss = 0.07387486\n",
      "Iteration 59, loss = 0.07303452\n",
      "Iteration 60, loss = 0.07221697\n",
      "Iteration 61, loss = 0.07145810\n",
      "Iteration 62, loss = 0.07069037\n",
      "Iteration 63, loss = 0.06994840\n",
      "Iteration 64, loss = 0.06923645\n",
      "Iteration 65, loss = 0.06853779\n",
      "Iteration 66, loss = 0.06786804\n",
      "Iteration 67, loss = 0.06720858\n",
      "Iteration 68, loss = 0.06657684\n",
      "Iteration 69, loss = 0.06596073\n",
      "Iteration 70, loss = 0.06535168\n",
      "Iteration 71, loss = 0.06476627\n",
      "Iteration 72, loss = 0.06420250\n",
      "Iteration 73, loss = 0.06362803\n",
      "Iteration 74, loss = 0.06309155\n",
      "Iteration 75, loss = 0.06256356\n",
      "Iteration 76, loss = 0.06204826\n",
      "Iteration 77, loss = 0.06155308\n",
      "Iteration 78, loss = 0.06105971\n",
      "Iteration 79, loss = 0.06057691\n",
      "Iteration 80, loss = 0.06012199\n",
      "Iteration 81, loss = 0.05965742\n",
      "Iteration 82, loss = 0.05921716\n",
      "Iteration 83, loss = 0.05877929\n",
      "Iteration 84, loss = 0.05834368\n",
      "Iteration 85, loss = 0.05792886\n",
      "Iteration 86, loss = 0.05752301\n",
      "Iteration 87, loss = 0.05712998\n",
      "Iteration 88, loss = 0.05673976\n",
      "Iteration 89, loss = 0.05636957\n",
      "Iteration 90, loss = 0.05599754\n",
      "Iteration 91, loss = 0.05562785\n",
      "Iteration 92, loss = 0.05527013\n",
      "Iteration 93, loss = 0.05492726\n",
      "Iteration 94, loss = 0.05458202\n",
      "Iteration 95, loss = 0.05424849\n",
      "Iteration 96, loss = 0.05394286\n",
      "Iteration 97, loss = 0.05360136\n",
      "Iteration 98, loss = 0.05330008\n",
      "Iteration 99, loss = 0.05298117\n",
      "Iteration 100, loss = 0.05268128\n",
      "Iteration 101, loss = 0.05239018\n",
      "Iteration 102, loss = 0.05209591\n",
      "Iteration 103, loss = 0.05181265\n",
      "Iteration 104, loss = 0.05154078\n",
      "Iteration 105, loss = 0.05125547\n",
      "Iteration 106, loss = 0.05098503\n",
      "Iteration 107, loss = 0.05071811\n",
      "Iteration 108, loss = 0.05045675\n",
      "Iteration 109, loss = 0.05020365\n",
      "Iteration 110, loss = 0.04995644\n",
      "Iteration 111, loss = 0.04969511\n",
      "Iteration 112, loss = 0.04945832\n",
      "Iteration 113, loss = 0.04922797\n",
      "Iteration 114, loss = 0.04898953\n",
      "Iteration 115, loss = 0.04875413\n",
      "Iteration 116, loss = 0.04852882\n",
      "Iteration 117, loss = 0.04830398\n",
      "Iteration 118, loss = 0.04807874\n",
      "Iteration 119, loss = 0.04786578\n",
      "Iteration 120, loss = 0.04765754\n",
      "Iteration 121, loss = 0.04744949\n",
      "Iteration 122, loss = 0.04724632\n",
      "Iteration 123, loss = 0.04703856\n",
      "Iteration 124, loss = 0.04684358\n",
      "Iteration 125, loss = 0.04664041\n",
      "Iteration 126, loss = 0.04644779\n",
      "Iteration 127, loss = 0.04626279\n",
      "Iteration 128, loss = 0.04607158\n",
      "Iteration 129, loss = 0.04588947\n",
      "Iteration 130, loss = 0.04570428\n",
      "Iteration 131, loss = 0.04552201\n",
      "Iteration 132, loss = 0.04534970\n",
      "Iteration 133, loss = 0.04517215\n",
      "Iteration 134, loss = 0.04500079\n",
      "Iteration 135, loss = 0.04483484\n",
      "Iteration 136, loss = 0.04467075\n",
      "Iteration 137, loss = 0.04449815\n",
      "Iteration 138, loss = 0.04433502\n",
      "Iteration 139, loss = 0.04417488\n",
      "Iteration 140, loss = 0.04401538\n",
      "Iteration 141, loss = 0.04386321\n",
      "Iteration 142, loss = 0.04370979\n",
      "Iteration 143, loss = 0.04356122\n",
      "Iteration 144, loss = 0.04340846\n",
      "Iteration 145, loss = 0.04325928\n",
      "Iteration 146, loss = 0.04311435\n",
      "Iteration 147, loss = 0.04297215\n",
      "Iteration 148, loss = 0.04282839\n",
      "Iteration 149, loss = 0.04268812\n",
      "Iteration 150, loss = 0.04255462\n",
      "Iteration 151, loss = 0.04241769\n",
      "Iteration 152, loss = 0.04227654\n",
      "Iteration 153, loss = 0.04215364\n",
      "Iteration 154, loss = 0.04201676\n",
      "Iteration 155, loss = 0.04189673\n",
      "Iteration 156, loss = 0.04175750\n",
      "Iteration 157, loss = 0.04163151\n",
      "Iteration 158, loss = 0.04150823\n",
      "Iteration 159, loss = 0.04138180\n",
      "Iteration 160, loss = 0.04126265\n",
      "Iteration 161, loss = 0.04113919\n",
      "Iteration 162, loss = 0.04102176\n",
      "Iteration 163, loss = 0.04090190\n",
      "Iteration 164, loss = 0.04078575\n",
      "Iteration 165, loss = 0.04067118\n",
      "Iteration 166, loss = 0.04055770\n",
      "Iteration 167, loss = 0.04044684\n",
      "Iteration 168, loss = 0.04033346\n",
      "Iteration 169, loss = 0.04022353\n",
      "Iteration 170, loss = 0.04011695\n",
      "Iteration 171, loss = 0.04000772\n",
      "Iteration 172, loss = 0.03990294\n",
      "Iteration 173, loss = 0.03979674\n",
      "Iteration 174, loss = 0.03969316\n",
      "Iteration 175, loss = 0.03959106\n",
      "Iteration 176, loss = 0.03949608\n",
      "Iteration 177, loss = 0.03939571\n",
      "Iteration 178, loss = 0.03928881\n",
      "Iteration 179, loss = 0.03919204\n",
      "Iteration 180, loss = 0.03909448\n",
      "Iteration 181, loss = 0.03900140\n",
      "Iteration 182, loss = 0.03890899\n",
      "Iteration 183, loss = 0.03881412\n",
      "Iteration 184, loss = 0.03871907\n",
      "Iteration 185, loss = 0.03862837\n",
      "Iteration 186, loss = 0.03854079\n",
      "Iteration 187, loss = 0.03844596\n",
      "Iteration 188, loss = 0.03836080\n",
      "Iteration 189, loss = 0.03827142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68493383\n",
      "Iteration 2, loss = 0.57130696\n",
      "Iteration 3, loss = 0.48046887\n",
      "Iteration 4, loss = 0.40994711\n",
      "Iteration 5, loss = 0.35488423\n",
      "Iteration 6, loss = 0.31154295\n",
      "Iteration 7, loss = 0.27722670\n",
      "Iteration 8, loss = 0.24971148\n",
      "Iteration 9, loss = 0.22716948\n",
      "Iteration 10, loss = 0.20836290\n",
      "Iteration 11, loss = 0.19242900\n",
      "Iteration 12, loss = 0.17875063\n",
      "Iteration 13, loss = 0.16692218\n",
      "Iteration 14, loss = 0.15652713\n",
      "Iteration 15, loss = 0.14742739\n",
      "Iteration 16, loss = 0.13928158\n",
      "Iteration 17, loss = 0.13206831\n",
      "Iteration 18, loss = 0.12563516\n",
      "Iteration 19, loss = 0.11984739\n",
      "Iteration 20, loss = 0.11463699\n",
      "Iteration 21, loss = 0.10992474\n",
      "Iteration 22, loss = 0.10561937\n",
      "Iteration 23, loss = 0.10170555\n",
      "Iteration 24, loss = 0.09814022\n",
      "Iteration 25, loss = 0.09481842\n",
      "Iteration 26, loss = 0.09181341\n",
      "Iteration 27, loss = 0.08899804\n",
      "Iteration 28, loss = 0.08640191\n",
      "Iteration 29, loss = 0.08401397\n",
      "Iteration 30, loss = 0.08175303\n",
      "Iteration 31, loss = 0.07966171\n",
      "Iteration 32, loss = 0.07770183\n",
      "Iteration 33, loss = 0.07585954\n",
      "Iteration 34, loss = 0.07414287\n",
      "Iteration 35, loss = 0.07252349\n",
      "Iteration 36, loss = 0.07099851\n",
      "Iteration 37, loss = 0.06955799\n",
      "Iteration 38, loss = 0.06818704\n",
      "Iteration 39, loss = 0.06690338\n",
      "Iteration 40, loss = 0.06566495\n",
      "Iteration 41, loss = 0.06450366\n",
      "Iteration 42, loss = 0.06339218\n",
      "Iteration 43, loss = 0.06234963\n",
      "Iteration 44, loss = 0.06133375\n",
      "Iteration 45, loss = 0.06037772\n",
      "Iteration 46, loss = 0.05948262\n",
      "Iteration 47, loss = 0.05860441\n",
      "Iteration 48, loss = 0.05776904\n",
      "Iteration 49, loss = 0.05697644\n",
      "Iteration 50, loss = 0.05619887\n",
      "Iteration 51, loss = 0.05546709\n",
      "Iteration 52, loss = 0.05476197\n",
      "Iteration 53, loss = 0.05408554\n",
      "Iteration 54, loss = 0.05343183\n",
      "Iteration 55, loss = 0.05281266\n",
      "Iteration 56, loss = 0.05221475\n",
      "Iteration 57, loss = 0.05163534\n",
      "Iteration 58, loss = 0.05107626\n",
      "Iteration 59, loss = 0.05054130\n",
      "Iteration 60, loss = 0.05003146\n",
      "Iteration 61, loss = 0.04952837\n",
      "Iteration 62, loss = 0.04904686\n",
      "Iteration 63, loss = 0.04858501\n",
      "Iteration 64, loss = 0.04813496\n",
      "Iteration 65, loss = 0.04770181\n",
      "Iteration 66, loss = 0.04729590\n",
      "Iteration 67, loss = 0.04687774\n",
      "Iteration 68, loss = 0.04648242\n",
      "Iteration 69, loss = 0.04609869\n",
      "Iteration 70, loss = 0.04572772\n",
      "Iteration 71, loss = 0.04536779\n",
      "Iteration 72, loss = 0.04502165\n",
      "Iteration 73, loss = 0.04468085\n",
      "Iteration 74, loss = 0.04436198\n",
      "Iteration 75, loss = 0.04403730\n",
      "Iteration 76, loss = 0.04372395\n",
      "Iteration 77, loss = 0.04342819\n",
      "Iteration 78, loss = 0.04313344\n",
      "Iteration 79, loss = 0.04285377\n",
      "Iteration 80, loss = 0.04257454\n",
      "Iteration 81, loss = 0.04230539\n",
      "Iteration 82, loss = 0.04205218\n",
      "Iteration 83, loss = 0.04179126\n",
      "Iteration 84, loss = 0.04154404\n",
      "Iteration 85, loss = 0.04130414\n",
      "Iteration 86, loss = 0.04106698\n",
      "Iteration 87, loss = 0.04083972\n",
      "Iteration 88, loss = 0.04061096\n",
      "Iteration 89, loss = 0.04039443\n",
      "Iteration 90, loss = 0.04018142\n",
      "Iteration 91, loss = 0.03997395\n",
      "Iteration 92, loss = 0.03977325\n",
      "Iteration 93, loss = 0.03957433\n",
      "Iteration 94, loss = 0.03937796\n",
      "Iteration 95, loss = 0.03918735\n",
      "Iteration 96, loss = 0.03900160\n",
      "Iteration 97, loss = 0.03882176\n",
      "Iteration 98, loss = 0.03864762\n",
      "Iteration 99, loss = 0.03847754\n",
      "Iteration 100, loss = 0.03830605\n",
      "Iteration 101, loss = 0.03814123\n",
      "Iteration 102, loss = 0.03797992\n",
      "Iteration 103, loss = 0.03782230\n",
      "Iteration 104, loss = 0.03766844\n",
      "Iteration 105, loss = 0.03751934\n",
      "Iteration 106, loss = 0.03737144\n",
      "Iteration 107, loss = 0.03722502\n",
      "Iteration 108, loss = 0.03708591\n",
      "Iteration 109, loss = 0.03694802\n",
      "Iteration 110, loss = 0.03680828\n",
      "Iteration 111, loss = 0.03667693\n",
      "Iteration 112, loss = 0.03654433\n",
      "Iteration 113, loss = 0.03641572\n",
      "Iteration 114, loss = 0.03629145\n",
      "Iteration 115, loss = 0.03616998\n",
      "Iteration 116, loss = 0.03604971\n",
      "Iteration 117, loss = 0.03593105\n",
      "Iteration 118, loss = 0.03581440\n",
      "Iteration 119, loss = 0.03569983\n",
      "Iteration 120, loss = 0.03558696\n",
      "Iteration 121, loss = 0.03548040\n",
      "Iteration 122, loss = 0.03537118\n",
      "Iteration 123, loss = 0.03526497\n",
      "Iteration 124, loss = 0.03516197\n",
      "Iteration 125, loss = 0.03505938\n",
      "Iteration 126, loss = 0.03495787\n",
      "Iteration 127, loss = 0.03485877\n",
      "Iteration 128, loss = 0.03476566\n",
      "Iteration 129, loss = 0.03466828\n",
      "Iteration 130, loss = 0.03457545\n",
      "Iteration 131, loss = 0.03448296\n",
      "Iteration 132, loss = 0.03439378\n",
      "Iteration 133, loss = 0.03430265\n",
      "Iteration 134, loss = 0.03421648\n",
      "Iteration 135, loss = 0.03413130\n",
      "Iteration 136, loss = 0.03404632\n",
      "Iteration 137, loss = 0.03396417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28242284\n",
      "Iteration 2, loss = 0.09176761\n",
      "Iteration 3, loss = 0.03374049\n",
      "Iteration 4, loss = 0.02141120\n",
      "Iteration 5, loss = 0.01564101\n",
      "Iteration 6, loss = 0.01600508\n",
      "Iteration 7, loss = 0.08390291\n",
      "Iteration 8, loss = 0.05172665\n",
      "Iteration 9, loss = 0.01777420\n",
      "Iteration 10, loss = 0.01267080\n",
      "Iteration 11, loss = 0.01744675\n",
      "Iteration 12, loss = 0.01199951\n",
      "Iteration 13, loss = 0.03417104\n",
      "Iteration 14, loss = 0.02124259\n",
      "Iteration 15, loss = 0.01736606\n",
      "Iteration 16, loss = 0.01227098\n",
      "Iteration 17, loss = 0.01148037\n",
      "Iteration 18, loss = 0.01499896\n",
      "Iteration 19, loss = 0.02098132\n",
      "Iteration 20, loss = 0.02119989\n",
      "Iteration 21, loss = 0.01279010\n",
      "Iteration 22, loss = 0.02345444\n",
      "Iteration 23, loss = 0.02760982\n",
      "Iteration 24, loss = 0.01457212\n",
      "Iteration 25, loss = 0.01374734\n",
      "Iteration 26, loss = 0.01076274\n",
      "Iteration 27, loss = 0.01073115\n",
      "Iteration 28, loss = 0.01142964\n",
      "Iteration 29, loss = 0.01244281\n",
      "Iteration 30, loss = 0.03136142\n",
      "Iteration 31, loss = 0.02201899\n",
      "Iteration 32, loss = 0.03423404\n",
      "Iteration 33, loss = 0.01562405\n",
      "Iteration 34, loss = 0.01313062\n",
      "Iteration 35, loss = 0.01335355\n",
      "Iteration 36, loss = 0.01529694\n",
      "Iteration 37, loss = 0.06209480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39214631\n",
      "Iteration 2, loss = 0.11258705\n",
      "Iteration 3, loss = 0.04507952\n",
      "Iteration 4, loss = 0.02568123\n",
      "Iteration 5, loss = 0.01865001\n",
      "Iteration 6, loss = 0.03044651\n",
      "Iteration 7, loss = 0.02018192\n",
      "Iteration 8, loss = 0.01344872\n",
      "Iteration 9, loss = 0.01214074\n",
      "Iteration 10, loss = 0.01269434\n",
      "Iteration 11, loss = 0.01930306\n",
      "Iteration 12, loss = 0.01580451\n",
      "Iteration 13, loss = 0.01182628\n",
      "Iteration 14, loss = 0.01211144\n",
      "Iteration 15, loss = 0.05661718\n",
      "Iteration 16, loss = 0.02412966\n",
      "Iteration 17, loss = 0.01330153\n",
      "Iteration 18, loss = 0.01301131\n",
      "Iteration 19, loss = 0.01276816\n",
      "Iteration 20, loss = 0.01097943\n",
      "Iteration 21, loss = 0.01560490\n",
      "Iteration 22, loss = 0.01694496\n",
      "Iteration 23, loss = 0.01418158\n",
      "Iteration 24, loss = 0.18227304\n",
      "Iteration 25, loss = 0.08708041\n",
      "Iteration 26, loss = 0.03414152\n",
      "Iteration 27, loss = 0.02549127\n",
      "Iteration 28, loss = 0.02061734\n",
      "Iteration 29, loss = 0.01772587\n",
      "Iteration 30, loss = 0.01569932\n",
      "Iteration 31, loss = 0.01532966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27360243\n",
      "Iteration 2, loss = 0.08380588\n",
      "Iteration 3, loss = 0.03455690\n",
      "Iteration 4, loss = 0.02204965\n",
      "Iteration 5, loss = 0.02774481\n",
      "Iteration 6, loss = 0.02316420\n",
      "Iteration 7, loss = 0.02321378\n",
      "Iteration 8, loss = 0.01399659\n",
      "Iteration 9, loss = 0.01874132\n",
      "Iteration 10, loss = 0.01647010\n",
      "Iteration 11, loss = 0.06980556\n",
      "Iteration 12, loss = 0.03616899\n",
      "Iteration 13, loss = 0.01906388\n",
      "Iteration 14, loss = 0.01596386\n",
      "Iteration 15, loss = 0.01276712\n",
      "Iteration 16, loss = 0.01657994\n",
      "Iteration 17, loss = 0.01983609\n",
      "Iteration 18, loss = 0.01984237\n",
      "Iteration 19, loss = 0.03297435\n",
      "Iteration 20, loss = 0.02380413\n",
      "Iteration 21, loss = 0.01357838\n",
      "Iteration 22, loss = 0.01597933\n",
      "Iteration 23, loss = 0.01594340\n",
      "Iteration 24, loss = 0.01345475\n",
      "Iteration 25, loss = 0.01999748\n",
      "Iteration 26, loss = 0.05455179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28874592\n",
      "Iteration 2, loss = 0.07998883\n",
      "Iteration 3, loss = 0.03252884\n",
      "Iteration 4, loss = 0.02379889\n",
      "Iteration 5, loss = 0.03300603\n",
      "Iteration 6, loss = 0.02333816\n",
      "Iteration 7, loss = 0.01345716\n",
      "Iteration 8, loss = 0.01225663\n",
      "Iteration 9, loss = 0.01387077\n",
      "Iteration 10, loss = 0.01264113\n",
      "Iteration 11, loss = 0.06906510\n",
      "Iteration 12, loss = 0.17599250\n",
      "Iteration 13, loss = 0.06349108\n",
      "Iteration 14, loss = 0.02818472\n",
      "Iteration 15, loss = 0.01774267\n",
      "Iteration 16, loss = 0.01415147\n",
      "Iteration 17, loss = 0.01248397\n",
      "Iteration 18, loss = 0.01160222\n",
      "Iteration 19, loss = 0.01138256\n",
      "Iteration 20, loss = 0.01121963\n",
      "Iteration 21, loss = 0.01166417\n",
      "Iteration 22, loss = 0.01156530\n",
      "Iteration 23, loss = 0.01729012\n",
      "Iteration 24, loss = 0.01344417\n",
      "Iteration 25, loss = 0.01240784\n",
      "Iteration 26, loss = 0.01094043\n",
      "Iteration 27, loss = 0.01048681\n",
      "Iteration 28, loss = 0.01235807\n",
      "Iteration 29, loss = 0.01213023\n",
      "Iteration 30, loss = 0.02549532\n",
      "Iteration 31, loss = 0.01872426\n",
      "Iteration 32, loss = 0.01126239\n",
      "Iteration 33, loss = 0.01156509\n",
      "Iteration 34, loss = 0.01088152\n",
      "Iteration 35, loss = 0.01050487\n",
      "Iteration 36, loss = 0.01116793\n",
      "Iteration 37, loss = 0.01059816\n",
      "Iteration 38, loss = 0.01259466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29132978\n",
      "Iteration 2, loss = 0.09407267\n",
      "Iteration 3, loss = 0.03522728\n",
      "Iteration 4, loss = 0.02343092\n",
      "Iteration 5, loss = 0.01688323\n",
      "Iteration 6, loss = 0.01737199\n",
      "Iteration 7, loss = 0.01640321\n",
      "Iteration 8, loss = 0.03171385\n",
      "Iteration 9, loss = 0.04452247\n",
      "Iteration 10, loss = 0.01595233\n",
      "Iteration 11, loss = 0.01206029\n",
      "Iteration 12, loss = 0.01357342\n",
      "Iteration 13, loss = 0.01208686\n",
      "Iteration 14, loss = 0.01452502\n",
      "Iteration 15, loss = 0.01389255\n",
      "Iteration 16, loss = 0.01608937\n",
      "Iteration 17, loss = 0.01327198\n",
      "Iteration 18, loss = 0.02343652\n",
      "Iteration 19, loss = 0.01549800\n",
      "Iteration 20, loss = 0.01500354\n",
      "Iteration 21, loss = 0.01085149\n",
      "Iteration 22, loss = 0.01185903\n",
      "Iteration 23, loss = 0.01280969\n",
      "Iteration 24, loss = 0.25602961\n",
      "Iteration 25, loss = 0.27486661\n",
      "Iteration 26, loss = 0.12037277\n",
      "Iteration 27, loss = 0.06453547\n",
      "Iteration 28, loss = 0.04094924\n",
      "Iteration 29, loss = 0.03016264\n",
      "Iteration 30, loss = 0.02378337\n",
      "Iteration 31, loss = 0.02036813\n",
      "Iteration 32, loss = 0.01753965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17682078\n",
      "Iteration 2, loss = 0.04859580\n",
      "Iteration 3, loss = 0.03970293\n",
      "Iteration 4, loss = 0.03577821\n",
      "Iteration 5, loss = 0.03308409\n",
      "Iteration 6, loss = 0.03133604\n",
      "Iteration 7, loss = 0.03002117\n",
      "Iteration 8, loss = 0.02892977\n",
      "Iteration 9, loss = 0.02812464\n",
      "Iteration 10, loss = 0.02721934\n",
      "Iteration 11, loss = 0.02644649\n",
      "Iteration 12, loss = 0.02577003\n",
      "Iteration 13, loss = 0.02516851\n",
      "Iteration 14, loss = 0.02453706\n",
      "Iteration 15, loss = 0.02394461\n",
      "Iteration 16, loss = 0.02338218\n",
      "Iteration 17, loss = 0.02286229\n",
      "Iteration 18, loss = 0.02235341\n",
      "Iteration 19, loss = 0.02188885\n",
      "Iteration 20, loss = 0.02141560\n",
      "Iteration 21, loss = 0.02096835\n",
      "Iteration 22, loss = 0.02053445\n",
      "Iteration 23, loss = 0.02012336\n",
      "Iteration 24, loss = 0.01973535\n",
      "Iteration 25, loss = 0.01933346\n",
      "Iteration 26, loss = 0.01898425\n",
      "Iteration 27, loss = 0.01861330\n",
      "Iteration 28, loss = 0.01827131\n",
      "Iteration 29, loss = 0.01793980\n",
      "Iteration 30, loss = 0.01763493\n",
      "Iteration 31, loss = 0.01731184\n",
      "Iteration 32, loss = 0.01702843\n",
      "Iteration 33, loss = 0.01675046\n",
      "Iteration 34, loss = 0.01645775\n",
      "Iteration 35, loss = 0.01621162\n",
      "Iteration 36, loss = 0.01593513\n",
      "Iteration 37, loss = 0.01567380\n",
      "Iteration 38, loss = 0.01543563\n",
      "Iteration 39, loss = 0.01519493\n",
      "Iteration 40, loss = 0.01498418\n",
      "Iteration 41, loss = 0.01478046\n",
      "Iteration 42, loss = 0.01454706\n",
      "Iteration 43, loss = 0.01434560\n",
      "Iteration 44, loss = 0.01414581\n",
      "Iteration 45, loss = 0.01395969\n",
      "Iteration 46, loss = 0.01377136\n",
      "Iteration 47, loss = 0.01359060\n",
      "Iteration 48, loss = 0.01343644\n",
      "Iteration 49, loss = 0.01327036\n",
      "Iteration 50, loss = 0.01310644\n",
      "Iteration 51, loss = 0.01294958\n",
      "Iteration 52, loss = 0.01279834\n",
      "Iteration 53, loss = 0.01266389\n",
      "Iteration 54, loss = 0.01252658\n",
      "Iteration 55, loss = 0.01237098\n",
      "Iteration 56, loss = 0.01225519\n",
      "Iteration 57, loss = 0.01213276\n",
      "Iteration 58, loss = 0.01202510\n",
      "Iteration 59, loss = 0.01190017\n",
      "Iteration 60, loss = 0.01177883\n",
      "Iteration 61, loss = 0.01166791\n",
      "Iteration 62, loss = 0.01157259\n",
      "Iteration 63, loss = 0.01146319\n",
      "Iteration 64, loss = 0.01137925\n",
      "Iteration 65, loss = 0.01128149\n",
      "Iteration 66, loss = 0.01117209\n",
      "Iteration 67, loss = 0.01109438\n",
      "Iteration 68, loss = 0.01100113\n",
      "Iteration 69, loss = 0.01093536\n",
      "Iteration 70, loss = 0.01084159\n",
      "Iteration 71, loss = 0.01076377\n",
      "Iteration 72, loss = 0.01068549\n",
      "Iteration 73, loss = 0.01060972\n",
      "Iteration 74, loss = 0.01055388\n",
      "Iteration 75, loss = 0.01047483\n",
      "Iteration 76, loss = 0.01042369\n",
      "Iteration 77, loss = 0.01034874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21995094\n",
      "Iteration 2, loss = 0.05219458\n",
      "Iteration 3, loss = 0.03949444\n",
      "Iteration 4, loss = 0.03531424\n",
      "Iteration 5, loss = 0.03322797\n",
      "Iteration 6, loss = 0.03175943\n",
      "Iteration 7, loss = 0.03063072\n",
      "Iteration 8, loss = 0.02970783\n",
      "Iteration 9, loss = 0.02890118\n",
      "Iteration 10, loss = 0.02816718\n",
      "Iteration 11, loss = 0.02745085\n",
      "Iteration 12, loss = 0.02680536\n",
      "Iteration 13, loss = 0.02618756\n",
      "Iteration 14, loss = 0.02561427\n",
      "Iteration 15, loss = 0.02505979\n",
      "Iteration 16, loss = 0.02451008\n",
      "Iteration 17, loss = 0.02399172\n",
      "Iteration 18, loss = 0.02350435\n",
      "Iteration 19, loss = 0.02302567\n",
      "Iteration 20, loss = 0.02257904\n",
      "Iteration 21, loss = 0.02212127\n",
      "Iteration 22, loss = 0.02170298\n",
      "Iteration 23, loss = 0.02130429\n",
      "Iteration 24, loss = 0.02091250\n",
      "Iteration 25, loss = 0.02050101\n",
      "Iteration 26, loss = 0.02016090\n",
      "Iteration 27, loss = 0.01979406\n",
      "Iteration 28, loss = 0.01945662\n",
      "Iteration 29, loss = 0.01911544\n",
      "Iteration 30, loss = 0.01881531\n",
      "Iteration 31, loss = 0.01848762\n",
      "Iteration 32, loss = 0.01819021\n",
      "Iteration 33, loss = 0.01790601\n",
      "Iteration 34, loss = 0.01762312\n",
      "Iteration 35, loss = 0.01735985\n",
      "Iteration 36, loss = 0.01709586\n",
      "Iteration 37, loss = 0.01684817\n",
      "Iteration 38, loss = 0.01660351\n",
      "Iteration 39, loss = 0.01637049\n",
      "Iteration 40, loss = 0.01614218\n",
      "Iteration 41, loss = 0.01593444\n",
      "Iteration 42, loss = 0.01571879\n",
      "Iteration 43, loss = 0.01551064\n",
      "Iteration 44, loss = 0.01531302\n",
      "Iteration 45, loss = 0.01512761\n",
      "Iteration 46, loss = 0.01494246\n",
      "Iteration 47, loss = 0.01476695\n",
      "Iteration 48, loss = 0.01460054\n",
      "Iteration 49, loss = 0.01442767\n",
      "Iteration 50, loss = 0.01427048\n",
      "Iteration 51, loss = 0.01411324\n",
      "Iteration 52, loss = 0.01396620\n",
      "Iteration 53, loss = 0.01382430\n",
      "Iteration 54, loss = 0.01368098\n",
      "Iteration 55, loss = 0.01354778\n",
      "Iteration 56, loss = 0.01342272\n",
      "Iteration 57, loss = 0.01329792\n",
      "Iteration 58, loss = 0.01316636\n",
      "Iteration 59, loss = 0.01305713\n",
      "Iteration 60, loss = 0.01293796\n",
      "Iteration 61, loss = 0.01282665\n",
      "Iteration 62, loss = 0.01272831\n",
      "Iteration 63, loss = 0.01263082\n",
      "Iteration 64, loss = 0.01252606\n",
      "Iteration 65, loss = 0.01243483\n",
      "Iteration 66, loss = 0.01233716\n",
      "Iteration 67, loss = 0.01224738\n",
      "Iteration 68, loss = 0.01216652\n",
      "Iteration 69, loss = 0.01208226\n",
      "Iteration 70, loss = 0.01199761\n",
      "Iteration 71, loss = 0.01191935\n",
      "Iteration 72, loss = 0.01184762\n",
      "Iteration 73, loss = 0.01178128\n",
      "Iteration 74, loss = 0.01170339\n",
      "Iteration 75, loss = 0.01163266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19700639\n",
      "Iteration 2, loss = 0.04858460\n",
      "Iteration 3, loss = 0.04061014\n",
      "Iteration 4, loss = 0.03647422\n",
      "Iteration 5, loss = 0.03394469\n",
      "Iteration 6, loss = 0.03208956\n",
      "Iteration 7, loss = 0.03055686\n",
      "Iteration 8, loss = 0.02952700\n",
      "Iteration 9, loss = 0.02861207\n",
      "Iteration 10, loss = 0.02777636\n",
      "Iteration 11, loss = 0.02703004\n",
      "Iteration 12, loss = 0.02624729\n",
      "Iteration 13, loss = 0.02576332\n",
      "Iteration 14, loss = 0.02505634\n",
      "Iteration 15, loss = 0.02443900\n",
      "Iteration 16, loss = 0.02382042\n",
      "Iteration 17, loss = 0.02342272\n",
      "Iteration 18, loss = 0.02286768\n",
      "Iteration 19, loss = 0.02234667\n",
      "Iteration 20, loss = 0.02190208\n",
      "Iteration 21, loss = 0.02142658\n",
      "Iteration 22, loss = 0.02100360\n",
      "Iteration 23, loss = 0.02059486\n",
      "Iteration 24, loss = 0.02020246\n",
      "Iteration 25, loss = 0.01978865\n",
      "Iteration 26, loss = 0.01945473\n",
      "Iteration 27, loss = 0.01905675\n",
      "Iteration 28, loss = 0.01871149\n",
      "Iteration 29, loss = 0.01837893\n",
      "Iteration 30, loss = 0.01805738\n",
      "Iteration 31, loss = 0.01774995\n",
      "Iteration 32, loss = 0.01745289\n",
      "Iteration 33, loss = 0.01716999\n",
      "Iteration 34, loss = 0.01687795\n",
      "Iteration 35, loss = 0.01662070\n",
      "Iteration 36, loss = 0.01636370\n",
      "Iteration 37, loss = 0.01609013\n",
      "Iteration 38, loss = 0.01585722\n",
      "Iteration 39, loss = 0.01561489\n",
      "Iteration 40, loss = 0.01539296\n",
      "Iteration 41, loss = 0.01516822\n",
      "Iteration 42, loss = 0.01495960\n",
      "Iteration 43, loss = 0.01477106\n",
      "Iteration 44, loss = 0.01455931\n",
      "Iteration 45, loss = 0.01438236\n",
      "Iteration 46, loss = 0.01420453\n",
      "Iteration 47, loss = 0.01400650\n",
      "Iteration 48, loss = 0.01384039\n",
      "Iteration 49, loss = 0.01367682\n",
      "Iteration 50, loss = 0.01350902\n",
      "Iteration 51, loss = 0.01337294\n",
      "Iteration 52, loss = 0.01321839\n",
      "Iteration 53, loss = 0.01306126\n",
      "Iteration 54, loss = 0.01292898\n",
      "Iteration 55, loss = 0.01278809\n",
      "Iteration 56, loss = 0.01267417\n",
      "Iteration 57, loss = 0.01253685\n",
      "Iteration 58, loss = 0.01240576\n",
      "Iteration 59, loss = 0.01229084\n",
      "Iteration 60, loss = 0.01218211\n",
      "Iteration 61, loss = 0.01207232\n",
      "Iteration 62, loss = 0.01196782\n",
      "Iteration 63, loss = 0.01186179\n",
      "Iteration 64, loss = 0.01176817\n",
      "Iteration 65, loss = 0.01166765\n",
      "Iteration 66, loss = 0.01157834\n",
      "Iteration 67, loss = 0.01149200\n",
      "Iteration 68, loss = 0.01139815\n",
      "Iteration 69, loss = 0.01131312\n",
      "Iteration 70, loss = 0.01124221\n",
      "Iteration 71, loss = 0.01116602\n",
      "Iteration 72, loss = 0.01111646\n",
      "Iteration 73, loss = 0.01101246\n",
      "Iteration 74, loss = 0.01093983\n",
      "Iteration 75, loss = 0.01088146\n",
      "Iteration 76, loss = 0.01080171\n",
      "Iteration 77, loss = 0.01074965\n",
      "Iteration 78, loss = 0.01069626\n",
      "Iteration 79, loss = 0.01061958\n",
      "Iteration 80, loss = 0.01056508\n",
      "Iteration 81, loss = 0.01050952\n",
      "Iteration 82, loss = 0.01045801\n",
      "Iteration 83, loss = 0.01040664\n",
      "Iteration 84, loss = 0.01036691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22529061\n",
      "Iteration 2, loss = 0.05111230\n",
      "Iteration 3, loss = 0.03876092\n",
      "Iteration 4, loss = 0.03498951\n",
      "Iteration 5, loss = 0.03311179\n",
      "Iteration 6, loss = 0.03169568\n",
      "Iteration 7, loss = 0.03064720\n",
      "Iteration 8, loss = 0.02972869\n",
      "Iteration 9, loss = 0.02894998\n",
      "Iteration 10, loss = 0.02819787\n",
      "Iteration 11, loss = 0.02751109\n",
      "Iteration 12, loss = 0.02685593\n",
      "Iteration 13, loss = 0.02626606\n",
      "Iteration 14, loss = 0.02566627\n",
      "Iteration 15, loss = 0.02510189\n",
      "Iteration 16, loss = 0.02457448\n",
      "Iteration 17, loss = 0.02406744\n",
      "Iteration 18, loss = 0.02356465\n",
      "Iteration 19, loss = 0.02308416\n",
      "Iteration 20, loss = 0.02263442\n",
      "Iteration 21, loss = 0.02219187\n",
      "Iteration 22, loss = 0.02177619\n",
      "Iteration 23, loss = 0.02134790\n",
      "Iteration 24, loss = 0.02096288\n",
      "Iteration 25, loss = 0.02056895\n",
      "Iteration 26, loss = 0.02019308\n",
      "Iteration 27, loss = 0.01984005\n",
      "Iteration 28, loss = 0.01949280\n",
      "Iteration 29, loss = 0.01916212\n",
      "Iteration 30, loss = 0.01884900\n",
      "Iteration 31, loss = 0.01852841\n",
      "Iteration 32, loss = 0.01823381\n",
      "Iteration 33, loss = 0.01794450\n",
      "Iteration 34, loss = 0.01766593\n",
      "Iteration 35, loss = 0.01739763\n",
      "Iteration 36, loss = 0.01713680\n",
      "Iteration 37, loss = 0.01688632\n",
      "Iteration 38, loss = 0.01664701\n",
      "Iteration 39, loss = 0.01641792\n",
      "Iteration 40, loss = 0.01618379\n",
      "Iteration 41, loss = 0.01596282\n",
      "Iteration 42, loss = 0.01575749\n",
      "Iteration 43, loss = 0.01554454\n",
      "Iteration 44, loss = 0.01535438\n",
      "Iteration 45, loss = 0.01515750\n",
      "Iteration 46, loss = 0.01497686\n",
      "Iteration 47, loss = 0.01480091\n",
      "Iteration 48, loss = 0.01463068\n",
      "Iteration 49, loss = 0.01446277\n",
      "Iteration 50, loss = 0.01430704\n",
      "Iteration 51, loss = 0.01415187\n",
      "Iteration 52, loss = 0.01401163\n",
      "Iteration 53, loss = 0.01385901\n",
      "Iteration 54, loss = 0.01371999\n",
      "Iteration 55, loss = 0.01358664\n",
      "Iteration 56, loss = 0.01345723\n",
      "Iteration 57, loss = 0.01332764\n",
      "Iteration 58, loss = 0.01320514\n",
      "Iteration 59, loss = 0.01308411\n",
      "Iteration 60, loss = 0.01297695\n",
      "Iteration 61, loss = 0.01286886\n",
      "Iteration 62, loss = 0.01276486\n",
      "Iteration 63, loss = 0.01266211\n",
      "Iteration 64, loss = 0.01255610\n",
      "Iteration 65, loss = 0.01246073\n",
      "Iteration 66, loss = 0.01236817\n",
      "Iteration 67, loss = 0.01227956\n",
      "Iteration 68, loss = 0.01220761\n",
      "Iteration 69, loss = 0.01211204\n",
      "Iteration 70, loss = 0.01203305\n",
      "Iteration 71, loss = 0.01195610\n",
      "Iteration 72, loss = 0.01188130\n",
      "Iteration 73, loss = 0.01180850\n",
      "Iteration 74, loss = 0.01173590\n",
      "Iteration 75, loss = 0.01167744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18989402\n",
      "Iteration 2, loss = 0.03822668\n",
      "Iteration 3, loss = 0.03355892\n",
      "Iteration 4, loss = 0.03183250\n",
      "Iteration 5, loss = 0.03063883\n",
      "Iteration 6, loss = 0.02960365\n",
      "Iteration 7, loss = 0.02870577\n",
      "Iteration 8, loss = 0.02785456\n",
      "Iteration 9, loss = 0.02708089\n",
      "Iteration 10, loss = 0.02634579\n",
      "Iteration 11, loss = 0.02565541\n",
      "Iteration 12, loss = 0.02499374\n",
      "Iteration 13, loss = 0.02440401\n",
      "Iteration 14, loss = 0.02379589\n",
      "Iteration 15, loss = 0.02320999\n",
      "Iteration 16, loss = 0.02266682\n",
      "Iteration 17, loss = 0.02213571\n",
      "Iteration 18, loss = 0.02165584\n",
      "Iteration 19, loss = 0.02115505\n",
      "Iteration 20, loss = 0.02068998\n",
      "Iteration 21, loss = 0.02023708\n",
      "Iteration 22, loss = 0.01981184\n",
      "Iteration 23, loss = 0.01939266\n",
      "Iteration 24, loss = 0.01900483\n",
      "Iteration 25, loss = 0.01860899\n",
      "Iteration 26, loss = 0.01823744\n",
      "Iteration 27, loss = 0.01787007\n",
      "Iteration 28, loss = 0.01753456\n",
      "Iteration 29, loss = 0.01718858\n",
      "Iteration 30, loss = 0.01687641\n",
      "Iteration 31, loss = 0.01654579\n",
      "Iteration 32, loss = 0.01626044\n",
      "Iteration 33, loss = 0.01595877\n",
      "Iteration 34, loss = 0.01567679\n",
      "Iteration 35, loss = 0.01540410\n",
      "Iteration 36, loss = 0.01514189\n",
      "Iteration 37, loss = 0.01489103\n",
      "Iteration 38, loss = 0.01463994\n",
      "Iteration 39, loss = 0.01441227\n",
      "Iteration 40, loss = 0.01417754\n",
      "Iteration 41, loss = 0.01397513\n",
      "Iteration 42, loss = 0.01374983\n",
      "Iteration 43, loss = 0.01355734\n",
      "Iteration 44, loss = 0.01335662\n",
      "Iteration 45, loss = 0.01315379\n",
      "Iteration 46, loss = 0.01297304\n",
      "Iteration 47, loss = 0.01280953\n",
      "Iteration 48, loss = 0.01262488\n",
      "Iteration 49, loss = 0.01245346\n",
      "Iteration 50, loss = 0.01229383\n",
      "Iteration 51, loss = 0.01213297\n",
      "Iteration 52, loss = 0.01198628\n",
      "Iteration 53, loss = 0.01185039\n",
      "Iteration 54, loss = 0.01170069\n",
      "Iteration 55, loss = 0.01156518\n",
      "Iteration 56, loss = 0.01143749\n",
      "Iteration 57, loss = 0.01131101\n",
      "Iteration 58, loss = 0.01118923\n",
      "Iteration 59, loss = 0.01107136\n",
      "Iteration 60, loss = 0.01095468\n",
      "Iteration 61, loss = 0.01084427\n",
      "Iteration 62, loss = 0.01074559\n",
      "Iteration 63, loss = 0.01064440\n",
      "Iteration 64, loss = 0.01053983\n",
      "Iteration 65, loss = 0.01044702\n",
      "Iteration 66, loss = 0.01034555\n",
      "Iteration 67, loss = 0.01025998\n",
      "Iteration 68, loss = 0.01018334\n",
      "Iteration 69, loss = 0.01009235\n",
      "Iteration 70, loss = 0.01001036\n",
      "Iteration 71, loss = 0.00994633\n",
      "Iteration 72, loss = 0.00985070\n",
      "Iteration 73, loss = 0.00979599\n",
      "Iteration 74, loss = 0.00971408\n",
      "Iteration 75, loss = 0.00964733\n",
      "Iteration 76, loss = 0.00959219\n",
      "Iteration 77, loss = 0.00951591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11892503\n",
      "Iteration 2, loss = 0.03096014\n",
      "Iteration 3, loss = 0.02225414\n",
      "Iteration 4, loss = 0.01768593\n",
      "Iteration 5, loss = 0.01497073\n",
      "Iteration 6, loss = 0.01318264\n",
      "Iteration 7, loss = 0.01195519\n",
      "Iteration 8, loss = 0.01121486\n",
      "Iteration 9, loss = 0.01070274\n",
      "Iteration 10, loss = 0.01033808\n",
      "Iteration 11, loss = 0.01025746\n",
      "Iteration 12, loss = 0.00998255\n",
      "Iteration 13, loss = 0.01124382\n",
      "Iteration 14, loss = 0.00984901\n",
      "Iteration 15, loss = 0.00939883\n",
      "Iteration 16, loss = 0.00961437\n",
      "Iteration 17, loss = 0.00940557\n",
      "Iteration 18, loss = 0.00976154\n",
      "Iteration 19, loss = 0.00927601\n",
      "Iteration 20, loss = 0.00934432\n",
      "Iteration 21, loss = 0.00906634\n",
      "Iteration 22, loss = 0.00953552\n",
      "Iteration 23, loss = 0.01016279\n",
      "Iteration 24, loss = 0.01081055\n",
      "Iteration 25, loss = 0.00962743\n",
      "Iteration 26, loss = 0.00917080\n",
      "Iteration 27, loss = 0.00893342\n",
      "Iteration 28, loss = 0.00904159\n",
      "Iteration 29, loss = 0.00886021\n",
      "Iteration 30, loss = 0.00934882\n",
      "Iteration 31, loss = 0.00930589\n",
      "Iteration 32, loss = 0.00937431\n",
      "Iteration 33, loss = 0.01360853\n",
      "Iteration 34, loss = 0.01249733\n",
      "Iteration 35, loss = 0.01051695\n",
      "Iteration 36, loss = 0.00958404\n",
      "Iteration 37, loss = 0.00905454\n",
      "Iteration 38, loss = 0.00891774\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13911376\n",
      "Iteration 2, loss = 0.03313122\n",
      "Iteration 3, loss = 0.02386632\n",
      "Iteration 4, loss = 0.01902252\n",
      "Iteration 5, loss = 0.01614608\n",
      "Iteration 6, loss = 0.01444603\n",
      "Iteration 7, loss = 0.01322613\n",
      "Iteration 8, loss = 0.01226239\n",
      "Iteration 9, loss = 0.01177782\n",
      "Iteration 10, loss = 0.01148449\n",
      "Iteration 11, loss = 0.01111403\n",
      "Iteration 12, loss = 0.01111043\n",
      "Iteration 13, loss = 0.01065385\n",
      "Iteration 14, loss = 0.01088467\n",
      "Iteration 15, loss = 0.01057654\n",
      "Iteration 16, loss = 0.01048485\n",
      "Iteration 17, loss = 0.01045080\n",
      "Iteration 18, loss = 0.01074181\n",
      "Iteration 19, loss = 0.01023691\n",
      "Iteration 20, loss = 0.01027186\n",
      "Iteration 21, loss = 0.01093331\n",
      "Iteration 22, loss = 0.01031704\n",
      "Iteration 23, loss = 0.01076913\n",
      "Iteration 24, loss = 0.01204096\n",
      "Iteration 25, loss = 0.01143762\n",
      "Iteration 26, loss = 0.01015361\n",
      "Iteration 27, loss = 0.01060280\n",
      "Iteration 28, loss = 0.01012196\n",
      "Iteration 29, loss = 0.00993091\n",
      "Iteration 30, loss = 0.01006235\n",
      "Iteration 31, loss = 0.01004957\n",
      "Iteration 32, loss = 0.01022697\n",
      "Iteration 33, loss = 0.01293487\n",
      "Iteration 34, loss = 0.01235107\n",
      "Iteration 35, loss = 0.01033131\n",
      "Iteration 36, loss = 0.01000364\n",
      "Iteration 37, loss = 0.01006392\n",
      "Iteration 38, loss = 0.01008316\n",
      "Iteration 39, loss = 0.01051914\n",
      "Iteration 40, loss = 0.01022479\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12622636\n",
      "Iteration 2, loss = 0.03179794\n",
      "Iteration 3, loss = 0.02264457\n",
      "Iteration 4, loss = 0.01803465\n",
      "Iteration 5, loss = 0.01590135\n",
      "Iteration 6, loss = 0.01399586\n",
      "Iteration 7, loss = 0.01243812\n",
      "Iteration 8, loss = 0.01140382\n",
      "Iteration 9, loss = 0.01164703\n",
      "Iteration 10, loss = 0.01102133\n",
      "Iteration 11, loss = 0.01069516\n",
      "Iteration 12, loss = 0.01079597\n",
      "Iteration 13, loss = 0.01067625\n",
      "Iteration 14, loss = 0.00999167\n",
      "Iteration 15, loss = 0.01084174\n",
      "Iteration 16, loss = 0.00981444\n",
      "Iteration 17, loss = 0.01014958\n",
      "Iteration 18, loss = 0.01032552\n",
      "Iteration 19, loss = 0.01052830\n",
      "Iteration 20, loss = 0.00965481\n",
      "Iteration 21, loss = 0.00989563\n",
      "Iteration 22, loss = 0.01042912\n",
      "Iteration 23, loss = 0.00968881\n",
      "Iteration 24, loss = 0.01263378\n",
      "Iteration 25, loss = 0.01326522\n",
      "Iteration 26, loss = 0.01056063\n",
      "Iteration 27, loss = 0.01012345\n",
      "Iteration 28, loss = 0.00994411\n",
      "Iteration 29, loss = 0.00951093\n",
      "Iteration 30, loss = 0.00980369\n",
      "Iteration 31, loss = 0.00965241\n",
      "Iteration 32, loss = 0.01107636\n",
      "Iteration 33, loss = 0.01140610\n",
      "Iteration 34, loss = 0.00977752\n",
      "Iteration 35, loss = 0.00953612\n",
      "Iteration 36, loss = 0.01025274\n",
      "Iteration 37, loss = 0.01027520\n",
      "Iteration 38, loss = 0.00982998\n",
      "Iteration 39, loss = 0.00943741\n",
      "Iteration 40, loss = 0.00977664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13818673\n",
      "Iteration 2, loss = 0.03420147\n",
      "Iteration 3, loss = 0.02418028\n",
      "Iteration 4, loss = 0.01914349\n",
      "Iteration 5, loss = 0.01632171\n",
      "Iteration 6, loss = 0.01430499\n",
      "Iteration 7, loss = 0.01312820\n",
      "Iteration 8, loss = 0.01238547\n",
      "Iteration 9, loss = 0.01178574\n",
      "Iteration 10, loss = 0.01129294\n",
      "Iteration 11, loss = 0.01098072\n",
      "Iteration 12, loss = 0.01067171\n",
      "Iteration 13, loss = 0.01058611\n",
      "Iteration 14, loss = 0.01053037\n",
      "Iteration 15, loss = 0.01035078\n",
      "Iteration 16, loss = 0.01071116\n",
      "Iteration 17, loss = 0.01046021\n",
      "Iteration 18, loss = 0.01019028\n",
      "Iteration 19, loss = 0.01064864\n",
      "Iteration 20, loss = 0.01100544\n",
      "Iteration 21, loss = 0.01297058\n",
      "Iteration 22, loss = 0.01258267\n",
      "Iteration 23, loss = 0.01033985\n",
      "Iteration 24, loss = 0.01011371\n",
      "Iteration 25, loss = 0.01029764\n",
      "Iteration 26, loss = 0.01039030\n",
      "Iteration 27, loss = 0.01014764\n",
      "Iteration 28, loss = 0.01004731\n",
      "Iteration 29, loss = 0.01037428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11148637\n",
      "Iteration 2, loss = 0.02698009\n",
      "Iteration 3, loss = 0.01983906\n",
      "Iteration 4, loss = 0.01588815\n",
      "Iteration 5, loss = 0.01357084\n",
      "Iteration 6, loss = 0.01220395\n",
      "Iteration 7, loss = 0.01103420\n",
      "Iteration 8, loss = 0.01026451\n",
      "Iteration 9, loss = 0.00992041\n",
      "Iteration 10, loss = 0.00943538\n",
      "Iteration 11, loss = 0.00967591\n",
      "Iteration 12, loss = 0.00925820\n",
      "Iteration 13, loss = 0.00922043\n",
      "Iteration 14, loss = 0.00976003\n",
      "Iteration 15, loss = 0.00880338\n",
      "Iteration 16, loss = 0.00969438\n",
      "Iteration 17, loss = 0.00860851\n",
      "Iteration 18, loss = 0.00999824\n",
      "Iteration 19, loss = 0.00896790\n",
      "Iteration 20, loss = 0.00869254\n",
      "Iteration 21, loss = 0.00891925\n",
      "Iteration 22, loss = 0.00830208\n",
      "Iteration 23, loss = 0.00920917\n",
      "Iteration 24, loss = 0.00920378\n",
      "Iteration 25, loss = 0.00876896\n",
      "Iteration 26, loss = 0.00938646\n",
      "Iteration 27, loss = 0.00855943\n",
      "Iteration 28, loss = 0.00841744\n",
      "Iteration 29, loss = 0.00967802\n",
      "Iteration 30, loss = 0.00909300\n",
      "Iteration 31, loss = 0.00887235\n",
      "Iteration 32, loss = 0.00967473\n",
      "Iteration 33, loss = 0.01097227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45487021\n",
      "Iteration 2, loss = 0.16357670\n",
      "Iteration 3, loss = 0.11046177\n",
      "Iteration 4, loss = 0.09042493\n",
      "Iteration 5, loss = 0.07878217\n",
      "Iteration 6, loss = 0.07070050\n",
      "Iteration 7, loss = 0.06473942\n",
      "Iteration 8, loss = 0.06032251\n",
      "Iteration 9, loss = 0.05685855\n",
      "Iteration 10, loss = 0.05404913\n",
      "Iteration 11, loss = 0.05168909\n",
      "Iteration 12, loss = 0.04975735\n",
      "Iteration 13, loss = 0.04815806\n",
      "Iteration 14, loss = 0.04667948\n",
      "Iteration 15, loss = 0.04543357\n",
      "Iteration 16, loss = 0.04428324\n",
      "Iteration 17, loss = 0.04329165\n",
      "Iteration 18, loss = 0.04238543\n",
      "Iteration 19, loss = 0.04154528\n",
      "Iteration 20, loss = 0.04078207\n",
      "Iteration 21, loss = 0.04010613\n",
      "Iteration 22, loss = 0.03947926\n",
      "Iteration 23, loss = 0.03888340\n",
      "Iteration 24, loss = 0.03834117\n",
      "Iteration 25, loss = 0.03783610\n",
      "Iteration 26, loss = 0.03735738\n",
      "Iteration 27, loss = 0.03691274\n",
      "Iteration 28, loss = 0.03648345\n",
      "Iteration 29, loss = 0.03610005\n",
      "Iteration 30, loss = 0.03572601\n",
      "Iteration 31, loss = 0.03539229\n",
      "Iteration 32, loss = 0.03504660\n",
      "Iteration 33, loss = 0.03474398\n",
      "Iteration 34, loss = 0.03443970\n",
      "Iteration 35, loss = 0.03417910\n",
      "Iteration 36, loss = 0.03388780\n",
      "Iteration 37, loss = 0.03362276\n",
      "Iteration 38, loss = 0.03335248\n",
      "Iteration 39, loss = 0.03311469\n",
      "Iteration 40, loss = 0.03293871\n",
      "Iteration 41, loss = 0.03267916\n",
      "Iteration 42, loss = 0.03245855\n",
      "Iteration 43, loss = 0.03226025\n",
      "Iteration 44, loss = 0.03205892\n",
      "Iteration 45, loss = 0.03187322\n",
      "Iteration 46, loss = 0.03168468\n",
      "Iteration 47, loss = 0.03151740\n",
      "Iteration 48, loss = 0.03135494\n",
      "Iteration 49, loss = 0.03117018\n",
      "Iteration 50, loss = 0.03101822\n",
      "Iteration 51, loss = 0.03085775\n",
      "Iteration 52, loss = 0.03071281\n",
      "Iteration 53, loss = 0.03056504\n",
      "Iteration 54, loss = 0.03043771\n",
      "Iteration 55, loss = 0.03028202\n",
      "Iteration 56, loss = 0.03014876\n",
      "Iteration 57, loss = 0.03004791\n",
      "Iteration 58, loss = 0.02988602\n",
      "Iteration 59, loss = 0.02976934\n",
      "Iteration 60, loss = 0.02964738\n",
      "Iteration 61, loss = 0.02953183\n",
      "Iteration 62, loss = 0.02940913\n",
      "Iteration 63, loss = 0.02929240\n",
      "Iteration 64, loss = 0.02918368\n",
      "Iteration 65, loss = 0.02907670\n",
      "Iteration 66, loss = 0.02896093\n",
      "Iteration 67, loss = 0.02886084\n",
      "Iteration 68, loss = 0.02876744\n",
      "Iteration 69, loss = 0.02865101\n",
      "Iteration 70, loss = 0.02855405\n",
      "Iteration 71, loss = 0.02845243\n",
      "Iteration 72, loss = 0.02835235\n",
      "Iteration 73, loss = 0.02826165\n",
      "Iteration 74, loss = 0.02817831\n",
      "Iteration 75, loss = 0.02807206\n",
      "Iteration 76, loss = 0.02798679\n",
      "Iteration 77, loss = 0.02788892\n",
      "Iteration 78, loss = 0.02780009\n",
      "Iteration 79, loss = 0.02772206\n",
      "Iteration 80, loss = 0.02762791\n",
      "Iteration 81, loss = 0.02754158\n",
      "Iteration 82, loss = 0.02745793\n",
      "Iteration 83, loss = 0.02737445\n",
      "Iteration 84, loss = 0.02730013\n",
      "Iteration 85, loss = 0.02720986\n",
      "Iteration 86, loss = 0.02713125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50600908\n",
      "Iteration 2, loss = 0.23518712\n",
      "Iteration 3, loss = 0.15874859\n",
      "Iteration 4, loss = 0.12470106\n",
      "Iteration 5, loss = 0.10463354\n",
      "Iteration 6, loss = 0.09050137\n",
      "Iteration 7, loss = 0.08000122\n",
      "Iteration 8, loss = 0.07200451\n",
      "Iteration 9, loss = 0.06583915\n",
      "Iteration 10, loss = 0.06116244\n",
      "Iteration 11, loss = 0.05730440\n",
      "Iteration 12, loss = 0.05429767\n",
      "Iteration 13, loss = 0.05179942\n",
      "Iteration 14, loss = 0.04969569\n",
      "Iteration 15, loss = 0.04790953\n",
      "Iteration 16, loss = 0.04630996\n",
      "Iteration 17, loss = 0.04498407\n",
      "Iteration 18, loss = 0.04380130\n",
      "Iteration 19, loss = 0.04272716\n",
      "Iteration 20, loss = 0.04180729\n",
      "Iteration 21, loss = 0.04096934\n",
      "Iteration 22, loss = 0.04023362\n",
      "Iteration 23, loss = 0.03954827\n",
      "Iteration 24, loss = 0.03893646\n",
      "Iteration 25, loss = 0.03833558\n",
      "Iteration 26, loss = 0.03784197\n",
      "Iteration 27, loss = 0.03735206\n",
      "Iteration 28, loss = 0.03690585\n",
      "Iteration 29, loss = 0.03649826\n",
      "Iteration 30, loss = 0.03611676\n",
      "Iteration 31, loss = 0.03575364\n",
      "Iteration 32, loss = 0.03541495\n",
      "Iteration 33, loss = 0.03511476\n",
      "Iteration 34, loss = 0.03481877\n",
      "Iteration 35, loss = 0.03454528\n",
      "Iteration 36, loss = 0.03427548\n",
      "Iteration 37, loss = 0.03402462\n",
      "Iteration 38, loss = 0.03379321\n",
      "Iteration 39, loss = 0.03356760\n",
      "Iteration 40, loss = 0.03337127\n",
      "Iteration 41, loss = 0.03315409\n",
      "Iteration 42, loss = 0.03296213\n",
      "Iteration 43, loss = 0.03277436\n",
      "Iteration 44, loss = 0.03259566\n",
      "Iteration 45, loss = 0.03241992\n",
      "Iteration 46, loss = 0.03225956\n",
      "Iteration 47, loss = 0.03209619\n",
      "Iteration 48, loss = 0.03194335\n",
      "Iteration 49, loss = 0.03179012\n",
      "Iteration 50, loss = 0.03164858\n",
      "Iteration 51, loss = 0.03150673\n",
      "Iteration 52, loss = 0.03138354\n",
      "Iteration 53, loss = 0.03124387\n",
      "Iteration 54, loss = 0.03111470\n",
      "Iteration 55, loss = 0.03098981\n",
      "Iteration 56, loss = 0.03086630\n",
      "Iteration 57, loss = 0.03075250\n",
      "Iteration 58, loss = 0.03063877\n",
      "Iteration 59, loss = 0.03052588\n",
      "Iteration 60, loss = 0.03041302\n",
      "Iteration 61, loss = 0.03031164\n",
      "Iteration 62, loss = 0.03020652\n",
      "Iteration 63, loss = 0.03009560\n",
      "Iteration 64, loss = 0.02999336\n",
      "Iteration 65, loss = 0.02989850\n",
      "Iteration 66, loss = 0.02979766\n",
      "Iteration 67, loss = 0.02970490\n",
      "Iteration 68, loss = 0.02960692\n",
      "Iteration 69, loss = 0.02951560\n",
      "Iteration 70, loss = 0.02942394\n",
      "Iteration 71, loss = 0.02933196\n",
      "Iteration 72, loss = 0.02924562\n",
      "Iteration 73, loss = 0.02916120\n",
      "Iteration 74, loss = 0.02907100\n",
      "Iteration 75, loss = 0.02898747\n",
      "Iteration 76, loss = 0.02890496\n",
      "Iteration 77, loss = 0.02882024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49800661\n",
      "Iteration 2, loss = 0.19815588\n",
      "Iteration 3, loss = 0.12629032\n",
      "Iteration 4, loss = 0.09764498\n",
      "Iteration 5, loss = 0.08220509\n",
      "Iteration 6, loss = 0.07244686\n",
      "Iteration 7, loss = 0.06598425\n",
      "Iteration 8, loss = 0.06113184\n",
      "Iteration 9, loss = 0.05759887\n",
      "Iteration 10, loss = 0.05470367\n",
      "Iteration 11, loss = 0.05248007\n",
      "Iteration 12, loss = 0.05061913\n",
      "Iteration 13, loss = 0.04911212\n",
      "Iteration 14, loss = 0.04774917\n",
      "Iteration 15, loss = 0.04652937\n",
      "Iteration 16, loss = 0.04552444\n",
      "Iteration 17, loss = 0.04460131\n",
      "Iteration 18, loss = 0.04373796\n",
      "Iteration 19, loss = 0.04294815\n",
      "Iteration 20, loss = 0.04224013\n",
      "Iteration 21, loss = 0.04158456\n",
      "Iteration 22, loss = 0.04100083\n",
      "Iteration 23, loss = 0.04041770\n",
      "Iteration 24, loss = 0.03990843\n",
      "Iteration 25, loss = 0.03945659\n",
      "Iteration 26, loss = 0.03888161\n",
      "Iteration 27, loss = 0.03841408\n",
      "Iteration 28, loss = 0.03798893\n",
      "Iteration 29, loss = 0.03757162\n",
      "Iteration 30, loss = 0.03717881\n",
      "Iteration 31, loss = 0.03677993\n",
      "Iteration 32, loss = 0.03642801\n",
      "Iteration 33, loss = 0.03605143\n",
      "Iteration 34, loss = 0.03574444\n",
      "Iteration 35, loss = 0.03540270\n",
      "Iteration 36, loss = 0.03507978\n",
      "Iteration 37, loss = 0.03476800\n",
      "Iteration 38, loss = 0.03451002\n",
      "Iteration 39, loss = 0.03421340\n",
      "Iteration 40, loss = 0.03395068\n",
      "Iteration 41, loss = 0.03368904\n",
      "Iteration 42, loss = 0.03344924\n",
      "Iteration 43, loss = 0.03322098\n",
      "Iteration 44, loss = 0.03299220\n",
      "Iteration 45, loss = 0.03281287\n",
      "Iteration 46, loss = 0.03257460\n",
      "Iteration 47, loss = 0.03237980\n",
      "Iteration 48, loss = 0.03217926\n",
      "Iteration 49, loss = 0.03200309\n",
      "Iteration 50, loss = 0.03182237\n",
      "Iteration 51, loss = 0.03165745\n",
      "Iteration 52, loss = 0.03147701\n",
      "Iteration 53, loss = 0.03131583\n",
      "Iteration 54, loss = 0.03116430\n",
      "Iteration 55, loss = 0.03100242\n",
      "Iteration 56, loss = 0.03086382\n",
      "Iteration 57, loss = 0.03072338\n",
      "Iteration 58, loss = 0.03057737\n",
      "Iteration 59, loss = 0.03044676\n",
      "Iteration 60, loss = 0.03031800\n",
      "Iteration 61, loss = 0.03018612\n",
      "Iteration 62, loss = 0.03006173\n",
      "Iteration 63, loss = 0.02993216\n",
      "Iteration 64, loss = 0.02982069\n",
      "Iteration 65, loss = 0.02969909\n",
      "Iteration 66, loss = 0.02959090\n",
      "Iteration 67, loss = 0.02947724\n",
      "Iteration 68, loss = 0.02936801\n",
      "Iteration 69, loss = 0.02926224\n",
      "Iteration 70, loss = 0.02916174\n",
      "Iteration 71, loss = 0.02906593\n",
      "Iteration 72, loss = 0.02895591\n",
      "Iteration 73, loss = 0.02886179\n",
      "Iteration 74, loss = 0.02875968\n",
      "Iteration 75, loss = 0.02866176\n",
      "Iteration 76, loss = 0.02856390\n",
      "Iteration 77, loss = 0.02846785\n",
      "Iteration 78, loss = 0.02838154\n",
      "Iteration 79, loss = 0.02829132\n",
      "Iteration 80, loss = 0.02819950\n",
      "Iteration 81, loss = 0.02811086\n",
      "Iteration 82, loss = 0.02802387\n",
      "Iteration 83, loss = 0.02793958\n",
      "Iteration 84, loss = 0.02785928\n",
      "Iteration 85, loss = 0.02777187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53191035\n",
      "Iteration 2, loss = 0.24565781\n",
      "Iteration 3, loss = 0.15836387\n",
      "Iteration 4, loss = 0.12191097\n",
      "Iteration 5, loss = 0.10126815\n",
      "Iteration 6, loss = 0.08751112\n",
      "Iteration 7, loss = 0.07745922\n",
      "Iteration 8, loss = 0.07023842\n",
      "Iteration 9, loss = 0.06461636\n",
      "Iteration 10, loss = 0.06020837\n",
      "Iteration 11, loss = 0.05666256\n",
      "Iteration 12, loss = 0.05375761\n",
      "Iteration 13, loss = 0.05141337\n",
      "Iteration 14, loss = 0.04935320\n",
      "Iteration 15, loss = 0.04757166\n",
      "Iteration 16, loss = 0.04607807\n",
      "Iteration 17, loss = 0.04476306\n",
      "Iteration 18, loss = 0.04358534\n",
      "Iteration 19, loss = 0.04254782\n",
      "Iteration 20, loss = 0.04164044\n",
      "Iteration 21, loss = 0.04083845\n",
      "Iteration 22, loss = 0.04017915\n",
      "Iteration 23, loss = 0.03942815\n",
      "Iteration 24, loss = 0.03881978\n",
      "Iteration 25, loss = 0.03824501\n",
      "Iteration 26, loss = 0.03772729\n",
      "Iteration 27, loss = 0.03726451\n",
      "Iteration 28, loss = 0.03683097\n",
      "Iteration 29, loss = 0.03642299\n",
      "Iteration 30, loss = 0.03607350\n",
      "Iteration 31, loss = 0.03569717\n",
      "Iteration 32, loss = 0.03537542\n",
      "Iteration 33, loss = 0.03507182\n",
      "Iteration 34, loss = 0.03478707\n",
      "Iteration 35, loss = 0.03451703\n",
      "Iteration 36, loss = 0.03424846\n",
      "Iteration 37, loss = 0.03400676\n",
      "Iteration 38, loss = 0.03379110\n",
      "Iteration 39, loss = 0.03356209\n",
      "Iteration 40, loss = 0.03334815\n",
      "Iteration 41, loss = 0.03315529\n",
      "Iteration 42, loss = 0.03296008\n",
      "Iteration 43, loss = 0.03278481\n",
      "Iteration 44, loss = 0.03259617\n",
      "Iteration 45, loss = 0.03243232\n",
      "Iteration 46, loss = 0.03227195\n",
      "Iteration 47, loss = 0.03212048\n",
      "Iteration 48, loss = 0.03196550\n",
      "Iteration 49, loss = 0.03182353\n",
      "Iteration 50, loss = 0.03168503\n",
      "Iteration 51, loss = 0.03154383\n",
      "Iteration 52, loss = 0.03140786\n",
      "Iteration 53, loss = 0.03128172\n",
      "Iteration 54, loss = 0.03115287\n",
      "Iteration 55, loss = 0.03103100\n",
      "Iteration 56, loss = 0.03090687\n",
      "Iteration 57, loss = 0.03079471\n",
      "Iteration 58, loss = 0.03068193\n",
      "Iteration 59, loss = 0.03057135\n",
      "Iteration 60, loss = 0.03046501\n",
      "Iteration 61, loss = 0.03036319\n",
      "Iteration 62, loss = 0.03024992\n",
      "Iteration 63, loss = 0.03015039\n",
      "Iteration 64, loss = 0.03004947\n",
      "Iteration 65, loss = 0.02994916\n",
      "Iteration 66, loss = 0.02985144\n",
      "Iteration 67, loss = 0.02976271\n",
      "Iteration 68, loss = 0.02966348\n",
      "Iteration 69, loss = 0.02957393\n",
      "Iteration 70, loss = 0.02948654\n",
      "Iteration 71, loss = 0.02939516\n",
      "Iteration 72, loss = 0.02930786\n",
      "Iteration 73, loss = 0.02922164\n",
      "Iteration 74, loss = 0.02913685\n",
      "Iteration 75, loss = 0.02905733\n",
      "Iteration 76, loss = 0.02896974\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49591667\n",
      "Iteration 2, loss = 0.20067834\n",
      "Iteration 3, loss = 0.12251779\n",
      "Iteration 4, loss = 0.09072814\n",
      "Iteration 5, loss = 0.07409129\n",
      "Iteration 6, loss = 0.06396891\n",
      "Iteration 7, loss = 0.05722852\n",
      "Iteration 8, loss = 0.05233882\n",
      "Iteration 9, loss = 0.04873755\n",
      "Iteration 10, loss = 0.04603289\n",
      "Iteration 11, loss = 0.04387364\n",
      "Iteration 12, loss = 0.04215925\n",
      "Iteration 13, loss = 0.04075796\n",
      "Iteration 14, loss = 0.03953874\n",
      "Iteration 15, loss = 0.03855681\n",
      "Iteration 16, loss = 0.03770409\n",
      "Iteration 17, loss = 0.03696866\n",
      "Iteration 18, loss = 0.03633237\n",
      "Iteration 19, loss = 0.03576385\n",
      "Iteration 20, loss = 0.03526842\n",
      "Iteration 21, loss = 0.03482006\n",
      "Iteration 22, loss = 0.03441184\n",
      "Iteration 23, loss = 0.03405019\n",
      "Iteration 24, loss = 0.03370912\n",
      "Iteration 25, loss = 0.03340275\n",
      "Iteration 26, loss = 0.03313008\n",
      "Iteration 27, loss = 0.03285789\n",
      "Iteration 28, loss = 0.03261215\n",
      "Iteration 29, loss = 0.03237638\n",
      "Iteration 30, loss = 0.03216189\n",
      "Iteration 31, loss = 0.03195065\n",
      "Iteration 32, loss = 0.03176055\n",
      "Iteration 33, loss = 0.03157378\n",
      "Iteration 34, loss = 0.03139547\n",
      "Iteration 35, loss = 0.03122279\n",
      "Iteration 36, loss = 0.03106154\n",
      "Iteration 37, loss = 0.03090638\n",
      "Iteration 38, loss = 0.03075631\n",
      "Iteration 39, loss = 0.03061209\n",
      "Iteration 40, loss = 0.03047881\n",
      "Iteration 41, loss = 0.03034115\n",
      "Iteration 42, loss = 0.03020319\n",
      "Iteration 43, loss = 0.03008855\n",
      "Iteration 44, loss = 0.02995688\n",
      "Iteration 45, loss = 0.02982941\n",
      "Iteration 46, loss = 0.02971728\n",
      "Iteration 47, loss = 0.02960511\n",
      "Iteration 48, loss = 0.02949843\n",
      "Iteration 49, loss = 0.02937585\n",
      "Iteration 50, loss = 0.02926864\n",
      "Iteration 51, loss = 0.02915472\n",
      "Iteration 52, loss = 0.02905067\n",
      "Iteration 53, loss = 0.02895041\n",
      "Iteration 54, loss = 0.02884714\n",
      "Iteration 55, loss = 0.02875335\n",
      "Iteration 56, loss = 0.02865044\n",
      "Iteration 57, loss = 0.02855582\n",
      "Iteration 58, loss = 0.02845834\n",
      "Iteration 59, loss = 0.02836807\n",
      "Iteration 60, loss = 0.02827608\n",
      "Iteration 61, loss = 0.02818135\n",
      "Iteration 62, loss = 0.02809364\n",
      "Iteration 63, loss = 0.02800571\n",
      "Iteration 64, loss = 0.02791547\n",
      "Iteration 65, loss = 0.02783359\n",
      "Iteration 66, loss = 0.02774846\n",
      "Iteration 67, loss = 0.02766527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43569611\n",
      "Iteration 2, loss = 0.14262277\n",
      "Iteration 3, loss = 0.07756198\n",
      "Iteration 4, loss = 0.05472315\n",
      "Iteration 5, loss = 0.04387513\n",
      "Iteration 6, loss = 0.03773386\n",
      "Iteration 7, loss = 0.03355242\n",
      "Iteration 8, loss = 0.03064921\n",
      "Iteration 9, loss = 0.02846951\n",
      "Iteration 10, loss = 0.02660430\n",
      "Iteration 11, loss = 0.02509393\n",
      "Iteration 12, loss = 0.02388103\n",
      "Iteration 13, loss = 0.02285762\n",
      "Iteration 14, loss = 0.02185503\n",
      "Iteration 15, loss = 0.02100398\n",
      "Iteration 16, loss = 0.02022604\n",
      "Iteration 17, loss = 0.01954257\n",
      "Iteration 18, loss = 0.01888393\n",
      "Iteration 19, loss = 0.01831088\n",
      "Iteration 20, loss = 0.01776124\n",
      "Iteration 21, loss = 0.01724128\n",
      "Iteration 22, loss = 0.01676518\n",
      "Iteration 23, loss = 0.01633246\n",
      "Iteration 24, loss = 0.01592195\n",
      "Iteration 25, loss = 0.01551435\n",
      "Iteration 26, loss = 0.01516750\n",
      "Iteration 27, loss = 0.01481679\n",
      "Iteration 28, loss = 0.01449490\n",
      "Iteration 29, loss = 0.01419318\n",
      "Iteration 30, loss = 0.01392743\n",
      "Iteration 31, loss = 0.01364744\n",
      "Iteration 32, loss = 0.01340563\n",
      "Iteration 33, loss = 0.01317505\n",
      "Iteration 34, loss = 0.01293039\n",
      "Iteration 35, loss = 0.01274227\n",
      "Iteration 36, loss = 0.01253046\n",
      "Iteration 37, loss = 0.01231183\n",
      "Iteration 38, loss = 0.01212046\n",
      "Iteration 39, loss = 0.01194578\n",
      "Iteration 40, loss = 0.01181673\n",
      "Iteration 41, loss = 0.01165693\n",
      "Iteration 42, loss = 0.01149792\n",
      "Iteration 43, loss = 0.01136316\n",
      "Iteration 44, loss = 0.01120380\n",
      "Iteration 45, loss = 0.01108421\n",
      "Iteration 46, loss = 0.01095882\n",
      "Iteration 47, loss = 0.01083934\n",
      "Iteration 48, loss = 0.01076916\n",
      "Iteration 49, loss = 0.01063736\n",
      "Iteration 50, loss = 0.01055921\n",
      "Iteration 51, loss = 0.01046218\n",
      "Iteration 52, loss = 0.01035953\n",
      "Iteration 53, loss = 0.01028236\n",
      "Iteration 54, loss = 0.01022218\n",
      "Iteration 55, loss = 0.01010455\n",
      "Iteration 56, loss = 0.01004362\n",
      "Iteration 57, loss = 0.00999623\n",
      "Iteration 58, loss = 0.00993537\n",
      "Iteration 59, loss = 0.00983945\n",
      "Iteration 60, loss = 0.00979464\n",
      "Iteration 61, loss = 0.00972110\n",
      "Iteration 62, loss = 0.00967827\n",
      "Iteration 63, loss = 0.00961328\n",
      "Iteration 64, loss = 0.00958202\n",
      "Iteration 65, loss = 0.00954763\n",
      "Iteration 66, loss = 0.00946982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47369577\n",
      "Iteration 2, loss = 0.18736740\n",
      "Iteration 3, loss = 0.10122782\n",
      "Iteration 4, loss = 0.06803596\n",
      "Iteration 5, loss = 0.05212925\n",
      "Iteration 6, loss = 0.04328844\n",
      "Iteration 7, loss = 0.03795196\n",
      "Iteration 8, loss = 0.03431921\n",
      "Iteration 9, loss = 0.03163582\n",
      "Iteration 10, loss = 0.02959158\n",
      "Iteration 11, loss = 0.02789480\n",
      "Iteration 12, loss = 0.02650877\n",
      "Iteration 13, loss = 0.02529903\n",
      "Iteration 14, loss = 0.02424873\n",
      "Iteration 15, loss = 0.02333215\n",
      "Iteration 16, loss = 0.02245235\n",
      "Iteration 17, loss = 0.02167379\n",
      "Iteration 18, loss = 0.02096427\n",
      "Iteration 19, loss = 0.02031271\n",
      "Iteration 20, loss = 0.01973758\n",
      "Iteration 21, loss = 0.01914480\n",
      "Iteration 22, loss = 0.01863235\n",
      "Iteration 23, loss = 0.01815649\n",
      "Iteration 24, loss = 0.01770818\n",
      "Iteration 25, loss = 0.01724580\n",
      "Iteration 26, loss = 0.01687455\n",
      "Iteration 27, loss = 0.01649579\n",
      "Iteration 28, loss = 0.01614410\n",
      "Iteration 29, loss = 0.01579197\n",
      "Iteration 30, loss = 0.01552789\n",
      "Iteration 31, loss = 0.01518787\n",
      "Iteration 32, loss = 0.01490692\n",
      "Iteration 33, loss = 0.01465236\n",
      "Iteration 34, loss = 0.01439405\n",
      "Iteration 35, loss = 0.01417463\n",
      "Iteration 36, loss = 0.01393881\n",
      "Iteration 37, loss = 0.01373542\n",
      "Iteration 38, loss = 0.01352868\n",
      "Iteration 39, loss = 0.01334394\n",
      "Iteration 40, loss = 0.01315808\n",
      "Iteration 41, loss = 0.01299930\n",
      "Iteration 42, loss = 0.01284290\n",
      "Iteration 43, loss = 0.01266837\n",
      "Iteration 44, loss = 0.01252649\n",
      "Iteration 45, loss = 0.01239777\n",
      "Iteration 46, loss = 0.01225892\n",
      "Iteration 47, loss = 0.01213249\n",
      "Iteration 48, loss = 0.01204745\n",
      "Iteration 49, loss = 0.01190310\n",
      "Iteration 50, loss = 0.01179462\n",
      "Iteration 51, loss = 0.01170021\n",
      "Iteration 52, loss = 0.01159641\n",
      "Iteration 53, loss = 0.01151280\n",
      "Iteration 54, loss = 0.01141699\n",
      "Iteration 55, loss = 0.01134108\n",
      "Iteration 56, loss = 0.01127485\n",
      "Iteration 57, loss = 0.01121530\n",
      "Iteration 58, loss = 0.01111086\n",
      "Iteration 59, loss = 0.01105410\n",
      "Iteration 60, loss = 0.01097508\n",
      "Iteration 61, loss = 0.01091430\n",
      "Iteration 62, loss = 0.01087158\n",
      "Iteration 63, loss = 0.01084208\n",
      "Iteration 64, loss = 0.01074865\n",
      "Iteration 65, loss = 0.01071502\n",
      "Iteration 66, loss = 0.01066747\n",
      "Iteration 67, loss = 0.01061232\n",
      "Iteration 68, loss = 0.01058892\n",
      "Iteration 69, loss = 0.01053287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46272853\n",
      "Iteration 2, loss = 0.17091179\n",
      "Iteration 3, loss = 0.09137280\n",
      "Iteration 4, loss = 0.06287711\n",
      "Iteration 5, loss = 0.04922145\n",
      "Iteration 6, loss = 0.04138147\n",
      "Iteration 7, loss = 0.03648506\n",
      "Iteration 8, loss = 0.03299329\n",
      "Iteration 9, loss = 0.03050242\n",
      "Iteration 10, loss = 0.02844258\n",
      "Iteration 11, loss = 0.02683562\n",
      "Iteration 12, loss = 0.02543488\n",
      "Iteration 13, loss = 0.02442794\n",
      "Iteration 14, loss = 0.02327388\n",
      "Iteration 15, loss = 0.02232698\n",
      "Iteration 16, loss = 0.02151205\n",
      "Iteration 17, loss = 0.02086848\n",
      "Iteration 18, loss = 0.02011082\n",
      "Iteration 19, loss = 0.01945747\n",
      "Iteration 20, loss = 0.01888801\n",
      "Iteration 21, loss = 0.01833423\n",
      "Iteration 22, loss = 0.01784217\n",
      "Iteration 23, loss = 0.01738471\n",
      "Iteration 24, loss = 0.01696199\n",
      "Iteration 25, loss = 0.01652617\n",
      "Iteration 26, loss = 0.01617213\n",
      "Iteration 27, loss = 0.01574449\n",
      "Iteration 28, loss = 0.01540918\n",
      "Iteration 29, loss = 0.01508475\n",
      "Iteration 30, loss = 0.01477375\n",
      "Iteration 31, loss = 0.01450066\n",
      "Iteration 32, loss = 0.01421886\n",
      "Iteration 33, loss = 0.01396538\n",
      "Iteration 34, loss = 0.01370353\n",
      "Iteration 35, loss = 0.01347867\n",
      "Iteration 36, loss = 0.01328007\n",
      "Iteration 37, loss = 0.01303342\n",
      "Iteration 38, loss = 0.01285783\n",
      "Iteration 39, loss = 0.01264758\n",
      "Iteration 40, loss = 0.01247810\n",
      "Iteration 41, loss = 0.01230263\n",
      "Iteration 42, loss = 0.01213129\n",
      "Iteration 43, loss = 0.01200437\n",
      "Iteration 44, loss = 0.01184991\n",
      "Iteration 45, loss = 0.01173188\n",
      "Iteration 46, loss = 0.01158970\n",
      "Iteration 47, loss = 0.01142714\n",
      "Iteration 48, loss = 0.01131533\n",
      "Iteration 49, loss = 0.01119486\n",
      "Iteration 50, loss = 0.01108292\n",
      "Iteration 51, loss = 0.01101922\n",
      "Iteration 52, loss = 0.01093583\n",
      "Iteration 53, loss = 0.01078886\n",
      "Iteration 54, loss = 0.01071518\n",
      "Iteration 55, loss = 0.01062233\n",
      "Iteration 56, loss = 0.01059289\n",
      "Iteration 57, loss = 0.01049149\n",
      "Iteration 58, loss = 0.01037095\n",
      "Iteration 59, loss = 0.01031558\n",
      "Iteration 60, loss = 0.01025518\n",
      "Iteration 61, loss = 0.01020733\n",
      "Iteration 62, loss = 0.01014610\n",
      "Iteration 63, loss = 0.01007651\n",
      "Iteration 64, loss = 0.01003989\n",
      "Iteration 65, loss = 0.00998073\n",
      "Iteration 66, loss = 0.00993254\n",
      "Iteration 67, loss = 0.00989131\n",
      "Iteration 68, loss = 0.00983457\n",
      "Iteration 69, loss = 0.00978260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48256437\n",
      "Iteration 2, loss = 0.19457746\n",
      "Iteration 3, loss = 0.10193679\n",
      "Iteration 4, loss = 0.06785334\n",
      "Iteration 5, loss = 0.05177194\n",
      "Iteration 6, loss = 0.04297058\n",
      "Iteration 7, loss = 0.03757301\n",
      "Iteration 8, loss = 0.03400833\n",
      "Iteration 9, loss = 0.03147787\n",
      "Iteration 10, loss = 0.02947839\n",
      "Iteration 11, loss = 0.02786388\n",
      "Iteration 12, loss = 0.02649633\n",
      "Iteration 13, loss = 0.02535662\n",
      "Iteration 14, loss = 0.02431460\n",
      "Iteration 15, loss = 0.02337591\n",
      "Iteration 16, loss = 0.02256003\n",
      "Iteration 17, loss = 0.02182828\n",
      "Iteration 18, loss = 0.02110109\n",
      "Iteration 19, loss = 0.02044472\n",
      "Iteration 20, loss = 0.01985470\n",
      "Iteration 21, loss = 0.01930229\n",
      "Iteration 22, loss = 0.01880202\n",
      "Iteration 23, loss = 0.01827559\n",
      "Iteration 24, loss = 0.01784752\n",
      "Iteration 25, loss = 0.01738771\n",
      "Iteration 26, loss = 0.01697683\n",
      "Iteration 27, loss = 0.01659962\n",
      "Iteration 28, loss = 0.01623807\n",
      "Iteration 29, loss = 0.01589547\n",
      "Iteration 30, loss = 0.01559434\n",
      "Iteration 31, loss = 0.01528137\n",
      "Iteration 32, loss = 0.01499088\n",
      "Iteration 33, loss = 0.01472092\n",
      "Iteration 34, loss = 0.01447235\n",
      "Iteration 35, loss = 0.01422471\n",
      "Iteration 36, loss = 0.01399719\n",
      "Iteration 37, loss = 0.01377599\n",
      "Iteration 38, loss = 0.01358283\n",
      "Iteration 39, loss = 0.01338477\n",
      "Iteration 40, loss = 0.01321132\n",
      "Iteration 41, loss = 0.01302087\n",
      "Iteration 42, loss = 0.01287179\n",
      "Iteration 43, loss = 0.01269057\n",
      "Iteration 44, loss = 0.01255274\n",
      "Iteration 45, loss = 0.01239741\n",
      "Iteration 46, loss = 0.01226689\n",
      "Iteration 47, loss = 0.01215336\n",
      "Iteration 48, loss = 0.01202217\n",
      "Iteration 49, loss = 0.01190038\n",
      "Iteration 50, loss = 0.01179617\n",
      "Iteration 51, loss = 0.01169050\n",
      "Iteration 52, loss = 0.01163043\n",
      "Iteration 53, loss = 0.01152283\n",
      "Iteration 54, loss = 0.01142842\n",
      "Iteration 55, loss = 0.01133686\n",
      "Iteration 56, loss = 0.01126240\n",
      "Iteration 57, loss = 0.01117320\n",
      "Iteration 58, loss = 0.01110449\n",
      "Iteration 59, loss = 0.01102527\n",
      "Iteration 60, loss = 0.01096405\n",
      "Iteration 61, loss = 0.01091726\n",
      "Iteration 62, loss = 0.01085770\n",
      "Iteration 63, loss = 0.01080515\n",
      "Iteration 64, loss = 0.01073294\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45504355\n",
      "Iteration 2, loss = 0.15251737\n",
      "Iteration 3, loss = 0.07207896\n",
      "Iteration 4, loss = 0.04737329\n",
      "Iteration 5, loss = 0.03737766\n",
      "Iteration 6, loss = 0.03211207\n",
      "Iteration 7, loss = 0.02872854\n",
      "Iteration 8, loss = 0.02643776\n",
      "Iteration 9, loss = 0.02476928\n",
      "Iteration 10, loss = 0.02346997\n",
      "Iteration 11, loss = 0.02238060\n",
      "Iteration 12, loss = 0.02144801\n",
      "Iteration 13, loss = 0.02066503\n",
      "Iteration 14, loss = 0.01989817\n",
      "Iteration 15, loss = 0.01922362\n",
      "Iteration 16, loss = 0.01858317\n",
      "Iteration 17, loss = 0.01800862\n",
      "Iteration 18, loss = 0.01749118\n",
      "Iteration 19, loss = 0.01698151\n",
      "Iteration 20, loss = 0.01650317\n",
      "Iteration 21, loss = 0.01605306\n",
      "Iteration 22, loss = 0.01564338\n",
      "Iteration 23, loss = 0.01526041\n",
      "Iteration 24, loss = 0.01489124\n",
      "Iteration 25, loss = 0.01452767\n",
      "Iteration 26, loss = 0.01418941\n",
      "Iteration 27, loss = 0.01386512\n",
      "Iteration 28, loss = 0.01357881\n",
      "Iteration 29, loss = 0.01328414\n",
      "Iteration 30, loss = 0.01301811\n",
      "Iteration 31, loss = 0.01272882\n",
      "Iteration 32, loss = 0.01252069\n",
      "Iteration 33, loss = 0.01227323\n",
      "Iteration 34, loss = 0.01204281\n",
      "Iteration 35, loss = 0.01183668\n",
      "Iteration 36, loss = 0.01163209\n",
      "Iteration 37, loss = 0.01145200\n",
      "Iteration 38, loss = 0.01125652\n",
      "Iteration 39, loss = 0.01110615\n",
      "Iteration 40, loss = 0.01092385\n",
      "Iteration 41, loss = 0.01078200\n",
      "Iteration 42, loss = 0.01063712\n",
      "Iteration 43, loss = 0.01051359\n",
      "Iteration 44, loss = 0.01039981\n",
      "Iteration 45, loss = 0.01022862\n",
      "Iteration 46, loss = 0.01012359\n",
      "Iteration 47, loss = 0.01004348\n",
      "Iteration 48, loss = 0.00990040\n",
      "Iteration 49, loss = 0.00979186\n",
      "Iteration 50, loss = 0.00970201\n",
      "Iteration 51, loss = 0.00959260\n",
      "Iteration 52, loss = 0.00951150\n",
      "Iteration 53, loss = 0.00944681\n",
      "Iteration 54, loss = 0.00933328\n",
      "Iteration 55, loss = 0.00927484\n",
      "Iteration 56, loss = 0.00920667\n",
      "Iteration 57, loss = 0.00912572\n",
      "Iteration 58, loss = 0.00906051\n",
      "Iteration 59, loss = 0.00899254\n",
      "Iteration 60, loss = 0.00890823\n",
      "Iteration 61, loss = 0.00888295\n",
      "Iteration 62, loss = 0.00881953\n",
      "Iteration 63, loss = 0.00877479\n",
      "Iteration 64, loss = 0.00872909\n",
      "Iteration 65, loss = 0.00868099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72493868\n",
      "Iteration 2, loss = 0.56911659\n",
      "Iteration 3, loss = 0.46702608\n",
      "Iteration 4, loss = 0.39418561\n",
      "Iteration 5, loss = 0.33961719\n",
      "Iteration 6, loss = 0.29815085\n",
      "Iteration 7, loss = 0.26610966\n",
      "Iteration 8, loss = 0.24082222\n",
      "Iteration 9, loss = 0.22052252\n",
      "Iteration 10, loss = 0.20386375\n",
      "Iteration 11, loss = 0.18996158\n",
      "Iteration 12, loss = 0.17814968\n",
      "Iteration 13, loss = 0.16806604\n",
      "Iteration 14, loss = 0.15925840\n",
      "Iteration 15, loss = 0.15161304\n",
      "Iteration 16, loss = 0.14488921\n",
      "Iteration 17, loss = 0.13892079\n",
      "Iteration 18, loss = 0.13359702\n",
      "Iteration 19, loss = 0.12883040\n",
      "Iteration 20, loss = 0.12453043\n",
      "Iteration 21, loss = 0.12064080\n",
      "Iteration 22, loss = 0.11709040\n",
      "Iteration 23, loss = 0.11387663\n",
      "Iteration 24, loss = 0.11091679\n",
      "Iteration 25, loss = 0.10816319\n",
      "Iteration 26, loss = 0.10562567\n",
      "Iteration 27, loss = 0.10326610\n",
      "Iteration 28, loss = 0.10106167\n",
      "Iteration 29, loss = 0.09901087\n",
      "Iteration 30, loss = 0.09707385\n",
      "Iteration 31, loss = 0.09526822\n",
      "Iteration 32, loss = 0.09355548\n",
      "Iteration 33, loss = 0.09193257\n",
      "Iteration 34, loss = 0.09039324\n",
      "Iteration 35, loss = 0.08895165\n",
      "Iteration 36, loss = 0.08753839\n",
      "Iteration 37, loss = 0.08618636\n",
      "Iteration 38, loss = 0.08492074\n",
      "Iteration 39, loss = 0.08370586\n",
      "Iteration 40, loss = 0.08254428\n",
      "Iteration 41, loss = 0.08142272\n",
      "Iteration 42, loss = 0.08034649\n",
      "Iteration 43, loss = 0.07930674\n",
      "Iteration 44, loss = 0.07830986\n",
      "Iteration 45, loss = 0.07736233\n",
      "Iteration 46, loss = 0.07643629\n",
      "Iteration 47, loss = 0.07553966\n",
      "Iteration 48, loss = 0.07468361\n",
      "Iteration 49, loss = 0.07383936\n",
      "Iteration 50, loss = 0.07304844\n",
      "Iteration 51, loss = 0.07227465\n",
      "Iteration 52, loss = 0.07153082\n",
      "Iteration 53, loss = 0.07080377\n",
      "Iteration 54, loss = 0.07010637\n",
      "Iteration 55, loss = 0.06942138\n",
      "Iteration 56, loss = 0.06876587\n",
      "Iteration 57, loss = 0.06815192\n",
      "Iteration 58, loss = 0.06750393\n",
      "Iteration 59, loss = 0.06690686\n",
      "Iteration 60, loss = 0.06633953\n",
      "Iteration 61, loss = 0.06577556\n",
      "Iteration 62, loss = 0.06522780\n",
      "Iteration 63, loss = 0.06469318\n",
      "Iteration 64, loss = 0.06419124\n",
      "Iteration 65, loss = 0.06369028\n",
      "Iteration 66, loss = 0.06320199\n",
      "Iteration 67, loss = 0.06273559\n",
      "Iteration 68, loss = 0.06228514\n",
      "Iteration 69, loss = 0.06182356\n",
      "Iteration 70, loss = 0.06139139\n",
      "Iteration 71, loss = 0.06095901\n",
      "Iteration 72, loss = 0.06054075\n",
      "Iteration 73, loss = 0.06013763\n",
      "Iteration 74, loss = 0.05974692\n",
      "Iteration 75, loss = 0.05936041\n",
      "Iteration 76, loss = 0.05899598\n",
      "Iteration 77, loss = 0.05862093\n",
      "Iteration 78, loss = 0.05825519\n",
      "Iteration 79, loss = 0.05791852\n",
      "Iteration 80, loss = 0.05757300\n",
      "Iteration 81, loss = 0.05723733\n",
      "Iteration 82, loss = 0.05691620\n",
      "Iteration 83, loss = 0.05659349\n",
      "Iteration 84, loss = 0.05629057\n",
      "Iteration 85, loss = 0.05597987\n",
      "Iteration 86, loss = 0.05568531\n",
      "Iteration 87, loss = 0.05539439\n",
      "Iteration 88, loss = 0.05511086\n",
      "Iteration 89, loss = 0.05483178\n",
      "Iteration 90, loss = 0.05456817\n",
      "Iteration 91, loss = 0.05430397\n",
      "Iteration 92, loss = 0.05403103\n",
      "Iteration 93, loss = 0.05377706\n",
      "Iteration 94, loss = 0.05352098\n",
      "Iteration 95, loss = 0.05327160\n",
      "Iteration 96, loss = 0.05302681\n",
      "Iteration 97, loss = 0.05279840\n",
      "Iteration 98, loss = 0.05255364\n",
      "Iteration 99, loss = 0.05232853\n",
      "Iteration 100, loss = 0.05210068\n",
      "Iteration 101, loss = 0.05187649\n",
      "Iteration 102, loss = 0.05166309\n",
      "Iteration 103, loss = 0.05144273\n",
      "Iteration 104, loss = 0.05123422\n",
      "Iteration 105, loss = 0.05102277\n",
      "Iteration 106, loss = 0.05082518\n",
      "Iteration 107, loss = 0.05062156\n",
      "Iteration 108, loss = 0.05042285\n",
      "Iteration 109, loss = 0.05023238\n",
      "Iteration 110, loss = 0.05004225\n",
      "Iteration 111, loss = 0.04985388\n",
      "Iteration 112, loss = 0.04967353\n",
      "Iteration 113, loss = 0.04949073\n",
      "Iteration 114, loss = 0.04931199\n",
      "Iteration 115, loss = 0.04913388\n",
      "Iteration 116, loss = 0.04896327\n",
      "Iteration 117, loss = 0.04879582\n",
      "Iteration 118, loss = 0.04862644\n",
      "Iteration 119, loss = 0.04846032\n",
      "Iteration 120, loss = 0.04829992\n",
      "Iteration 121, loss = 0.04814188\n",
      "Iteration 122, loss = 0.04798546\n",
      "Iteration 123, loss = 0.04782870\n",
      "Iteration 124, loss = 0.04767445\n",
      "Iteration 125, loss = 0.04752230\n",
      "Iteration 126, loss = 0.04737181\n",
      "Iteration 127, loss = 0.04723037\n",
      "Iteration 128, loss = 0.04708462\n",
      "Iteration 129, loss = 0.04694009\n",
      "Iteration 130, loss = 0.04679919\n",
      "Iteration 131, loss = 0.04666237\n",
      "Iteration 132, loss = 0.04652318\n",
      "Iteration 133, loss = 0.04638667\n",
      "Iteration 134, loss = 0.04624850\n",
      "Iteration 135, loss = 0.04611811\n",
      "Iteration 136, loss = 0.04598570\n",
      "Iteration 137, loss = 0.04586082\n",
      "Iteration 138, loss = 0.04573273\n",
      "Iteration 139, loss = 0.04560633\n",
      "Iteration 140, loss = 0.04548330\n",
      "Iteration 141, loss = 0.04536450\n",
      "Iteration 142, loss = 0.04524047\n",
      "Iteration 143, loss = 0.04512888\n",
      "Iteration 144, loss = 0.04500285\n",
      "Iteration 145, loss = 0.04488590\n",
      "Iteration 146, loss = 0.04477393\n",
      "Iteration 147, loss = 0.04465441\n",
      "Iteration 148, loss = 0.04454129\n",
      "Iteration 149, loss = 0.04443241\n",
      "Iteration 150, loss = 0.04432191\n",
      "Iteration 151, loss = 0.04421498\n",
      "Iteration 152, loss = 0.04411096\n",
      "Iteration 153, loss = 0.04399860\n",
      "Iteration 154, loss = 0.04389753\n",
      "Iteration 155, loss = 0.04379215\n",
      "Iteration 156, loss = 0.04368706\n",
      "Iteration 157, loss = 0.04359173\n",
      "Iteration 158, loss = 0.04348578\n",
      "Iteration 159, loss = 0.04338829\n",
      "Iteration 160, loss = 0.04329143\n",
      "Iteration 161, loss = 0.04319451\n",
      "Iteration 162, loss = 0.04309708\n",
      "Iteration 163, loss = 0.04299736\n",
      "Iteration 164, loss = 0.04290715\n",
      "Iteration 165, loss = 0.04280865\n",
      "Iteration 166, loss = 0.04271933\n",
      "Iteration 167, loss = 0.04262945\n",
      "Iteration 168, loss = 0.04253482\n",
      "Iteration 169, loss = 0.04244553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74190950\n",
      "Iteration 2, loss = 0.60030923\n",
      "Iteration 3, loss = 0.51313402\n",
      "Iteration 4, loss = 0.45183488\n",
      "Iteration 5, loss = 0.40548607\n",
      "Iteration 6, loss = 0.36930713\n",
      "Iteration 7, loss = 0.34004793\n",
      "Iteration 8, loss = 0.31570381\n",
      "Iteration 9, loss = 0.29497091\n",
      "Iteration 10, loss = 0.27703097\n",
      "Iteration 11, loss = 0.26125827\n",
      "Iteration 12, loss = 0.24727765\n",
      "Iteration 13, loss = 0.23477884\n",
      "Iteration 14, loss = 0.22354194\n",
      "Iteration 15, loss = 0.21342724\n",
      "Iteration 16, loss = 0.20424747\n",
      "Iteration 17, loss = 0.19591976\n",
      "Iteration 18, loss = 0.18832491\n",
      "Iteration 19, loss = 0.18141954\n",
      "Iteration 20, loss = 0.17505827\n",
      "Iteration 21, loss = 0.16919395\n",
      "Iteration 22, loss = 0.16383695\n",
      "Iteration 23, loss = 0.15887425\n",
      "Iteration 24, loss = 0.15430681\n",
      "Iteration 25, loss = 0.15003809\n",
      "Iteration 26, loss = 0.14604836\n",
      "Iteration 27, loss = 0.14234848\n",
      "Iteration 28, loss = 0.13887473\n",
      "Iteration 29, loss = 0.13559576\n",
      "Iteration 30, loss = 0.13252061\n",
      "Iteration 31, loss = 0.12960428\n",
      "Iteration 32, loss = 0.12686303\n",
      "Iteration 33, loss = 0.12425380\n",
      "Iteration 34, loss = 0.12176040\n",
      "Iteration 35, loss = 0.11940382\n",
      "Iteration 36, loss = 0.11713971\n",
      "Iteration 37, loss = 0.11497500\n",
      "Iteration 38, loss = 0.11290244\n",
      "Iteration 39, loss = 0.11090744\n",
      "Iteration 40, loss = 0.10900135\n",
      "Iteration 41, loss = 0.10717410\n",
      "Iteration 42, loss = 0.10540903\n",
      "Iteration 43, loss = 0.10370964\n",
      "Iteration 44, loss = 0.10207154\n",
      "Iteration 45, loss = 0.10048358\n",
      "Iteration 46, loss = 0.09895620\n",
      "Iteration 47, loss = 0.09747304\n",
      "Iteration 48, loss = 0.09603199\n",
      "Iteration 49, loss = 0.09465881\n",
      "Iteration 50, loss = 0.09331873\n",
      "Iteration 51, loss = 0.09201526\n",
      "Iteration 52, loss = 0.09076202\n",
      "Iteration 53, loss = 0.08952654\n",
      "Iteration 54, loss = 0.08835868\n",
      "Iteration 55, loss = 0.08721794\n",
      "Iteration 56, loss = 0.08609800\n",
      "Iteration 57, loss = 0.08501873\n",
      "Iteration 58, loss = 0.08396922\n",
      "Iteration 59, loss = 0.08294287\n",
      "Iteration 60, loss = 0.08197582\n",
      "Iteration 61, loss = 0.08101807\n",
      "Iteration 62, loss = 0.08009521\n",
      "Iteration 63, loss = 0.07918202\n",
      "Iteration 64, loss = 0.07831077\n",
      "Iteration 65, loss = 0.07745811\n",
      "Iteration 66, loss = 0.07663945\n",
      "Iteration 67, loss = 0.07583295\n",
      "Iteration 68, loss = 0.07505719\n",
      "Iteration 69, loss = 0.07428882\n",
      "Iteration 70, loss = 0.07355033\n",
      "Iteration 71, loss = 0.07283384\n",
      "Iteration 72, loss = 0.07212281\n",
      "Iteration 73, loss = 0.07144580\n",
      "Iteration 74, loss = 0.07078393\n",
      "Iteration 75, loss = 0.07014809\n",
      "Iteration 76, loss = 0.06951614\n",
      "Iteration 77, loss = 0.06890845\n",
      "Iteration 78, loss = 0.06829671\n",
      "Iteration 79, loss = 0.06772351\n",
      "Iteration 80, loss = 0.06715357\n",
      "Iteration 81, loss = 0.06660047\n",
      "Iteration 82, loss = 0.06606857\n",
      "Iteration 83, loss = 0.06554259\n",
      "Iteration 84, loss = 0.06503027\n",
      "Iteration 85, loss = 0.06453076\n",
      "Iteration 86, loss = 0.06404077\n",
      "Iteration 87, loss = 0.06356743\n",
      "Iteration 88, loss = 0.06310151\n",
      "Iteration 89, loss = 0.06264701\n",
      "Iteration 90, loss = 0.06221780\n",
      "Iteration 91, loss = 0.06179328\n",
      "Iteration 92, loss = 0.06135621\n",
      "Iteration 93, loss = 0.06094393\n",
      "Iteration 94, loss = 0.06053685\n",
      "Iteration 95, loss = 0.06013812\n",
      "Iteration 96, loss = 0.05974935\n",
      "Iteration 97, loss = 0.05938172\n",
      "Iteration 98, loss = 0.05899813\n",
      "Iteration 99, loss = 0.05864535\n",
      "Iteration 100, loss = 0.05828461\n",
      "Iteration 101, loss = 0.05792995\n",
      "Iteration 102, loss = 0.05758960\n",
      "Iteration 103, loss = 0.05725468\n",
      "Iteration 104, loss = 0.05693224\n",
      "Iteration 105, loss = 0.05660229\n",
      "Iteration 106, loss = 0.05628463\n",
      "Iteration 107, loss = 0.05598033\n",
      "Iteration 108, loss = 0.05567452\n",
      "Iteration 109, loss = 0.05538051\n",
      "Iteration 110, loss = 0.05507934\n",
      "Iteration 111, loss = 0.05479643\n",
      "Iteration 112, loss = 0.05451046\n",
      "Iteration 113, loss = 0.05423900\n",
      "Iteration 114, loss = 0.05397084\n",
      "Iteration 115, loss = 0.05369963\n",
      "Iteration 116, loss = 0.05343781\n",
      "Iteration 117, loss = 0.05318478\n",
      "Iteration 118, loss = 0.05292899\n",
      "Iteration 119, loss = 0.05268167\n",
      "Iteration 120, loss = 0.05244333\n",
      "Iteration 121, loss = 0.05220328\n",
      "Iteration 122, loss = 0.05197076\n",
      "Iteration 123, loss = 0.05173833\n",
      "Iteration 124, loss = 0.05151194\n",
      "Iteration 125, loss = 0.05128851\n",
      "Iteration 126, loss = 0.05106665\n",
      "Iteration 127, loss = 0.05085093\n",
      "Iteration 128, loss = 0.05064952\n",
      "Iteration 129, loss = 0.05042965\n",
      "Iteration 130, loss = 0.05022686\n",
      "Iteration 131, loss = 0.05002470\n",
      "Iteration 132, loss = 0.04982382\n",
      "Iteration 133, loss = 0.04962666\n",
      "Iteration 134, loss = 0.04943115\n",
      "Iteration 135, loss = 0.04924538\n",
      "Iteration 136, loss = 0.04905792\n",
      "Iteration 137, loss = 0.04887381\n",
      "Iteration 138, loss = 0.04869091\n",
      "Iteration 139, loss = 0.04851182\n",
      "Iteration 140, loss = 0.04833561\n",
      "Iteration 141, loss = 0.04816352\n",
      "Iteration 142, loss = 0.04799058\n",
      "Iteration 143, loss = 0.04783066\n",
      "Iteration 144, loss = 0.04765389\n",
      "Iteration 145, loss = 0.04749310\n",
      "Iteration 146, loss = 0.04733209\n",
      "Iteration 147, loss = 0.04717087\n",
      "Iteration 148, loss = 0.04702231\n",
      "Iteration 149, loss = 0.04686424\n",
      "Iteration 150, loss = 0.04670980\n",
      "Iteration 151, loss = 0.04656354\n",
      "Iteration 152, loss = 0.04643024\n",
      "Iteration 153, loss = 0.04626533\n",
      "Iteration 154, loss = 0.04612690\n",
      "Iteration 155, loss = 0.04599382\n",
      "Iteration 156, loss = 0.04583952\n",
      "Iteration 157, loss = 0.04570498\n",
      "Iteration 158, loss = 0.04556662\n",
      "Iteration 159, loss = 0.04543477\n",
      "Iteration 160, loss = 0.04530183\n",
      "Iteration 161, loss = 0.04517500\n",
      "Iteration 162, loss = 0.04504337\n",
      "Iteration 163, loss = 0.04491468\n",
      "Iteration 164, loss = 0.04479337\n",
      "Iteration 165, loss = 0.04466389\n",
      "Iteration 166, loss = 0.04454183\n",
      "Iteration 167, loss = 0.04442416\n",
      "Iteration 168, loss = 0.04430299\n",
      "Iteration 169, loss = 0.04418230\n",
      "Iteration 170, loss = 0.04406608\n",
      "Iteration 171, loss = 0.04395176\n",
      "Iteration 172, loss = 0.04383859\n",
      "Iteration 173, loss = 0.04372715\n",
      "Iteration 174, loss = 0.04361664\n",
      "Iteration 175, loss = 0.04350649\n",
      "Iteration 176, loss = 0.04339867\n",
      "Iteration 177, loss = 0.04329186\n",
      "Iteration 178, loss = 0.04318969\n",
      "Iteration 179, loss = 0.04308966\n",
      "Iteration 180, loss = 0.04297816\n",
      "Iteration 181, loss = 0.04287449\n",
      "Iteration 182, loss = 0.04277739\n",
      "Iteration 183, loss = 0.04267531\n",
      "Iteration 184, loss = 0.04258045\n",
      "Iteration 185, loss = 0.04247894\n",
      "Iteration 186, loss = 0.04238232\n",
      "Iteration 187, loss = 0.04229048\n",
      "Iteration 188, loss = 0.04219358\n",
      "Iteration 189, loss = 0.04210105\n",
      "Iteration 190, loss = 0.04201204\n",
      "Iteration 191, loss = 0.04191930\n",
      "Iteration 192, loss = 0.04182935\n",
      "Iteration 193, loss = 0.04174472\n",
      "Iteration 194, loss = 0.04166203\n",
      "Iteration 195, loss = 0.04156957\n",
      "Iteration 196, loss = 0.04148546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74808939\n",
      "Iteration 2, loss = 0.60511774\n",
      "Iteration 3, loss = 0.50928088\n",
      "Iteration 4, loss = 0.43860297\n",
      "Iteration 5, loss = 0.38428244\n",
      "Iteration 6, loss = 0.34210796\n",
      "Iteration 7, loss = 0.30876457\n",
      "Iteration 8, loss = 0.28190067\n",
      "Iteration 9, loss = 0.25974890\n",
      "Iteration 10, loss = 0.24096397\n",
      "Iteration 11, loss = 0.22486221\n",
      "Iteration 12, loss = 0.21090227\n",
      "Iteration 13, loss = 0.19866562\n",
      "Iteration 14, loss = 0.18779246\n",
      "Iteration 15, loss = 0.17812300\n",
      "Iteration 16, loss = 0.16949514\n",
      "Iteration 17, loss = 0.16169936\n",
      "Iteration 18, loss = 0.15468511\n",
      "Iteration 19, loss = 0.14836273\n",
      "Iteration 20, loss = 0.14262513\n",
      "Iteration 21, loss = 0.13736675\n",
      "Iteration 22, loss = 0.13258944\n",
      "Iteration 23, loss = 0.12821061\n",
      "Iteration 24, loss = 0.12420722\n",
      "Iteration 25, loss = 0.12049695\n",
      "Iteration 26, loss = 0.11706036\n",
      "Iteration 27, loss = 0.11388497\n",
      "Iteration 28, loss = 0.11093409\n",
      "Iteration 29, loss = 0.10817561\n",
      "Iteration 30, loss = 0.10563010\n",
      "Iteration 31, loss = 0.10318898\n",
      "Iteration 32, loss = 0.10092325\n",
      "Iteration 33, loss = 0.09880780\n",
      "Iteration 34, loss = 0.09680647\n",
      "Iteration 35, loss = 0.09492002\n",
      "Iteration 36, loss = 0.09313726\n",
      "Iteration 37, loss = 0.09143379\n",
      "Iteration 38, loss = 0.08983132\n",
      "Iteration 39, loss = 0.08831391\n",
      "Iteration 40, loss = 0.08686536\n",
      "Iteration 41, loss = 0.08549263\n",
      "Iteration 42, loss = 0.08417805\n",
      "Iteration 43, loss = 0.08292813\n",
      "Iteration 44, loss = 0.08173552\n",
      "Iteration 45, loss = 0.08058950\n",
      "Iteration 46, loss = 0.07949839\n",
      "Iteration 47, loss = 0.07845538\n",
      "Iteration 48, loss = 0.07745747\n",
      "Iteration 49, loss = 0.07649278\n",
      "Iteration 50, loss = 0.07557609\n",
      "Iteration 51, loss = 0.07467666\n",
      "Iteration 52, loss = 0.07380649\n",
      "Iteration 53, loss = 0.07298564\n",
      "Iteration 54, loss = 0.07218609\n",
      "Iteration 55, loss = 0.07142684\n",
      "Iteration 56, loss = 0.07068913\n",
      "Iteration 57, loss = 0.06997601\n",
      "Iteration 58, loss = 0.06928625\n",
      "Iteration 59, loss = 0.06863012\n",
      "Iteration 60, loss = 0.06798356\n",
      "Iteration 61, loss = 0.06737172\n",
      "Iteration 62, loss = 0.06677899\n",
      "Iteration 63, loss = 0.06619142\n",
      "Iteration 64, loss = 0.06562954\n",
      "Iteration 65, loss = 0.06508838\n",
      "Iteration 66, loss = 0.06456570\n",
      "Iteration 67, loss = 0.06405148\n",
      "Iteration 68, loss = 0.06355736\n",
      "Iteration 69, loss = 0.06307428\n",
      "Iteration 70, loss = 0.06261942\n",
      "Iteration 71, loss = 0.06216675\n",
      "Iteration 72, loss = 0.06172162\n",
      "Iteration 73, loss = 0.06130443\n",
      "Iteration 74, loss = 0.06088918\n",
      "Iteration 75, loss = 0.06048580\n",
      "Iteration 76, loss = 0.06009704\n",
      "Iteration 77, loss = 0.05970879\n",
      "Iteration 78, loss = 0.05933698\n",
      "Iteration 79, loss = 0.05897063\n",
      "Iteration 80, loss = 0.05862163\n",
      "Iteration 81, loss = 0.05827647\n",
      "Iteration 82, loss = 0.05794036\n",
      "Iteration 83, loss = 0.05761374\n",
      "Iteration 84, loss = 0.05729845\n",
      "Iteration 85, loss = 0.05698680\n",
      "Iteration 86, loss = 0.05668499\n",
      "Iteration 87, loss = 0.05639019\n",
      "Iteration 88, loss = 0.05609743\n",
      "Iteration 89, loss = 0.05581949\n",
      "Iteration 90, loss = 0.05553852\n",
      "Iteration 91, loss = 0.05527689\n",
      "Iteration 92, loss = 0.05500649\n",
      "Iteration 93, loss = 0.05474873\n",
      "Iteration 94, loss = 0.05449499\n",
      "Iteration 95, loss = 0.05424877\n",
      "Iteration 96, loss = 0.05400604\n",
      "Iteration 97, loss = 0.05377355\n",
      "Iteration 98, loss = 0.05354139\n",
      "Iteration 99, loss = 0.05331781\n",
      "Iteration 100, loss = 0.05309643\n",
      "Iteration 101, loss = 0.05287846\n",
      "Iteration 102, loss = 0.05266433\n",
      "Iteration 103, loss = 0.05245664\n",
      "Iteration 104, loss = 0.05225089\n",
      "Iteration 105, loss = 0.05204680\n",
      "Iteration 106, loss = 0.05185191\n",
      "Iteration 107, loss = 0.05165850\n",
      "Iteration 108, loss = 0.05146814\n",
      "Iteration 109, loss = 0.05128232\n",
      "Iteration 110, loss = 0.05109638\n",
      "Iteration 111, loss = 0.05092118\n",
      "Iteration 112, loss = 0.05074105\n",
      "Iteration 113, loss = 0.05058292\n",
      "Iteration 114, loss = 0.05040058\n",
      "Iteration 115, loss = 0.05023684\n",
      "Iteration 116, loss = 0.05007228\n",
      "Iteration 117, loss = 0.04990499\n",
      "Iteration 118, loss = 0.04975008\n",
      "Iteration 119, loss = 0.04959362\n",
      "Iteration 120, loss = 0.04944140\n",
      "Iteration 121, loss = 0.04929476\n",
      "Iteration 122, loss = 0.04914429\n",
      "Iteration 123, loss = 0.04898820\n",
      "Iteration 124, loss = 0.04884657\n",
      "Iteration 125, loss = 0.04869980\n",
      "Iteration 126, loss = 0.04855912\n",
      "Iteration 127, loss = 0.04842272\n",
      "Iteration 128, loss = 0.04829268\n",
      "Iteration 129, loss = 0.04815047\n",
      "Iteration 130, loss = 0.04801716\n",
      "Iteration 131, loss = 0.04788801\n",
      "Iteration 132, loss = 0.04775815\n",
      "Iteration 133, loss = 0.04763171\n",
      "Iteration 134, loss = 0.04750869\n",
      "Iteration 135, loss = 0.04738844\n",
      "Iteration 136, loss = 0.04725907\n",
      "Iteration 137, loss = 0.04714110\n",
      "Iteration 138, loss = 0.04702184\n",
      "Iteration 139, loss = 0.04690900\n",
      "Iteration 140, loss = 0.04679068\n",
      "Iteration 141, loss = 0.04666884\n",
      "Iteration 142, loss = 0.04655637\n",
      "Iteration 143, loss = 0.04644987\n",
      "Iteration 144, loss = 0.04633609\n",
      "Iteration 145, loss = 0.04622599\n",
      "Iteration 146, loss = 0.04611857\n",
      "Iteration 147, loss = 0.04601067\n",
      "Iteration 148, loss = 0.04590684\n",
      "Iteration 149, loss = 0.04580397\n",
      "Iteration 150, loss = 0.04569955\n",
      "Iteration 151, loss = 0.04559978\n",
      "Iteration 152, loss = 0.04550444\n",
      "Iteration 153, loss = 0.04540078\n",
      "Iteration 154, loss = 0.04530374\n",
      "Iteration 155, loss = 0.04521205\n",
      "Iteration 156, loss = 0.04510794\n",
      "Iteration 157, loss = 0.04501450\n",
      "Iteration 158, loss = 0.04492013\n",
      "Iteration 159, loss = 0.04482512\n",
      "Iteration 160, loss = 0.04473415\n",
      "Iteration 161, loss = 0.04464611\n",
      "Iteration 162, loss = 0.04455331\n",
      "Iteration 163, loss = 0.04446442\n",
      "Iteration 164, loss = 0.04437740\n",
      "Iteration 165, loss = 0.04429218\n",
      "Iteration 166, loss = 0.04420375\n",
      "Iteration 167, loss = 0.04411897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74764243\n",
      "Iteration 2, loss = 0.62197266\n",
      "Iteration 3, loss = 0.54171818\n",
      "Iteration 4, loss = 0.48169933\n",
      "Iteration 5, loss = 0.43362556\n",
      "Iteration 6, loss = 0.39442592\n",
      "Iteration 7, loss = 0.36200802\n",
      "Iteration 8, loss = 0.33473394\n",
      "Iteration 9, loss = 0.31132391\n",
      "Iteration 10, loss = 0.29094920\n",
      "Iteration 11, loss = 0.27297059\n",
      "Iteration 12, loss = 0.25707049\n",
      "Iteration 13, loss = 0.24281520\n",
      "Iteration 14, loss = 0.23010061\n",
      "Iteration 15, loss = 0.21869092\n",
      "Iteration 16, loss = 0.20837731\n",
      "Iteration 17, loss = 0.19911444\n",
      "Iteration 18, loss = 0.19065964\n",
      "Iteration 19, loss = 0.18305135\n",
      "Iteration 20, loss = 0.17607933\n",
      "Iteration 21, loss = 0.16970947\n",
      "Iteration 22, loss = 0.16392776\n",
      "Iteration 23, loss = 0.15855434\n",
      "Iteration 24, loss = 0.15364984\n",
      "Iteration 25, loss = 0.14909119\n",
      "Iteration 26, loss = 0.14485724\n",
      "Iteration 27, loss = 0.14091754\n",
      "Iteration 28, loss = 0.13727570\n",
      "Iteration 29, loss = 0.13383946\n",
      "Iteration 30, loss = 0.13062870\n",
      "Iteration 31, loss = 0.12760472\n",
      "Iteration 32, loss = 0.12473110\n",
      "Iteration 33, loss = 0.12203106\n",
      "Iteration 34, loss = 0.11947593\n",
      "Iteration 35, loss = 0.11704387\n",
      "Iteration 36, loss = 0.11473258\n",
      "Iteration 37, loss = 0.11253250\n",
      "Iteration 38, loss = 0.11044966\n",
      "Iteration 39, loss = 0.10842906\n",
      "Iteration 40, loss = 0.10649135\n",
      "Iteration 41, loss = 0.10467314\n",
      "Iteration 42, loss = 0.10289182\n",
      "Iteration 43, loss = 0.10121303\n",
      "Iteration 44, loss = 0.09957588\n",
      "Iteration 45, loss = 0.09800954\n",
      "Iteration 46, loss = 0.09650336\n",
      "Iteration 47, loss = 0.09507499\n",
      "Iteration 48, loss = 0.09365205\n",
      "Iteration 49, loss = 0.09232207\n",
      "Iteration 50, loss = 0.09102121\n",
      "Iteration 51, loss = 0.08974661\n",
      "Iteration 52, loss = 0.08852585\n",
      "Iteration 53, loss = 0.08736902\n",
      "Iteration 54, loss = 0.08622411\n",
      "Iteration 55, loss = 0.08512260\n",
      "Iteration 56, loss = 0.08405634\n",
      "Iteration 57, loss = 0.08302860\n",
      "Iteration 58, loss = 0.08203973\n",
      "Iteration 59, loss = 0.08107819\n",
      "Iteration 60, loss = 0.08012696\n",
      "Iteration 61, loss = 0.07922855\n",
      "Iteration 62, loss = 0.07833848\n",
      "Iteration 63, loss = 0.07749083\n",
      "Iteration 64, loss = 0.07664772\n",
      "Iteration 65, loss = 0.07585144\n",
      "Iteration 66, loss = 0.07506830\n",
      "Iteration 67, loss = 0.07431756\n",
      "Iteration 68, loss = 0.07357772\n",
      "Iteration 69, loss = 0.07285944\n",
      "Iteration 70, loss = 0.07216726\n",
      "Iteration 71, loss = 0.07148696\n",
      "Iteration 72, loss = 0.07081919\n",
      "Iteration 73, loss = 0.07017773\n",
      "Iteration 74, loss = 0.06955373\n",
      "Iteration 75, loss = 0.06895083\n",
      "Iteration 76, loss = 0.06835555\n",
      "Iteration 77, loss = 0.06779214\n",
      "Iteration 78, loss = 0.06721031\n",
      "Iteration 79, loss = 0.06666065\n",
      "Iteration 80, loss = 0.06612325\n",
      "Iteration 81, loss = 0.06560727\n",
      "Iteration 82, loss = 0.06509508\n",
      "Iteration 83, loss = 0.06459931\n",
      "Iteration 84, loss = 0.06411785\n",
      "Iteration 85, loss = 0.06363799\n",
      "Iteration 86, loss = 0.06316888\n",
      "Iteration 87, loss = 0.06273326\n",
      "Iteration 88, loss = 0.06227631\n",
      "Iteration 89, loss = 0.06184351\n",
      "Iteration 90, loss = 0.06142105\n",
      "Iteration 91, loss = 0.06100864\n",
      "Iteration 92, loss = 0.06059906\n",
      "Iteration 93, loss = 0.06020846\n",
      "Iteration 94, loss = 0.05982289\n",
      "Iteration 95, loss = 0.05945571\n",
      "Iteration 96, loss = 0.05908020\n",
      "Iteration 97, loss = 0.05870595\n",
      "Iteration 98, loss = 0.05835185\n",
      "Iteration 99, loss = 0.05801062\n",
      "Iteration 100, loss = 0.05766161\n",
      "Iteration 101, loss = 0.05732688\n",
      "Iteration 102, loss = 0.05699539\n",
      "Iteration 103, loss = 0.05667303\n",
      "Iteration 104, loss = 0.05635903\n",
      "Iteration 105, loss = 0.05604936\n",
      "Iteration 106, loss = 0.05575042\n",
      "Iteration 107, loss = 0.05544776\n",
      "Iteration 108, loss = 0.05516432\n",
      "Iteration 109, loss = 0.05487774\n",
      "Iteration 110, loss = 0.05459169\n",
      "Iteration 111, loss = 0.05431745\n",
      "Iteration 112, loss = 0.05404741\n",
      "Iteration 113, loss = 0.05378167\n",
      "Iteration 114, loss = 0.05352858\n",
      "Iteration 115, loss = 0.05326450\n",
      "Iteration 116, loss = 0.05301261\n",
      "Iteration 117, loss = 0.05276800\n",
      "Iteration 118, loss = 0.05252381\n",
      "Iteration 119, loss = 0.05228222\n",
      "Iteration 120, loss = 0.05205512\n",
      "Iteration 121, loss = 0.05182142\n",
      "Iteration 122, loss = 0.05159926\n",
      "Iteration 123, loss = 0.05136661\n",
      "Iteration 124, loss = 0.05114781\n",
      "Iteration 125, loss = 0.05093149\n",
      "Iteration 126, loss = 0.05071890\n",
      "Iteration 127, loss = 0.05050639\n",
      "Iteration 128, loss = 0.05030333\n",
      "Iteration 129, loss = 0.05009697\n",
      "Iteration 130, loss = 0.04989805\n",
      "Iteration 131, loss = 0.04970489\n",
      "Iteration 132, loss = 0.04951252\n",
      "Iteration 133, loss = 0.04932113\n",
      "Iteration 134, loss = 0.04912994\n",
      "Iteration 135, loss = 0.04894772\n",
      "Iteration 136, loss = 0.04876122\n",
      "Iteration 137, loss = 0.04859449\n",
      "Iteration 138, loss = 0.04840800\n",
      "Iteration 139, loss = 0.04823721\n",
      "Iteration 140, loss = 0.04806076\n",
      "Iteration 141, loss = 0.04789176\n",
      "Iteration 142, loss = 0.04772277\n",
      "Iteration 143, loss = 0.04756513\n",
      "Iteration 144, loss = 0.04740033\n",
      "Iteration 145, loss = 0.04724012\n",
      "Iteration 146, loss = 0.04708843\n",
      "Iteration 147, loss = 0.04693118\n",
      "Iteration 148, loss = 0.04677810\n",
      "Iteration 149, loss = 0.04662547\n",
      "Iteration 150, loss = 0.04647670\n",
      "Iteration 151, loss = 0.04633492\n",
      "Iteration 152, loss = 0.04618946\n",
      "Iteration 153, loss = 0.04604233\n",
      "Iteration 154, loss = 0.04590925\n",
      "Iteration 155, loss = 0.04576854\n",
      "Iteration 156, loss = 0.04562630\n",
      "Iteration 157, loss = 0.04549297\n",
      "Iteration 158, loss = 0.04536033\n",
      "Iteration 159, loss = 0.04523038\n",
      "Iteration 160, loss = 0.04510578\n",
      "Iteration 161, loss = 0.04497365\n",
      "Iteration 162, loss = 0.04484956\n",
      "Iteration 163, loss = 0.04472190\n",
      "Iteration 164, loss = 0.04460050\n",
      "Iteration 165, loss = 0.04448573\n",
      "Iteration 166, loss = 0.04436181\n",
      "Iteration 167, loss = 0.04424916\n",
      "Iteration 168, loss = 0.04413167\n",
      "Iteration 169, loss = 0.04400715\n",
      "Iteration 170, loss = 0.04389433\n",
      "Iteration 171, loss = 0.04378087\n",
      "Iteration 172, loss = 0.04367260\n",
      "Iteration 173, loss = 0.04356089\n",
      "Iteration 174, loss = 0.04345261\n",
      "Iteration 175, loss = 0.04334315\n",
      "Iteration 176, loss = 0.04324052\n",
      "Iteration 177, loss = 0.04313372\n",
      "Iteration 178, loss = 0.04302887\n",
      "Iteration 179, loss = 0.04292906\n",
      "Iteration 180, loss = 0.04282738\n",
      "Iteration 181, loss = 0.04272557\n",
      "Iteration 182, loss = 0.04262496\n",
      "Iteration 183, loss = 0.04252744\n",
      "Iteration 184, loss = 0.04243843\n",
      "Iteration 185, loss = 0.04233338\n",
      "Iteration 186, loss = 0.04224084\n",
      "Iteration 187, loss = 0.04215201\n",
      "Iteration 188, loss = 0.04205476\n",
      "Iteration 189, loss = 0.04197219\n",
      "Iteration 190, loss = 0.04187236\n",
      "Iteration 191, loss = 0.04178702\n",
      "Iteration 192, loss = 0.04169602\n",
      "Iteration 193, loss = 0.04160858\n",
      "Iteration 194, loss = 0.04152296\n",
      "Iteration 195, loss = 0.04144040\n",
      "Iteration 196, loss = 0.04135415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73986520\n",
      "Iteration 2, loss = 0.59451719\n",
      "Iteration 3, loss = 0.50426240\n",
      "Iteration 4, loss = 0.43844684\n",
      "Iteration 5, loss = 0.38733284\n",
      "Iteration 6, loss = 0.34686406\n",
      "Iteration 7, loss = 0.31434440\n",
      "Iteration 8, loss = 0.28761814\n",
      "Iteration 9, loss = 0.26503945\n",
      "Iteration 10, loss = 0.24568972\n",
      "Iteration 11, loss = 0.22885516\n",
      "Iteration 12, loss = 0.21407113\n",
      "Iteration 13, loss = 0.20098125\n",
      "Iteration 14, loss = 0.18932116\n",
      "Iteration 15, loss = 0.17889156\n",
      "Iteration 16, loss = 0.16951499\n",
      "Iteration 17, loss = 0.16110904\n",
      "Iteration 18, loss = 0.15351529\n",
      "Iteration 19, loss = 0.14657076\n",
      "Iteration 20, loss = 0.14028655\n",
      "Iteration 21, loss = 0.13457112\n",
      "Iteration 22, loss = 0.12930820\n",
      "Iteration 23, loss = 0.12453309\n",
      "Iteration 24, loss = 0.12007334\n",
      "Iteration 25, loss = 0.11601510\n",
      "Iteration 26, loss = 0.11225250\n",
      "Iteration 27, loss = 0.10876249\n",
      "Iteration 28, loss = 0.10551771\n",
      "Iteration 29, loss = 0.10250986\n",
      "Iteration 30, loss = 0.09970218\n",
      "Iteration 31, loss = 0.09707905\n",
      "Iteration 32, loss = 0.09461258\n",
      "Iteration 33, loss = 0.09232020\n",
      "Iteration 34, loss = 0.09013677\n",
      "Iteration 35, loss = 0.08808890\n",
      "Iteration 36, loss = 0.08616906\n",
      "Iteration 37, loss = 0.08434141\n",
      "Iteration 38, loss = 0.08260710\n",
      "Iteration 39, loss = 0.08097851\n",
      "Iteration 40, loss = 0.07943907\n",
      "Iteration 41, loss = 0.07796644\n",
      "Iteration 42, loss = 0.07656304\n",
      "Iteration 43, loss = 0.07522826\n",
      "Iteration 44, loss = 0.07396996\n",
      "Iteration 45, loss = 0.07274029\n",
      "Iteration 46, loss = 0.07159153\n",
      "Iteration 47, loss = 0.07048642\n",
      "Iteration 48, loss = 0.06942589\n",
      "Iteration 49, loss = 0.06841745\n",
      "Iteration 50, loss = 0.06745570\n",
      "Iteration 51, loss = 0.06651722\n",
      "Iteration 52, loss = 0.06562479\n",
      "Iteration 53, loss = 0.06476702\n",
      "Iteration 54, loss = 0.06394184\n",
      "Iteration 55, loss = 0.06315797\n",
      "Iteration 56, loss = 0.06238464\n",
      "Iteration 57, loss = 0.06165433\n",
      "Iteration 58, loss = 0.06095553\n",
      "Iteration 59, loss = 0.06026153\n",
      "Iteration 60, loss = 0.05960649\n",
      "Iteration 61, loss = 0.05896601\n",
      "Iteration 62, loss = 0.05835275\n",
      "Iteration 63, loss = 0.05776088\n",
      "Iteration 64, loss = 0.05718485\n",
      "Iteration 65, loss = 0.05662969\n",
      "Iteration 66, loss = 0.05610383\n",
      "Iteration 67, loss = 0.05557597\n",
      "Iteration 68, loss = 0.05506941\n",
      "Iteration 69, loss = 0.05458353\n",
      "Iteration 70, loss = 0.05411103\n",
      "Iteration 71, loss = 0.05365531\n",
      "Iteration 72, loss = 0.05321161\n",
      "Iteration 73, loss = 0.05278566\n",
      "Iteration 74, loss = 0.05236087\n",
      "Iteration 75, loss = 0.05195449\n",
      "Iteration 76, loss = 0.05155918\n",
      "Iteration 77, loss = 0.05117800\n",
      "Iteration 78, loss = 0.05080760\n",
      "Iteration 79, loss = 0.05044124\n",
      "Iteration 80, loss = 0.05009045\n",
      "Iteration 81, loss = 0.04975458\n",
      "Iteration 82, loss = 0.04941317\n",
      "Iteration 83, loss = 0.04909502\n",
      "Iteration 84, loss = 0.04877322\n",
      "Iteration 85, loss = 0.04846495\n",
      "Iteration 86, loss = 0.04816871\n",
      "Iteration 87, loss = 0.04787734\n",
      "Iteration 88, loss = 0.04758877\n",
      "Iteration 89, loss = 0.04731658\n",
      "Iteration 90, loss = 0.04704617\n",
      "Iteration 91, loss = 0.04678355\n",
      "Iteration 92, loss = 0.04652520\n",
      "Iteration 93, loss = 0.04627786\n",
      "Iteration 94, loss = 0.04603295\n",
      "Iteration 95, loss = 0.04579127\n",
      "Iteration 96, loss = 0.04555968\n",
      "Iteration 97, loss = 0.04533273\n",
      "Iteration 98, loss = 0.04510857\n",
      "Iteration 99, loss = 0.04489222\n",
      "Iteration 100, loss = 0.04467979\n",
      "Iteration 101, loss = 0.04447224\n",
      "Iteration 102, loss = 0.04426411\n",
      "Iteration 103, loss = 0.04406556\n",
      "Iteration 104, loss = 0.04386934\n",
      "Iteration 105, loss = 0.04368142\n",
      "Iteration 106, loss = 0.04349972\n",
      "Iteration 107, loss = 0.04331063\n",
      "Iteration 108, loss = 0.04313468\n",
      "Iteration 109, loss = 0.04295757\n",
      "Iteration 110, loss = 0.04279063\n",
      "Iteration 111, loss = 0.04262149\n",
      "Iteration 112, loss = 0.04245608\n",
      "Iteration 113, loss = 0.04229333\n",
      "Iteration 114, loss = 0.04213532\n",
      "Iteration 115, loss = 0.04198502\n",
      "Iteration 116, loss = 0.04182791\n",
      "Iteration 117, loss = 0.04167964\n",
      "Iteration 118, loss = 0.04153265\n",
      "Iteration 119, loss = 0.04139054\n",
      "Iteration 120, loss = 0.04125103\n",
      "Iteration 121, loss = 0.04111402\n",
      "Iteration 122, loss = 0.04097482\n",
      "Iteration 123, loss = 0.04084746\n",
      "Iteration 124, loss = 0.04071263\n",
      "Iteration 125, loss = 0.04058522\n",
      "Iteration 126, loss = 0.04045737\n",
      "Iteration 127, loss = 0.04033439\n",
      "Iteration 128, loss = 0.04021245\n",
      "Iteration 129, loss = 0.04009285\n",
      "Iteration 130, loss = 0.03997694\n",
      "Iteration 131, loss = 0.03985990\n",
      "Iteration 132, loss = 0.03974647\n",
      "Iteration 133, loss = 0.03963604\n",
      "Iteration 134, loss = 0.03952710\n",
      "Iteration 135, loss = 0.03941772\n",
      "Iteration 136, loss = 0.03931388\n",
      "Iteration 137, loss = 0.03920830\n",
      "Iteration 138, loss = 0.03910684\n",
      "Iteration 139, loss = 0.03900789\n",
      "Iteration 140, loss = 0.03890649\n",
      "Iteration 141, loss = 0.03881075\n",
      "Iteration 142, loss = 0.03871190\n",
      "Iteration 143, loss = 0.03862024\n",
      "Iteration 144, loss = 0.03852538\n",
      "Iteration 145, loss = 0.03843299\n",
      "Iteration 146, loss = 0.03834315\n",
      "Iteration 147, loss = 0.03825623\n",
      "Iteration 148, loss = 0.03816662\n",
      "Iteration 149, loss = 0.03808123\n",
      "Iteration 150, loss = 0.03799616\n",
      "Iteration 151, loss = 0.03791211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33643948\n",
      "Iteration 2, loss = 0.10532024\n",
      "Iteration 3, loss = 0.03764391\n",
      "Iteration 4, loss = 0.02010642\n",
      "Iteration 5, loss = 0.03716770\n",
      "Iteration 6, loss = 0.01692400\n",
      "Iteration 7, loss = 0.01335621\n",
      "Iteration 8, loss = 0.04905081\n",
      "Iteration 9, loss = 0.05167050\n",
      "Iteration 10, loss = 0.01820177\n",
      "Iteration 11, loss = 0.01175695\n",
      "Iteration 12, loss = 0.01318208\n",
      "Iteration 13, loss = 0.01145623\n",
      "Iteration 14, loss = 0.01562965\n",
      "Iteration 15, loss = 0.01525470\n",
      "Iteration 16, loss = 0.01665417\n",
      "Iteration 17, loss = 0.02528270\n",
      "Iteration 18, loss = 0.01870957\n",
      "Iteration 19, loss = 0.02126055\n",
      "Iteration 20, loss = 0.01365873\n",
      "Iteration 21, loss = 0.01313702\n",
      "Iteration 22, loss = 0.01398995\n",
      "Iteration 23, loss = 0.06510327\n",
      "Iteration 24, loss = 0.02815581\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35560540\n",
      "Iteration 2, loss = 0.11393731\n",
      "Iteration 3, loss = 0.04008952\n",
      "Iteration 4, loss = 0.04370641\n",
      "Iteration 5, loss = 0.02065929\n",
      "Iteration 6, loss = 0.01420888\n",
      "Iteration 7, loss = 0.01240381\n",
      "Iteration 8, loss = 0.01561590\n",
      "Iteration 9, loss = 0.01468381\n",
      "Iteration 10, loss = 0.05542896\n",
      "Iteration 11, loss = 0.03345614\n",
      "Iteration 12, loss = 0.01571591\n",
      "Iteration 13, loss = 0.01296628\n",
      "Iteration 14, loss = 0.01209875\n",
      "Iteration 15, loss = 0.01308143\n",
      "Iteration 16, loss = 0.03266997\n",
      "Iteration 17, loss = 0.04257351\n",
      "Iteration 18, loss = 0.02362832\n",
      "Iteration 19, loss = 0.01327395\n",
      "Iteration 20, loss = 0.01331094\n",
      "Iteration 21, loss = 0.01280081\n",
      "Iteration 22, loss = 0.03255686\n",
      "Iteration 23, loss = 0.09797365\n",
      "Iteration 24, loss = 0.03215968\n",
      "Iteration 25, loss = 0.01481127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24697771\n",
      "Iteration 2, loss = 0.06498207\n",
      "Iteration 3, loss = 0.02208536\n",
      "Iteration 4, loss = 0.02114662\n",
      "Iteration 5, loss = 0.04097100\n",
      "Iteration 6, loss = 0.02335912\n",
      "Iteration 7, loss = 0.05062997\n",
      "Iteration 8, loss = 0.03415069\n",
      "Iteration 9, loss = 0.01400701\n",
      "Iteration 10, loss = 0.01284192\n",
      "Iteration 11, loss = 0.01439602\n",
      "Iteration 12, loss = 0.01414246\n",
      "Iteration 13, loss = 0.01246710\n",
      "Iteration 14, loss = 0.01914789\n",
      "Iteration 15, loss = 0.01615148\n",
      "Iteration 16, loss = 0.01131373\n",
      "Iteration 17, loss = 0.02221521\n",
      "Iteration 18, loss = 0.01869407\n",
      "Iteration 19, loss = 0.01616357\n",
      "Iteration 20, loss = 0.01409117\n",
      "Iteration 21, loss = 0.01195857\n",
      "Iteration 22, loss = 0.05746718\n",
      "Iteration 23, loss = 0.02780960\n",
      "Iteration 24, loss = 0.01303774\n",
      "Iteration 25, loss = 0.01526185\n",
      "Iteration 26, loss = 0.01248352\n",
      "Iteration 27, loss = 0.01087299\n",
      "Iteration 28, loss = 0.01159497\n",
      "Iteration 29, loss = 0.01532657\n",
      "Iteration 30, loss = 0.01173525\n",
      "Iteration 31, loss = 0.01426991\n",
      "Iteration 32, loss = 0.01185398\n",
      "Iteration 33, loss = 0.01059444\n",
      "Iteration 34, loss = 0.01047858\n",
      "Iteration 35, loss = 0.01022497\n",
      "Iteration 36, loss = 0.01131482\n",
      "Iteration 37, loss = 0.06633532\n",
      "Iteration 38, loss = 0.02906783\n",
      "Iteration 39, loss = 0.01406924\n",
      "Iteration 40, loss = 0.01107753\n",
      "Iteration 41, loss = 0.01031684\n",
      "Iteration 42, loss = 0.01121604\n",
      "Iteration 43, loss = 0.01117337\n",
      "Iteration 44, loss = 0.00991617\n",
      "Iteration 45, loss = 0.01315740\n",
      "Iteration 46, loss = 0.01458616\n",
      "Iteration 47, loss = 0.01265263\n",
      "Iteration 48, loss = 0.01013955\n",
      "Iteration 49, loss = 0.01014762\n",
      "Iteration 50, loss = 0.01000247\n",
      "Iteration 51, loss = 0.01051107\n",
      "Iteration 52, loss = 0.01058452\n",
      "Iteration 53, loss = 0.01254345\n",
      "Iteration 54, loss = 0.01370049\n",
      "Iteration 55, loss = 0.01093462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34530365\n",
      "Iteration 2, loss = 0.09177061\n",
      "Iteration 3, loss = 0.03134673\n",
      "Iteration 4, loss = 0.02032546\n",
      "Iteration 5, loss = 0.01790327\n",
      "Iteration 6, loss = 0.01712849\n",
      "Iteration 7, loss = 0.01538442\n",
      "Iteration 8, loss = 0.33715876\n",
      "Iteration 9, loss = 0.18955480\n",
      "Iteration 10, loss = 0.08986460\n",
      "Iteration 11, loss = 0.05191365\n",
      "Iteration 12, loss = 0.03344638\n",
      "Iteration 13, loss = 0.02385014\n",
      "Iteration 14, loss = 0.01901600\n",
      "Iteration 15, loss = 0.01567671\n",
      "Iteration 16, loss = 0.01385184\n",
      "Iteration 17, loss = 0.01263913\n",
      "Iteration 18, loss = 0.01186847\n",
      "Iteration 19, loss = 0.01143853\n",
      "Iteration 20, loss = 0.01095562\n",
      "Iteration 21, loss = 0.01076115\n",
      "Iteration 22, loss = 0.01077159\n",
      "Iteration 23, loss = 0.01070710\n",
      "Iteration 24, loss = 0.01047111\n",
      "Iteration 25, loss = 0.01077093\n",
      "Iteration 26, loss = 0.01040686\n",
      "Iteration 27, loss = 0.01071086\n",
      "Iteration 28, loss = 0.01068633\n",
      "Iteration 29, loss = 0.01061193\n",
      "Iteration 30, loss = 0.01282934\n",
      "Iteration 31, loss = 0.01187493\n",
      "Iteration 32, loss = 0.01126238\n",
      "Iteration 33, loss = 0.01060380\n",
      "Iteration 34, loss = 0.01062498\n",
      "Iteration 35, loss = 0.01031455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46617051\n",
      "Iteration 2, loss = 0.16337461\n",
      "Iteration 3, loss = 0.06181988\n",
      "Iteration 4, loss = 0.03146686\n",
      "Iteration 5, loss = 0.02264182\n",
      "Iteration 6, loss = 0.01603930\n",
      "Iteration 7, loss = 0.01768028\n",
      "Iteration 8, loss = 0.01872336\n",
      "Iteration 9, loss = 0.01243940\n",
      "Iteration 10, loss = 0.01245412\n",
      "Iteration 11, loss = 0.01424945\n",
      "Iteration 12, loss = 0.01538302\n",
      "Iteration 13, loss = 0.01348913\n",
      "Iteration 14, loss = 0.01245683\n",
      "Iteration 15, loss = 0.01452082\n",
      "Iteration 16, loss = 0.01887197\n",
      "Iteration 17, loss = 0.01458948\n",
      "Iteration 18, loss = 0.01488589\n",
      "Iteration 19, loss = 0.01379062\n",
      "Iteration 20, loss = 0.01748314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19863645\n",
      "Iteration 2, loss = 0.05178986\n",
      "Iteration 3, loss = 0.04308861\n",
      "Iteration 4, loss = 0.03857099\n",
      "Iteration 5, loss = 0.03584700\n",
      "Iteration 6, loss = 0.03398923\n",
      "Iteration 7, loss = 0.03268349\n",
      "Iteration 8, loss = 0.03144527\n",
      "Iteration 9, loss = 0.03045743\n",
      "Iteration 10, loss = 0.02953171\n",
      "Iteration 11, loss = 0.02872555\n",
      "Iteration 12, loss = 0.02800040\n",
      "Iteration 13, loss = 0.02729285\n",
      "Iteration 14, loss = 0.02657255\n",
      "Iteration 15, loss = 0.02595505\n",
      "Iteration 16, loss = 0.02533947\n",
      "Iteration 17, loss = 0.02475139\n",
      "Iteration 18, loss = 0.02418734\n",
      "Iteration 19, loss = 0.02365068\n",
      "Iteration 20, loss = 0.02312481\n",
      "Iteration 21, loss = 0.02261576\n",
      "Iteration 22, loss = 0.02212568\n",
      "Iteration 23, loss = 0.02165663\n",
      "Iteration 24, loss = 0.02121175\n",
      "Iteration 25, loss = 0.02077757\n",
      "Iteration 26, loss = 0.02037695\n",
      "Iteration 27, loss = 0.01995544\n",
      "Iteration 28, loss = 0.01959593\n",
      "Iteration 29, loss = 0.01920553\n",
      "Iteration 30, loss = 0.01885729\n",
      "Iteration 31, loss = 0.01850213\n",
      "Iteration 32, loss = 0.01815852\n",
      "Iteration 33, loss = 0.01783095\n",
      "Iteration 34, loss = 0.01751150\n",
      "Iteration 35, loss = 0.01720485\n",
      "Iteration 36, loss = 0.01691505\n",
      "Iteration 37, loss = 0.01663122\n",
      "Iteration 38, loss = 0.01635067\n",
      "Iteration 39, loss = 0.01608801\n",
      "Iteration 40, loss = 0.01583785\n",
      "Iteration 41, loss = 0.01559963\n",
      "Iteration 42, loss = 0.01535588\n",
      "Iteration 43, loss = 0.01514481\n",
      "Iteration 44, loss = 0.01489674\n",
      "Iteration 45, loss = 0.01468832\n",
      "Iteration 46, loss = 0.01448492\n",
      "Iteration 47, loss = 0.01428845\n",
      "Iteration 48, loss = 0.01408221\n",
      "Iteration 49, loss = 0.01390111\n",
      "Iteration 50, loss = 0.01371617\n",
      "Iteration 51, loss = 0.01353765\n",
      "Iteration 52, loss = 0.01337009\n",
      "Iteration 53, loss = 0.01322635\n",
      "Iteration 54, loss = 0.01305931\n",
      "Iteration 55, loss = 0.01290109\n",
      "Iteration 56, loss = 0.01276693\n",
      "Iteration 57, loss = 0.01262685\n",
      "Iteration 58, loss = 0.01247615\n",
      "Iteration 59, loss = 0.01234945\n",
      "Iteration 60, loss = 0.01222096\n",
      "Iteration 61, loss = 0.01209762\n",
      "Iteration 62, loss = 0.01197255\n",
      "Iteration 63, loss = 0.01185077\n",
      "Iteration 64, loss = 0.01174054\n",
      "Iteration 65, loss = 0.01163906\n",
      "Iteration 66, loss = 0.01153269\n",
      "Iteration 67, loss = 0.01143103\n",
      "Iteration 68, loss = 0.01134281\n",
      "Iteration 69, loss = 0.01124752\n",
      "Iteration 70, loss = 0.01116033\n",
      "Iteration 71, loss = 0.01106346\n",
      "Iteration 72, loss = 0.01098263\n",
      "Iteration 73, loss = 0.01089243\n",
      "Iteration 74, loss = 0.01081441\n",
      "Iteration 75, loss = 0.01074438\n",
      "Iteration 76, loss = 0.01068343\n",
      "Iteration 77, loss = 0.01059045\n",
      "Iteration 78, loss = 0.01052692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23862783\n",
      "Iteration 2, loss = 0.05638933\n",
      "Iteration 3, loss = 0.04294736\n",
      "Iteration 4, loss = 0.03844904\n",
      "Iteration 5, loss = 0.03603815\n",
      "Iteration 6, loss = 0.03441487\n",
      "Iteration 7, loss = 0.03322061\n",
      "Iteration 8, loss = 0.03218770\n",
      "Iteration 9, loss = 0.03131015\n",
      "Iteration 10, loss = 0.03046979\n",
      "Iteration 11, loss = 0.02969332\n",
      "Iteration 12, loss = 0.02898290\n",
      "Iteration 13, loss = 0.02832767\n",
      "Iteration 14, loss = 0.02762428\n",
      "Iteration 15, loss = 0.02701385\n",
      "Iteration 16, loss = 0.02639237\n",
      "Iteration 17, loss = 0.02582329\n",
      "Iteration 18, loss = 0.02525891\n",
      "Iteration 19, loss = 0.02473245\n",
      "Iteration 20, loss = 0.02420925\n",
      "Iteration 21, loss = 0.02371621\n",
      "Iteration 22, loss = 0.02323159\n",
      "Iteration 23, loss = 0.02276925\n",
      "Iteration 24, loss = 0.02232255\n",
      "Iteration 25, loss = 0.02189353\n",
      "Iteration 26, loss = 0.02148637\n",
      "Iteration 27, loss = 0.02107956\n",
      "Iteration 28, loss = 0.02069975\n",
      "Iteration 29, loss = 0.02032511\n",
      "Iteration 30, loss = 0.01997378\n",
      "Iteration 31, loss = 0.01961547\n",
      "Iteration 32, loss = 0.01929058\n",
      "Iteration 33, loss = 0.01895367\n",
      "Iteration 34, loss = 0.01864487\n",
      "Iteration 35, loss = 0.01833549\n",
      "Iteration 36, loss = 0.01804164\n",
      "Iteration 37, loss = 0.01776364\n",
      "Iteration 38, loss = 0.01748563\n",
      "Iteration 39, loss = 0.01722718\n",
      "Iteration 40, loss = 0.01697365\n",
      "Iteration 41, loss = 0.01672340\n",
      "Iteration 42, loss = 0.01648796\n",
      "Iteration 43, loss = 0.01626459\n",
      "Iteration 44, loss = 0.01604257\n",
      "Iteration 45, loss = 0.01583018\n",
      "Iteration 46, loss = 0.01562551\n",
      "Iteration 47, loss = 0.01541790\n",
      "Iteration 48, loss = 0.01522756\n",
      "Iteration 49, loss = 0.01504021\n",
      "Iteration 50, loss = 0.01486357\n",
      "Iteration 51, loss = 0.01468805\n",
      "Iteration 52, loss = 0.01452179\n",
      "Iteration 53, loss = 0.01436935\n",
      "Iteration 54, loss = 0.01420138\n",
      "Iteration 55, loss = 0.01405151\n",
      "Iteration 56, loss = 0.01390067\n",
      "Iteration 57, loss = 0.01375910\n",
      "Iteration 58, loss = 0.01363660\n",
      "Iteration 59, loss = 0.01349152\n",
      "Iteration 60, loss = 0.01336311\n",
      "Iteration 61, loss = 0.01324093\n",
      "Iteration 62, loss = 0.01312583\n",
      "Iteration 63, loss = 0.01300704\n",
      "Iteration 64, loss = 0.01289539\n",
      "Iteration 65, loss = 0.01279057\n",
      "Iteration 66, loss = 0.01268963\n",
      "Iteration 67, loss = 0.01258441\n",
      "Iteration 68, loss = 0.01248767\n",
      "Iteration 69, loss = 0.01239569\n",
      "Iteration 70, loss = 0.01230640\n",
      "Iteration 71, loss = 0.01222608\n",
      "Iteration 72, loss = 0.01213877\n",
      "Iteration 73, loss = 0.01205114\n",
      "Iteration 74, loss = 0.01197058\n",
      "Iteration 75, loss = 0.01190058\n",
      "Iteration 76, loss = 0.01182949\n",
      "Iteration 77, loss = 0.01175244\n",
      "Iteration 78, loss = 0.01168190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21115862\n",
      "Iteration 2, loss = 0.05186286\n",
      "Iteration 3, loss = 0.04361573\n",
      "Iteration 4, loss = 0.03932515\n",
      "Iteration 5, loss = 0.03652335\n",
      "Iteration 6, loss = 0.03446416\n",
      "Iteration 7, loss = 0.03306413\n",
      "Iteration 8, loss = 0.03192444\n",
      "Iteration 9, loss = 0.03105833\n",
      "Iteration 10, loss = 0.03012244\n",
      "Iteration 11, loss = 0.02931759\n",
      "Iteration 12, loss = 0.02854087\n",
      "Iteration 13, loss = 0.02779641\n",
      "Iteration 14, loss = 0.02708394\n",
      "Iteration 15, loss = 0.02644120\n",
      "Iteration 16, loss = 0.02584503\n",
      "Iteration 17, loss = 0.02523729\n",
      "Iteration 18, loss = 0.02464577\n",
      "Iteration 19, loss = 0.02413536\n",
      "Iteration 20, loss = 0.02359543\n",
      "Iteration 21, loss = 0.02304868\n",
      "Iteration 22, loss = 0.02259156\n",
      "Iteration 23, loss = 0.02208591\n",
      "Iteration 24, loss = 0.02162245\n",
      "Iteration 25, loss = 0.02121627\n",
      "Iteration 26, loss = 0.02076296\n",
      "Iteration 27, loss = 0.02040554\n",
      "Iteration 28, loss = 0.01999164\n",
      "Iteration 29, loss = 0.01960521\n",
      "Iteration 30, loss = 0.01924191\n",
      "Iteration 31, loss = 0.01888048\n",
      "Iteration 32, loss = 0.01857990\n",
      "Iteration 33, loss = 0.01822640\n",
      "Iteration 34, loss = 0.01790784\n",
      "Iteration 35, loss = 0.01761417\n",
      "Iteration 36, loss = 0.01730959\n",
      "Iteration 37, loss = 0.01703548\n",
      "Iteration 38, loss = 0.01676632\n",
      "Iteration 39, loss = 0.01650395\n",
      "Iteration 40, loss = 0.01623192\n",
      "Iteration 41, loss = 0.01599567\n",
      "Iteration 42, loss = 0.01575974\n",
      "Iteration 43, loss = 0.01552544\n",
      "Iteration 44, loss = 0.01529745\n",
      "Iteration 45, loss = 0.01507815\n",
      "Iteration 46, loss = 0.01486688\n",
      "Iteration 47, loss = 0.01468425\n",
      "Iteration 48, loss = 0.01447149\n",
      "Iteration 49, loss = 0.01428581\n",
      "Iteration 50, loss = 0.01410734\n",
      "Iteration 51, loss = 0.01393862\n",
      "Iteration 52, loss = 0.01380071\n",
      "Iteration 53, loss = 0.01361486\n",
      "Iteration 54, loss = 0.01346268\n",
      "Iteration 55, loss = 0.01330226\n",
      "Iteration 56, loss = 0.01316877\n",
      "Iteration 57, loss = 0.01301686\n",
      "Iteration 58, loss = 0.01287275\n",
      "Iteration 59, loss = 0.01274302\n",
      "Iteration 60, loss = 0.01260118\n",
      "Iteration 61, loss = 0.01252075\n",
      "Iteration 62, loss = 0.01237053\n",
      "Iteration 63, loss = 0.01225935\n",
      "Iteration 64, loss = 0.01214620\n",
      "Iteration 65, loss = 0.01202710\n",
      "Iteration 66, loss = 0.01193671\n",
      "Iteration 67, loss = 0.01184207\n",
      "Iteration 68, loss = 0.01174543\n",
      "Iteration 69, loss = 0.01164315\n",
      "Iteration 70, loss = 0.01157380\n",
      "Iteration 71, loss = 0.01150090\n",
      "Iteration 72, loss = 0.01138529\n",
      "Iteration 73, loss = 0.01129682\n",
      "Iteration 74, loss = 0.01121193\n",
      "Iteration 75, loss = 0.01114538\n",
      "Iteration 76, loss = 0.01106286\n",
      "Iteration 77, loss = 0.01100107\n",
      "Iteration 78, loss = 0.01092938\n",
      "Iteration 79, loss = 0.01086303\n",
      "Iteration 80, loss = 0.01079518\n",
      "Iteration 81, loss = 0.01073576\n",
      "Iteration 82, loss = 0.01070970\n",
      "Iteration 83, loss = 0.01062840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23730345\n",
      "Iteration 2, loss = 0.05330539\n",
      "Iteration 3, loss = 0.04198066\n",
      "Iteration 4, loss = 0.03802944\n",
      "Iteration 5, loss = 0.03578474\n",
      "Iteration 6, loss = 0.03434347\n",
      "Iteration 7, loss = 0.03318636\n",
      "Iteration 8, loss = 0.03222961\n",
      "Iteration 9, loss = 0.03133049\n",
      "Iteration 10, loss = 0.03051923\n",
      "Iteration 11, loss = 0.02975994\n",
      "Iteration 12, loss = 0.02905133\n",
      "Iteration 13, loss = 0.02836666\n",
      "Iteration 14, loss = 0.02769999\n",
      "Iteration 15, loss = 0.02707237\n",
      "Iteration 16, loss = 0.02647862\n",
      "Iteration 17, loss = 0.02590330\n",
      "Iteration 18, loss = 0.02533813\n",
      "Iteration 19, loss = 0.02479706\n",
      "Iteration 20, loss = 0.02427990\n",
      "Iteration 21, loss = 0.02378358\n",
      "Iteration 22, loss = 0.02329811\n",
      "Iteration 23, loss = 0.02283954\n",
      "Iteration 24, loss = 0.02239248\n",
      "Iteration 25, loss = 0.02196813\n",
      "Iteration 26, loss = 0.02154700\n",
      "Iteration 27, loss = 0.02114521\n",
      "Iteration 28, loss = 0.02075790\n",
      "Iteration 29, loss = 0.02037595\n",
      "Iteration 30, loss = 0.02002309\n",
      "Iteration 31, loss = 0.01966779\n",
      "Iteration 32, loss = 0.01933413\n",
      "Iteration 33, loss = 0.01902177\n",
      "Iteration 34, loss = 0.01869214\n",
      "Iteration 35, loss = 0.01839194\n",
      "Iteration 36, loss = 0.01810040\n",
      "Iteration 37, loss = 0.01781597\n",
      "Iteration 38, loss = 0.01753842\n",
      "Iteration 39, loss = 0.01727618\n",
      "Iteration 40, loss = 0.01702284\n",
      "Iteration 41, loss = 0.01677224\n",
      "Iteration 42, loss = 0.01653852\n",
      "Iteration 43, loss = 0.01631098\n",
      "Iteration 44, loss = 0.01609589\n",
      "Iteration 45, loss = 0.01587951\n",
      "Iteration 46, loss = 0.01566121\n",
      "Iteration 47, loss = 0.01546489\n",
      "Iteration 48, loss = 0.01527220\n",
      "Iteration 49, loss = 0.01508630\n",
      "Iteration 50, loss = 0.01490165\n",
      "Iteration 51, loss = 0.01473886\n",
      "Iteration 52, loss = 0.01456057\n",
      "Iteration 53, loss = 0.01440405\n",
      "Iteration 54, loss = 0.01424282\n",
      "Iteration 55, loss = 0.01409122\n",
      "Iteration 56, loss = 0.01394812\n",
      "Iteration 57, loss = 0.01380551\n",
      "Iteration 58, loss = 0.01366716\n",
      "Iteration 59, loss = 0.01353192\n",
      "Iteration 60, loss = 0.01340527\n",
      "Iteration 61, loss = 0.01328908\n",
      "Iteration 62, loss = 0.01317919\n",
      "Iteration 63, loss = 0.01305780\n",
      "Iteration 64, loss = 0.01294148\n",
      "Iteration 65, loss = 0.01283207\n",
      "Iteration 66, loss = 0.01273127\n",
      "Iteration 67, loss = 0.01262779\n",
      "Iteration 68, loss = 0.01252765\n",
      "Iteration 69, loss = 0.01243423\n",
      "Iteration 70, loss = 0.01234220\n",
      "Iteration 71, loss = 0.01226488\n",
      "Iteration 72, loss = 0.01217222\n",
      "Iteration 73, loss = 0.01209305\n",
      "Iteration 74, loss = 0.01201251\n",
      "Iteration 75, loss = 0.01193630\n",
      "Iteration 76, loss = 0.01186381\n",
      "Iteration 77, loss = 0.01179592\n",
      "Iteration 78, loss = 0.01172269\n",
      "Iteration 79, loss = 0.01165627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19727000\n",
      "Iteration 2, loss = 0.04063844\n",
      "Iteration 3, loss = 0.03637920\n",
      "Iteration 4, loss = 0.03454680\n",
      "Iteration 5, loss = 0.03317040\n",
      "Iteration 6, loss = 0.03207030\n",
      "Iteration 7, loss = 0.03111415\n",
      "Iteration 8, loss = 0.03022005\n",
      "Iteration 9, loss = 0.02936108\n",
      "Iteration 10, loss = 0.02854676\n",
      "Iteration 11, loss = 0.02779274\n",
      "Iteration 12, loss = 0.02707683\n",
      "Iteration 13, loss = 0.02640645\n",
      "Iteration 14, loss = 0.02572046\n",
      "Iteration 15, loss = 0.02507771\n",
      "Iteration 16, loss = 0.02451296\n",
      "Iteration 17, loss = 0.02390059\n",
      "Iteration 18, loss = 0.02333885\n",
      "Iteration 19, loss = 0.02281105\n",
      "Iteration 20, loss = 0.02228380\n",
      "Iteration 21, loss = 0.02177700\n",
      "Iteration 22, loss = 0.02128782\n",
      "Iteration 23, loss = 0.02082042\n",
      "Iteration 24, loss = 0.02038225\n",
      "Iteration 25, loss = 0.01993610\n",
      "Iteration 26, loss = 0.01954837\n",
      "Iteration 27, loss = 0.01912780\n",
      "Iteration 28, loss = 0.01873015\n",
      "Iteration 29, loss = 0.01835260\n",
      "Iteration 30, loss = 0.01799239\n",
      "Iteration 31, loss = 0.01763930\n",
      "Iteration 32, loss = 0.01731006\n",
      "Iteration 33, loss = 0.01698070\n",
      "Iteration 34, loss = 0.01665814\n",
      "Iteration 35, loss = 0.01635834\n",
      "Iteration 36, loss = 0.01605998\n",
      "Iteration 37, loss = 0.01577928\n",
      "Iteration 38, loss = 0.01550960\n",
      "Iteration 39, loss = 0.01523985\n",
      "Iteration 40, loss = 0.01498288\n",
      "Iteration 41, loss = 0.01473876\n",
      "Iteration 42, loss = 0.01449832\n",
      "Iteration 43, loss = 0.01426735\n",
      "Iteration 44, loss = 0.01404087\n",
      "Iteration 45, loss = 0.01382840\n",
      "Iteration 46, loss = 0.01362495\n",
      "Iteration 47, loss = 0.01342050\n",
      "Iteration 48, loss = 0.01322286\n",
      "Iteration 49, loss = 0.01303470\n",
      "Iteration 50, loss = 0.01285187\n",
      "Iteration 51, loss = 0.01267956\n",
      "Iteration 52, loss = 0.01251793\n",
      "Iteration 53, loss = 0.01235009\n",
      "Iteration 54, loss = 0.01219799\n",
      "Iteration 55, loss = 0.01205126\n",
      "Iteration 56, loss = 0.01189390\n",
      "Iteration 57, loss = 0.01175123\n",
      "Iteration 58, loss = 0.01161255\n",
      "Iteration 59, loss = 0.01148384\n",
      "Iteration 60, loss = 0.01135671\n",
      "Iteration 61, loss = 0.01122770\n",
      "Iteration 62, loss = 0.01111056\n",
      "Iteration 63, loss = 0.01099319\n",
      "Iteration 64, loss = 0.01087611\n",
      "Iteration 65, loss = 0.01077918\n",
      "Iteration 66, loss = 0.01066838\n",
      "Iteration 67, loss = 0.01056936\n",
      "Iteration 68, loss = 0.01047143\n",
      "Iteration 69, loss = 0.01038074\n",
      "Iteration 70, loss = 0.01028616\n",
      "Iteration 71, loss = 0.01020435\n",
      "Iteration 72, loss = 0.01012021\n",
      "Iteration 73, loss = 0.01005693\n",
      "Iteration 74, loss = 0.00995248\n",
      "Iteration 75, loss = 0.00987965\n",
      "Iteration 76, loss = 0.00980555\n",
      "Iteration 77, loss = 0.00974002\n",
      "Iteration 78, loss = 0.00966793\n",
      "Iteration 79, loss = 0.00959704\n",
      "Iteration 80, loss = 0.00953992\n",
      "Iteration 81, loss = 0.00947313\n",
      "Iteration 82, loss = 0.00942623\n",
      "Iteration 83, loss = 0.00935045\n",
      "Iteration 84, loss = 0.00929727\n",
      "Iteration 85, loss = 0.00924834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12313735\n",
      "Iteration 2, loss = 0.03223844\n",
      "Iteration 3, loss = 0.02196495\n",
      "Iteration 4, loss = 0.01727224\n",
      "Iteration 5, loss = 0.01435692\n",
      "Iteration 6, loss = 0.01264226\n",
      "Iteration 7, loss = 0.01162568\n",
      "Iteration 8, loss = 0.01094934\n",
      "Iteration 9, loss = 0.01083790\n",
      "Iteration 10, loss = 0.01017384\n",
      "Iteration 11, loss = 0.00989447\n",
      "Iteration 12, loss = 0.01085604\n",
      "Iteration 13, loss = 0.00979892\n",
      "Iteration 14, loss = 0.01049374\n",
      "Iteration 15, loss = 0.00982008\n",
      "Iteration 16, loss = 0.00933395\n",
      "Iteration 17, loss = 0.00951137\n",
      "Iteration 18, loss = 0.01095039\n",
      "Iteration 19, loss = 0.01061740\n",
      "Iteration 20, loss = 0.01218506\n",
      "Iteration 21, loss = 0.00989970\n",
      "Iteration 22, loss = 0.00947002\n",
      "Iteration 23, loss = 0.00898089\n",
      "Iteration 24, loss = 0.00908364\n",
      "Iteration 25, loss = 0.00961429\n",
      "Iteration 26, loss = 0.00986875\n",
      "Iteration 27, loss = 0.00963236\n",
      "Iteration 28, loss = 0.00982802\n",
      "Iteration 29, loss = 0.00954236\n",
      "Iteration 30, loss = 0.01011476\n",
      "Iteration 31, loss = 0.00927982\n",
      "Iteration 32, loss = 0.00957610\n",
      "Iteration 33, loss = 0.01005113\n",
      "Iteration 34, loss = 0.01024158\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.14108866\n",
      "Iteration 2, loss = 0.03438977\n",
      "Iteration 3, loss = 0.02386089\n",
      "Iteration 4, loss = 0.01856792\n",
      "Iteration 5, loss = 0.01582884\n",
      "Iteration 6, loss = 0.01393011\n",
      "Iteration 7, loss = 0.01249926\n",
      "Iteration 8, loss = 0.01191886\n",
      "Iteration 9, loss = 0.01138794\n",
      "Iteration 10, loss = 0.01085929\n",
      "Iteration 11, loss = 0.01102345\n",
      "Iteration 12, loss = 0.01059646\n",
      "Iteration 13, loss = 0.01052278\n",
      "Iteration 14, loss = 0.01048212\n",
      "Iteration 15, loss = 0.01041579\n",
      "Iteration 16, loss = 0.01073460\n",
      "Iteration 17, loss = 0.01077932\n",
      "Iteration 18, loss = 0.01067459\n",
      "Iteration 19, loss = 0.01113750\n",
      "Iteration 20, loss = 0.01065030\n",
      "Iteration 21, loss = 0.01046371\n",
      "Iteration 22, loss = 0.01088738\n",
      "Iteration 23, loss = 0.01030201\n",
      "Iteration 24, loss = 0.01059152\n",
      "Iteration 25, loss = 0.01027022\n",
      "Iteration 26, loss = 0.01026025\n",
      "Iteration 27, loss = 0.01131249\n",
      "Iteration 28, loss = 0.01094917\n",
      "Iteration 29, loss = 0.01014505\n",
      "Iteration 30, loss = 0.00991996\n",
      "Iteration 31, loss = 0.01048212\n",
      "Iteration 32, loss = 0.01075633\n",
      "Iteration 33, loss = 0.01035230\n",
      "Iteration 34, loss = 0.01003329\n",
      "Iteration 35, loss = 0.01235767\n",
      "Iteration 36, loss = 0.01207356\n",
      "Iteration 37, loss = 0.01066965\n",
      "Iteration 38, loss = 0.01038399\n",
      "Iteration 39, loss = 0.01008024\n",
      "Iteration 40, loss = 0.01095058\n",
      "Iteration 41, loss = 0.01031215\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12935661\n",
      "Iteration 2, loss = 0.03217536\n",
      "Iteration 3, loss = 0.02278569\n",
      "Iteration 4, loss = 0.01791633\n",
      "Iteration 5, loss = 0.01574352\n",
      "Iteration 6, loss = 0.01393482\n",
      "Iteration 7, loss = 0.01201380\n",
      "Iteration 8, loss = 0.01142279\n",
      "Iteration 9, loss = 0.01114382\n",
      "Iteration 10, loss = 0.01068285\n",
      "Iteration 11, loss = 0.01047314\n",
      "Iteration 12, loss = 0.01080751\n",
      "Iteration 13, loss = 0.01075135\n",
      "Iteration 14, loss = 0.01063841\n",
      "Iteration 15, loss = 0.00983341\n",
      "Iteration 16, loss = 0.01018997\n",
      "Iteration 17, loss = 0.01083073\n",
      "Iteration 18, loss = 0.01035935\n",
      "Iteration 19, loss = 0.01110789\n",
      "Iteration 20, loss = 0.01168208\n",
      "Iteration 21, loss = 0.01699740\n",
      "Iteration 22, loss = 0.01402250\n",
      "Iteration 23, loss = 0.01026616\n",
      "Iteration 24, loss = 0.01001603\n",
      "Iteration 25, loss = 0.01003815\n",
      "Iteration 26, loss = 0.00961868\n",
      "Iteration 27, loss = 0.00967153\n",
      "Iteration 28, loss = 0.01053713\n",
      "Iteration 29, loss = 0.01063151\n",
      "Iteration 30, loss = 0.01023235\n",
      "Iteration 31, loss = 0.01230867\n",
      "Iteration 32, loss = 0.01004721\n",
      "Iteration 33, loss = 0.01012941\n",
      "Iteration 34, loss = 0.01045880\n",
      "Iteration 35, loss = 0.00945336\n",
      "Iteration 36, loss = 0.00986001\n",
      "Iteration 37, loss = 0.01260906\n",
      "Iteration 38, loss = 0.01208117\n",
      "Iteration 39, loss = 0.00953621\n",
      "Iteration 40, loss = 0.00958549\n",
      "Iteration 41, loss = 0.00977247\n",
      "Iteration 42, loss = 0.01072629\n",
      "Iteration 43, loss = 0.01065701\n",
      "Iteration 44, loss = 0.00961400\n",
      "Iteration 45, loss = 0.01110821\n",
      "Iteration 46, loss = 0.01117080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13485980\n",
      "Iteration 2, loss = 0.03379239\n",
      "Iteration 3, loss = 0.02337307\n",
      "Iteration 4, loss = 0.01832231\n",
      "Iteration 5, loss = 0.01542394\n",
      "Iteration 6, loss = 0.01360145\n",
      "Iteration 7, loss = 0.01233961\n",
      "Iteration 8, loss = 0.01226807\n",
      "Iteration 9, loss = 0.01150690\n",
      "Iteration 10, loss = 0.01086318\n",
      "Iteration 11, loss = 0.01072332\n",
      "Iteration 12, loss = 0.01064316\n",
      "Iteration 13, loss = 0.01060843\n",
      "Iteration 14, loss = 0.01037443\n",
      "Iteration 15, loss = 0.01018559\n",
      "Iteration 16, loss = 0.01067175\n",
      "Iteration 17, loss = 0.01029773\n",
      "Iteration 18, loss = 0.01047684\n",
      "Iteration 19, loss = 0.01053364\n",
      "Iteration 20, loss = 0.01135643\n",
      "Iteration 21, loss = 0.01019493\n",
      "Iteration 22, loss = 0.01009803\n",
      "Iteration 23, loss = 0.01023538\n",
      "Iteration 24, loss = 0.01114648\n",
      "Iteration 25, loss = 0.01225233\n",
      "Iteration 26, loss = 0.01039255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11373873\n",
      "Iteration 2, loss = 0.02936879\n",
      "Iteration 3, loss = 0.02047071\n",
      "Iteration 4, loss = 0.01569230\n",
      "Iteration 5, loss = 0.01356731\n",
      "Iteration 6, loss = 0.01187316\n",
      "Iteration 7, loss = 0.01101746\n",
      "Iteration 8, loss = 0.01072964\n",
      "Iteration 9, loss = 0.00969675\n",
      "Iteration 10, loss = 0.01010228\n",
      "Iteration 11, loss = 0.00942733\n",
      "Iteration 12, loss = 0.00934445\n",
      "Iteration 13, loss = 0.01015796\n",
      "Iteration 14, loss = 0.00987633\n",
      "Iteration 15, loss = 0.01018899\n",
      "Iteration 16, loss = 0.01072214\n",
      "Iteration 17, loss = 0.00998442\n",
      "Iteration 18, loss = 0.00944767\n",
      "Iteration 19, loss = 0.00846375\n",
      "Iteration 20, loss = 0.00863586\n",
      "Iteration 21, loss = 0.00955736\n",
      "Iteration 22, loss = 0.00923590\n",
      "Iteration 23, loss = 0.00916874\n",
      "Iteration 24, loss = 0.00850520\n",
      "Iteration 25, loss = 0.01097264\n",
      "Iteration 26, loss = 0.00856215\n",
      "Iteration 27, loss = 0.01000519\n",
      "Iteration 28, loss = 0.00976784\n",
      "Iteration 29, loss = 0.00869976\n",
      "Iteration 30, loss = 0.00887507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51602405\n",
      "Iteration 2, loss = 0.17845313\n",
      "Iteration 3, loss = 0.11665125\n",
      "Iteration 4, loss = 0.09459501\n",
      "Iteration 5, loss = 0.08240332\n",
      "Iteration 6, loss = 0.07403793\n",
      "Iteration 7, loss = 0.06808938\n",
      "Iteration 8, loss = 0.06352076\n",
      "Iteration 9, loss = 0.06003885\n",
      "Iteration 10, loss = 0.05722832\n",
      "Iteration 11, loss = 0.05491677\n",
      "Iteration 12, loss = 0.05301434\n",
      "Iteration 13, loss = 0.05141667\n",
      "Iteration 14, loss = 0.04989030\n",
      "Iteration 15, loss = 0.04867822\n",
      "Iteration 16, loss = 0.04754687\n",
      "Iteration 17, loss = 0.04655162\n",
      "Iteration 18, loss = 0.04569973\n",
      "Iteration 19, loss = 0.04475353\n",
      "Iteration 20, loss = 0.04394186\n",
      "Iteration 21, loss = 0.04323219\n",
      "Iteration 22, loss = 0.04255202\n",
      "Iteration 23, loss = 0.04192354\n",
      "Iteration 24, loss = 0.04133713\n",
      "Iteration 25, loss = 0.04080546\n",
      "Iteration 26, loss = 0.04032562\n",
      "Iteration 27, loss = 0.03983162\n",
      "Iteration 28, loss = 0.03943160\n",
      "Iteration 29, loss = 0.03897844\n",
      "Iteration 30, loss = 0.03858916\n",
      "Iteration 31, loss = 0.03821842\n",
      "Iteration 32, loss = 0.03788828\n",
      "Iteration 33, loss = 0.03752626\n",
      "Iteration 34, loss = 0.03719775\n",
      "Iteration 35, loss = 0.03688910\n",
      "Iteration 36, loss = 0.03662218\n",
      "Iteration 37, loss = 0.03633429\n",
      "Iteration 38, loss = 0.03606480\n",
      "Iteration 39, loss = 0.03584091\n",
      "Iteration 40, loss = 0.03558610\n",
      "Iteration 41, loss = 0.03536561\n",
      "Iteration 42, loss = 0.03512964\n",
      "Iteration 43, loss = 0.03492376\n",
      "Iteration 44, loss = 0.03471098\n",
      "Iteration 45, loss = 0.03451810\n",
      "Iteration 46, loss = 0.03432546\n",
      "Iteration 47, loss = 0.03414633\n",
      "Iteration 48, loss = 0.03396684\n",
      "Iteration 49, loss = 0.03379364\n",
      "Iteration 50, loss = 0.03363622\n",
      "Iteration 51, loss = 0.03347019\n",
      "Iteration 52, loss = 0.03333404\n",
      "Iteration 53, loss = 0.03316065\n",
      "Iteration 54, loss = 0.03301146\n",
      "Iteration 55, loss = 0.03287665\n",
      "Iteration 56, loss = 0.03273912\n",
      "Iteration 57, loss = 0.03260061\n",
      "Iteration 58, loss = 0.03245155\n",
      "Iteration 59, loss = 0.03233461\n",
      "Iteration 60, loss = 0.03219431\n",
      "Iteration 61, loss = 0.03207228\n",
      "Iteration 62, loss = 0.03194140\n",
      "Iteration 63, loss = 0.03182297\n",
      "Iteration 64, loss = 0.03170158\n",
      "Iteration 65, loss = 0.03158909\n",
      "Iteration 66, loss = 0.03146745\n",
      "Iteration 67, loss = 0.03136591\n",
      "Iteration 68, loss = 0.03125145\n",
      "Iteration 69, loss = 0.03114065\n",
      "Iteration 70, loss = 0.03103601\n",
      "Iteration 71, loss = 0.03092804\n",
      "Iteration 72, loss = 0.03082344\n",
      "Iteration 73, loss = 0.03071933\n",
      "Iteration 74, loss = 0.03061967\n",
      "Iteration 75, loss = 0.03052830\n",
      "Iteration 76, loss = 0.03042614\n",
      "Iteration 77, loss = 0.03032494\n",
      "Iteration 78, loss = 0.03023050\n",
      "Iteration 79, loss = 0.03013965\n",
      "Iteration 80, loss = 0.03004352\n",
      "Iteration 81, loss = 0.02995177\n",
      "Iteration 82, loss = 0.02986864\n",
      "Iteration 83, loss = 0.02976915\n",
      "Iteration 84, loss = 0.02968240\n",
      "Iteration 85, loss = 0.02959270\n",
      "Iteration 86, loss = 0.02950914\n",
      "Iteration 87, loss = 0.02941662\n",
      "Iteration 88, loss = 0.02933345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56115711\n",
      "Iteration 2, loss = 0.24615219\n",
      "Iteration 3, loss = 0.16431255\n",
      "Iteration 4, loss = 0.12921507\n",
      "Iteration 5, loss = 0.10890286\n",
      "Iteration 6, loss = 0.09455230\n",
      "Iteration 7, loss = 0.08380622\n",
      "Iteration 8, loss = 0.07571542\n",
      "Iteration 9, loss = 0.06963907\n",
      "Iteration 10, loss = 0.06473079\n",
      "Iteration 11, loss = 0.06088584\n",
      "Iteration 12, loss = 0.05788750\n",
      "Iteration 13, loss = 0.05539860\n",
      "Iteration 14, loss = 0.05308085\n",
      "Iteration 15, loss = 0.05132462\n",
      "Iteration 16, loss = 0.04969390\n",
      "Iteration 17, loss = 0.04831065\n",
      "Iteration 18, loss = 0.04716418\n",
      "Iteration 19, loss = 0.04600108\n",
      "Iteration 20, loss = 0.04502360\n",
      "Iteration 21, loss = 0.04415741\n",
      "Iteration 22, loss = 0.04336966\n",
      "Iteration 23, loss = 0.04265611\n",
      "Iteration 24, loss = 0.04201064\n",
      "Iteration 25, loss = 0.04141186\n",
      "Iteration 26, loss = 0.04088824\n",
      "Iteration 27, loss = 0.04038028\n",
      "Iteration 28, loss = 0.03994492\n",
      "Iteration 29, loss = 0.03949666\n",
      "Iteration 30, loss = 0.03909189\n",
      "Iteration 31, loss = 0.03873633\n",
      "Iteration 32, loss = 0.03838359\n",
      "Iteration 33, loss = 0.03804424\n",
      "Iteration 34, loss = 0.03772842\n",
      "Iteration 35, loss = 0.03742126\n",
      "Iteration 36, loss = 0.03715580\n",
      "Iteration 37, loss = 0.03689420\n",
      "Iteration 38, loss = 0.03663565\n",
      "Iteration 39, loss = 0.03640570\n",
      "Iteration 40, loss = 0.03618235\n",
      "Iteration 41, loss = 0.03595390\n",
      "Iteration 42, loss = 0.03574879\n",
      "Iteration 43, loss = 0.03555865\n",
      "Iteration 44, loss = 0.03536596\n",
      "Iteration 45, loss = 0.03518829\n",
      "Iteration 46, loss = 0.03500237\n",
      "Iteration 47, loss = 0.03483080\n",
      "Iteration 48, loss = 0.03466716\n",
      "Iteration 49, loss = 0.03450951\n",
      "Iteration 50, loss = 0.03435859\n",
      "Iteration 51, loss = 0.03420620\n",
      "Iteration 52, loss = 0.03408009\n",
      "Iteration 53, loss = 0.03392525\n",
      "Iteration 54, loss = 0.03378400\n",
      "Iteration 55, loss = 0.03365575\n",
      "Iteration 56, loss = 0.03351706\n",
      "Iteration 57, loss = 0.03339480\n",
      "Iteration 58, loss = 0.03326822\n",
      "Iteration 59, loss = 0.03314896\n",
      "Iteration 60, loss = 0.03302285\n",
      "Iteration 61, loss = 0.03290943\n",
      "Iteration 62, loss = 0.03279458\n",
      "Iteration 63, loss = 0.03267932\n",
      "Iteration 64, loss = 0.03256975\n",
      "Iteration 65, loss = 0.03246198\n",
      "Iteration 66, loss = 0.03235819\n",
      "Iteration 67, loss = 0.03225088\n",
      "Iteration 68, loss = 0.03214689\n",
      "Iteration 69, loss = 0.03204766\n",
      "Iteration 70, loss = 0.03194867\n",
      "Iteration 71, loss = 0.03185365\n",
      "Iteration 72, loss = 0.03175437\n",
      "Iteration 73, loss = 0.03165290\n",
      "Iteration 74, loss = 0.03155863\n",
      "Iteration 75, loss = 0.03146931\n",
      "Iteration 76, loss = 0.03137427\n",
      "Iteration 77, loss = 0.03128289\n",
      "Iteration 78, loss = 0.03119447\n",
      "Iteration 79, loss = 0.03110240\n",
      "Iteration 80, loss = 0.03101645\n",
      "Iteration 81, loss = 0.03092777\n",
      "Iteration 82, loss = 0.03084621\n",
      "Iteration 83, loss = 0.03075562\n",
      "Iteration 84, loss = 0.03067228\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54772946\n",
      "Iteration 2, loss = 0.20734189\n",
      "Iteration 3, loss = 0.13079730\n",
      "Iteration 4, loss = 0.10088447\n",
      "Iteration 5, loss = 0.08564139\n",
      "Iteration 6, loss = 0.07595516\n",
      "Iteration 7, loss = 0.06923993\n",
      "Iteration 8, loss = 0.06445190\n",
      "Iteration 9, loss = 0.06078512\n",
      "Iteration 10, loss = 0.05793749\n",
      "Iteration 11, loss = 0.05553356\n",
      "Iteration 12, loss = 0.05372470\n",
      "Iteration 13, loss = 0.05203871\n",
      "Iteration 14, loss = 0.05065957\n",
      "Iteration 15, loss = 0.04944582\n",
      "Iteration 16, loss = 0.04836669\n",
      "Iteration 17, loss = 0.04737024\n",
      "Iteration 18, loss = 0.04646722\n",
      "Iteration 19, loss = 0.04563986\n",
      "Iteration 20, loss = 0.04491025\n",
      "Iteration 21, loss = 0.04424961\n",
      "Iteration 22, loss = 0.04359600\n",
      "Iteration 23, loss = 0.04296697\n",
      "Iteration 24, loss = 0.04243129\n",
      "Iteration 25, loss = 0.04188335\n",
      "Iteration 26, loss = 0.04142144\n",
      "Iteration 27, loss = 0.04092256\n",
      "Iteration 28, loss = 0.04046890\n",
      "Iteration 29, loss = 0.04003718\n",
      "Iteration 30, loss = 0.03963536\n",
      "Iteration 31, loss = 0.03927470\n",
      "Iteration 32, loss = 0.03889488\n",
      "Iteration 33, loss = 0.03852495\n",
      "Iteration 34, loss = 0.03818158\n",
      "Iteration 35, loss = 0.03786329\n",
      "Iteration 36, loss = 0.03755105\n",
      "Iteration 37, loss = 0.03727319\n",
      "Iteration 38, loss = 0.03700224\n",
      "Iteration 39, loss = 0.03671803\n",
      "Iteration 40, loss = 0.03645404\n",
      "Iteration 41, loss = 0.03621674\n",
      "Iteration 42, loss = 0.03598712\n",
      "Iteration 43, loss = 0.03577170\n",
      "Iteration 44, loss = 0.03553216\n",
      "Iteration 45, loss = 0.03529948\n",
      "Iteration 46, loss = 0.03510956\n",
      "Iteration 47, loss = 0.03491403\n",
      "Iteration 48, loss = 0.03470369\n",
      "Iteration 49, loss = 0.03452378\n",
      "Iteration 50, loss = 0.03434704\n",
      "Iteration 51, loss = 0.03418938\n",
      "Iteration 52, loss = 0.03402990\n",
      "Iteration 53, loss = 0.03386097\n",
      "Iteration 54, loss = 0.03369755\n",
      "Iteration 55, loss = 0.03354140\n",
      "Iteration 56, loss = 0.03340396\n",
      "Iteration 57, loss = 0.03324445\n",
      "Iteration 58, loss = 0.03310157\n",
      "Iteration 59, loss = 0.03296489\n",
      "Iteration 60, loss = 0.03283313\n",
      "Iteration 61, loss = 0.03270841\n",
      "Iteration 62, loss = 0.03256518\n",
      "Iteration 63, loss = 0.03244469\n",
      "Iteration 64, loss = 0.03232025\n",
      "Iteration 65, loss = 0.03220458\n",
      "Iteration 66, loss = 0.03208886\n",
      "Iteration 67, loss = 0.03196114\n",
      "Iteration 68, loss = 0.03185247\n",
      "Iteration 69, loss = 0.03174352\n",
      "Iteration 70, loss = 0.03163643\n",
      "Iteration 71, loss = 0.03153194\n",
      "Iteration 72, loss = 0.03141291\n",
      "Iteration 73, loss = 0.03130442\n",
      "Iteration 74, loss = 0.03120110\n",
      "Iteration 75, loss = 0.03109691\n",
      "Iteration 76, loss = 0.03100171\n",
      "Iteration 77, loss = 0.03089667\n",
      "Iteration 78, loss = 0.03079957\n",
      "Iteration 79, loss = 0.03070954\n",
      "Iteration 80, loss = 0.03060702\n",
      "Iteration 81, loss = 0.03051947\n",
      "Iteration 82, loss = 0.03042699\n",
      "Iteration 83, loss = 0.03032813\n",
      "Iteration 84, loss = 0.03024299\n",
      "Iteration 85, loss = 0.03014869\n",
      "Iteration 86, loss = 0.03005241\n",
      "Iteration 87, loss = 0.02997216\n",
      "Iteration 88, loss = 0.02988079\n",
      "Iteration 89, loss = 0.02979413\n",
      "Iteration 90, loss = 0.02970816\n",
      "Iteration 91, loss = 0.02962172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56936934\n",
      "Iteration 2, loss = 0.25590209\n",
      "Iteration 3, loss = 0.16217385\n",
      "Iteration 4, loss = 0.12417165\n",
      "Iteration 5, loss = 0.10314763\n",
      "Iteration 6, loss = 0.08953915\n",
      "Iteration 7, loss = 0.07965261\n",
      "Iteration 8, loss = 0.07255170\n",
      "Iteration 9, loss = 0.06708884\n",
      "Iteration 10, loss = 0.06282573\n",
      "Iteration 11, loss = 0.05930295\n",
      "Iteration 12, loss = 0.05648771\n",
      "Iteration 13, loss = 0.05418270\n",
      "Iteration 14, loss = 0.05205599\n",
      "Iteration 15, loss = 0.05037745\n",
      "Iteration 16, loss = 0.04885743\n",
      "Iteration 17, loss = 0.04758728\n",
      "Iteration 18, loss = 0.04640525\n",
      "Iteration 19, loss = 0.04536398\n",
      "Iteration 20, loss = 0.04444893\n",
      "Iteration 21, loss = 0.04362467\n",
      "Iteration 22, loss = 0.04291316\n",
      "Iteration 23, loss = 0.04225438\n",
      "Iteration 24, loss = 0.04160470\n",
      "Iteration 25, loss = 0.04105249\n",
      "Iteration 26, loss = 0.04053650\n",
      "Iteration 27, loss = 0.04004522\n",
      "Iteration 28, loss = 0.03961757\n",
      "Iteration 29, loss = 0.03920686\n",
      "Iteration 30, loss = 0.03881863\n",
      "Iteration 31, loss = 0.03845279\n",
      "Iteration 32, loss = 0.03814349\n",
      "Iteration 33, loss = 0.03783935\n",
      "Iteration 34, loss = 0.03751532\n",
      "Iteration 35, loss = 0.03724555\n",
      "Iteration 36, loss = 0.03698213\n",
      "Iteration 37, loss = 0.03673387\n",
      "Iteration 38, loss = 0.03649441\n",
      "Iteration 39, loss = 0.03626175\n",
      "Iteration 40, loss = 0.03605216\n",
      "Iteration 41, loss = 0.03583970\n",
      "Iteration 42, loss = 0.03565076\n",
      "Iteration 43, loss = 0.03546801\n",
      "Iteration 44, loss = 0.03527979\n",
      "Iteration 45, loss = 0.03509613\n",
      "Iteration 46, loss = 0.03492593\n",
      "Iteration 47, loss = 0.03476731\n",
      "Iteration 48, loss = 0.03461393\n",
      "Iteration 49, loss = 0.03445120\n",
      "Iteration 50, loss = 0.03430089\n",
      "Iteration 51, loss = 0.03416265\n",
      "Iteration 52, loss = 0.03401408\n",
      "Iteration 53, loss = 0.03388091\n",
      "Iteration 54, loss = 0.03374547\n",
      "Iteration 55, loss = 0.03361679\n",
      "Iteration 56, loss = 0.03349438\n",
      "Iteration 57, loss = 0.03336455\n",
      "Iteration 58, loss = 0.03324483\n",
      "Iteration 59, loss = 0.03312499\n",
      "Iteration 60, loss = 0.03300859\n",
      "Iteration 61, loss = 0.03289874\n",
      "Iteration 62, loss = 0.03279010\n",
      "Iteration 63, loss = 0.03267315\n",
      "Iteration 64, loss = 0.03256367\n",
      "Iteration 65, loss = 0.03245592\n",
      "Iteration 66, loss = 0.03235342\n",
      "Iteration 67, loss = 0.03224781\n",
      "Iteration 68, loss = 0.03214578\n",
      "Iteration 69, loss = 0.03204662\n",
      "Iteration 70, loss = 0.03195415\n",
      "Iteration 71, loss = 0.03185531\n",
      "Iteration 72, loss = 0.03175739\n",
      "Iteration 73, loss = 0.03166365\n",
      "Iteration 74, loss = 0.03157033\n",
      "Iteration 75, loss = 0.03147820\n",
      "Iteration 76, loss = 0.03138738\n",
      "Iteration 77, loss = 0.03129823\n",
      "Iteration 78, loss = 0.03120895\n",
      "Iteration 79, loss = 0.03112208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53561269\n",
      "Iteration 2, loss = 0.20559546\n",
      "Iteration 3, loss = 0.12284637\n",
      "Iteration 4, loss = 0.09128479\n",
      "Iteration 5, loss = 0.07502815\n",
      "Iteration 6, loss = 0.06539807\n",
      "Iteration 7, loss = 0.05894422\n",
      "Iteration 8, loss = 0.05435232\n",
      "Iteration 9, loss = 0.05095996\n",
      "Iteration 10, loss = 0.04832914\n",
      "Iteration 11, loss = 0.04630356\n",
      "Iteration 12, loss = 0.04462729\n",
      "Iteration 13, loss = 0.04327176\n",
      "Iteration 14, loss = 0.04215493\n",
      "Iteration 15, loss = 0.04119550\n",
      "Iteration 16, loss = 0.04039515\n",
      "Iteration 17, loss = 0.03966586\n",
      "Iteration 18, loss = 0.03905483\n",
      "Iteration 19, loss = 0.03850375\n",
      "Iteration 20, loss = 0.03802762\n",
      "Iteration 21, loss = 0.03757398\n",
      "Iteration 22, loss = 0.03716937\n",
      "Iteration 23, loss = 0.03681033\n",
      "Iteration 24, loss = 0.03648248\n",
      "Iteration 25, loss = 0.03619325\n",
      "Iteration 26, loss = 0.03588926\n",
      "Iteration 27, loss = 0.03562572\n",
      "Iteration 28, loss = 0.03536899\n",
      "Iteration 29, loss = 0.03513789\n",
      "Iteration 30, loss = 0.03491459\n",
      "Iteration 31, loss = 0.03470517\n",
      "Iteration 32, loss = 0.03451472\n",
      "Iteration 33, loss = 0.03431839\n",
      "Iteration 34, loss = 0.03413233\n",
      "Iteration 35, loss = 0.03395978\n",
      "Iteration 36, loss = 0.03378842\n",
      "Iteration 37, loss = 0.03363016\n",
      "Iteration 38, loss = 0.03347740\n",
      "Iteration 39, loss = 0.03332075\n",
      "Iteration 40, loss = 0.03317415\n",
      "Iteration 41, loss = 0.03302537\n",
      "Iteration 42, loss = 0.03289993\n",
      "Iteration 43, loss = 0.03275650\n",
      "Iteration 44, loss = 0.03262095\n",
      "Iteration 45, loss = 0.03249413\n",
      "Iteration 46, loss = 0.03236576\n",
      "Iteration 47, loss = 0.03224400\n",
      "Iteration 48, loss = 0.03212056\n",
      "Iteration 49, loss = 0.03200042\n",
      "Iteration 50, loss = 0.03188951\n",
      "Iteration 51, loss = 0.03177226\n",
      "Iteration 52, loss = 0.03166261\n",
      "Iteration 53, loss = 0.03155603\n",
      "Iteration 54, loss = 0.03144450\n",
      "Iteration 55, loss = 0.03134203\n",
      "Iteration 56, loss = 0.03123405\n",
      "Iteration 57, loss = 0.03111816\n",
      "Iteration 58, loss = 0.03102142\n",
      "Iteration 59, loss = 0.03092222\n",
      "Iteration 60, loss = 0.03081890\n",
      "Iteration 61, loss = 0.03072041\n",
      "Iteration 62, loss = 0.03062628\n",
      "Iteration 63, loss = 0.03052839\n",
      "Iteration 64, loss = 0.03044005\n",
      "Iteration 65, loss = 0.03034406\n",
      "Iteration 66, loss = 0.03024430\n",
      "Iteration 67, loss = 0.03015236\n",
      "Iteration 68, loss = 0.03006516\n",
      "Iteration 69, loss = 0.02997092\n",
      "Iteration 70, loss = 0.02988109\n",
      "Iteration 71, loss = 0.02979307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46005206\n",
      "Iteration 2, loss = 0.13616134\n",
      "Iteration 3, loss = 0.07109774\n",
      "Iteration 4, loss = 0.05161017\n",
      "Iteration 5, loss = 0.04293136\n",
      "Iteration 6, loss = 0.03760364\n",
      "Iteration 7, loss = 0.03397977\n",
      "Iteration 8, loss = 0.03120454\n",
      "Iteration 9, loss = 0.02896565\n",
      "Iteration 10, loss = 0.02723781\n",
      "Iteration 11, loss = 0.02571062\n",
      "Iteration 12, loss = 0.02442971\n",
      "Iteration 13, loss = 0.02335148\n",
      "Iteration 14, loss = 0.02227011\n",
      "Iteration 15, loss = 0.02137417\n",
      "Iteration 16, loss = 0.02053741\n",
      "Iteration 17, loss = 0.01980469\n",
      "Iteration 18, loss = 0.01908167\n",
      "Iteration 19, loss = 0.01845119\n",
      "Iteration 20, loss = 0.01785837\n",
      "Iteration 21, loss = 0.01730571\n",
      "Iteration 22, loss = 0.01678193\n",
      "Iteration 23, loss = 0.01630124\n",
      "Iteration 24, loss = 0.01586585\n",
      "Iteration 25, loss = 0.01545086\n",
      "Iteration 26, loss = 0.01509569\n",
      "Iteration 27, loss = 0.01470247\n",
      "Iteration 28, loss = 0.01442692\n",
      "Iteration 29, loss = 0.01407135\n",
      "Iteration 30, loss = 0.01382045\n",
      "Iteration 31, loss = 0.01351370\n",
      "Iteration 32, loss = 0.01321396\n",
      "Iteration 33, loss = 0.01297408\n",
      "Iteration 34, loss = 0.01271231\n",
      "Iteration 35, loss = 0.01248369\n",
      "Iteration 36, loss = 0.01228307\n",
      "Iteration 37, loss = 0.01208839\n",
      "Iteration 38, loss = 0.01187551\n",
      "Iteration 39, loss = 0.01171095\n",
      "Iteration 40, loss = 0.01156228\n",
      "Iteration 41, loss = 0.01143628\n",
      "Iteration 42, loss = 0.01124903\n",
      "Iteration 43, loss = 0.01115053\n",
      "Iteration 44, loss = 0.01096996\n",
      "Iteration 45, loss = 0.01086795\n",
      "Iteration 46, loss = 0.01075065\n",
      "Iteration 47, loss = 0.01066321\n",
      "Iteration 48, loss = 0.01054022\n",
      "Iteration 49, loss = 0.01043941\n",
      "Iteration 50, loss = 0.01036478\n",
      "Iteration 51, loss = 0.01022706\n",
      "Iteration 52, loss = 0.01015359\n",
      "Iteration 53, loss = 0.01008042\n",
      "Iteration 54, loss = 0.01002798\n",
      "Iteration 55, loss = 0.00995221\n",
      "Iteration 56, loss = 0.00988034\n",
      "Iteration 57, loss = 0.00982188\n",
      "Iteration 58, loss = 0.00974757\n",
      "Iteration 59, loss = 0.00966960\n",
      "Iteration 60, loss = 0.00967429\n",
      "Iteration 61, loss = 0.00957756\n",
      "Iteration 62, loss = 0.00956826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49881804\n",
      "Iteration 2, loss = 0.17834988\n",
      "Iteration 3, loss = 0.09418843\n",
      "Iteration 4, loss = 0.06396036\n",
      "Iteration 5, loss = 0.05042674\n",
      "Iteration 6, loss = 0.04292200\n",
      "Iteration 7, loss = 0.03817006\n",
      "Iteration 8, loss = 0.03478881\n",
      "Iteration 9, loss = 0.03222624\n",
      "Iteration 10, loss = 0.03017644\n",
      "Iteration 11, loss = 0.02843885\n",
      "Iteration 12, loss = 0.02701879\n",
      "Iteration 13, loss = 0.02583228\n",
      "Iteration 14, loss = 0.02458716\n",
      "Iteration 15, loss = 0.02359130\n",
      "Iteration 16, loss = 0.02264057\n",
      "Iteration 17, loss = 0.02183439\n",
      "Iteration 18, loss = 0.02103384\n",
      "Iteration 19, loss = 0.02033335\n",
      "Iteration 20, loss = 0.01968536\n",
      "Iteration 21, loss = 0.01908176\n",
      "Iteration 22, loss = 0.01850648\n",
      "Iteration 23, loss = 0.01798234\n",
      "Iteration 24, loss = 0.01749311\n",
      "Iteration 25, loss = 0.01704369\n",
      "Iteration 26, loss = 0.01663468\n",
      "Iteration 27, loss = 0.01622831\n",
      "Iteration 28, loss = 0.01587978\n",
      "Iteration 29, loss = 0.01552358\n",
      "Iteration 30, loss = 0.01525363\n",
      "Iteration 31, loss = 0.01487002\n",
      "Iteration 32, loss = 0.01460743\n",
      "Iteration 33, loss = 0.01431573\n",
      "Iteration 34, loss = 0.01406641\n",
      "Iteration 35, loss = 0.01381011\n",
      "Iteration 36, loss = 0.01358903\n",
      "Iteration 37, loss = 0.01339123\n",
      "Iteration 38, loss = 0.01316275\n",
      "Iteration 39, loss = 0.01298936\n",
      "Iteration 40, loss = 0.01280958\n",
      "Iteration 41, loss = 0.01264039\n",
      "Iteration 42, loss = 0.01247655\n",
      "Iteration 43, loss = 0.01234374\n",
      "Iteration 44, loss = 0.01218922\n",
      "Iteration 45, loss = 0.01207737\n",
      "Iteration 46, loss = 0.01192702\n",
      "Iteration 47, loss = 0.01179658\n",
      "Iteration 48, loss = 0.01169464\n",
      "Iteration 49, loss = 0.01157866\n",
      "Iteration 50, loss = 0.01148008\n",
      "Iteration 51, loss = 0.01137566\n",
      "Iteration 52, loss = 0.01129868\n",
      "Iteration 53, loss = 0.01122234\n",
      "Iteration 54, loss = 0.01113906\n",
      "Iteration 55, loss = 0.01106524\n",
      "Iteration 56, loss = 0.01097665\n",
      "Iteration 57, loss = 0.01093358\n",
      "Iteration 58, loss = 0.01089124\n",
      "Iteration 59, loss = 0.01080016\n",
      "Iteration 60, loss = 0.01074081\n",
      "Iteration 61, loss = 0.01066382\n",
      "Iteration 62, loss = 0.01062408\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48238274\n",
      "Iteration 2, loss = 0.15732689\n",
      "Iteration 3, loss = 0.08215849\n",
      "Iteration 4, loss = 0.05817112\n",
      "Iteration 5, loss = 0.04728438\n",
      "Iteration 6, loss = 0.04087305\n",
      "Iteration 7, loss = 0.03655873\n",
      "Iteration 8, loss = 0.03347834\n",
      "Iteration 9, loss = 0.03108978\n",
      "Iteration 10, loss = 0.02914516\n",
      "Iteration 11, loss = 0.02742522\n",
      "Iteration 12, loss = 0.02610662\n",
      "Iteration 13, loss = 0.02478372\n",
      "Iteration 14, loss = 0.02366705\n",
      "Iteration 15, loss = 0.02267721\n",
      "Iteration 16, loss = 0.02183926\n",
      "Iteration 17, loss = 0.02096593\n",
      "Iteration 18, loss = 0.02018092\n",
      "Iteration 19, loss = 0.01953606\n",
      "Iteration 20, loss = 0.01888272\n",
      "Iteration 21, loss = 0.01828615\n",
      "Iteration 22, loss = 0.01775909\n",
      "Iteration 23, loss = 0.01718877\n",
      "Iteration 24, loss = 0.01672917\n",
      "Iteration 25, loss = 0.01629595\n",
      "Iteration 26, loss = 0.01584817\n",
      "Iteration 27, loss = 0.01551101\n",
      "Iteration 28, loss = 0.01512141\n",
      "Iteration 29, loss = 0.01477054\n",
      "Iteration 30, loss = 0.01444660\n",
      "Iteration 31, loss = 0.01412683\n",
      "Iteration 32, loss = 0.01390660\n",
      "Iteration 33, loss = 0.01358858\n",
      "Iteration 34, loss = 0.01332140\n",
      "Iteration 35, loss = 0.01310809\n",
      "Iteration 36, loss = 0.01288431\n",
      "Iteration 37, loss = 0.01268423\n",
      "Iteration 38, loss = 0.01247837\n",
      "Iteration 39, loss = 0.01232055\n",
      "Iteration 40, loss = 0.01210458\n",
      "Iteration 41, loss = 0.01195420\n",
      "Iteration 42, loss = 0.01179469\n",
      "Iteration 43, loss = 0.01161283\n",
      "Iteration 44, loss = 0.01150191\n",
      "Iteration 45, loss = 0.01134441\n",
      "Iteration 46, loss = 0.01120640\n",
      "Iteration 47, loss = 0.01112867\n",
      "Iteration 48, loss = 0.01098614\n",
      "Iteration 49, loss = 0.01086171\n",
      "Iteration 50, loss = 0.01077790\n",
      "Iteration 51, loss = 0.01069358\n",
      "Iteration 52, loss = 0.01064519\n",
      "Iteration 53, loss = 0.01057330\n",
      "Iteration 54, loss = 0.01046991\n",
      "Iteration 55, loss = 0.01036498\n",
      "Iteration 56, loss = 0.01034368\n",
      "Iteration 57, loss = 0.01023564\n",
      "Iteration 58, loss = 0.01013510\n",
      "Iteration 59, loss = 0.01009635\n",
      "Iteration 60, loss = 0.00999300\n",
      "Iteration 61, loss = 0.00998064\n",
      "Iteration 62, loss = 0.00992874\n",
      "Iteration 63, loss = 0.00989021\n",
      "Iteration 64, loss = 0.00982826\n",
      "Iteration 65, loss = 0.00975501\n",
      "Iteration 66, loss = 0.00974563\n",
      "Iteration 67, loss = 0.00973929\n",
      "Iteration 68, loss = 0.00974028\n",
      "Iteration 69, loss = 0.00968700\n",
      "Iteration 70, loss = 0.00962176\n",
      "Iteration 71, loss = 0.00958606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49363148\n",
      "Iteration 2, loss = 0.18170527\n",
      "Iteration 3, loss = 0.09388379\n",
      "Iteration 4, loss = 0.06421763\n",
      "Iteration 5, loss = 0.05069794\n",
      "Iteration 6, loss = 0.04338443\n",
      "Iteration 7, loss = 0.03846735\n",
      "Iteration 8, loss = 0.03514605\n",
      "Iteration 9, loss = 0.03253767\n",
      "Iteration 10, loss = 0.03046552\n",
      "Iteration 11, loss = 0.02872014\n",
      "Iteration 12, loss = 0.02729922\n",
      "Iteration 13, loss = 0.02600157\n",
      "Iteration 14, loss = 0.02483375\n",
      "Iteration 15, loss = 0.02381327\n",
      "Iteration 16, loss = 0.02290169\n",
      "Iteration 17, loss = 0.02204581\n",
      "Iteration 18, loss = 0.02124878\n",
      "Iteration 19, loss = 0.02051930\n",
      "Iteration 20, loss = 0.01986177\n",
      "Iteration 21, loss = 0.01924763\n",
      "Iteration 22, loss = 0.01867504\n",
      "Iteration 23, loss = 0.01813865\n",
      "Iteration 24, loss = 0.01764630\n",
      "Iteration 25, loss = 0.01720182\n",
      "Iteration 26, loss = 0.01676361\n",
      "Iteration 27, loss = 0.01635275\n",
      "Iteration 28, loss = 0.01597626\n",
      "Iteration 29, loss = 0.01560090\n",
      "Iteration 30, loss = 0.01529138\n",
      "Iteration 31, loss = 0.01495565\n",
      "Iteration 32, loss = 0.01466923\n",
      "Iteration 33, loss = 0.01440090\n",
      "Iteration 34, loss = 0.01413347\n",
      "Iteration 35, loss = 0.01388174\n",
      "Iteration 36, loss = 0.01365766\n",
      "Iteration 37, loss = 0.01344550\n",
      "Iteration 38, loss = 0.01321847\n",
      "Iteration 39, loss = 0.01303198\n",
      "Iteration 40, loss = 0.01286342\n",
      "Iteration 41, loss = 0.01267105\n",
      "Iteration 42, loss = 0.01252575\n",
      "Iteration 43, loss = 0.01238034\n",
      "Iteration 44, loss = 0.01222262\n",
      "Iteration 45, loss = 0.01211056\n",
      "Iteration 46, loss = 0.01196086\n",
      "Iteration 47, loss = 0.01182991\n",
      "Iteration 48, loss = 0.01171818\n",
      "Iteration 49, loss = 0.01162473\n",
      "Iteration 50, loss = 0.01149967\n",
      "Iteration 51, loss = 0.01142347\n",
      "Iteration 52, loss = 0.01131290\n",
      "Iteration 53, loss = 0.01124980\n",
      "Iteration 54, loss = 0.01115459\n",
      "Iteration 55, loss = 0.01106847\n",
      "Iteration 56, loss = 0.01101298\n",
      "Iteration 57, loss = 0.01092898\n",
      "Iteration 58, loss = 0.01086190\n",
      "Iteration 59, loss = 0.01080266\n",
      "Iteration 60, loss = 0.01074130\n",
      "Iteration 61, loss = 0.01074663\n",
      "Iteration 62, loss = 0.01069390\n",
      "Iteration 63, loss = 0.01059244\n",
      "Iteration 64, loss = 0.01056611\n",
      "Iteration 65, loss = 0.01050658\n",
      "Iteration 66, loss = 0.01046489\n",
      "Iteration 67, loss = 0.01042869\n",
      "Iteration 68, loss = 0.01038254\n",
      "Iteration 69, loss = 0.01034171\n",
      "Iteration 70, loss = 0.01032550\n",
      "Iteration 71, loss = 0.01030935\n",
      "Iteration 72, loss = 0.01024550\n",
      "Iteration 73, loss = 0.01024152\n",
      "Iteration 74, loss = 0.01021192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47553309\n",
      "Iteration 2, loss = 0.14633174\n",
      "Iteration 3, loss = 0.06886998\n",
      "Iteration 4, loss = 0.04653479\n",
      "Iteration 5, loss = 0.03734176\n",
      "Iteration 6, loss = 0.03250589\n",
      "Iteration 7, loss = 0.02944267\n",
      "Iteration 8, loss = 0.02721924\n",
      "Iteration 9, loss = 0.02555915\n",
      "Iteration 10, loss = 0.02418264\n",
      "Iteration 11, loss = 0.02302926\n",
      "Iteration 12, loss = 0.02200858\n",
      "Iteration 13, loss = 0.02111967\n",
      "Iteration 14, loss = 0.02024318\n",
      "Iteration 15, loss = 0.01947395\n",
      "Iteration 16, loss = 0.01880239\n",
      "Iteration 17, loss = 0.01812511\n",
      "Iteration 18, loss = 0.01753143\n",
      "Iteration 19, loss = 0.01698716\n",
      "Iteration 20, loss = 0.01644208\n",
      "Iteration 21, loss = 0.01594184\n",
      "Iteration 22, loss = 0.01547436\n",
      "Iteration 23, loss = 0.01503355\n",
      "Iteration 24, loss = 0.01464855\n",
      "Iteration 25, loss = 0.01424430\n",
      "Iteration 26, loss = 0.01393881\n",
      "Iteration 27, loss = 0.01357001\n",
      "Iteration 28, loss = 0.01323040\n",
      "Iteration 29, loss = 0.01292220\n",
      "Iteration 30, loss = 0.01265002\n",
      "Iteration 31, loss = 0.01237274\n",
      "Iteration 32, loss = 0.01213451\n",
      "Iteration 33, loss = 0.01191384\n",
      "Iteration 34, loss = 0.01164598\n",
      "Iteration 35, loss = 0.01144753\n",
      "Iteration 36, loss = 0.01123380\n",
      "Iteration 37, loss = 0.01104910\n",
      "Iteration 38, loss = 0.01089907\n",
      "Iteration 39, loss = 0.01071225\n",
      "Iteration 40, loss = 0.01055158\n",
      "Iteration 41, loss = 0.01040386\n",
      "Iteration 42, loss = 0.01025247\n",
      "Iteration 43, loss = 0.01011919\n",
      "Iteration 44, loss = 0.00997924\n",
      "Iteration 45, loss = 0.00985734\n",
      "Iteration 46, loss = 0.00975434\n",
      "Iteration 47, loss = 0.00965272\n",
      "Iteration 48, loss = 0.00952598\n",
      "Iteration 49, loss = 0.00943163\n",
      "Iteration 50, loss = 0.00933030\n",
      "Iteration 51, loss = 0.00925535\n",
      "Iteration 52, loss = 0.00919555\n",
      "Iteration 53, loss = 0.00909377\n",
      "Iteration 54, loss = 0.00902825\n",
      "Iteration 55, loss = 0.00897220\n",
      "Iteration 56, loss = 0.00892051\n",
      "Iteration 57, loss = 0.00883657\n",
      "Iteration 58, loss = 0.00877282\n",
      "Iteration 59, loss = 0.00872046\n",
      "Iteration 60, loss = 0.00865739\n",
      "Iteration 61, loss = 0.00859366\n",
      "Iteration 62, loss = 0.00855404\n",
      "Iteration 63, loss = 0.00850241\n",
      "Iteration 64, loss = 0.00845632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78629606\n",
      "Iteration 2, loss = 0.65029399\n",
      "Iteration 3, loss = 0.54373776\n",
      "Iteration 4, loss = 0.46021006\n",
      "Iteration 5, loss = 0.39356278\n",
      "Iteration 6, loss = 0.34081124\n",
      "Iteration 7, loss = 0.29950106\n",
      "Iteration 8, loss = 0.26709074\n",
      "Iteration 9, loss = 0.24150926\n",
      "Iteration 10, loss = 0.22101289\n",
      "Iteration 11, loss = 0.20431503\n",
      "Iteration 12, loss = 0.19039468\n",
      "Iteration 13, loss = 0.17871874\n",
      "Iteration 14, loss = 0.16871456\n",
      "Iteration 15, loss = 0.16009030\n",
      "Iteration 16, loss = 0.15255121\n",
      "Iteration 17, loss = 0.14596513\n",
      "Iteration 18, loss = 0.14013167\n",
      "Iteration 19, loss = 0.13493993\n",
      "Iteration 20, loss = 0.13028145\n",
      "Iteration 21, loss = 0.12611154\n",
      "Iteration 22, loss = 0.12231794\n",
      "Iteration 23, loss = 0.11889378\n",
      "Iteration 24, loss = 0.11573687\n",
      "Iteration 25, loss = 0.11282534\n",
      "Iteration 26, loss = 0.11015527\n",
      "Iteration 27, loss = 0.10767516\n",
      "Iteration 28, loss = 0.10535976\n",
      "Iteration 29, loss = 0.10320413\n",
      "Iteration 30, loss = 0.10118123\n",
      "Iteration 31, loss = 0.09930035\n",
      "Iteration 32, loss = 0.09750835\n",
      "Iteration 33, loss = 0.09581776\n",
      "Iteration 34, loss = 0.09423608\n",
      "Iteration 35, loss = 0.09271781\n",
      "Iteration 36, loss = 0.09128008\n",
      "Iteration 37, loss = 0.08990638\n",
      "Iteration 38, loss = 0.08860668\n",
      "Iteration 39, loss = 0.08738557\n",
      "Iteration 40, loss = 0.08615867\n",
      "Iteration 41, loss = 0.08500535\n",
      "Iteration 42, loss = 0.08390194\n",
      "Iteration 43, loss = 0.08284643\n",
      "Iteration 44, loss = 0.08183304\n",
      "Iteration 45, loss = 0.08085463\n",
      "Iteration 46, loss = 0.07992348\n",
      "Iteration 47, loss = 0.07901043\n",
      "Iteration 48, loss = 0.07815760\n",
      "Iteration 49, loss = 0.07730913\n",
      "Iteration 50, loss = 0.07650623\n",
      "Iteration 51, loss = 0.07572100\n",
      "Iteration 52, loss = 0.07497601\n",
      "Iteration 53, loss = 0.07424403\n",
      "Iteration 54, loss = 0.07353417\n",
      "Iteration 55, loss = 0.07286422\n",
      "Iteration 56, loss = 0.07219448\n",
      "Iteration 57, loss = 0.07155480\n",
      "Iteration 58, loss = 0.07093552\n",
      "Iteration 59, loss = 0.07036350\n",
      "Iteration 60, loss = 0.06975506\n",
      "Iteration 61, loss = 0.06919290\n",
      "Iteration 62, loss = 0.06864418\n",
      "Iteration 63, loss = 0.06811037\n",
      "Iteration 64, loss = 0.06759283\n",
      "Iteration 65, loss = 0.06708972\n",
      "Iteration 66, loss = 0.06659857\n",
      "Iteration 67, loss = 0.06612756\n",
      "Iteration 68, loss = 0.06565875\n",
      "Iteration 69, loss = 0.06521067\n",
      "Iteration 70, loss = 0.06477607\n",
      "Iteration 71, loss = 0.06435428\n",
      "Iteration 72, loss = 0.06393674\n",
      "Iteration 73, loss = 0.06352646\n",
      "Iteration 74, loss = 0.06313849\n",
      "Iteration 75, loss = 0.06275588\n",
      "Iteration 76, loss = 0.06238038\n",
      "Iteration 77, loss = 0.06201053\n",
      "Iteration 78, loss = 0.06165148\n",
      "Iteration 79, loss = 0.06130569\n",
      "Iteration 80, loss = 0.06096867\n",
      "Iteration 81, loss = 0.06062778\n",
      "Iteration 82, loss = 0.06031268\n",
      "Iteration 83, loss = 0.05998599\n",
      "Iteration 84, loss = 0.05967384\n",
      "Iteration 85, loss = 0.05936446\n",
      "Iteration 86, loss = 0.05908046\n",
      "Iteration 87, loss = 0.05877732\n",
      "Iteration 88, loss = 0.05849031\n",
      "Iteration 89, loss = 0.05821004\n",
      "Iteration 90, loss = 0.05793349\n",
      "Iteration 91, loss = 0.05767406\n",
      "Iteration 92, loss = 0.05741086\n",
      "Iteration 93, loss = 0.05714885\n",
      "Iteration 94, loss = 0.05689805\n",
      "Iteration 95, loss = 0.05664870\n",
      "Iteration 96, loss = 0.05640610\n",
      "Iteration 97, loss = 0.05617107\n",
      "Iteration 98, loss = 0.05594195\n",
      "Iteration 99, loss = 0.05570537\n",
      "Iteration 100, loss = 0.05548064\n",
      "Iteration 101, loss = 0.05526344\n",
      "Iteration 102, loss = 0.05504488\n",
      "Iteration 103, loss = 0.05483205\n",
      "Iteration 104, loss = 0.05461390\n",
      "Iteration 105, loss = 0.05441128\n",
      "Iteration 106, loss = 0.05420317\n",
      "Iteration 107, loss = 0.05400433\n",
      "Iteration 108, loss = 0.05380661\n",
      "Iteration 109, loss = 0.05361662\n",
      "Iteration 110, loss = 0.05343884\n",
      "Iteration 111, loss = 0.05324157\n",
      "Iteration 112, loss = 0.05305193\n",
      "Iteration 113, loss = 0.05287503\n",
      "Iteration 114, loss = 0.05269655\n",
      "Iteration 115, loss = 0.05251752\n",
      "Iteration 116, loss = 0.05235017\n",
      "Iteration 117, loss = 0.05217607\n",
      "Iteration 118, loss = 0.05200553\n",
      "Iteration 119, loss = 0.05183934\n",
      "Iteration 120, loss = 0.05167770\n",
      "Iteration 121, loss = 0.05151624\n",
      "Iteration 122, loss = 0.05136204\n",
      "Iteration 123, loss = 0.05120063\n",
      "Iteration 124, loss = 0.05104690\n",
      "Iteration 125, loss = 0.05089207\n",
      "Iteration 126, loss = 0.05074393\n",
      "Iteration 127, loss = 0.05059824\n",
      "Iteration 128, loss = 0.05044929\n",
      "Iteration 129, loss = 0.05030475\n",
      "Iteration 130, loss = 0.05016249\n",
      "Iteration 131, loss = 0.05002099\n",
      "Iteration 132, loss = 0.04988735\n",
      "Iteration 133, loss = 0.04974740\n",
      "Iteration 134, loss = 0.04961190\n",
      "Iteration 135, loss = 0.04948478\n",
      "Iteration 136, loss = 0.04934928\n",
      "Iteration 137, loss = 0.04921429\n",
      "Iteration 138, loss = 0.04908296\n",
      "Iteration 139, loss = 0.04895809\n",
      "Iteration 140, loss = 0.04883227\n",
      "Iteration 141, loss = 0.04870990\n",
      "Iteration 142, loss = 0.04858662\n",
      "Iteration 143, loss = 0.04846264\n",
      "Iteration 144, loss = 0.04834041\n",
      "Iteration 145, loss = 0.04822540\n",
      "Iteration 146, loss = 0.04810779\n",
      "Iteration 147, loss = 0.04798796\n",
      "Iteration 148, loss = 0.04787194\n",
      "Iteration 149, loss = 0.04775733\n",
      "Iteration 150, loss = 0.04764583\n",
      "Iteration 151, loss = 0.04753135\n",
      "Iteration 152, loss = 0.04742194\n",
      "Iteration 153, loss = 0.04731050\n",
      "Iteration 154, loss = 0.04720511\n",
      "Iteration 155, loss = 0.04709971\n",
      "Iteration 156, loss = 0.04698847\n",
      "Iteration 157, loss = 0.04688591\n",
      "Iteration 158, loss = 0.04678317\n",
      "Iteration 159, loss = 0.04668016\n",
      "Iteration 160, loss = 0.04657765\n",
      "Iteration 161, loss = 0.04648106\n",
      "Iteration 162, loss = 0.04637690\n",
      "Iteration 163, loss = 0.04628090\n",
      "Iteration 164, loss = 0.04617999\n",
      "Iteration 165, loss = 0.04608590\n",
      "Iteration 166, loss = 0.04598953\n",
      "Iteration 167, loss = 0.04589437\n",
      "Iteration 168, loss = 0.04579901\n",
      "Iteration 169, loss = 0.04570676\n",
      "Iteration 170, loss = 0.04561391\n",
      "Iteration 171, loss = 0.04552601\n",
      "Iteration 172, loss = 0.04543680\n",
      "Iteration 173, loss = 0.04534805\n",
      "Iteration 174, loss = 0.04525851\n",
      "Iteration 175, loss = 0.04517138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79966415\n",
      "Iteration 2, loss = 0.67862317\n",
      "Iteration 3, loss = 0.58489818\n",
      "Iteration 4, loss = 0.51248159\n",
      "Iteration 5, loss = 0.45440084\n",
      "Iteration 6, loss = 0.40735929\n",
      "Iteration 7, loss = 0.36946083\n",
      "Iteration 8, loss = 0.33832404\n",
      "Iteration 9, loss = 0.31263511\n",
      "Iteration 10, loss = 0.29109818\n",
      "Iteration 11, loss = 0.27277292\n",
      "Iteration 12, loss = 0.25689563\n",
      "Iteration 13, loss = 0.24310083\n",
      "Iteration 14, loss = 0.23090726\n",
      "Iteration 15, loss = 0.22006599\n",
      "Iteration 16, loss = 0.21035440\n",
      "Iteration 17, loss = 0.20168979\n",
      "Iteration 18, loss = 0.19383309\n",
      "Iteration 19, loss = 0.18670457\n",
      "Iteration 20, loss = 0.18020981\n",
      "Iteration 21, loss = 0.17429280\n",
      "Iteration 22, loss = 0.16884647\n",
      "Iteration 23, loss = 0.16383087\n",
      "Iteration 24, loss = 0.15921251\n",
      "Iteration 25, loss = 0.15488357\n",
      "Iteration 26, loss = 0.15089450\n",
      "Iteration 27, loss = 0.14712905\n",
      "Iteration 28, loss = 0.14364702\n",
      "Iteration 29, loss = 0.14034179\n",
      "Iteration 30, loss = 0.13721403\n",
      "Iteration 31, loss = 0.13429717\n",
      "Iteration 32, loss = 0.13151474\n",
      "Iteration 33, loss = 0.12885555\n",
      "Iteration 34, loss = 0.12635210\n",
      "Iteration 35, loss = 0.12395494\n",
      "Iteration 36, loss = 0.12164606\n",
      "Iteration 37, loss = 0.11947102\n",
      "Iteration 38, loss = 0.11736734\n",
      "Iteration 39, loss = 0.11536888\n",
      "Iteration 40, loss = 0.11341675\n",
      "Iteration 41, loss = 0.11154976\n",
      "Iteration 42, loss = 0.10974179\n",
      "Iteration 43, loss = 0.10801193\n",
      "Iteration 44, loss = 0.10634796\n",
      "Iteration 45, loss = 0.10473009\n",
      "Iteration 46, loss = 0.10316646\n",
      "Iteration 47, loss = 0.10165552\n",
      "Iteration 48, loss = 0.10021801\n",
      "Iteration 49, loss = 0.09878621\n",
      "Iteration 50, loss = 0.09742970\n",
      "Iteration 51, loss = 0.09610150\n",
      "Iteration 52, loss = 0.09483700\n",
      "Iteration 53, loss = 0.09359339\n",
      "Iteration 54, loss = 0.09239331\n",
      "Iteration 55, loss = 0.09123970\n",
      "Iteration 56, loss = 0.09009841\n",
      "Iteration 57, loss = 0.08900792\n",
      "Iteration 58, loss = 0.08794493\n",
      "Iteration 59, loss = 0.08693677\n",
      "Iteration 60, loss = 0.08591230\n",
      "Iteration 61, loss = 0.08492892\n",
      "Iteration 62, loss = 0.08398537\n",
      "Iteration 63, loss = 0.08307369\n",
      "Iteration 64, loss = 0.08217657\n",
      "Iteration 65, loss = 0.08131179\n",
      "Iteration 66, loss = 0.08047499\n",
      "Iteration 67, loss = 0.07965463\n",
      "Iteration 68, loss = 0.07884885\n",
      "Iteration 69, loss = 0.07807848\n",
      "Iteration 70, loss = 0.07733879\n",
      "Iteration 71, loss = 0.07660682\n",
      "Iteration 72, loss = 0.07589108\n",
      "Iteration 73, loss = 0.07518683\n",
      "Iteration 74, loss = 0.07451642\n",
      "Iteration 75, loss = 0.07386110\n",
      "Iteration 76, loss = 0.07322179\n",
      "Iteration 77, loss = 0.07259333\n",
      "Iteration 78, loss = 0.07198710\n",
      "Iteration 79, loss = 0.07138829\n",
      "Iteration 80, loss = 0.07082941\n",
      "Iteration 81, loss = 0.07024509\n",
      "Iteration 82, loss = 0.06971362\n",
      "Iteration 83, loss = 0.06916869\n",
      "Iteration 84, loss = 0.06864459\n",
      "Iteration 85, loss = 0.06812919\n",
      "Iteration 86, loss = 0.06762881\n",
      "Iteration 87, loss = 0.06714525\n",
      "Iteration 88, loss = 0.06667127\n",
      "Iteration 89, loss = 0.06620311\n",
      "Iteration 90, loss = 0.06575063\n",
      "Iteration 91, loss = 0.06531234\n",
      "Iteration 92, loss = 0.06487837\n",
      "Iteration 93, loss = 0.06445180\n",
      "Iteration 94, loss = 0.06403397\n",
      "Iteration 95, loss = 0.06363711\n",
      "Iteration 96, loss = 0.06323731\n",
      "Iteration 97, loss = 0.06285758\n",
      "Iteration 98, loss = 0.06249055\n",
      "Iteration 99, loss = 0.06211027\n",
      "Iteration 100, loss = 0.06174295\n",
      "Iteration 101, loss = 0.06138910\n",
      "Iteration 102, loss = 0.06104062\n",
      "Iteration 103, loss = 0.06070746\n",
      "Iteration 104, loss = 0.06035761\n",
      "Iteration 105, loss = 0.06003358\n",
      "Iteration 106, loss = 0.05970860\n",
      "Iteration 107, loss = 0.05939189\n",
      "Iteration 108, loss = 0.05908329\n",
      "Iteration 109, loss = 0.05878013\n",
      "Iteration 110, loss = 0.05848920\n",
      "Iteration 111, loss = 0.05819453\n",
      "Iteration 112, loss = 0.05791059\n",
      "Iteration 113, loss = 0.05762844\n",
      "Iteration 114, loss = 0.05735646\n",
      "Iteration 115, loss = 0.05707630\n",
      "Iteration 116, loss = 0.05681461\n",
      "Iteration 117, loss = 0.05655632\n",
      "Iteration 118, loss = 0.05630520\n",
      "Iteration 119, loss = 0.05604467\n",
      "Iteration 120, loss = 0.05579778\n",
      "Iteration 121, loss = 0.05556040\n",
      "Iteration 122, loss = 0.05531973\n",
      "Iteration 123, loss = 0.05507984\n",
      "Iteration 124, loss = 0.05485022\n",
      "Iteration 125, loss = 0.05461852\n",
      "Iteration 126, loss = 0.05440200\n",
      "Iteration 127, loss = 0.05418230\n",
      "Iteration 128, loss = 0.05396248\n",
      "Iteration 129, loss = 0.05375045\n",
      "Iteration 130, loss = 0.05353931\n",
      "Iteration 131, loss = 0.05333451\n",
      "Iteration 132, loss = 0.05313423\n",
      "Iteration 133, loss = 0.05293040\n",
      "Iteration 134, loss = 0.05273624\n",
      "Iteration 135, loss = 0.05254862\n",
      "Iteration 136, loss = 0.05235912\n",
      "Iteration 137, loss = 0.05217035\n",
      "Iteration 138, loss = 0.05198374\n",
      "Iteration 139, loss = 0.05180763\n",
      "Iteration 140, loss = 0.05162175\n",
      "Iteration 141, loss = 0.05144595\n",
      "Iteration 142, loss = 0.05127950\n",
      "Iteration 143, loss = 0.05110178\n",
      "Iteration 144, loss = 0.05093277\n",
      "Iteration 145, loss = 0.05077501\n",
      "Iteration 146, loss = 0.05060361\n",
      "Iteration 147, loss = 0.05044367\n",
      "Iteration 148, loss = 0.05028303\n",
      "Iteration 149, loss = 0.05012544\n",
      "Iteration 150, loss = 0.04997339\n",
      "Iteration 151, loss = 0.04981745\n",
      "Iteration 152, loss = 0.04967047\n",
      "Iteration 153, loss = 0.04951883\n",
      "Iteration 154, loss = 0.04937365\n",
      "Iteration 155, loss = 0.04922935\n",
      "Iteration 156, loss = 0.04908337\n",
      "Iteration 157, loss = 0.04894770\n",
      "Iteration 158, loss = 0.04881015\n",
      "Iteration 159, loss = 0.04866938\n",
      "Iteration 160, loss = 0.04853906\n",
      "Iteration 161, loss = 0.04840476\n",
      "Iteration 162, loss = 0.04826867\n",
      "Iteration 163, loss = 0.04813944\n",
      "Iteration 164, loss = 0.04800950\n",
      "Iteration 165, loss = 0.04788438\n",
      "Iteration 166, loss = 0.04775806\n",
      "Iteration 167, loss = 0.04763865\n",
      "Iteration 168, loss = 0.04751189\n",
      "Iteration 169, loss = 0.04739211\n",
      "Iteration 170, loss = 0.04727526\n",
      "Iteration 171, loss = 0.04716298\n",
      "Iteration 172, loss = 0.04704529\n",
      "Iteration 173, loss = 0.04693029\n",
      "Iteration 174, loss = 0.04681408\n",
      "Iteration 175, loss = 0.04670124\n",
      "Iteration 176, loss = 0.04659171\n",
      "Iteration 177, loss = 0.04648845\n",
      "Iteration 178, loss = 0.04637535\n",
      "Iteration 179, loss = 0.04626680\n",
      "Iteration 180, loss = 0.04616459\n",
      "Iteration 181, loss = 0.04606095\n",
      "Iteration 182, loss = 0.04595794\n",
      "Iteration 183, loss = 0.04585645\n",
      "Iteration 184, loss = 0.04575676\n",
      "Iteration 185, loss = 0.04566119\n",
      "Iteration 186, loss = 0.04555952\n",
      "Iteration 187, loss = 0.04546357\n",
      "Iteration 188, loss = 0.04536667\n",
      "Iteration 189, loss = 0.04527186\n",
      "Iteration 190, loss = 0.04517748\n",
      "Iteration 191, loss = 0.04508620\n",
      "Iteration 192, loss = 0.04499413\n",
      "Iteration 193, loss = 0.04490320\n",
      "Iteration 194, loss = 0.04481267\n",
      "Iteration 195, loss = 0.04472636\n",
      "Iteration 196, loss = 0.04463873\n",
      "Iteration 197, loss = 0.04454993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79948849\n",
      "Iteration 2, loss = 0.67622094\n",
      "Iteration 3, loss = 0.57726361\n",
      "Iteration 4, loss = 0.49809445\n",
      "Iteration 5, loss = 0.43290547\n",
      "Iteration 6, loss = 0.37966739\n",
      "Iteration 7, loss = 0.33704233\n",
      "Iteration 8, loss = 0.30292250\n",
      "Iteration 9, loss = 0.27556660\n",
      "Iteration 10, loss = 0.25326296\n",
      "Iteration 11, loss = 0.23475684\n",
      "Iteration 12, loss = 0.21914054\n",
      "Iteration 13, loss = 0.20578992\n",
      "Iteration 14, loss = 0.19417011\n",
      "Iteration 15, loss = 0.18399201\n",
      "Iteration 16, loss = 0.17486625\n",
      "Iteration 17, loss = 0.16685046\n",
      "Iteration 18, loss = 0.15954568\n",
      "Iteration 19, loss = 0.15302517\n",
      "Iteration 20, loss = 0.14714092\n",
      "Iteration 21, loss = 0.14175658\n",
      "Iteration 22, loss = 0.13686397\n",
      "Iteration 23, loss = 0.13237878\n",
      "Iteration 24, loss = 0.12827242\n",
      "Iteration 25, loss = 0.12449817\n",
      "Iteration 26, loss = 0.12099634\n",
      "Iteration 27, loss = 0.11774968\n",
      "Iteration 28, loss = 0.11474536\n",
      "Iteration 29, loss = 0.11195385\n",
      "Iteration 30, loss = 0.10933971\n",
      "Iteration 31, loss = 0.10690710\n",
      "Iteration 32, loss = 0.10463122\n",
      "Iteration 33, loss = 0.10247505\n",
      "Iteration 34, loss = 0.10044709\n",
      "Iteration 35, loss = 0.09856322\n",
      "Iteration 36, loss = 0.09677051\n",
      "Iteration 37, loss = 0.09507408\n",
      "Iteration 38, loss = 0.09346877\n",
      "Iteration 39, loss = 0.09194078\n",
      "Iteration 40, loss = 0.09048368\n",
      "Iteration 41, loss = 0.08908484\n",
      "Iteration 42, loss = 0.08776531\n",
      "Iteration 43, loss = 0.08650348\n",
      "Iteration 44, loss = 0.08529947\n",
      "Iteration 45, loss = 0.08415117\n",
      "Iteration 46, loss = 0.08305563\n",
      "Iteration 47, loss = 0.08199548\n",
      "Iteration 48, loss = 0.08098451\n",
      "Iteration 49, loss = 0.08000613\n",
      "Iteration 50, loss = 0.07908422\n",
      "Iteration 51, loss = 0.07818674\n",
      "Iteration 52, loss = 0.07732947\n",
      "Iteration 53, loss = 0.07649473\n",
      "Iteration 54, loss = 0.07569849\n",
      "Iteration 55, loss = 0.07492522\n",
      "Iteration 56, loss = 0.07419187\n",
      "Iteration 57, loss = 0.07347340\n",
      "Iteration 58, loss = 0.07278749\n",
      "Iteration 59, loss = 0.07212267\n",
      "Iteration 60, loss = 0.07147656\n",
      "Iteration 61, loss = 0.07084800\n",
      "Iteration 62, loss = 0.07024692\n",
      "Iteration 63, loss = 0.06967337\n",
      "Iteration 64, loss = 0.06910600\n",
      "Iteration 65, loss = 0.06856925\n",
      "Iteration 66, loss = 0.06803637\n",
      "Iteration 67, loss = 0.06751600\n",
      "Iteration 68, loss = 0.06701263\n",
      "Iteration 69, loss = 0.06652506\n",
      "Iteration 70, loss = 0.06605963\n",
      "Iteration 71, loss = 0.06560152\n",
      "Iteration 72, loss = 0.06515152\n",
      "Iteration 73, loss = 0.06471530\n",
      "Iteration 74, loss = 0.06429601\n",
      "Iteration 75, loss = 0.06388921\n",
      "Iteration 76, loss = 0.06348993\n",
      "Iteration 77, loss = 0.06310303\n",
      "Iteration 78, loss = 0.06271794\n",
      "Iteration 79, loss = 0.06234768\n",
      "Iteration 80, loss = 0.06199100\n",
      "Iteration 81, loss = 0.06164182\n",
      "Iteration 82, loss = 0.06130115\n",
      "Iteration 83, loss = 0.06096267\n",
      "Iteration 84, loss = 0.06064213\n",
      "Iteration 85, loss = 0.06032034\n",
      "Iteration 86, loss = 0.06000700\n",
      "Iteration 87, loss = 0.05970333\n",
      "Iteration 88, loss = 0.05941468\n",
      "Iteration 89, loss = 0.05911744\n",
      "Iteration 90, loss = 0.05883564\n",
      "Iteration 91, loss = 0.05855962\n",
      "Iteration 92, loss = 0.05828570\n",
      "Iteration 93, loss = 0.05801995\n",
      "Iteration 94, loss = 0.05775504\n",
      "Iteration 95, loss = 0.05751062\n",
      "Iteration 96, loss = 0.05725408\n",
      "Iteration 97, loss = 0.05701189\n",
      "Iteration 98, loss = 0.05677930\n",
      "Iteration 99, loss = 0.05654531\n",
      "Iteration 100, loss = 0.05631212\n",
      "Iteration 101, loss = 0.05608535\n",
      "Iteration 102, loss = 0.05586345\n",
      "Iteration 103, loss = 0.05564400\n",
      "Iteration 104, loss = 0.05542987\n",
      "Iteration 105, loss = 0.05522797\n",
      "Iteration 106, loss = 0.05501522\n",
      "Iteration 107, loss = 0.05481273\n",
      "Iteration 108, loss = 0.05461352\n",
      "Iteration 109, loss = 0.05442478\n",
      "Iteration 110, loss = 0.05423604\n",
      "Iteration 111, loss = 0.05404539\n",
      "Iteration 112, loss = 0.05385662\n",
      "Iteration 113, loss = 0.05367798\n",
      "Iteration 114, loss = 0.05349532\n",
      "Iteration 115, loss = 0.05331850\n",
      "Iteration 116, loss = 0.05314536\n",
      "Iteration 117, loss = 0.05297626\n",
      "Iteration 118, loss = 0.05280833\n",
      "Iteration 119, loss = 0.05265066\n",
      "Iteration 120, loss = 0.05248306\n",
      "Iteration 121, loss = 0.05232832\n",
      "Iteration 122, loss = 0.05216560\n",
      "Iteration 123, loss = 0.05201771\n",
      "Iteration 124, loss = 0.05187733\n",
      "Iteration 125, loss = 0.05171494\n",
      "Iteration 126, loss = 0.05157678\n",
      "Iteration 127, loss = 0.05141847\n",
      "Iteration 128, loss = 0.05127587\n",
      "Iteration 129, loss = 0.05113303\n",
      "Iteration 130, loss = 0.05099283\n",
      "Iteration 131, loss = 0.05085640\n",
      "Iteration 132, loss = 0.05071713\n",
      "Iteration 133, loss = 0.05058612\n",
      "Iteration 134, loss = 0.05045626\n",
      "Iteration 135, loss = 0.05032209\n",
      "Iteration 136, loss = 0.05019725\n",
      "Iteration 137, loss = 0.05006827\n",
      "Iteration 138, loss = 0.04995171\n",
      "Iteration 139, loss = 0.04982715\n",
      "Iteration 140, loss = 0.04970357\n",
      "Iteration 141, loss = 0.04959283\n",
      "Iteration 142, loss = 0.04945954\n",
      "Iteration 143, loss = 0.04934084\n",
      "Iteration 144, loss = 0.04923140\n",
      "Iteration 145, loss = 0.04911300\n",
      "Iteration 146, loss = 0.04899812\n",
      "Iteration 147, loss = 0.04888637\n",
      "Iteration 148, loss = 0.04877607\n",
      "Iteration 149, loss = 0.04866694\n",
      "Iteration 150, loss = 0.04855788\n",
      "Iteration 151, loss = 0.04845262\n",
      "Iteration 152, loss = 0.04834692\n",
      "Iteration 153, loss = 0.04824258\n",
      "Iteration 154, loss = 0.04814246\n",
      "Iteration 155, loss = 0.04803846\n",
      "Iteration 156, loss = 0.04793610\n",
      "Iteration 157, loss = 0.04783582\n",
      "Iteration 158, loss = 0.04773838\n",
      "Iteration 159, loss = 0.04764132\n",
      "Iteration 160, loss = 0.04754426\n",
      "Iteration 161, loss = 0.04744616\n",
      "Iteration 162, loss = 0.04735609\n",
      "Iteration 163, loss = 0.04726228\n",
      "Iteration 164, loss = 0.04716455\n",
      "Iteration 165, loss = 0.04707376\n",
      "Iteration 166, loss = 0.04698584\n",
      "Iteration 167, loss = 0.04690110\n",
      "Iteration 168, loss = 0.04680333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78133641\n",
      "Iteration 2, loss = 0.67726132\n",
      "Iteration 3, loss = 0.59465497\n",
      "Iteration 4, loss = 0.52893860\n",
      "Iteration 5, loss = 0.47452634\n",
      "Iteration 6, loss = 0.42844137\n",
      "Iteration 7, loss = 0.38986752\n",
      "Iteration 8, loss = 0.35717947\n",
      "Iteration 9, loss = 0.32946505\n",
      "Iteration 10, loss = 0.30581296\n",
      "Iteration 11, loss = 0.28531112\n",
      "Iteration 12, loss = 0.26750755\n",
      "Iteration 13, loss = 0.25195459\n",
      "Iteration 14, loss = 0.23818873\n",
      "Iteration 15, loss = 0.22597713\n",
      "Iteration 16, loss = 0.21503707\n",
      "Iteration 17, loss = 0.20525363\n",
      "Iteration 18, loss = 0.19637435\n",
      "Iteration 19, loss = 0.18841132\n",
      "Iteration 20, loss = 0.18116374\n",
      "Iteration 21, loss = 0.17453303\n",
      "Iteration 22, loss = 0.16851250\n",
      "Iteration 23, loss = 0.16294832\n",
      "Iteration 24, loss = 0.15781033\n",
      "Iteration 25, loss = 0.15311095\n",
      "Iteration 26, loss = 0.14872046\n",
      "Iteration 27, loss = 0.14468709\n",
      "Iteration 28, loss = 0.14088469\n",
      "Iteration 29, loss = 0.13732878\n",
      "Iteration 30, loss = 0.13398995\n",
      "Iteration 31, loss = 0.13087106\n",
      "Iteration 32, loss = 0.12793658\n",
      "Iteration 33, loss = 0.12517922\n",
      "Iteration 34, loss = 0.12249834\n",
      "Iteration 35, loss = 0.12001433\n",
      "Iteration 36, loss = 0.11766084\n",
      "Iteration 37, loss = 0.11539481\n",
      "Iteration 38, loss = 0.11325950\n",
      "Iteration 39, loss = 0.11120965\n",
      "Iteration 40, loss = 0.10924294\n",
      "Iteration 41, loss = 0.10736986\n",
      "Iteration 42, loss = 0.10558697\n",
      "Iteration 43, loss = 0.10386968\n",
      "Iteration 44, loss = 0.10223242\n",
      "Iteration 45, loss = 0.10062752\n",
      "Iteration 46, loss = 0.09910649\n",
      "Iteration 47, loss = 0.09764744\n",
      "Iteration 48, loss = 0.09624349\n",
      "Iteration 49, loss = 0.09487464\n",
      "Iteration 50, loss = 0.09355998\n",
      "Iteration 51, loss = 0.09230854\n",
      "Iteration 52, loss = 0.09107709\n",
      "Iteration 53, loss = 0.08990780\n",
      "Iteration 54, loss = 0.08878104\n",
      "Iteration 55, loss = 0.08766628\n",
      "Iteration 56, loss = 0.08661890\n",
      "Iteration 57, loss = 0.08558096\n",
      "Iteration 58, loss = 0.08459601\n",
      "Iteration 59, loss = 0.08363252\n",
      "Iteration 60, loss = 0.08269435\n",
      "Iteration 61, loss = 0.08181134\n",
      "Iteration 62, loss = 0.08092416\n",
      "Iteration 63, loss = 0.08006806\n",
      "Iteration 64, loss = 0.07924768\n",
      "Iteration 65, loss = 0.07844824\n",
      "Iteration 66, loss = 0.07768372\n",
      "Iteration 67, loss = 0.07691126\n",
      "Iteration 68, loss = 0.07618313\n",
      "Iteration 69, loss = 0.07547507\n",
      "Iteration 70, loss = 0.07482285\n",
      "Iteration 71, loss = 0.07411463\n",
      "Iteration 72, loss = 0.07346731\n",
      "Iteration 73, loss = 0.07282171\n",
      "Iteration 74, loss = 0.07220282\n",
      "Iteration 75, loss = 0.07159876\n",
      "Iteration 76, loss = 0.07100968\n",
      "Iteration 77, loss = 0.07044446\n",
      "Iteration 78, loss = 0.06988013\n",
      "Iteration 79, loss = 0.06933362\n",
      "Iteration 80, loss = 0.06881088\n",
      "Iteration 81, loss = 0.06829908\n",
      "Iteration 82, loss = 0.06779147\n",
      "Iteration 83, loss = 0.06729778\n",
      "Iteration 84, loss = 0.06683085\n",
      "Iteration 85, loss = 0.06634771\n",
      "Iteration 86, loss = 0.06588568\n",
      "Iteration 87, loss = 0.06543721\n",
      "Iteration 88, loss = 0.06500546\n",
      "Iteration 89, loss = 0.06457418\n",
      "Iteration 90, loss = 0.06415006\n",
      "Iteration 91, loss = 0.06375448\n",
      "Iteration 92, loss = 0.06334182\n",
      "Iteration 93, loss = 0.06295113\n",
      "Iteration 94, loss = 0.06256856\n",
      "Iteration 95, loss = 0.06219200\n",
      "Iteration 96, loss = 0.06183367\n",
      "Iteration 97, loss = 0.06146632\n",
      "Iteration 98, loss = 0.06111200\n",
      "Iteration 99, loss = 0.06077774\n",
      "Iteration 100, loss = 0.06043057\n",
      "Iteration 101, loss = 0.06009738\n",
      "Iteration 102, loss = 0.05977401\n",
      "Iteration 103, loss = 0.05945137\n",
      "Iteration 104, loss = 0.05913814\n",
      "Iteration 105, loss = 0.05883231\n",
      "Iteration 106, loss = 0.05853557\n",
      "Iteration 107, loss = 0.05823051\n",
      "Iteration 108, loss = 0.05794071\n",
      "Iteration 109, loss = 0.05765631\n",
      "Iteration 110, loss = 0.05737537\n",
      "Iteration 111, loss = 0.05709821\n",
      "Iteration 112, loss = 0.05683030\n",
      "Iteration 113, loss = 0.05656599\n",
      "Iteration 114, loss = 0.05630471\n",
      "Iteration 115, loss = 0.05604102\n",
      "Iteration 116, loss = 0.05579302\n",
      "Iteration 117, loss = 0.05554897\n",
      "Iteration 118, loss = 0.05530332\n",
      "Iteration 119, loss = 0.05507182\n",
      "Iteration 120, loss = 0.05483702\n",
      "Iteration 121, loss = 0.05461096\n",
      "Iteration 122, loss = 0.05437525\n",
      "Iteration 123, loss = 0.05415308\n",
      "Iteration 124, loss = 0.05395203\n",
      "Iteration 125, loss = 0.05372193\n",
      "Iteration 126, loss = 0.05350388\n",
      "Iteration 127, loss = 0.05329940\n",
      "Iteration 128, loss = 0.05309641\n",
      "Iteration 129, loss = 0.05289198\n",
      "Iteration 130, loss = 0.05269571\n",
      "Iteration 131, loss = 0.05249965\n",
      "Iteration 132, loss = 0.05230904\n",
      "Iteration 133, loss = 0.05211179\n",
      "Iteration 134, loss = 0.05192954\n",
      "Iteration 135, loss = 0.05174634\n",
      "Iteration 136, loss = 0.05156539\n",
      "Iteration 137, loss = 0.05138913\n",
      "Iteration 138, loss = 0.05122011\n",
      "Iteration 139, loss = 0.05105356\n",
      "Iteration 140, loss = 0.05087209\n",
      "Iteration 141, loss = 0.05070707\n",
      "Iteration 142, loss = 0.05053837\n",
      "Iteration 143, loss = 0.05037644\n",
      "Iteration 144, loss = 0.05021409\n",
      "Iteration 145, loss = 0.05005557\n",
      "Iteration 146, loss = 0.04990183\n",
      "Iteration 147, loss = 0.04974902\n",
      "Iteration 148, loss = 0.04959976\n",
      "Iteration 149, loss = 0.04944284\n",
      "Iteration 150, loss = 0.04930041\n",
      "Iteration 151, loss = 0.04915080\n",
      "Iteration 152, loss = 0.04900870\n",
      "Iteration 153, loss = 0.04886626\n",
      "Iteration 154, loss = 0.04872826\n",
      "Iteration 155, loss = 0.04858978\n",
      "Iteration 156, loss = 0.04845205\n",
      "Iteration 157, loss = 0.04831651\n",
      "Iteration 158, loss = 0.04818534\n",
      "Iteration 159, loss = 0.04805182\n",
      "Iteration 160, loss = 0.04792889\n",
      "Iteration 161, loss = 0.04779432\n",
      "Iteration 162, loss = 0.04767307\n",
      "Iteration 163, loss = 0.04754366\n",
      "Iteration 164, loss = 0.04742424\n",
      "Iteration 165, loss = 0.04730034\n",
      "Iteration 166, loss = 0.04718136\n",
      "Iteration 167, loss = 0.04707481\n",
      "Iteration 168, loss = 0.04694577\n",
      "Iteration 169, loss = 0.04682919\n",
      "Iteration 170, loss = 0.04671835\n",
      "Iteration 171, loss = 0.04660160\n",
      "Iteration 172, loss = 0.04649302\n",
      "Iteration 173, loss = 0.04638191\n",
      "Iteration 174, loss = 0.04627508\n",
      "Iteration 175, loss = 0.04616612\n",
      "Iteration 176, loss = 0.04606318\n",
      "Iteration 177, loss = 0.04595612\n",
      "Iteration 178, loss = 0.04585883\n",
      "Iteration 179, loss = 0.04574989\n",
      "Iteration 180, loss = 0.04564870\n",
      "Iteration 181, loss = 0.04554675\n",
      "Iteration 182, loss = 0.04545096\n",
      "Iteration 183, loss = 0.04535237\n",
      "Iteration 184, loss = 0.04525406\n",
      "Iteration 185, loss = 0.04516368\n",
      "Iteration 186, loss = 0.04506750\n",
      "Iteration 187, loss = 0.04497333\n",
      "Iteration 188, loss = 0.04488094\n",
      "Iteration 189, loss = 0.04478986\n",
      "Iteration 190, loss = 0.04469660\n",
      "Iteration 191, loss = 0.04460850\n",
      "Iteration 192, loss = 0.04452163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78230137\n",
      "Iteration 2, loss = 0.65771579\n",
      "Iteration 3, loss = 0.56130554\n",
      "Iteration 4, loss = 0.48645185\n",
      "Iteration 5, loss = 0.42644366\n",
      "Iteration 6, loss = 0.37745710\n",
      "Iteration 7, loss = 0.33777850\n",
      "Iteration 8, loss = 0.30524531\n",
      "Iteration 9, loss = 0.27838814\n",
      "Iteration 10, loss = 0.25575067\n",
      "Iteration 11, loss = 0.23658475\n",
      "Iteration 12, loss = 0.22009198\n",
      "Iteration 13, loss = 0.20575313\n",
      "Iteration 14, loss = 0.19323796\n",
      "Iteration 15, loss = 0.18220729\n",
      "Iteration 16, loss = 0.17238498\n",
      "Iteration 17, loss = 0.16363616\n",
      "Iteration 18, loss = 0.15576937\n",
      "Iteration 19, loss = 0.14870645\n",
      "Iteration 20, loss = 0.14231879\n",
      "Iteration 21, loss = 0.13649975\n",
      "Iteration 22, loss = 0.13118430\n",
      "Iteration 23, loss = 0.12633910\n",
      "Iteration 24, loss = 0.12191736\n",
      "Iteration 25, loss = 0.11783967\n",
      "Iteration 26, loss = 0.11408478\n",
      "Iteration 27, loss = 0.11058180\n",
      "Iteration 28, loss = 0.10734738\n",
      "Iteration 29, loss = 0.10435081\n",
      "Iteration 30, loss = 0.10152689\n",
      "Iteration 31, loss = 0.09892595\n",
      "Iteration 32, loss = 0.09647760\n",
      "Iteration 33, loss = 0.09417208\n",
      "Iteration 34, loss = 0.09200820\n",
      "Iteration 35, loss = 0.08999584\n",
      "Iteration 36, loss = 0.08807017\n",
      "Iteration 37, loss = 0.08628504\n",
      "Iteration 38, loss = 0.08454705\n",
      "Iteration 39, loss = 0.08293408\n",
      "Iteration 40, loss = 0.08139980\n",
      "Iteration 41, loss = 0.07995365\n",
      "Iteration 42, loss = 0.07857023\n",
      "Iteration 43, loss = 0.07724669\n",
      "Iteration 44, loss = 0.07599568\n",
      "Iteration 45, loss = 0.07480791\n",
      "Iteration 46, loss = 0.07366616\n",
      "Iteration 47, loss = 0.07257935\n",
      "Iteration 48, loss = 0.07153821\n",
      "Iteration 49, loss = 0.07054422\n",
      "Iteration 50, loss = 0.06959594\n",
      "Iteration 51, loss = 0.06868312\n",
      "Iteration 52, loss = 0.06780825\n",
      "Iteration 53, loss = 0.06697146\n",
      "Iteration 54, loss = 0.06616251\n",
      "Iteration 55, loss = 0.06539020\n",
      "Iteration 56, loss = 0.06463758\n",
      "Iteration 57, loss = 0.06391490\n",
      "Iteration 58, loss = 0.06322530\n",
      "Iteration 59, loss = 0.06256783\n",
      "Iteration 60, loss = 0.06191878\n",
      "Iteration 61, loss = 0.06129608\n",
      "Iteration 62, loss = 0.06070263\n",
      "Iteration 63, loss = 0.06011963\n",
      "Iteration 64, loss = 0.05957682\n",
      "Iteration 65, loss = 0.05901683\n",
      "Iteration 66, loss = 0.05849521\n",
      "Iteration 67, loss = 0.05799166\n",
      "Iteration 68, loss = 0.05750245\n",
      "Iteration 69, loss = 0.05702252\n",
      "Iteration 70, loss = 0.05656230\n",
      "Iteration 71, loss = 0.05611476\n",
      "Iteration 72, loss = 0.05568148\n",
      "Iteration 73, loss = 0.05526822\n",
      "Iteration 74, loss = 0.05485374\n",
      "Iteration 75, loss = 0.05445448\n",
      "Iteration 76, loss = 0.05407327\n",
      "Iteration 77, loss = 0.05370270\n",
      "Iteration 78, loss = 0.05333644\n",
      "Iteration 79, loss = 0.05298070\n",
      "Iteration 80, loss = 0.05263614\n",
      "Iteration 81, loss = 0.05230999\n",
      "Iteration 82, loss = 0.05198145\n",
      "Iteration 83, loss = 0.05165859\n",
      "Iteration 84, loss = 0.05135280\n",
      "Iteration 85, loss = 0.05104983\n",
      "Iteration 86, loss = 0.05075997\n",
      "Iteration 87, loss = 0.05047409\n",
      "Iteration 88, loss = 0.05019729\n",
      "Iteration 89, loss = 0.04992430\n",
      "Iteration 90, loss = 0.04966125\n",
      "Iteration 91, loss = 0.04939992\n",
      "Iteration 92, loss = 0.04915371\n",
      "Iteration 93, loss = 0.04890468\n",
      "Iteration 94, loss = 0.04866225\n",
      "Iteration 95, loss = 0.04842977\n",
      "Iteration 96, loss = 0.04820294\n",
      "Iteration 97, loss = 0.04798821\n",
      "Iteration 98, loss = 0.04775926\n",
      "Iteration 99, loss = 0.04754994\n",
      "Iteration 100, loss = 0.04734234\n",
      "Iteration 101, loss = 0.04713926\n",
      "Iteration 102, loss = 0.04693454\n",
      "Iteration 103, loss = 0.04674032\n",
      "Iteration 104, loss = 0.04654828\n",
      "Iteration 105, loss = 0.04636219\n",
      "Iteration 106, loss = 0.04618253\n",
      "Iteration 107, loss = 0.04600124\n",
      "Iteration 108, loss = 0.04582664\n",
      "Iteration 109, loss = 0.04565582\n",
      "Iteration 110, loss = 0.04548863\n",
      "Iteration 111, loss = 0.04532268\n",
      "Iteration 112, loss = 0.04516666\n",
      "Iteration 113, loss = 0.04500520\n",
      "Iteration 114, loss = 0.04485156\n",
      "Iteration 115, loss = 0.04470116\n",
      "Iteration 116, loss = 0.04455243\n",
      "Iteration 117, loss = 0.04440649\n",
      "Iteration 118, loss = 0.04426395\n",
      "Iteration 119, loss = 0.04412323\n",
      "Iteration 120, loss = 0.04398790\n",
      "Iteration 121, loss = 0.04384994\n",
      "Iteration 122, loss = 0.04371747\n",
      "Iteration 123, loss = 0.04358580\n",
      "Iteration 124, loss = 0.04345888\n",
      "Iteration 125, loss = 0.04333313\n",
      "Iteration 126, loss = 0.04320860\n",
      "Iteration 127, loss = 0.04308670\n",
      "Iteration 128, loss = 0.04296950\n",
      "Iteration 129, loss = 0.04284951\n",
      "Iteration 130, loss = 0.04273357\n",
      "Iteration 131, loss = 0.04261926\n",
      "Iteration 132, loss = 0.04251128\n",
      "Iteration 133, loss = 0.04239930\n",
      "Iteration 134, loss = 0.04229300\n",
      "Iteration 135, loss = 0.04218930\n",
      "Iteration 136, loss = 0.04208659\n",
      "Iteration 137, loss = 0.04198418\n",
      "Iteration 138, loss = 0.04188130\n",
      "Iteration 139, loss = 0.04178434\n",
      "Iteration 140, loss = 0.04168714\n",
      "Iteration 141, loss = 0.04159035\n",
      "Iteration 142, loss = 0.04149700\n",
      "Iteration 143, loss = 0.04140452\n",
      "Iteration 144, loss = 0.04131397\n",
      "Iteration 145, loss = 0.04122602\n",
      "Iteration 146, loss = 0.04113552\n",
      "Iteration 147, loss = 0.04104995\n",
      "Iteration 148, loss = 0.04096394\n",
      "Iteration 149, loss = 0.04088065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(alpha=0.1, hidden_layer_sizes=60,\n",
       "                                     learning_rate_init=0.1, max_iter=1000,\n",
       "                                     random_state=1, solver='lbfgs',\n",
       "                                     verbose=10),\n",
       "             param_grid={'hidden_layer_sizes': [20, 40, 60, 80, 100],\n",
       "                         'learning_rate_init': (0.1, 0.01, 0.001),\n",
       "                         'solver': ['adam', 'lbfgs', 'sgd']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokladnosc dla danych treningowych: 100.0%\n",
      "Dokladnosc dla danych testowych: 100.0%\n",
      "Najlepsze parametry:  {'hidden_layer_sizes': 60, 'learning_rate_init': 0.1, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dokladnosc dla GridSearcha: \" + str(clf.score(x, y)*100) + \"%\")\n",
    "print(\"Najlepsze parametry:  \" + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron wielowarstwowy dla najlepszych parametrow\n",
    "mlp2 = MLPClassifier(alpha=0.1, hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'], \n",
    "                    learning_rate_init=clf.best_params_['learning_rate_init'],\n",
    "                    max_iter=1000, random_state=1, solver=clf.best_params_['solver'])\n",
    "mlp2.out_activation = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening Perceptrona z najlepszymi wartosciami\n",
    "mlp2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trening\n",
    "mlp2.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokladnosc dla danych treningowych: 100.0%\n",
      "Dokladnosc dla danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Dokladnosc dla danych treningowych: \" + str(mlp2.score(x_train, y_train)*100) + \"%\")\n",
    "print(\"Dokladnosc dla danych testowych: \" + str(mlp2.score(x_test, y_test)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'loss_curve_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0bb0fe6fdeeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Wykres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_curve_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mx_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlp2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_curve_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'loss_curve_'"
     ]
    }
   ],
   "source": [
    "# Wykres\n",
    "x_label = []\n",
    "for x in range(len(mlp2.loss_curve_)):\n",
    "    x_label.append(x)\n",
    "plt.plot(x_label, mlp2.loss_curve_)\n",
    "plt.grid(which='both')\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Numerical value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       852\n",
      "           1       1.00      1.00      1.00       773\n",
      "\n",
      "    accuracy                           1.00      1625\n",
      "   macro avg       1.00      1.00      1.00      1625\n",
      "weighted avg       1.00      1.00      1.00      1625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Raport klasyfikacji\n",
    "y_true, y_pred = y_test, clf.predict(x_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
