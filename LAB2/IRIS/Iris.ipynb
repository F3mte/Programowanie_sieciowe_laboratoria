{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "seed = np.random.seed\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal length</th>\n",
       "      <th>Sepal width</th>\n",
       "      <th>Petal length</th>\n",
       "      <th>Petal width</th>\n",
       "      <th>Class label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal length  Sepal width  Petal length  Petal width     Class label\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wczytanie bazy danych kwiatów\n",
    "df = pd.read_csv('iris.data', header = None)\n",
    "df.columns = ['Sepal length' , 'Sepal width', 'Petal length', 'Petal width', 'Class label']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stworzenie listy dwuwymiarowej z wartosci wejsciowymi dla kazdej klasy\n",
    "x = df[['Sepal length' , 'Sepal width', 'Petal length', 'Petal width']].values\n",
    "display(x)\n",
    "# Zmiana nazw klas na liczby z przedziału 0-2\n",
    "y = pd.factorize(df['Class label'])[0]\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zmiana kształtu na liste dwuwymiarowa, ksztalt jest taki, wymagany przez funkcje fit\n",
    "yy = y.reshape(-1, 1)\n",
    "display(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zakodowanie numerow klas wedlug goracej jedynki \n",
    "enc.fit(yy)\n",
    "yy_encoded = enc.transform(yy).toarray()\n",
    "display(yy_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4, 3.1, 5.5, 1.8],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [4.6, 3.2, 1.4, 0.2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Podzial na wartosci do trenowania i testowania\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, yy_encoded, test_size = 0.2, random_state = 0)\n",
    "display(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron wielowarstwowy\n",
    "mlp = MLPClassifier(alpha=0.1, hidden_layer_sizes=40, learning_rate_init=0.01,\n",
    "              max_iter=1000, random_state=1, solver='sgd', verbose=10)\n",
    "mlp.out_activation = 'softmax'# jakos wplywa na wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adam', 'lbfgs', 'sgd']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parametramy ktore posluza nam do znalezienia najlepszego klasyfikatora\n",
    "parametry = {'learning_rate_init' : (0.1, 0.01, 0.001), 'hidden_layer_sizes' : [20, 40, 60, 80, 100], \n",
    "            'solver' : ['adam', 'lbfgs', 'sgd']}\n",
    "# Siatka do przeszukiwania podanych parametrow\n",
    "clf = GridSearchCV(mlp, parametry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.63603705\n",
      "Iteration 3, loss = 1.93882652\n",
      "Iteration 4, loss = 1.26094722\n",
      "Iteration 5, loss = 0.70787225\n",
      "Iteration 6, loss = 0.88739333\n",
      "Iteration 7, loss = 0.76889081\n",
      "Iteration 8, loss = 0.55110124\n",
      "Iteration 9, loss = 0.60850024\n",
      "Iteration 10, loss = 0.50377267\n",
      "Iteration 11, loss = 0.54514560\n",
      "Iteration 12, loss = 0.45028719\n",
      "Iteration 13, loss = 0.41710675\n",
      "Iteration 14, loss = 0.42833162\n",
      "Iteration 15, loss = 0.39651726\n",
      "Iteration 16, loss = 0.36351265\n",
      "Iteration 17, loss = 0.33624602\n",
      "Iteration 18, loss = 0.30414803\n",
      "Iteration 19, loss = 0.29021965\n",
      "Iteration 20, loss = 0.28394947\n",
      "Iteration 21, loss = 0.26640358\n",
      "Iteration 22, loss = 0.25626798\n",
      "Iteration 23, loss = 0.24288436\n",
      "Iteration 24, loss = 0.22333325\n",
      "Iteration 25, loss = 0.21934740\n",
      "Iteration 26, loss = 0.19578285\n",
      "Iteration 27, loss = 0.19289828\n",
      "Iteration 28, loss = 0.17518830\n",
      "Iteration 29, loss = 0.17052570\n",
      "Iteration 30, loss = 0.16073473\n",
      "Iteration 31, loss = 0.15252600\n",
      "Iteration 32, loss = 0.14918515\n",
      "Iteration 33, loss = 0.13924380\n",
      "Iteration 34, loss = 0.13815875\n",
      "Iteration 35, loss = 0.13010022\n",
      "Iteration 36, loss = 0.12820384\n",
      "Iteration 37, loss = 0.12322578\n",
      "Iteration 38, loss = 0.11973222\n",
      "Iteration 39, loss = 0.11765225\n",
      "Iteration 40, loss = 0.11315392\n",
      "Iteration 41, loss = 0.11264788\n",
      "Iteration 42, loss = 0.10850581\n",
      "Iteration 43, loss = 0.10822259\n",
      "Iteration 44, loss = 0.10518424\n",
      "Iteration 45, loss = 0.10450215\n",
      "Iteration 46, loss = 0.10257329\n",
      "Iteration 47, loss = 0.10152370\n",
      "Iteration 48, loss = 0.10047888\n",
      "Iteration 49, loss = 0.09908380\n",
      "Iteration 50, loss = 0.09858135\n",
      "Iteration 51, loss = 0.09714209\n",
      "Iteration 52, loss = 0.09684855\n",
      "Iteration 53, loss = 0.09545874\n",
      "Iteration 54, loss = 0.09527730\n",
      "Iteration 55, loss = 0.09401489\n",
      "Iteration 56, loss = 0.09382281\n",
      "Iteration 57, loss = 0.09272938\n",
      "Iteration 58, loss = 0.09254772\n",
      "Iteration 59, loss = 0.09159223\n",
      "Iteration 60, loss = 0.09139389\n",
      "Iteration 61, loss = 0.09058171\n",
      "Iteration 62, loss = 0.09039194\n",
      "Iteration 63, loss = 0.08967984\n",
      "Iteration 64, loss = 0.08948359\n",
      "Iteration 65, loss = 0.08886757\n",
      "Iteration 66, loss = 0.08867306\n",
      "Iteration 67, loss = 0.08812773\n",
      "Iteration 68, loss = 0.08791834\n",
      "Iteration 69, loss = 0.08744364\n",
      "Iteration 70, loss = 0.08722007\n",
      "Iteration 71, loss = 0.08681076\n",
      "Iteration 72, loss = 0.08656105\n",
      "Iteration 73, loss = 0.08621700\n",
      "Iteration 74, loss = 0.08594342\n",
      "Iteration 75, loss = 0.08566247\n",
      "Iteration 76, loss = 0.08536871\n",
      "Iteration 77, loss = 0.08513793\n",
      "Iteration 78, loss = 0.08483630\n",
      "Iteration 79, loss = 0.08463785\n",
      "Iteration 80, loss = 0.08434866\n",
      "Iteration 81, loss = 0.08415995\n",
      "Iteration 82, loss = 0.08390068\n",
      "Iteration 83, loss = 0.08370167\n",
      "Iteration 84, loss = 0.08348267\n",
      "Iteration 85, loss = 0.08326819\n",
      "Iteration 86, loss = 0.08308319\n",
      "Iteration 87, loss = 0.08286352\n",
      "Iteration 88, loss = 0.08269347\n",
      "Iteration 89, loss = 0.08248688\n",
      "Iteration 90, loss = 0.08231226\n",
      "Iteration 91, loss = 0.08213091\n",
      "Iteration 92, loss = 0.08194665\n",
      "Iteration 93, loss = 0.08178523\n",
      "Iteration 94, loss = 0.08160286\n",
      "Iteration 95, loss = 0.08144544\n",
      "Iteration 96, loss = 0.08127847\n",
      "Iteration 97, loss = 0.08111531\n",
      "Iteration 98, loss = 0.08096443\n",
      "Iteration 99, loss = 0.08080184\n",
      "Iteration 100, loss = 0.08065487\n",
      "Iteration 101, loss = 0.08050402\n",
      "Iteration 102, loss = 0.08035394\n",
      "Iteration 103, loss = 0.08021393\n",
      "Iteration 104, loss = 0.08006746\n",
      "Iteration 105, loss = 0.07992888\n",
      "Iteration 106, loss = 0.07979194\n",
      "Iteration 107, loss = 0.07965242\n",
      "Iteration 108, loss = 0.07952068\n",
      "Iteration 109, loss = 0.07938699\n",
      "Iteration 110, loss = 0.07925460\n",
      "Iteration 111, loss = 0.07912749\n",
      "Iteration 112, loss = 0.07899817\n",
      "Iteration 113, loss = 0.07887184\n",
      "Iteration 114, loss = 0.07874869\n",
      "Iteration 115, loss = 0.07862415\n",
      "Iteration 116, loss = 0.07850282\n",
      "Iteration 117, loss = 0.07838355\n",
      "Iteration 118, loss = 0.07826365\n",
      "Iteration 119, loss = 0.07814660\n",
      "Iteration 120, loss = 0.07803124\n",
      "Iteration 121, loss = 0.07791576\n",
      "Iteration 122, loss = 0.07780257\n",
      "Iteration 123, loss = 0.07769098\n",
      "Iteration 124, loss = 0.07757954\n",
      "Iteration 125, loss = 0.07746989\n",
      "Iteration 126, loss = 0.07736190\n",
      "Iteration 127, loss = 0.07725425\n",
      "Iteration 128, loss = 0.07714788\n",
      "Iteration 129, loss = 0.07704321\n",
      "Iteration 130, loss = 0.07693916\n",
      "Iteration 131, loss = 0.07683599\n",
      "Iteration 132, loss = 0.07673435\n",
      "Iteration 133, loss = 0.07663367\n",
      "Iteration 134, loss = 0.07653371\n",
      "Iteration 135, loss = 0.07643497\n",
      "Iteration 136, loss = 0.07633744\n",
      "Iteration 137, loss = 0.07624067\n",
      "Iteration 138, loss = 0.07614470\n",
      "Iteration 139, loss = 0.07604986\n",
      "Iteration 140, loss = 0.07595600\n",
      "Iteration 141, loss = 0.07586285\n",
      "Iteration 142, loss = 0.07577056\n",
      "Iteration 143, loss = 0.07567930\n",
      "Iteration 144, loss = 0.07558891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.64520827\n",
      "Iteration 3, loss = 1.95495568\n",
      "Iteration 4, loss = 1.32473910\n",
      "Iteration 5, loss = 0.70812378\n",
      "Iteration 6, loss = 0.93758302\n",
      "Iteration 7, loss = 0.77334959\n",
      "Iteration 8, loss = 0.55812748\n",
      "Iteration 9, loss = 0.59926122\n",
      "Iteration 10, loss = 0.49833634\n",
      "Iteration 11, loss = 0.54375739\n",
      "Iteration 12, loss = 0.44792328\n",
      "Iteration 13, loss = 0.40432591\n",
      "Iteration 14, loss = 0.41576066\n",
      "Iteration 15, loss = 0.39230111\n",
      "Iteration 16, loss = 0.35765759\n",
      "Iteration 17, loss = 0.32419809\n",
      "Iteration 18, loss = 0.29261385\n",
      "Iteration 19, loss = 0.27567179\n",
      "Iteration 20, loss = 0.26815267\n",
      "Iteration 21, loss = 0.25609971\n",
      "Iteration 22, loss = 0.24498696\n",
      "Iteration 23, loss = 0.23020261\n",
      "Iteration 24, loss = 0.21161994\n",
      "Iteration 25, loss = 0.20353204\n",
      "Iteration 26, loss = 0.18400852\n",
      "Iteration 27, loss = 0.17838641\n",
      "Iteration 28, loss = 0.16136661\n",
      "Iteration 29, loss = 0.15870733\n",
      "Iteration 30, loss = 0.14506915\n",
      "Iteration 31, loss = 0.14352940\n",
      "Iteration 32, loss = 0.13218521\n",
      "Iteration 33, loss = 0.13068117\n",
      "Iteration 34, loss = 0.12189161\n",
      "Iteration 35, loss = 0.12048101\n",
      "Iteration 36, loss = 0.11401238\n",
      "Iteration 37, loss = 0.11207184\n",
      "Iteration 38, loss = 0.10767458\n",
      "Iteration 39, loss = 0.10583667\n",
      "Iteration 40, loss = 0.10190422\n",
      "Iteration 41, loss = 0.10069379\n",
      "Iteration 42, loss = 0.09770880\n",
      "Iteration 43, loss = 0.09682165\n",
      "Iteration 44, loss = 0.09416945\n",
      "Iteration 45, loss = 0.09385923\n",
      "Iteration 46, loss = 0.09140505\n",
      "Iteration 47, loss = 0.09140818\n",
      "Iteration 48, loss = 0.08950815\n",
      "Iteration 49, loss = 0.08891861\n",
      "Iteration 50, loss = 0.08837523\n",
      "Iteration 51, loss = 0.08679545\n",
      "Iteration 52, loss = 0.08679365\n",
      "Iteration 53, loss = 0.08606718\n",
      "Iteration 54, loss = 0.08487322\n",
      "Iteration 55, loss = 0.08473698\n",
      "Iteration 56, loss = 0.08453309\n",
      "Iteration 57, loss = 0.08366868\n",
      "Iteration 58, loss = 0.08285572\n",
      "Iteration 59, loss = 0.08268054\n",
      "Iteration 60, loss = 0.08282334\n",
      "Iteration 61, loss = 0.08272090\n",
      "Iteration 62, loss = 0.08245120\n",
      "Iteration 63, loss = 0.08168284\n",
      "Iteration 64, loss = 0.08097641\n",
      "Iteration 65, loss = 0.08042664\n",
      "Iteration 66, loss = 0.08010920\n",
      "Iteration 67, loss = 0.07998214\n",
      "Iteration 68, loss = 0.08013598\n",
      "Iteration 69, loss = 0.08121261\n",
      "Iteration 70, loss = 0.08351335\n",
      "Iteration 71, loss = 0.08780634\n",
      "Iteration 72, loss = 0.08215477\n",
      "Iteration 73, loss = 0.07855064\n",
      "Iteration 74, loss = 0.08108866\n",
      "Iteration 75, loss = 0.08159062\n",
      "Iteration 76, loss = 0.07904603\n",
      "Iteration 77, loss = 0.07818868\n",
      "Iteration 78, loss = 0.08021292\n",
      "Iteration 79, loss = 0.07998548\n",
      "Iteration 80, loss = 0.07746552\n",
      "Iteration 81, loss = 0.07889095\n",
      "Iteration 82, loss = 0.08010908\n",
      "Iteration 83, loss = 0.07730489\n",
      "Iteration 84, loss = 0.07772637\n",
      "Iteration 85, loss = 0.07934949\n",
      "Iteration 86, loss = 0.07716264\n",
      "Iteration 87, loss = 0.07675488\n",
      "Iteration 88, loss = 0.07809074\n",
      "Iteration 89, loss = 0.07694547\n",
      "Iteration 90, loss = 0.07605593\n",
      "Iteration 91, loss = 0.07672407\n",
      "Iteration 92, loss = 0.07661317\n",
      "Iteration 93, loss = 0.07580561\n",
      "Iteration 94, loss = 0.07564869\n",
      "Iteration 95, loss = 0.07603164\n",
      "Iteration 96, loss = 0.07589873\n",
      "Iteration 97, loss = 0.07521492\n",
      "Iteration 98, loss = 0.07517000\n",
      "Iteration 99, loss = 0.07551071\n",
      "Iteration 100, loss = 0.07526035\n",
      "Iteration 101, loss = 0.07477580\n",
      "Iteration 102, loss = 0.07455375\n",
      "Iteration 103, loss = 0.07469218\n",
      "Iteration 104, loss = 0.07482454\n",
      "Iteration 105, loss = 0.07456566\n",
      "Iteration 106, loss = 0.07420280\n",
      "Iteration 107, loss = 0.07395677\n",
      "Iteration 108, loss = 0.07390494\n",
      "Iteration 109, loss = 0.07396044\n",
      "Iteration 110, loss = 0.07398565\n",
      "Iteration 111, loss = 0.07396224\n",
      "Iteration 112, loss = 0.07380225\n",
      "Iteration 113, loss = 0.07363824\n",
      "Iteration 114, loss = 0.07343786\n",
      "Iteration 115, loss = 0.07328185\n",
      "Iteration 116, loss = 0.07314302\n",
      "Iteration 117, loss = 0.07304971\n",
      "Iteration 118, loss = 0.07298777\n",
      "Iteration 119, loss = 0.07301494\n",
      "Iteration 120, loss = 0.07316171\n",
      "Iteration 121, loss = 0.07370828\n",
      "Iteration 122, loss = 0.07471330\n",
      "Iteration 123, loss = 0.07763663\n",
      "Iteration 124, loss = 0.07960031\n",
      "Iteration 125, loss = 0.08373473\n",
      "Iteration 126, loss = 0.07693610\n",
      "Iteration 127, loss = 0.07251432\n",
      "Iteration 128, loss = 0.07280613\n",
      "Iteration 129, loss = 0.07580790\n",
      "Iteration 130, loss = 0.07750949\n",
      "Iteration 131, loss = 0.07309916\n",
      "Iteration 132, loss = 0.07193160\n",
      "Iteration 133, loss = 0.07439290\n",
      "Iteration 134, loss = 0.07433965\n",
      "Iteration 135, loss = 0.07250259\n",
      "Iteration 136, loss = 0.07152190\n",
      "Iteration 137, loss = 0.07284763\n",
      "Iteration 138, loss = 0.07374380\n",
      "Iteration 139, loss = 0.07197213\n",
      "Iteration 140, loss = 0.07129048\n",
      "Iteration 141, loss = 0.07227914\n",
      "Iteration 142, loss = 0.07244157\n",
      "Iteration 143, loss = 0.07161308\n",
      "Iteration 144, loss = 0.07099635\n",
      "Iteration 145, loss = 0.07146998\n",
      "Iteration 146, loss = 0.07197443\n",
      "Iteration 147, loss = 0.07137424\n",
      "Iteration 148, loss = 0.07078616\n",
      "Iteration 149, loss = 0.07087719\n",
      "Iteration 150, loss = 0.07123177\n",
      "Iteration 151, loss = 0.07124433\n",
      "Iteration 152, loss = 0.07075640\n",
      "Iteration 153, loss = 0.07046377\n",
      "Iteration 154, loss = 0.07058467\n",
      "Iteration 155, loss = 0.07077919\n",
      "Iteration 156, loss = 0.07078226\n",
      "Iteration 157, loss = 0.07047150\n",
      "Iteration 158, loss = 0.07020879\n",
      "Iteration 159, loss = 0.07015796\n",
      "Iteration 160, loss = 0.07026199\n",
      "Iteration 161, loss = 0.07036351\n",
      "Iteration 162, loss = 0.07030715\n",
      "Iteration 163, loss = 0.07017697\n",
      "Iteration 164, loss = 0.06998252\n",
      "Iteration 165, loss = 0.06983721\n",
      "Iteration 166, loss = 0.06976501\n",
      "Iteration 167, loss = 0.06975735\n",
      "Iteration 168, loss = 0.06978933\n",
      "Iteration 169, loss = 0.06982928\n",
      "Iteration 170, loss = 0.06990503\n",
      "Iteration 171, loss = 0.06996696\n",
      "Iteration 172, loss = 0.07015201\n",
      "Iteration 173, loss = 0.07034331\n",
      "Iteration 174, loss = 0.07086694\n",
      "Iteration 175, loss = 0.07140698\n",
      "Iteration 176, loss = 0.07284579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.62579625\n",
      "Iteration 3, loss = 1.92004172\n",
      "Iteration 4, loss = 1.24625045\n",
      "Iteration 5, loss = 0.69858285\n",
      "Iteration 6, loss = 0.88047414\n",
      "Iteration 7, loss = 0.75578185\n",
      "Iteration 8, loss = 0.54001266\n",
      "Iteration 9, loss = 0.59446964\n",
      "Iteration 10, loss = 0.49030851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.52949937\n",
      "Iteration 12, loss = 0.43364632\n",
      "Iteration 13, loss = 0.39843760\n",
      "Iteration 14, loss = 0.40669817\n",
      "Iteration 15, loss = 0.37244418\n",
      "Iteration 16, loss = 0.33878240\n",
      "Iteration 17, loss = 0.30972273\n",
      "Iteration 18, loss = 0.27553319\n",
      "Iteration 19, loss = 0.25995758\n",
      "Iteration 20, loss = 0.25024366\n",
      "Iteration 21, loss = 0.23115848\n",
      "Iteration 22, loss = 0.22162107\n",
      "Iteration 23, loss = 0.20771854\n",
      "Iteration 24, loss = 0.18972032\n",
      "Iteration 25, loss = 0.17952391\n",
      "Iteration 26, loss = 0.16575608\n",
      "Iteration 27, loss = 0.15765176\n",
      "Iteration 28, loss = 0.14524776\n",
      "Iteration 29, loss = 0.13624340\n",
      "Iteration 30, loss = 0.12639888\n",
      "Iteration 31, loss = 0.12046397\n",
      "Iteration 32, loss = 0.11447235\n",
      "Iteration 33, loss = 0.10680675\n",
      "Iteration 34, loss = 0.10480696\n",
      "Iteration 35, loss = 0.09673563\n",
      "Iteration 36, loss = 0.09672952\n",
      "Iteration 37, loss = 0.08995512\n",
      "Iteration 38, loss = 0.08926283\n",
      "Iteration 39, loss = 0.08573684\n",
      "Iteration 40, loss = 0.08309268\n",
      "Iteration 41, loss = 0.08244855\n",
      "Iteration 42, loss = 0.07885152\n",
      "Iteration 43, loss = 0.07904344\n",
      "Iteration 44, loss = 0.07628242\n",
      "Iteration 45, loss = 0.07568275\n",
      "Iteration 46, loss = 0.07442143\n",
      "Iteration 47, loss = 0.07292394\n",
      "Iteration 48, loss = 0.07272481\n",
      "Iteration 49, loss = 0.07083939\n",
      "Iteration 50, loss = 0.07094611\n",
      "Iteration 51, loss = 0.06928453\n",
      "Iteration 52, loss = 0.06925679\n",
      "Iteration 53, loss = 0.06801909\n",
      "Iteration 54, loss = 0.06774312\n",
      "Iteration 55, loss = 0.06682265\n",
      "Iteration 56, loss = 0.06647061\n",
      "Iteration 57, loss = 0.06574690\n",
      "Iteration 58, loss = 0.06536511\n",
      "Iteration 59, loss = 0.06470150\n",
      "Iteration 60, loss = 0.06442845\n",
      "Iteration 61, loss = 0.06375912\n",
      "Iteration 62, loss = 0.06359471\n",
      "Iteration 63, loss = 0.06288652\n",
      "Iteration 64, loss = 0.06279286\n",
      "Iteration 65, loss = 0.06214252\n",
      "Iteration 66, loss = 0.06200123\n",
      "Iteration 67, loss = 0.06154122\n",
      "Iteration 68, loss = 0.06118242\n",
      "Iteration 69, loss = 0.06099539\n",
      "Iteration 70, loss = 0.06049015\n",
      "Iteration 71, loss = 0.06033218\n",
      "Iteration 72, loss = 0.06001220\n",
      "Iteration 73, loss = 0.05965185\n",
      "Iteration 74, loss = 0.05951344\n",
      "Iteration 75, loss = 0.05919864\n",
      "Iteration 76, loss = 0.05891311\n",
      "Iteration 77, loss = 0.05877599\n",
      "Iteration 78, loss = 0.05853068\n",
      "Iteration 79, loss = 0.05825802\n",
      "Iteration 80, loss = 0.05810584\n",
      "Iteration 81, loss = 0.05795354\n",
      "Iteration 82, loss = 0.05773382\n",
      "Iteration 83, loss = 0.05753230\n",
      "Iteration 84, loss = 0.05739045\n",
      "Iteration 85, loss = 0.05726792\n",
      "Iteration 86, loss = 0.05712155\n",
      "Iteration 87, loss = 0.05695480\n",
      "Iteration 88, loss = 0.05679783\n",
      "Iteration 89, loss = 0.05665539\n",
      "Iteration 90, loss = 0.05652690\n",
      "Iteration 91, loss = 0.05647211\n",
      "Iteration 92, loss = 0.05702420\n",
      "Iteration 93, loss = 0.05879010\n",
      "Iteration 94, loss = 0.06126124\n",
      "Iteration 95, loss = 0.05811466\n",
      "Iteration 96, loss = 0.05586124\n",
      "Iteration 97, loss = 0.05756367\n",
      "Iteration 98, loss = 0.05696762\n",
      "Iteration 99, loss = 0.05562119\n",
      "Iteration 100, loss = 0.05666633\n",
      "Iteration 101, loss = 0.05605012\n",
      "Iteration 102, loss = 0.05539899\n",
      "Iteration 103, loss = 0.05607066\n",
      "Iteration 104, loss = 0.05539568\n",
      "Iteration 105, loss = 0.05519307\n",
      "Iteration 106, loss = 0.05557069\n",
      "Iteration 107, loss = 0.05494560\n",
      "Iteration 108, loss = 0.05497396\n",
      "Iteration 109, loss = 0.05514295\n",
      "Iteration 110, loss = 0.05462096\n",
      "Iteration 111, loss = 0.05473311\n",
      "Iteration 112, loss = 0.05479869\n",
      "Iteration 113, loss = 0.05436167\n",
      "Iteration 114, loss = 0.05446185\n",
      "Iteration 115, loss = 0.05450280\n",
      "Iteration 116, loss = 0.05413680\n",
      "Iteration 117, loss = 0.05415655\n",
      "Iteration 118, loss = 0.05420690\n",
      "Iteration 119, loss = 0.05392052\n",
      "Iteration 120, loss = 0.05386290\n",
      "Iteration 121, loss = 0.05392349\n",
      "Iteration 122, loss = 0.05371043\n",
      "Iteration 123, loss = 0.05359217\n",
      "Iteration 124, loss = 0.05363665\n",
      "Iteration 125, loss = 0.05351688\n",
      "Iteration 126, loss = 0.05335691\n",
      "Iteration 127, loss = 0.05334358\n",
      "Iteration 128, loss = 0.05331445\n",
      "Iteration 129, loss = 0.05318097\n",
      "Iteration 130, loss = 0.05308127\n",
      "Iteration 131, loss = 0.05306627\n",
      "Iteration 132, loss = 0.05301975\n",
      "Iteration 133, loss = 0.05290358\n",
      "Iteration 134, loss = 0.05281414\n",
      "Iteration 135, loss = 0.05278136\n",
      "Iteration 136, loss = 0.05275251\n",
      "Iteration 137, loss = 0.05267507\n",
      "Iteration 138, loss = 0.05257549\n",
      "Iteration 139, loss = 0.05249424\n",
      "Iteration 140, loss = 0.05244699\n",
      "Iteration 141, loss = 0.05241906\n",
      "Iteration 142, loss = 0.05237727\n",
      "Iteration 143, loss = 0.05231194\n",
      "Iteration 144, loss = 0.05223068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.63726219\n",
      "Iteration 3, loss = 1.93757501\n",
      "Iteration 4, loss = 1.24883043\n",
      "Iteration 5, loss = 0.70228831\n",
      "Iteration 6, loss = 0.88977336\n",
      "Iteration 7, loss = 0.76460150\n",
      "Iteration 8, loss = 0.54573048\n",
      "Iteration 9, loss = 0.60116468\n",
      "Iteration 10, loss = 0.49300700\n",
      "Iteration 11, loss = 0.53371396\n",
      "Iteration 12, loss = 0.43936585\n",
      "Iteration 13, loss = 0.40197016\n",
      "Iteration 14, loss = 0.41367249\n",
      "Iteration 15, loss = 0.37866755\n",
      "Iteration 16, loss = 0.34021342\n",
      "Iteration 17, loss = 0.31278669\n",
      "Iteration 18, loss = 0.28081768\n",
      "Iteration 19, loss = 0.26476012\n",
      "Iteration 20, loss = 0.25796202\n",
      "Iteration 21, loss = 0.23962728\n",
      "Iteration 22, loss = 0.22721135\n",
      "Iteration 23, loss = 0.21443018\n",
      "Iteration 24, loss = 0.19393056\n",
      "Iteration 25, loss = 0.18377139\n",
      "Iteration 26, loss = 0.16586470\n",
      "Iteration 27, loss = 0.15575878\n",
      "Iteration 28, loss = 0.14565915\n",
      "Iteration 29, loss = 0.13413015\n",
      "Iteration 30, loss = 0.12870006\n",
      "Iteration 31, loss = 0.11933528\n",
      "Iteration 32, loss = 0.11451265\n",
      "Iteration 33, loss = 0.10888979\n",
      "Iteration 34, loss = 0.10288173\n",
      "Iteration 35, loss = 0.09992949\n",
      "Iteration 36, loss = 0.09447641\n",
      "Iteration 37, loss = 0.09170405\n",
      "Iteration 38, loss = 0.08825575\n",
      "Iteration 39, loss = 0.08472749\n",
      "Iteration 40, loss = 0.08290262\n",
      "Iteration 41, loss = 0.07964068\n",
      "Iteration 42, loss = 0.07798145\n",
      "Iteration 43, loss = 0.07598490\n",
      "Iteration 44, loss = 0.07393996\n",
      "Iteration 45, loss = 0.07292265\n",
      "Iteration 46, loss = 0.07104307\n",
      "Iteration 47, loss = 0.07010879\n",
      "Iteration 48, loss = 0.06893626\n",
      "Iteration 49, loss = 0.06773296\n",
      "Iteration 50, loss = 0.06708811\n",
      "Iteration 51, loss = 0.06589786\n",
      "Iteration 52, loss = 0.06528567\n",
      "Iteration 53, loss = 0.06442876\n",
      "Iteration 54, loss = 0.06362525\n",
      "Iteration 55, loss = 0.06307405\n",
      "Iteration 56, loss = 0.06221979\n",
      "Iteration 57, loss = 0.06174734\n",
      "Iteration 58, loss = 0.06105005\n",
      "Iteration 59, loss = 0.06050287\n",
      "Iteration 60, loss = 0.06001536\n",
      "Iteration 61, loss = 0.05942141\n",
      "Iteration 62, loss = 0.05905020\n",
      "Iteration 63, loss = 0.05851015\n",
      "Iteration 64, loss = 0.05814316\n",
      "Iteration 65, loss = 0.05771816\n",
      "Iteration 66, loss = 0.05732011\n",
      "Iteration 67, loss = 0.05699159\n",
      "Iteration 68, loss = 0.05658793\n",
      "Iteration 69, loss = 0.05630087\n",
      "Iteration 70, loss = 0.05593069\n",
      "Iteration 71, loss = 0.05564464\n",
      "Iteration 72, loss = 0.05532562\n",
      "Iteration 73, loss = 0.05502858\n",
      "Iteration 74, loss = 0.05475560\n",
      "Iteration 75, loss = 0.05445791\n",
      "Iteration 76, loss = 0.05421415\n",
      "Iteration 77, loss = 0.05393130\n",
      "Iteration 78, loss = 0.05370090\n",
      "Iteration 79, loss = 0.05344351\n",
      "Iteration 80, loss = 0.05321882\n",
      "Iteration 81, loss = 0.05298771\n",
      "Iteration 82, loss = 0.05276743\n",
      "Iteration 83, loss = 0.05255839\n",
      "Iteration 84, loss = 0.05234479\n",
      "Iteration 85, loss = 0.05215174\n",
      "Iteration 86, loss = 0.05194709\n",
      "Iteration 87, loss = 0.05176437\n",
      "Iteration 88, loss = 0.05157045\n",
      "Iteration 89, loss = 0.05139551\n",
      "Iteration 90, loss = 0.05121268\n",
      "Iteration 91, loss = 0.05104340\n",
      "Iteration 92, loss = 0.05087018\n",
      "Iteration 93, loss = 0.05070541\n",
      "Iteration 94, loss = 0.05054075\n",
      "Iteration 95, loss = 0.05038057\n",
      "Iteration 96, loss = 0.05022343\n",
      "Iteration 97, loss = 0.05006808\n",
      "Iteration 98, loss = 0.04991747\n",
      "Iteration 99, loss = 0.04976714\n",
      "Iteration 100, loss = 0.04962223\n",
      "Iteration 101, loss = 0.04947690\n",
      "Iteration 102, loss = 0.04933703\n",
      "Iteration 103, loss = 0.04919653\n",
      "Iteration 104, loss = 0.04906112\n",
      "Iteration 105, loss = 0.04892516\n",
      "Iteration 106, loss = 0.04879377\n",
      "Iteration 107, loss = 0.04866202\n",
      "Iteration 108, loss = 0.04853430\n",
      "Iteration 109, loss = 0.04840644\n",
      "Iteration 110, loss = 0.04828210\n",
      "Iteration 111, loss = 0.04815785\n",
      "Iteration 112, loss = 0.04803669\n",
      "Iteration 113, loss = 0.04791580\n",
      "Iteration 114, loss = 0.04779766\n",
      "Iteration 115, loss = 0.04767995\n",
      "Iteration 116, loss = 0.04756469\n",
      "Iteration 117, loss = 0.04744998\n",
      "Iteration 118, loss = 0.04733747\n",
      "Iteration 119, loss = 0.04722563\n",
      "Iteration 120, loss = 0.04711576\n",
      "Iteration 121, loss = 0.04700665\n",
      "Iteration 122, loss = 0.04689931\n",
      "Iteration 123, loss = 0.04679280\n",
      "Iteration 124, loss = 0.04668789\n",
      "Iteration 125, loss = 0.04658387\n",
      "Iteration 126, loss = 0.04648128\n",
      "Iteration 127, loss = 0.04637965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 128, loss = 0.04627930\n",
      "Iteration 129, loss = 0.04617996\n",
      "Iteration 130, loss = 0.04608180\n",
      "Iteration 131, loss = 0.04598467\n",
      "Iteration 132, loss = 0.04588861\n",
      "Iteration 133, loss = 0.04579362\n",
      "Iteration 134, loss = 0.04569960\n",
      "Iteration 135, loss = 0.04560668\n",
      "Iteration 136, loss = 0.04551466\n",
      "Iteration 137, loss = 0.04542374\n",
      "Iteration 138, loss = 0.04533369\n",
      "Iteration 139, loss = 0.04524471\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.62612155\n",
      "Iteration 3, loss = 1.92755262\n",
      "Iteration 4, loss = 1.23194013\n",
      "Iteration 5, loss = 0.69896170\n",
      "Iteration 6, loss = 0.88010305\n",
      "Iteration 7, loss = 0.74552400\n",
      "Iteration 8, loss = 0.54415391\n",
      "Iteration 9, loss = 0.58917165\n",
      "Iteration 10, loss = 0.48990720\n",
      "Iteration 11, loss = 0.52470553\n",
      "Iteration 12, loss = 0.43607263\n",
      "Iteration 13, loss = 0.40994819\n",
      "Iteration 14, loss = 0.41353957\n",
      "Iteration 15, loss = 0.37882989\n",
      "Iteration 16, loss = 0.35019742\n",
      "Iteration 17, loss = 0.32145194\n",
      "Iteration 18, loss = 0.29090634\n",
      "Iteration 19, loss = 0.27873093\n",
      "Iteration 20, loss = 0.26845942\n",
      "Iteration 21, loss = 0.25221172\n",
      "Iteration 22, loss = 0.24437740\n",
      "Iteration 23, loss = 0.22933358\n",
      "Iteration 24, loss = 0.21587975\n",
      "Iteration 25, loss = 0.20453350\n",
      "Iteration 26, loss = 0.18932208\n",
      "Iteration 27, loss = 0.18162208\n",
      "Iteration 28, loss = 0.16879440\n",
      "Iteration 29, loss = 0.16201676\n",
      "Iteration 30, loss = 0.15334931\n",
      "Iteration 31, loss = 0.14704148\n",
      "Iteration 32, loss = 0.14171110\n",
      "Iteration 33, loss = 0.13538514\n",
      "Iteration 34, loss = 0.13212144\n",
      "Iteration 35, loss = 0.12665199\n",
      "Iteration 36, loss = 0.12439362\n",
      "Iteration 37, loss = 0.11994120\n",
      "Iteration 38, loss = 0.11784276\n",
      "Iteration 39, loss = 0.11461731\n",
      "Iteration 40, loss = 0.11246846\n",
      "Iteration 41, loss = 0.11023940\n",
      "Iteration 42, loss = 0.10808340\n",
      "Iteration 43, loss = 0.10658999\n",
      "Iteration 44, loss = 0.10455186\n",
      "Iteration 45, loss = 0.10353460\n",
      "Iteration 46, loss = 0.10173695\n",
      "Iteration 47, loss = 0.10097344\n",
      "Iteration 48, loss = 0.09945976\n",
      "Iteration 49, loss = 0.09883817\n",
      "Iteration 50, loss = 0.09758436\n",
      "Iteration 51, loss = 0.09702996\n",
      "Iteration 52, loss = 0.09600028\n",
      "Iteration 53, loss = 0.09548509\n",
      "Iteration 54, loss = 0.09461454\n",
      "Iteration 55, loss = 0.09412591\n",
      "Iteration 56, loss = 0.09337532\n",
      "Iteration 57, loss = 0.09291000\n",
      "Iteration 58, loss = 0.09223864\n",
      "Iteration 59, loss = 0.09179874\n",
      "Iteration 60, loss = 0.09119233\n",
      "Iteration 61, loss = 0.09078274\n",
      "Iteration 62, loss = 0.09022544\n",
      "Iteration 63, loss = 0.08984738\n",
      "Iteration 64, loss = 0.08933851\n",
      "Iteration 65, loss = 0.08899404\n",
      "Iteration 66, loss = 0.08852775\n",
      "Iteration 67, loss = 0.08821099\n",
      "Iteration 68, loss = 0.08778905\n",
      "Iteration 69, loss = 0.08749588\n",
      "Iteration 70, loss = 0.08711444\n",
      "Iteration 71, loss = 0.08683453\n",
      "Iteration 72, loss = 0.08649409\n",
      "Iteration 73, loss = 0.08622289\n",
      "Iteration 74, loss = 0.08592013\n",
      "Iteration 75, loss = 0.08565204\n",
      "Iteration 76, loss = 0.08538268\n",
      "Iteration 77, loss = 0.08511730\n",
      "Iteration 78, loss = 0.08487540\n",
      "Iteration 79, loss = 0.08461617\n",
      "Iteration 80, loss = 0.08439447\n",
      "Iteration 81, loss = 0.08414707\n",
      "Iteration 82, loss = 0.08393702\n",
      "Iteration 83, loss = 0.08370758\n",
      "Iteration 84, loss = 0.08350313\n",
      "Iteration 85, loss = 0.08329437\n",
      "Iteration 86, loss = 0.08309305\n",
      "Iteration 87, loss = 0.08290306\n",
      "Iteration 88, loss = 0.08270699\n",
      "Iteration 89, loss = 0.08252975\n",
      "Iteration 90, loss = 0.08234346\n",
      "Iteration 91, loss = 0.08217241\n",
      "Iteration 92, loss = 0.08199890\n",
      "Iteration 93, loss = 0.08183057\n",
      "Iteration 94, loss = 0.08166899\n",
      "Iteration 95, loss = 0.08150448\n",
      "Iteration 96, loss = 0.08135045\n",
      "Iteration 97, loss = 0.08119313\n",
      "Iteration 98, loss = 0.08104253\n",
      "Iteration 99, loss = 0.08089406\n",
      "Iteration 100, loss = 0.08074598\n",
      "Iteration 101, loss = 0.08060437\n",
      "Iteration 102, loss = 0.08046106\n",
      "Iteration 103, loss = 0.08032298\n",
      "Iteration 104, loss = 0.08018628\n",
      "Iteration 105, loss = 0.08005065\n",
      "Iteration 106, loss = 0.07991945\n",
      "Iteration 107, loss = 0.07978782\n",
      "Iteration 108, loss = 0.07965971\n",
      "Iteration 109, loss = 0.07953328\n",
      "Iteration 110, loss = 0.07940762\n",
      "Iteration 111, loss = 0.07928529\n",
      "Iteration 112, loss = 0.07916333\n",
      "Iteration 113, loss = 0.07904344\n",
      "Iteration 114, loss = 0.07892559\n",
      "Iteration 115, loss = 0.07880826\n",
      "Iteration 116, loss = 0.07869331\n",
      "Iteration 117, loss = 0.07857945\n",
      "Iteration 118, loss = 0.07846665\n",
      "Iteration 119, loss = 0.07835589\n",
      "Iteration 120, loss = 0.07824589\n",
      "Iteration 121, loss = 0.07813732\n",
      "Iteration 122, loss = 0.07803032\n",
      "Iteration 123, loss = 0.07792411\n",
      "Iteration 124, loss = 0.07781940\n",
      "Iteration 125, loss = 0.07771596\n",
      "Iteration 126, loss = 0.07761340\n",
      "Iteration 127, loss = 0.07751225\n",
      "Iteration 128, loss = 0.07741219\n",
      "Iteration 129, loss = 0.07731307\n",
      "Iteration 130, loss = 0.07721525\n",
      "Iteration 131, loss = 0.07711843\n",
      "Iteration 132, loss = 0.07702256\n",
      "Iteration 133, loss = 0.07692787\n",
      "Iteration 134, loss = 0.07683414\n",
      "Iteration 135, loss = 0.07674133\n",
      "Iteration 136, loss = 0.07664961\n",
      "Iteration 137, loss = 0.07655881\n",
      "Iteration 138, loss = 0.07646890\n",
      "Iteration 139, loss = 0.07638001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.74611452\n",
      "Iteration 3, loss = 2.25607157\n",
      "Iteration 4, loss = 0.81103117\n",
      "Iteration 5, loss = 0.70520789\n",
      "Iteration 6, loss = 0.62683946\n",
      "Iteration 7, loss = 0.55559087\n",
      "Iteration 8, loss = 0.50180878\n",
      "Iteration 9, loss = 0.44681033\n",
      "Iteration 10, loss = 0.41370850\n",
      "Iteration 11, loss = 0.38751443\n",
      "Iteration 12, loss = 0.38334437\n",
      "Iteration 13, loss = 0.58228081\n",
      "Iteration 14, loss = 1.70088146\n",
      "Iteration 15, loss = 0.52263764\n",
      "Iteration 16, loss = 0.57009265\n",
      "Iteration 17, loss = 0.40606152\n",
      "Iteration 18, loss = 0.39718730\n",
      "Iteration 19, loss = 0.34433320\n",
      "Iteration 20, loss = 0.28724144\n",
      "Iteration 21, loss = 0.24413513\n",
      "Iteration 22, loss = 0.21670587\n",
      "Iteration 23, loss = 0.21546748\n",
      "Iteration 24, loss = 0.34348772\n",
      "Iteration 25, loss = 1.20392288\n",
      "Iteration 26, loss = 2.37709306\n",
      "Iteration 27, loss = 0.46460359\n",
      "Iteration 28, loss = 0.58101832\n",
      "Iteration 29, loss = 0.56208869\n",
      "Iteration 30, loss = 0.50218889\n",
      "Iteration 31, loss = 0.47778004\n",
      "Iteration 32, loss = 0.46416111\n",
      "Iteration 33, loss = 0.46085829\n",
      "Iteration 34, loss = 0.45124523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.73643596\n",
      "Iteration 3, loss = 2.17567420\n",
      "Iteration 4, loss = 0.79795878\n",
      "Iteration 5, loss = 0.71635699\n",
      "Iteration 6, loss = 0.64037064\n",
      "Iteration 7, loss = 0.55524331\n",
      "Iteration 8, loss = 0.48606317\n",
      "Iteration 9, loss = 0.44429341\n",
      "Iteration 10, loss = 0.40692315\n",
      "Iteration 11, loss = 0.37729929\n",
      "Iteration 12, loss = 0.35129985\n",
      "Iteration 13, loss = 0.34372729\n",
      "Iteration 14, loss = 0.63384852\n",
      "Iteration 15, loss = 2.34064976\n",
      "Iteration 16, loss = 0.32631214\n",
      "Iteration 17, loss = 0.56943901\n",
      "Iteration 18, loss = 0.45356293\n",
      "Iteration 19, loss = 0.44996298\n",
      "Iteration 20, loss = 0.43915185\n",
      "Iteration 21, loss = 0.39328150\n",
      "Iteration 22, loss = 0.36877797\n",
      "Iteration 23, loss = 0.32943385\n",
      "Iteration 24, loss = 0.31032801\n",
      "Iteration 25, loss = 0.30157229\n",
      "Iteration 26, loss = 0.34289706\n",
      "Iteration 27, loss = 0.69240874\n",
      "Iteration 28, loss = 1.70135164\n",
      "Iteration 29, loss = 3.11960644\n",
      "Iteration 30, loss = 0.52126770\n",
      "Iteration 31, loss = 0.58178722\n",
      "Iteration 32, loss = 0.60668031\n",
      "Iteration 33, loss = 0.59676958\n",
      "Iteration 34, loss = 0.54868485\n",
      "Iteration 35, loss = 0.45362418\n",
      "Iteration 36, loss = 0.39540691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.69721183\n",
      "Iteration 3, loss = 2.07663100\n",
      "Iteration 4, loss = 0.77501764\n",
      "Iteration 5, loss = 0.71697587\n",
      "Iteration 6, loss = 0.64066717\n",
      "Iteration 7, loss = 0.57112884\n",
      "Iteration 8, loss = 0.52739669\n",
      "Iteration 9, loss = 0.49349083\n",
      "Iteration 10, loss = 0.42258252\n",
      "Iteration 11, loss = 0.39125488\n",
      "Iteration 12, loss = 0.36296598\n",
      "Iteration 13, loss = 0.37622954\n",
      "Iteration 14, loss = 0.69808306\n",
      "Iteration 15, loss = 1.56721413\n",
      "Iteration 16, loss = 1.04165900\n",
      "Iteration 17, loss = 0.35560586\n",
      "Iteration 18, loss = 0.41055184\n",
      "Iteration 19, loss = 0.37151966\n",
      "Iteration 20, loss = 0.32204708\n",
      "Iteration 21, loss = 0.26353601\n",
      "Iteration 22, loss = 0.21639168\n",
      "Iteration 23, loss = 0.18719900\n",
      "Iteration 24, loss = 0.17218689\n",
      "Iteration 25, loss = 0.18175182\n",
      "Iteration 26, loss = 0.32499313\n",
      "Iteration 27, loss = 1.35186452\n",
      "Iteration 28, loss = 2.73028352\n",
      "Iteration 29, loss = 0.40824345\n",
      "Iteration 30, loss = 0.65883544\n",
      "Iteration 31, loss = 0.57790794\n",
      "Iteration 32, loss = 0.47113496\n",
      "Iteration 33, loss = 0.41861494\n",
      "Iteration 34, loss = 0.40416199\n",
      "Iteration 35, loss = 0.40937926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.71786270\n",
      "Iteration 3, loss = 2.12602753\n",
      "Iteration 4, loss = 0.79224175\n",
      "Iteration 5, loss = 0.71788835\n",
      "Iteration 6, loss = 0.64069355\n",
      "Iteration 7, loss = 0.56474153\n",
      "Iteration 8, loss = 0.49976716\n",
      "Iteration 9, loss = 0.44903283\n",
      "Iteration 10, loss = 0.40978637\n",
      "Iteration 11, loss = 0.38617853\n",
      "Iteration 12, loss = 0.41049890\n",
      "Iteration 13, loss = 0.77849549\n",
      "Iteration 14, loss = 1.71882135\n",
      "Iteration 15, loss = 0.34316064\n",
      "Iteration 16, loss = 0.33362881\n",
      "Iteration 17, loss = 0.32743728\n",
      "Iteration 18, loss = 0.29723853\n",
      "Iteration 19, loss = 0.25857249\n",
      "Iteration 20, loss = 0.26673518\n",
      "Iteration 21, loss = 0.38526152\n",
      "Iteration 22, loss = 1.04883153\n",
      "Iteration 23, loss = 0.28366976\n",
      "Iteration 24, loss = 0.30107483\n",
      "Iteration 25, loss = 0.24594129\n",
      "Iteration 26, loss = 0.20291767\n",
      "Iteration 27, loss = 0.19735958\n",
      "Iteration 28, loss = 0.34940441\n",
      "Iteration 29, loss = 0.80266675\n",
      "Iteration 30, loss = 1.67184397\n",
      "Iteration 31, loss = 0.40866222\n",
      "Iteration 32, loss = 0.52554152\n",
      "Iteration 33, loss = 0.47287122\n",
      "Iteration 34, loss = 0.42871743\n",
      "Iteration 35, loss = 0.43305927\n",
      "Iteration 36, loss = 0.43460188\n",
      "Iteration 37, loss = 0.42018012\n",
      "Iteration 38, loss = 0.39960748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.71998789\n",
      "Iteration 3, loss = 2.06683061\n",
      "Iteration 4, loss = 0.76387600\n",
      "Iteration 5, loss = 0.70679143\n",
      "Iteration 6, loss = 0.62492841\n",
      "Iteration 7, loss = 0.55510594\n",
      "Iteration 8, loss = 0.50163689\n",
      "Iteration 9, loss = 0.44684755\n",
      "Iteration 10, loss = 0.40780330\n",
      "Iteration 11, loss = 0.38299248\n",
      "Iteration 12, loss = 0.38742480\n",
      "Iteration 13, loss = 0.66112506\n",
      "Iteration 14, loss = 1.72991915\n",
      "Iteration 15, loss = 0.38644524\n",
      "Iteration 16, loss = 0.36508215\n",
      "Iteration 17, loss = 0.35347713\n",
      "Iteration 18, loss = 0.29318144\n",
      "Iteration 19, loss = 0.26583641\n",
      "Iteration 20, loss = 0.24345795\n",
      "Iteration 21, loss = 0.27103200\n",
      "Iteration 22, loss = 0.58928365\n",
      "Iteration 23, loss = 1.24035660\n",
      "Iteration 24, loss = 1.30642846\n",
      "Iteration 25, loss = 0.44743758\n",
      "Iteration 26, loss = 0.49876979\n",
      "Iteration 27, loss = 0.53849692\n",
      "Iteration 28, loss = 0.52839631\n",
      "Iteration 29, loss = 0.49233334\n",
      "Iteration 30, loss = 0.45880156\n",
      "Iteration 31, loss = 0.44741753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.10881881\n",
      "Iteration 3, loss = 0.92189471\n",
      "Iteration 4, loss = 0.80397058\n",
      "Iteration 5, loss = 0.75264896\n",
      "Iteration 6, loss = 0.74995610\n",
      "Iteration 7, loss = 0.76274982\n",
      "Iteration 8, loss = 0.77002771\n",
      "Iteration 9, loss = 0.76544293\n",
      "Iteration 10, loss = 0.74995365\n",
      "Iteration 11, loss = 0.72660538\n",
      "Iteration 12, loss = 0.69836352\n",
      "Iteration 13, loss = 0.66777574\n",
      "Iteration 14, loss = 0.63731303\n",
      "Iteration 15, loss = 0.60913715\n",
      "Iteration 16, loss = 0.58468719\n",
      "Iteration 17, loss = 0.56502523\n",
      "Iteration 18, loss = 0.55004478\n",
      "Iteration 19, loss = 0.53902593\n",
      "Iteration 20, loss = 0.53203667\n",
      "Iteration 21, loss = 0.52753027\n",
      "Iteration 22, loss = 0.52117705\n",
      "Iteration 23, loss = 0.51285645\n",
      "Iteration 24, loss = 0.50327578\n",
      "Iteration 25, loss = 0.49280687\n",
      "Iteration 26, loss = 0.48188366\n",
      "Iteration 27, loss = 0.47166194\n",
      "Iteration 28, loss = 0.46333518\n",
      "Iteration 29, loss = 0.45620842\n",
      "Iteration 30, loss = 0.44959417\n",
      "Iteration 31, loss = 0.44335037\n",
      "Iteration 32, loss = 0.43717104\n",
      "Iteration 33, loss = 0.43115011\n",
      "Iteration 34, loss = 0.42566837\n",
      "Iteration 35, loss = 0.42090974\n",
      "Iteration 36, loss = 0.41681556\n",
      "Iteration 37, loss = 0.41323062\n",
      "Iteration 38, loss = 0.40956117\n",
      "Iteration 39, loss = 0.40551904\n",
      "Iteration 40, loss = 0.40103214\n",
      "Iteration 41, loss = 0.39621728\n",
      "Iteration 42, loss = 0.39130716\n",
      "Iteration 43, loss = 0.38656403\n",
      "Iteration 44, loss = 0.38227730\n",
      "Iteration 45, loss = 0.37821384\n",
      "Iteration 46, loss = 0.37419822\n",
      "Iteration 47, loss = 0.37016716\n",
      "Iteration 48, loss = 0.36613637\n",
      "Iteration 49, loss = 0.36216177\n",
      "Iteration 50, loss = 0.35829101\n",
      "Iteration 51, loss = 0.35456314\n",
      "Iteration 52, loss = 0.35098740\n",
      "Iteration 53, loss = 0.34744013\n",
      "Iteration 54, loss = 0.34378966\n",
      "Iteration 55, loss = 0.34004017\n",
      "Iteration 56, loss = 0.33625298\n",
      "Iteration 57, loss = 0.33250898\n",
      "Iteration 58, loss = 0.32874289\n",
      "Iteration 59, loss = 0.32477382\n",
      "Iteration 60, loss = 0.32054758\n",
      "Iteration 61, loss = 0.31613401\n",
      "Iteration 62, loss = 0.31168007\n",
      "Iteration 63, loss = 0.30729550\n",
      "Iteration 64, loss = 0.30291268\n",
      "Iteration 65, loss = 0.29834587\n",
      "Iteration 66, loss = 0.29369123\n",
      "Iteration 67, loss = 0.28910930\n",
      "Iteration 68, loss = 0.28445468\n",
      "Iteration 69, loss = 0.27959058\n",
      "Iteration 70, loss = 0.27457399\n",
      "Iteration 71, loss = 0.26956551\n",
      "Iteration 72, loss = 0.26458594\n",
      "Iteration 73, loss = 0.25951455\n",
      "Iteration 74, loss = 0.25435719\n",
      "Iteration 75, loss = 0.24924853\n",
      "Iteration 76, loss = 0.24415483\n",
      "Iteration 77, loss = 0.23898037\n",
      "Iteration 78, loss = 0.23386720\n",
      "Iteration 79, loss = 0.22885210\n",
      "Iteration 80, loss = 0.22384771\n",
      "Iteration 81, loss = 0.21892862\n",
      "Iteration 82, loss = 0.21413520\n",
      "Iteration 83, loss = 0.20939278\n",
      "Iteration 84, loss = 0.20471853\n",
      "Iteration 85, loss = 0.20019232\n",
      "Iteration 86, loss = 0.19567618\n",
      "Iteration 87, loss = 0.19118431\n",
      "Iteration 88, loss = 0.18676736\n",
      "Iteration 89, loss = 0.18249848\n",
      "Iteration 90, loss = 0.17818563\n",
      "Iteration 91, loss = 0.17402141\n",
      "Iteration 92, loss = 0.16991963\n",
      "Iteration 93, loss = 0.16588442\n",
      "Iteration 94, loss = 0.16201491\n",
      "Iteration 95, loss = 0.15830025\n",
      "Iteration 96, loss = 0.15467226\n",
      "Iteration 97, loss = 0.15111072\n",
      "Iteration 98, loss = 0.14771200\n",
      "Iteration 99, loss = 0.14443772\n",
      "Iteration 100, loss = 0.14129095\n",
      "Iteration 101, loss = 0.13824265\n",
      "Iteration 102, loss = 0.13532682\n",
      "Iteration 103, loss = 0.13253372\n",
      "Iteration 104, loss = 0.12985709\n",
      "Iteration 105, loss = 0.12731756\n",
      "Iteration 106, loss = 0.12488386\n",
      "Iteration 107, loss = 0.12259018\n",
      "Iteration 108, loss = 0.12039272\n",
      "Iteration 109, loss = 0.11829551\n",
      "Iteration 110, loss = 0.11631724\n",
      "Iteration 111, loss = 0.11442794\n",
      "Iteration 112, loss = 0.11264401\n",
      "Iteration 113, loss = 0.11096245\n",
      "Iteration 114, loss = 0.10935419\n",
      "Iteration 115, loss = 0.10783447\n",
      "Iteration 116, loss = 0.10640382\n",
      "Iteration 117, loss = 0.10504453\n",
      "Iteration 118, loss = 0.10375278\n",
      "Iteration 119, loss = 0.10253683\n",
      "Iteration 120, loss = 0.10138638\n",
      "Iteration 121, loss = 0.10029152\n",
      "Iteration 122, loss = 0.09925342\n",
      "Iteration 123, loss = 0.09827399\n",
      "Iteration 124, loss = 0.09734769\n",
      "Iteration 125, loss = 0.09646700\n",
      "Iteration 126, loss = 0.09562977\n",
      "Iteration 127, loss = 0.09483561\n",
      "Iteration 128, loss = 0.09408224\n",
      "Iteration 129, loss = 0.09336706\n",
      "Iteration 130, loss = 0.09268875\n",
      "Iteration 131, loss = 0.09204474\n",
      "Iteration 132, loss = 0.09143260\n",
      "Iteration 133, loss = 0.09085109\n",
      "Iteration 134, loss = 0.09030166\n",
      "Iteration 135, loss = 0.08978718\n",
      "Iteration 136, loss = 0.08929299\n",
      "Iteration 137, loss = 0.08881676\n",
      "Iteration 138, loss = 0.08833481\n",
      "Iteration 139, loss = 0.08787168\n",
      "Iteration 140, loss = 0.08744859\n",
      "Iteration 141, loss = 0.08706020\n",
      "Iteration 142, loss = 0.08668295\n",
      "Iteration 143, loss = 0.08630041\n",
      "Iteration 144, loss = 0.08592439\n",
      "Iteration 145, loss = 0.08556740\n",
      "Iteration 146, loss = 0.08523745\n",
      "Iteration 147, loss = 0.08492286\n",
      "Iteration 148, loss = 0.08461288\n",
      "Iteration 149, loss = 0.08429716\n",
      "Iteration 150, loss = 0.08398712\n",
      "Iteration 151, loss = 0.08369513\n",
      "Iteration 152, loss = 0.08342116\n",
      "Iteration 153, loss = 0.08315290\n",
      "Iteration 154, loss = 0.08288296\n",
      "Iteration 155, loss = 0.08261528\n",
      "Iteration 156, loss = 0.08235914\n",
      "Iteration 157, loss = 0.08211586\n",
      "Iteration 158, loss = 0.08188046\n",
      "Iteration 159, loss = 0.08164737\n",
      "Iteration 160, loss = 0.08141769\n",
      "Iteration 161, loss = 0.08119098\n",
      "Iteration 162, loss = 0.08097203\n",
      "Iteration 163, loss = 0.08076240\n",
      "Iteration 164, loss = 0.08056086\n",
      "Iteration 165, loss = 0.08036505\n",
      "Iteration 166, loss = 0.08017363\n",
      "Iteration 167, loss = 0.07998715\n",
      "Iteration 168, loss = 0.07980431\n",
      "Iteration 169, loss = 0.07962502\n",
      "Iteration 170, loss = 0.07945061\n",
      "Iteration 171, loss = 0.07928209\n",
      "Iteration 172, loss = 0.07911958\n",
      "Iteration 173, loss = 0.07896272\n",
      "Iteration 174, loss = 0.07881083\n",
      "Iteration 175, loss = 0.07866385\n",
      "Iteration 176, loss = 0.07852114\n",
      "Iteration 177, loss = 0.07838384\n",
      "Iteration 178, loss = 0.07825073\n",
      "Iteration 179, loss = 0.07812088\n",
      "Iteration 180, loss = 0.07799675\n",
      "Iteration 181, loss = 0.07787458\n",
      "Iteration 182, loss = 0.07775236\n",
      "Iteration 183, loss = 0.07762858\n",
      "Iteration 184, loss = 0.07750281\n",
      "Iteration 185, loss = 0.07737844\n",
      "Iteration 186, loss = 0.07725976\n",
      "Iteration 187, loss = 0.07714857\n",
      "Iteration 188, loss = 0.07704482\n",
      "Iteration 189, loss = 0.07694497\n",
      "Iteration 190, loss = 0.07684577\n",
      "Iteration 191, loss = 0.07674552\n",
      "Iteration 192, loss = 0.07664479\n",
      "Iteration 193, loss = 0.07654297\n",
      "Iteration 194, loss = 0.07644212\n",
      "Iteration 195, loss = 0.07634315\n",
      "Iteration 196, loss = 0.07624791\n",
      "Iteration 197, loss = 0.07615624\n",
      "Iteration 198, loss = 0.07606773\n",
      "Iteration 199, loss = 0.07598055\n",
      "Iteration 200, loss = 0.07589548\n",
      "Iteration 201, loss = 0.07581186\n",
      "Iteration 202, loss = 0.07572912\n",
      "Iteration 203, loss = 0.07564719\n",
      "Iteration 204, loss = 0.07556615\n",
      "Iteration 205, loss = 0.07548613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.11413451\n",
      "Iteration 3, loss = 0.92793012\n",
      "Iteration 4, loss = 0.80998178\n",
      "Iteration 5, loss = 0.75750312\n",
      "Iteration 6, loss = 0.75388362\n",
      "Iteration 7, loss = 0.76720327\n",
      "Iteration 8, loss = 0.77489683\n",
      "Iteration 9, loss = 0.77005324\n",
      "Iteration 10, loss = 0.75376704\n",
      "Iteration 11, loss = 0.72951577\n",
      "Iteration 12, loss = 0.70055247\n",
      "Iteration 13, loss = 0.66952323\n",
      "Iteration 14, loss = 0.63893205\n",
      "Iteration 15, loss = 0.61069653\n",
      "Iteration 16, loss = 0.58639254\n",
      "Iteration 17, loss = 0.56675859\n",
      "Iteration 18, loss = 0.55177650\n",
      "Iteration 19, loss = 0.54062279\n",
      "Iteration 20, loss = 0.53292497\n",
      "Iteration 21, loss = 0.52806697\n",
      "Iteration 22, loss = 0.52141290\n",
      "Iteration 23, loss = 0.51269107\n",
      "Iteration 24, loss = 0.50265596\n",
      "Iteration 25, loss = 0.49162669\n",
      "Iteration 26, loss = 0.48029758\n",
      "Iteration 27, loss = 0.46984836\n",
      "Iteration 28, loss = 0.46103213\n",
      "Iteration 29, loss = 0.45349407\n",
      "Iteration 30, loss = 0.44655707\n",
      "Iteration 31, loss = 0.43992528\n",
      "Iteration 32, loss = 0.43354344\n",
      "Iteration 33, loss = 0.42772568\n",
      "Iteration 34, loss = 0.42241949\n",
      "Iteration 35, loss = 0.41783145\n",
      "Iteration 36, loss = 0.41368252\n",
      "Iteration 37, loss = 0.40966511\n",
      "Iteration 38, loss = 0.40548177\n",
      "Iteration 39, loss = 0.40091922\n",
      "Iteration 40, loss = 0.39598220\n",
      "Iteration 41, loss = 0.39083784\n",
      "Iteration 42, loss = 0.38573698\n",
      "Iteration 43, loss = 0.38085822\n",
      "Iteration 44, loss = 0.37628300\n",
      "Iteration 45, loss = 0.37203743\n",
      "Iteration 46, loss = 0.36784176\n",
      "Iteration 47, loss = 0.36364653\n",
      "Iteration 48, loss = 0.35944285\n",
      "Iteration 49, loss = 0.35534096\n",
      "Iteration 50, loss = 0.35144169\n",
      "Iteration 51, loss = 0.34764923\n",
      "Iteration 52, loss = 0.34389719\n",
      "Iteration 53, loss = 0.34013761\n",
      "Iteration 54, loss = 0.33627329\n",
      "Iteration 55, loss = 0.33239094\n",
      "Iteration 56, loss = 0.32852046\n",
      "Iteration 57, loss = 0.32468844\n",
      "Iteration 58, loss = 0.32088983\n",
      "Iteration 59, loss = 0.31712692\n",
      "Iteration 60, loss = 0.31338097\n",
      "Iteration 61, loss = 0.30962893\n",
      "Iteration 62, loss = 0.30588213\n",
      "Iteration 63, loss = 0.30219502\n",
      "Iteration 64, loss = 0.29858095\n",
      "Iteration 65, loss = 0.29500180\n",
      "Iteration 66, loss = 0.29142725\n",
      "Iteration 67, loss = 0.28785175\n",
      "Iteration 68, loss = 0.28429674\n",
      "Iteration 69, loss = 0.28077208\n",
      "Iteration 70, loss = 0.27728288\n",
      "Iteration 71, loss = 0.27382222\n",
      "Iteration 72, loss = 0.27037211\n",
      "Iteration 73, loss = 0.26694690\n",
      "Iteration 74, loss = 0.26355031\n",
      "Iteration 75, loss = 0.26019384\n",
      "Iteration 76, loss = 0.25687960\n",
      "Iteration 77, loss = 0.25359778\n",
      "Iteration 78, loss = 0.25033535\n",
      "Iteration 79, loss = 0.24709653\n",
      "Iteration 80, loss = 0.24388118\n",
      "Iteration 81, loss = 0.24069180\n",
      "Iteration 82, loss = 0.23752897\n",
      "Iteration 83, loss = 0.23439115\n",
      "Iteration 84, loss = 0.23127672\n",
      "Iteration 85, loss = 0.22819248\n",
      "Iteration 86, loss = 0.22513712\n",
      "Iteration 87, loss = 0.22211970\n",
      "Iteration 88, loss = 0.21911952\n",
      "Iteration 89, loss = 0.21609410\n",
      "Iteration 90, loss = 0.21309684\n",
      "Iteration 91, loss = 0.21006197\n",
      "Iteration 92, loss = 0.20699339\n",
      "Iteration 93, loss = 0.20387602\n",
      "Iteration 94, loss = 0.20070416\n",
      "Iteration 95, loss = 0.19747954\n",
      "Iteration 96, loss = 0.19420179\n",
      "Iteration 97, loss = 0.19087227\n",
      "Iteration 98, loss = 0.18750282\n",
      "Iteration 99, loss = 0.18409578\n",
      "Iteration 100, loss = 0.18065928\n",
      "Iteration 101, loss = 0.17720161\n",
      "Iteration 102, loss = 0.17373346\n",
      "Iteration 103, loss = 0.17026128\n",
      "Iteration 104, loss = 0.16678804\n",
      "Iteration 105, loss = 0.16333066\n",
      "Iteration 106, loss = 0.15990301\n",
      "Iteration 107, loss = 0.15652356\n",
      "Iteration 108, loss = 0.15319354\n",
      "Iteration 109, loss = 0.14993929\n",
      "Iteration 110, loss = 0.14673970\n",
      "Iteration 111, loss = 0.14364599\n",
      "Iteration 112, loss = 0.14059985\n",
      "Iteration 113, loss = 0.13766323\n",
      "Iteration 114, loss = 0.13479895\n",
      "Iteration 115, loss = 0.13204031\n",
      "Iteration 116, loss = 0.12937176\n",
      "Iteration 117, loss = 0.12682570\n",
      "Iteration 118, loss = 0.12437089\n",
      "Iteration 119, loss = 0.12201103\n",
      "Iteration 120, loss = 0.11975307\n",
      "Iteration 121, loss = 0.11758528\n",
      "Iteration 122, loss = 0.11552882\n",
      "Iteration 123, loss = 0.11355690\n",
      "Iteration 124, loss = 0.11169937\n",
      "Iteration 125, loss = 0.10990936\n",
      "Iteration 126, loss = 0.10822646\n",
      "Iteration 127, loss = 0.10662820\n",
      "Iteration 128, loss = 0.10510036\n",
      "Iteration 129, loss = 0.10366871\n",
      "Iteration 130, loss = 0.10230988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 131, loss = 0.10102170\n",
      "Iteration 132, loss = 0.09981317\n",
      "Iteration 133, loss = 0.09866069\n",
      "Iteration 134, loss = 0.09757206\n",
      "Iteration 135, loss = 0.09655030\n",
      "Iteration 136, loss = 0.09557329\n",
      "Iteration 137, loss = 0.09465383\n",
      "Iteration 138, loss = 0.09378990\n",
      "Iteration 139, loss = 0.09296373\n",
      "Iteration 140, loss = 0.09218099\n",
      "Iteration 141, loss = 0.09144655\n",
      "Iteration 142, loss = 0.09074964\n",
      "Iteration 143, loss = 0.09008669\n",
      "Iteration 144, loss = 0.08945733\n",
      "Iteration 145, loss = 0.08886422\n",
      "Iteration 146, loss = 0.08829860\n",
      "Iteration 147, loss = 0.08775930\n",
      "Iteration 148, loss = 0.08725160\n",
      "Iteration 149, loss = 0.08676892\n",
      "Iteration 150, loss = 0.08630643\n",
      "Iteration 151, loss = 0.08586663\n",
      "Iteration 152, loss = 0.08544749\n",
      "Iteration 153, loss = 0.08504882\n",
      "Iteration 154, loss = 0.08466849\n",
      "Iteration 155, loss = 0.08430565\n",
      "Iteration 156, loss = 0.08395628\n",
      "Iteration 157, loss = 0.08362246\n",
      "Iteration 158, loss = 0.08330550\n",
      "Iteration 159, loss = 0.08300145\n",
      "Iteration 160, loss = 0.08270765\n",
      "Iteration 161, loss = 0.08242454\n",
      "Iteration 162, loss = 0.08215320\n",
      "Iteration 163, loss = 0.08189367\n",
      "Iteration 164, loss = 0.08164547\n",
      "Iteration 165, loss = 0.08141464\n",
      "Iteration 166, loss = 0.08120039\n",
      "Iteration 167, loss = 0.08098680\n",
      "Iteration 168, loss = 0.08075942\n",
      "Iteration 169, loss = 0.08052448\n",
      "Iteration 170, loss = 0.08032380\n",
      "Iteration 171, loss = 0.08014907\n",
      "Iteration 172, loss = 0.07995536\n",
      "Iteration 173, loss = 0.07975606\n",
      "Iteration 174, loss = 0.07958043\n",
      "Iteration 175, loss = 0.07941698\n",
      "Iteration 176, loss = 0.07924677\n",
      "Iteration 177, loss = 0.07907301\n",
      "Iteration 178, loss = 0.07892237\n",
      "Iteration 179, loss = 0.07877786\n",
      "Iteration 180, loss = 0.07861108\n",
      "Iteration 181, loss = 0.07846727\n",
      "Iteration 182, loss = 0.07834545\n",
      "Iteration 183, loss = 0.07818970\n",
      "Iteration 184, loss = 0.07804206\n",
      "Iteration 185, loss = 0.07792010\n",
      "Iteration 186, loss = 0.07778954\n",
      "Iteration 187, loss = 0.07765332\n",
      "Iteration 188, loss = 0.07752929\n",
      "Iteration 189, loss = 0.07741383\n",
      "Iteration 190, loss = 0.07729340\n",
      "Iteration 191, loss = 0.07716981\n",
      "Iteration 192, loss = 0.07705771\n",
      "Iteration 193, loss = 0.07694870\n",
      "Iteration 194, loss = 0.07683671\n",
      "Iteration 195, loss = 0.07672635\n",
      "Iteration 196, loss = 0.07662379\n",
      "Iteration 197, loss = 0.07652351\n",
      "Iteration 198, loss = 0.07641638\n",
      "Iteration 199, loss = 0.07631685\n",
      "Iteration 200, loss = 0.07622173\n",
      "Iteration 201, loss = 0.07612073\n",
      "Iteration 202, loss = 0.07602533\n",
      "Iteration 203, loss = 0.07593477\n",
      "Iteration 204, loss = 0.07584011\n",
      "Iteration 205, loss = 0.07574841\n",
      "Iteration 206, loss = 0.07566162\n",
      "Iteration 207, loss = 0.07557231\n",
      "Iteration 208, loss = 0.07548413\n",
      "Iteration 209, loss = 0.07540051\n",
      "Iteration 210, loss = 0.07531623\n",
      "Iteration 211, loss = 0.07523179\n",
      "Iteration 212, loss = 0.07515079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.10504854\n",
      "Iteration 3, loss = 0.91906857\n",
      "Iteration 4, loss = 0.80154579\n",
      "Iteration 5, loss = 0.74947201\n",
      "Iteration 6, loss = 0.74474179\n",
      "Iteration 7, loss = 0.75706005\n",
      "Iteration 8, loss = 0.76416864\n",
      "Iteration 9, loss = 0.75936111\n",
      "Iteration 10, loss = 0.74363884\n",
      "Iteration 11, loss = 0.72022758\n",
      "Iteration 12, loss = 0.69217705\n",
      "Iteration 13, loss = 0.66196998\n",
      "Iteration 14, loss = 0.63202970\n",
      "Iteration 15, loss = 0.60429842\n",
      "Iteration 16, loss = 0.58019552\n",
      "Iteration 17, loss = 0.56067207\n",
      "Iteration 18, loss = 0.54574477\n",
      "Iteration 19, loss = 0.53459834\n",
      "Iteration 20, loss = 0.52597742\n",
      "Iteration 21, loss = 0.52062233\n",
      "Iteration 22, loss = 0.51422579\n",
      "Iteration 23, loss = 0.50564855\n",
      "Iteration 24, loss = 0.49575231\n",
      "Iteration 25, loss = 0.48506747\n",
      "Iteration 26, loss = 0.47379896\n",
      "Iteration 27, loss = 0.46355012\n",
      "Iteration 28, loss = 0.45482538\n",
      "Iteration 29, loss = 0.44699226\n",
      "Iteration 30, loss = 0.43976602\n",
      "Iteration 31, loss = 0.43295537\n",
      "Iteration 32, loss = 0.42664882\n",
      "Iteration 33, loss = 0.42070179\n",
      "Iteration 34, loss = 0.41541353\n",
      "Iteration 35, loss = 0.41082599\n",
      "Iteration 36, loss = 0.40657706\n",
      "Iteration 37, loss = 0.40232410\n",
      "Iteration 38, loss = 0.39782745\n",
      "Iteration 39, loss = 0.39296547\n",
      "Iteration 40, loss = 0.38778506\n",
      "Iteration 41, loss = 0.38243855\n",
      "Iteration 42, loss = 0.37716160\n",
      "Iteration 43, loss = 0.37209264\n",
      "Iteration 44, loss = 0.36727114\n",
      "Iteration 45, loss = 0.36273103\n",
      "Iteration 46, loss = 0.35831413\n",
      "Iteration 47, loss = 0.35394786\n",
      "Iteration 48, loss = 0.34962969\n",
      "Iteration 49, loss = 0.34537884\n",
      "Iteration 50, loss = 0.34119934\n",
      "Iteration 51, loss = 0.33705221\n",
      "Iteration 52, loss = 0.33299037\n",
      "Iteration 53, loss = 0.32894099\n",
      "Iteration 54, loss = 0.32476090\n",
      "Iteration 55, loss = 0.32058923\n",
      "Iteration 56, loss = 0.31639600\n",
      "Iteration 57, loss = 0.31216674\n",
      "Iteration 58, loss = 0.30797845\n",
      "Iteration 59, loss = 0.30374433\n",
      "Iteration 60, loss = 0.29937106\n",
      "Iteration 61, loss = 0.29488307\n",
      "Iteration 62, loss = 0.29029158\n",
      "Iteration 63, loss = 0.28559319\n",
      "Iteration 64, loss = 0.28080892\n",
      "Iteration 65, loss = 0.27594244\n",
      "Iteration 66, loss = 0.27095922\n",
      "Iteration 67, loss = 0.26584132\n",
      "Iteration 68, loss = 0.26063354\n",
      "Iteration 69, loss = 0.25533364\n",
      "Iteration 70, loss = 0.24993255\n",
      "Iteration 71, loss = 0.24445641\n",
      "Iteration 72, loss = 0.23912273\n",
      "Iteration 73, loss = 0.23372351\n",
      "Iteration 74, loss = 0.22846082\n",
      "Iteration 75, loss = 0.22302568\n",
      "Iteration 76, loss = 0.21755629\n",
      "Iteration 77, loss = 0.21220403\n",
      "Iteration 78, loss = 0.20679616\n",
      "Iteration 79, loss = 0.20154747\n",
      "Iteration 80, loss = 0.19633944\n",
      "Iteration 81, loss = 0.19112867\n",
      "Iteration 82, loss = 0.18604045\n",
      "Iteration 83, loss = 0.18105623\n",
      "Iteration 84, loss = 0.17615238\n",
      "Iteration 85, loss = 0.17137816\n",
      "Iteration 86, loss = 0.16671100\n",
      "Iteration 87, loss = 0.16213053\n",
      "Iteration 88, loss = 0.15771097\n",
      "Iteration 89, loss = 0.15342444\n",
      "Iteration 90, loss = 0.14927299\n",
      "Iteration 91, loss = 0.14528765\n",
      "Iteration 92, loss = 0.14141281\n",
      "Iteration 93, loss = 0.13771089\n",
      "Iteration 94, loss = 0.13415193\n",
      "Iteration 95, loss = 0.13071897\n",
      "Iteration 96, loss = 0.12745821\n",
      "Iteration 97, loss = 0.12430201\n",
      "Iteration 98, loss = 0.12132015\n",
      "Iteration 99, loss = 0.11846705\n",
      "Iteration 100, loss = 0.11574618\n",
      "Iteration 101, loss = 0.11317351\n",
      "Iteration 102, loss = 0.11070859\n",
      "Iteration 103, loss = 0.10838520\n",
      "Iteration 104, loss = 0.10614979\n",
      "Iteration 105, loss = 0.10406681\n",
      "Iteration 106, loss = 0.10204650\n",
      "Iteration 107, loss = 0.10016095\n",
      "Iteration 108, loss = 0.09835190\n",
      "Iteration 109, loss = 0.09665597\n",
      "Iteration 110, loss = 0.09502348\n",
      "Iteration 111, loss = 0.09349068\n",
      "Iteration 112, loss = 0.09202714\n",
      "Iteration 113, loss = 0.09063514\n",
      "Iteration 114, loss = 0.08931595\n",
      "Iteration 115, loss = 0.08805902\n",
      "Iteration 116, loss = 0.08687275\n",
      "Iteration 117, loss = 0.08572945\n",
      "Iteration 118, loss = 0.08465542\n",
      "Iteration 119, loss = 0.08362083\n",
      "Iteration 120, loss = 0.08264336\n",
      "Iteration 121, loss = 0.08170580\n",
      "Iteration 122, loss = 0.08081303\n",
      "Iteration 123, loss = 0.07996390\n",
      "Iteration 124, loss = 0.07914429\n",
      "Iteration 125, loss = 0.07836792\n",
      "Iteration 126, loss = 0.07761971\n",
      "Iteration 127, loss = 0.07690805\n",
      "Iteration 128, loss = 0.07622214\n",
      "Iteration 129, loss = 0.07556680\n",
      "Iteration 130, loss = 0.07493718\n",
      "Iteration 131, loss = 0.07433287\n",
      "Iteration 132, loss = 0.07375456\n",
      "Iteration 133, loss = 0.07319688\n",
      "Iteration 134, loss = 0.07266235\n",
      "Iteration 135, loss = 0.07214551\n",
      "Iteration 136, loss = 0.07165300\n",
      "Iteration 137, loss = 0.07117429\n",
      "Iteration 138, loss = 0.07071696\n",
      "Iteration 139, loss = 0.07027408\n",
      "Iteration 140, loss = 0.06985058\n",
      "Iteration 141, loss = 0.06943881\n",
      "Iteration 142, loss = 0.06904282\n",
      "Iteration 143, loss = 0.06865875\n",
      "Iteration 144, loss = 0.06828944\n",
      "Iteration 145, loss = 0.06793056\n",
      "Iteration 146, loss = 0.06758585\n",
      "Iteration 147, loss = 0.06725001\n",
      "Iteration 148, loss = 0.06692558\n",
      "Iteration 149, loss = 0.06661139\n",
      "Iteration 150, loss = 0.06630670\n",
      "Iteration 151, loss = 0.06601061\n",
      "Iteration 152, loss = 0.06572359\n",
      "Iteration 153, loss = 0.06544470\n",
      "Iteration 154, loss = 0.06517418\n",
      "Iteration 155, loss = 0.06491208\n",
      "Iteration 156, loss = 0.06465633\n",
      "Iteration 157, loss = 0.06440831\n",
      "Iteration 158, loss = 0.06416691\n",
      "Iteration 159, loss = 0.06393252\n",
      "Iteration 160, loss = 0.06370371\n",
      "Iteration 161, loss = 0.06348166\n",
      "Iteration 162, loss = 0.06326506\n",
      "Iteration 163, loss = 0.06305372\n",
      "Iteration 164, loss = 0.06284826\n",
      "Iteration 165, loss = 0.06264813\n",
      "Iteration 166, loss = 0.06245224\n",
      "Iteration 167, loss = 0.06226191\n",
      "Iteration 168, loss = 0.06207594\n",
      "Iteration 169, loss = 0.06189397\n",
      "Iteration 170, loss = 0.06171603\n",
      "Iteration 171, loss = 0.06154228\n",
      "Iteration 172, loss = 0.06137365\n",
      "Iteration 173, loss = 0.06120781\n",
      "Iteration 174, loss = 0.06104635\n",
      "Iteration 175, loss = 0.06088895\n",
      "Iteration 176, loss = 0.06073489\n",
      "Iteration 177, loss = 0.06058395\n",
      "Iteration 178, loss = 0.06043619\n",
      "Iteration 179, loss = 0.06029186\n",
      "Iteration 180, loss = 0.06015100\n",
      "Iteration 181, loss = 0.06001232\n",
      "Iteration 182, loss = 0.05987686\n",
      "Iteration 183, loss = 0.05974442\n",
      "Iteration 184, loss = 0.05961474\n",
      "Iteration 185, loss = 0.05948747\n",
      "Iteration 186, loss = 0.05936264\n",
      "Iteration 187, loss = 0.05924031\n",
      "Iteration 188, loss = 0.05912027\n",
      "Iteration 189, loss = 0.05900253\n",
      "Iteration 190, loss = 0.05888723\n",
      "Iteration 191, loss = 0.05877389\n",
      "Iteration 192, loss = 0.05866275\n",
      "Iteration 193, loss = 0.05855363\n",
      "Iteration 194, loss = 0.05844647\n",
      "Iteration 195, loss = 0.05834143\n",
      "Iteration 196, loss = 0.05823810\n",
      "Iteration 197, loss = 0.05813661\n",
      "Iteration 198, loss = 0.05803699\n",
      "Iteration 199, loss = 0.05793907\n",
      "Iteration 200, loss = 0.05784279\n",
      "Iteration 201, loss = 0.05774810\n",
      "Iteration 202, loss = 0.05765496\n",
      "Iteration 203, loss = 0.05756335\n",
      "Iteration 204, loss = 0.05747321\n",
      "Iteration 205, loss = 0.05738450\n",
      "Iteration 206, loss = 0.05729720\n",
      "Iteration 207, loss = 0.05721125\n",
      "Iteration 208, loss = 0.05712663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.10424458\n",
      "Iteration 3, loss = 0.91840542\n",
      "Iteration 4, loss = 0.80151386\n",
      "Iteration 5, loss = 0.75021197\n",
      "Iteration 6, loss = 0.74679324\n",
      "Iteration 7, loss = 0.76000234\n",
      "Iteration 8, loss = 0.76731795\n",
      "Iteration 9, loss = 0.76224539\n",
      "Iteration 10, loss = 0.74596423\n",
      "Iteration 11, loss = 0.72185971\n",
      "Iteration 12, loss = 0.69314822\n",
      "Iteration 13, loss = 0.66237883\n",
      "Iteration 14, loss = 0.63206771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.60415735\n",
      "Iteration 16, loss = 0.58000455\n",
      "Iteration 17, loss = 0.56076632\n",
      "Iteration 18, loss = 0.54613165\n",
      "Iteration 19, loss = 0.53517805\n",
      "Iteration 20, loss = 0.52701369\n",
      "Iteration 21, loss = 0.52217093\n",
      "Iteration 22, loss = 0.51555171\n",
      "Iteration 23, loss = 0.50680143\n",
      "Iteration 24, loss = 0.49632411\n",
      "Iteration 25, loss = 0.48511068\n",
      "Iteration 26, loss = 0.47334176\n",
      "Iteration 27, loss = 0.46284285\n",
      "Iteration 28, loss = 0.45432110\n",
      "Iteration 29, loss = 0.44704591\n",
      "Iteration 30, loss = 0.44019304\n",
      "Iteration 31, loss = 0.43352964\n",
      "Iteration 32, loss = 0.42709673\n",
      "Iteration 33, loss = 0.42105014\n",
      "Iteration 34, loss = 0.41543048\n",
      "Iteration 35, loss = 0.41060473\n",
      "Iteration 36, loss = 0.40629637\n",
      "Iteration 37, loss = 0.40214774\n",
      "Iteration 38, loss = 0.39786130\n",
      "Iteration 39, loss = 0.39318639\n",
      "Iteration 40, loss = 0.38807375\n",
      "Iteration 41, loss = 0.38273962\n",
      "Iteration 42, loss = 0.37746428\n",
      "Iteration 43, loss = 0.37243023\n",
      "Iteration 44, loss = 0.36765280\n",
      "Iteration 45, loss = 0.36313361\n",
      "Iteration 46, loss = 0.35876079\n",
      "Iteration 47, loss = 0.35437798\n",
      "Iteration 48, loss = 0.34998914\n",
      "Iteration 49, loss = 0.34565060\n",
      "Iteration 50, loss = 0.34142281\n",
      "Iteration 51, loss = 0.33737705\n",
      "Iteration 52, loss = 0.33336075\n",
      "Iteration 53, loss = 0.32930436\n",
      "Iteration 54, loss = 0.32517013\n",
      "Iteration 55, loss = 0.32096965\n",
      "Iteration 56, loss = 0.31680222\n",
      "Iteration 57, loss = 0.31273844\n",
      "Iteration 58, loss = 0.30869905\n",
      "Iteration 59, loss = 0.30448588\n",
      "Iteration 60, loss = 0.30005534\n",
      "Iteration 61, loss = 0.29548704\n",
      "Iteration 62, loss = 0.29085544\n",
      "Iteration 63, loss = 0.28624340\n",
      "Iteration 64, loss = 0.28160534\n",
      "Iteration 65, loss = 0.27678925\n",
      "Iteration 66, loss = 0.27180458\n",
      "Iteration 67, loss = 0.26679399\n",
      "Iteration 68, loss = 0.26176336\n",
      "Iteration 69, loss = 0.25659783\n",
      "Iteration 70, loss = 0.25130222\n",
      "Iteration 71, loss = 0.24597763\n",
      "Iteration 72, loss = 0.24072342\n",
      "Iteration 73, loss = 0.23537467\n",
      "Iteration 74, loss = 0.23005482\n",
      "Iteration 75, loss = 0.22474443\n",
      "Iteration 76, loss = 0.21936312\n",
      "Iteration 77, loss = 0.21397045\n",
      "Iteration 78, loss = 0.20864724\n",
      "Iteration 79, loss = 0.20334677\n",
      "Iteration 80, loss = 0.19811089\n",
      "Iteration 81, loss = 0.19294749\n",
      "Iteration 82, loss = 0.18782047\n",
      "Iteration 83, loss = 0.18277225\n",
      "Iteration 84, loss = 0.17782642\n",
      "Iteration 85, loss = 0.17296017\n",
      "Iteration 86, loss = 0.16820699\n",
      "Iteration 87, loss = 0.16355360\n",
      "Iteration 88, loss = 0.15902103\n",
      "Iteration 89, loss = 0.15463236\n",
      "Iteration 90, loss = 0.15038486\n",
      "Iteration 91, loss = 0.14628286\n",
      "Iteration 92, loss = 0.14230692\n",
      "Iteration 93, loss = 0.13846456\n",
      "Iteration 94, loss = 0.13477391\n",
      "Iteration 95, loss = 0.13122281\n",
      "Iteration 96, loss = 0.12780373\n",
      "Iteration 97, loss = 0.12454823\n",
      "Iteration 98, loss = 0.12141726\n",
      "Iteration 99, loss = 0.11842072\n",
      "Iteration 100, loss = 0.11557374\n",
      "Iteration 101, loss = 0.11284123\n",
      "Iteration 102, loss = 0.11023761\n",
      "Iteration 103, loss = 0.10775959\n",
      "Iteration 104, loss = 0.10539865\n",
      "Iteration 105, loss = 0.10314804\n",
      "Iteration 106, loss = 0.10100157\n",
      "Iteration 107, loss = 0.09895649\n",
      "Iteration 108, loss = 0.09700979\n",
      "Iteration 109, loss = 0.09515567\n",
      "Iteration 110, loss = 0.09338741\n",
      "Iteration 111, loss = 0.09170096\n",
      "Iteration 112, loss = 0.09009253\n",
      "Iteration 113, loss = 0.08855745\n",
      "Iteration 114, loss = 0.08709181\n",
      "Iteration 115, loss = 0.08569172\n",
      "Iteration 116, loss = 0.08435435\n",
      "Iteration 117, loss = 0.08307480\n",
      "Iteration 118, loss = 0.08185467\n",
      "Iteration 119, loss = 0.08068037\n",
      "Iteration 120, loss = 0.07955904\n",
      "Iteration 121, loss = 0.07848526\n",
      "Iteration 122, loss = 0.07745564\n",
      "Iteration 123, loss = 0.07646681\n",
      "Iteration 124, loss = 0.07551795\n",
      "Iteration 125, loss = 0.07460299\n",
      "Iteration 126, loss = 0.07372028\n",
      "Iteration 127, loss = 0.07286422\n",
      "Iteration 128, loss = 0.07203229\n",
      "Iteration 129, loss = 0.07121984\n",
      "Iteration 130, loss = 0.07041879\n",
      "Iteration 131, loss = 0.06962379\n",
      "Iteration 132, loss = 0.06882839\n",
      "Iteration 133, loss = 0.06803621\n",
      "Iteration 134, loss = 0.06725276\n",
      "Iteration 135, loss = 0.06647361\n",
      "Iteration 136, loss = 0.06570772\n",
      "Iteration 137, loss = 0.06496152\n",
      "Iteration 138, loss = 0.06422863\n",
      "Iteration 139, loss = 0.06351577\n",
      "Iteration 140, loss = 0.06281265\n",
      "Iteration 141, loss = 0.06211301\n",
      "Iteration 142, loss = 0.06141693\n",
      "Iteration 143, loss = 0.06076050\n",
      "Iteration 144, loss = 0.06005723\n",
      "Iteration 145, loss = 0.05939998\n",
      "Iteration 146, loss = 0.05877309\n",
      "Iteration 147, loss = 0.05815787\n",
      "Iteration 148, loss = 0.05767850\n",
      "Iteration 149, loss = 0.05726784\n",
      "Iteration 150, loss = 0.05661363\n",
      "Iteration 151, loss = 0.05605616\n",
      "Iteration 152, loss = 0.05578629\n",
      "Iteration 153, loss = 0.05513907\n",
      "Iteration 154, loss = 0.05487849\n",
      "Iteration 155, loss = 0.05437947\n",
      "Iteration 156, loss = 0.05402909\n",
      "Iteration 157, loss = 0.05368535\n",
      "Iteration 158, loss = 0.05327106\n",
      "Iteration 159, loss = 0.05300777\n",
      "Iteration 160, loss = 0.05257949\n",
      "Iteration 161, loss = 0.05234698\n",
      "Iteration 162, loss = 0.05195357\n",
      "Iteration 163, loss = 0.05171422\n",
      "Iteration 164, loss = 0.05137729\n",
      "Iteration 165, loss = 0.05110930\n",
      "Iteration 166, loss = 0.05082855\n",
      "Iteration 167, loss = 0.05053644\n",
      "Iteration 168, loss = 0.05030274\n",
      "Iteration 169, loss = 0.05000801\n",
      "Iteration 170, loss = 0.04979583\n",
      "Iteration 171, loss = 0.04951907\n",
      "Iteration 172, loss = 0.04930252\n",
      "Iteration 173, loss = 0.04906136\n",
      "Iteration 174, loss = 0.04883339\n",
      "Iteration 175, loss = 0.04862856\n",
      "Iteration 176, loss = 0.04839633\n",
      "Iteration 177, loss = 0.04820755\n",
      "Iteration 178, loss = 0.04799041\n",
      "Iteration 179, loss = 0.04779993\n",
      "Iteration 180, loss = 0.04761078\n",
      "Iteration 181, loss = 0.04741498\n",
      "Iteration 182, loss = 0.04724435\n",
      "Iteration 183, loss = 0.04705713\n",
      "Iteration 184, loss = 0.04688833\n",
      "Iteration 185, loss = 0.04672102\n",
      "Iteration 186, loss = 0.04654799\n",
      "Iteration 187, loss = 0.04639295\n",
      "Iteration 188, loss = 0.04623026\n",
      "Iteration 189, loss = 0.04607476\n",
      "Iteration 190, loss = 0.04592707\n",
      "Iteration 191, loss = 0.04577332\n",
      "Iteration 192, loss = 0.04563038\n",
      "Iteration 193, loss = 0.04549019\n",
      "Iteration 194, loss = 0.04534819\n",
      "Iteration 195, loss = 0.04521508\n",
      "Iteration 196, loss = 0.04508148\n",
      "Iteration 197, loss = 0.04494883\n",
      "Iteration 198, loss = 0.04482336\n",
      "Iteration 199, loss = 0.04469777\n",
      "Iteration 200, loss = 0.04457367\n",
      "Iteration 201, loss = 0.04445518\n",
      "Iteration 202, loss = 0.04433688\n",
      "Iteration 203, loss = 0.04421988\n",
      "Iteration 204, loss = 0.04410733\n",
      "Iteration 205, loss = 0.04399568\n",
      "Iteration 206, loss = 0.04388537\n",
      "Iteration 207, loss = 0.04377877\n",
      "Iteration 208, loss = 0.04367392\n",
      "Iteration 209, loss = 0.04356955\n",
      "Iteration 210, loss = 0.04346767\n",
      "Iteration 211, loss = 0.04336832\n",
      "Iteration 212, loss = 0.04327017\n",
      "Iteration 213, loss = 0.04317336\n",
      "Iteration 214, loss = 0.04307892\n",
      "Iteration 215, loss = 0.04298603\n",
      "Iteration 216, loss = 0.04289425\n",
      "Iteration 217, loss = 0.04280410\n",
      "Iteration 218, loss = 0.04271593\n",
      "Iteration 219, loss = 0.04262912\n",
      "Iteration 220, loss = 0.04254329\n",
      "Iteration 221, loss = 0.04245901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.10699475\n",
      "Iteration 3, loss = 0.91863603\n",
      "Iteration 4, loss = 0.79961857\n",
      "Iteration 5, loss = 0.74649225\n",
      "Iteration 6, loss = 0.74174011\n",
      "Iteration 7, loss = 0.75543673\n",
      "Iteration 8, loss = 0.76407826\n",
      "Iteration 9, loss = 0.76006199\n",
      "Iteration 10, loss = 0.74434368\n",
      "Iteration 11, loss = 0.72042874\n",
      "Iteration 12, loss = 0.69165981\n",
      "Iteration 13, loss = 0.66074814\n",
      "Iteration 14, loss = 0.63027899\n",
      "Iteration 15, loss = 0.60220760\n",
      "Iteration 16, loss = 0.57783076\n",
      "Iteration 17, loss = 0.55826890\n",
      "Iteration 18, loss = 0.54339804\n",
      "Iteration 19, loss = 0.53236738\n",
      "Iteration 20, loss = 0.52498518\n",
      "Iteration 21, loss = 0.52025717\n",
      "Iteration 22, loss = 0.51378762\n",
      "Iteration 23, loss = 0.50551124\n",
      "Iteration 24, loss = 0.49592355\n",
      "Iteration 25, loss = 0.48607422\n",
      "Iteration 26, loss = 0.47584552\n",
      "Iteration 27, loss = 0.46569928\n",
      "Iteration 28, loss = 0.45598851\n",
      "Iteration 29, loss = 0.44811514\n",
      "Iteration 30, loss = 0.44163571\n",
      "Iteration 31, loss = 0.43538036\n",
      "Iteration 32, loss = 0.42920033\n",
      "Iteration 33, loss = 0.42301584\n",
      "Iteration 34, loss = 0.41744580\n",
      "Iteration 35, loss = 0.41271729\n",
      "Iteration 36, loss = 0.40855932\n",
      "Iteration 37, loss = 0.40466424\n",
      "Iteration 38, loss = 0.40077971\n",
      "Iteration 39, loss = 0.39658584\n",
      "Iteration 40, loss = 0.39201534\n",
      "Iteration 41, loss = 0.38719735\n",
      "Iteration 42, loss = 0.38232817\n",
      "Iteration 43, loss = 0.37760923\n",
      "Iteration 44, loss = 0.37316185\n",
      "Iteration 45, loss = 0.36887648\n",
      "Iteration 46, loss = 0.36466588\n",
      "Iteration 47, loss = 0.36044641\n",
      "Iteration 48, loss = 0.35624778\n",
      "Iteration 49, loss = 0.35217943\n",
      "Iteration 50, loss = 0.34833421\n",
      "Iteration 51, loss = 0.34465625\n",
      "Iteration 52, loss = 0.34107240\n",
      "Iteration 53, loss = 0.33744151\n",
      "Iteration 54, loss = 0.33366609\n",
      "Iteration 55, loss = 0.32981794\n",
      "Iteration 56, loss = 0.32603877\n",
      "Iteration 57, loss = 0.32236064\n",
      "Iteration 58, loss = 0.31870912\n",
      "Iteration 59, loss = 0.31495017\n",
      "Iteration 60, loss = 0.31102963\n",
      "Iteration 61, loss = 0.30698901\n",
      "Iteration 62, loss = 0.30294038\n",
      "Iteration 63, loss = 0.29893008\n",
      "Iteration 64, loss = 0.29482149\n",
      "Iteration 65, loss = 0.29070229\n",
      "Iteration 66, loss = 0.28654929\n",
      "Iteration 67, loss = 0.28229130\n",
      "Iteration 68, loss = 0.27784550\n",
      "Iteration 69, loss = 0.27323991\n",
      "Iteration 70, loss = 0.26857165\n",
      "Iteration 71, loss = 0.26387936\n",
      "Iteration 72, loss = 0.25912126\n",
      "Iteration 73, loss = 0.25425972\n",
      "Iteration 74, loss = 0.24940212\n",
      "Iteration 75, loss = 0.24456895\n",
      "Iteration 76, loss = 0.23969175\n",
      "Iteration 77, loss = 0.23481546\n",
      "Iteration 78, loss = 0.23000444\n",
      "Iteration 79, loss = 0.22518200\n",
      "Iteration 80, loss = 0.22043822\n",
      "Iteration 81, loss = 0.21566345\n",
      "Iteration 82, loss = 0.21097802\n",
      "Iteration 83, loss = 0.20636383\n",
      "Iteration 84, loss = 0.20184423\n",
      "Iteration 85, loss = 0.19740856\n",
      "Iteration 86, loss = 0.19305432\n",
      "Iteration 87, loss = 0.18881105\n",
      "Iteration 88, loss = 0.18467859\n",
      "Iteration 89, loss = 0.18066679\n",
      "Iteration 90, loss = 0.17675430\n",
      "Iteration 91, loss = 0.17291986\n",
      "Iteration 92, loss = 0.16918761\n",
      "Iteration 93, loss = 0.16553305\n",
      "Iteration 94, loss = 0.16197851\n",
      "Iteration 95, loss = 0.15847309\n",
      "Iteration 96, loss = 0.15507273\n",
      "Iteration 97, loss = 0.15173079\n",
      "Iteration 98, loss = 0.14842633\n",
      "Iteration 99, loss = 0.14518169\n",
      "Iteration 100, loss = 0.14205400\n",
      "Iteration 101, loss = 0.13902345\n",
      "Iteration 102, loss = 0.13604379\n",
      "Iteration 103, loss = 0.13320443\n",
      "Iteration 104, loss = 0.13043119\n",
      "Iteration 105, loss = 0.12773165\n",
      "Iteration 106, loss = 0.12514369\n",
      "Iteration 107, loss = 0.12269297\n",
      "Iteration 108, loss = 0.12027555\n",
      "Iteration 109, loss = 0.11799985\n",
      "Iteration 110, loss = 0.11583431\n",
      "Iteration 111, loss = 0.11375128\n",
      "Iteration 112, loss = 0.11176735\n",
      "Iteration 113, loss = 0.10989737\n",
      "Iteration 114, loss = 0.10812639\n",
      "Iteration 115, loss = 0.10644098\n",
      "Iteration 116, loss = 0.10483862\n",
      "Iteration 117, loss = 0.10333295\n",
      "Iteration 118, loss = 0.10191266\n",
      "Iteration 119, loss = 0.10056648\n",
      "Iteration 120, loss = 0.09929310\n",
      "Iteration 121, loss = 0.09810222\n",
      "Iteration 122, loss = 0.09699664\n",
      "Iteration 123, loss = 0.09603032\n",
      "Iteration 124, loss = 0.09510056\n",
      "Iteration 125, loss = 0.09414574\n",
      "Iteration 126, loss = 0.09321035\n",
      "Iteration 127, loss = 0.09250319\n",
      "Iteration 128, loss = 0.09178194\n",
      "Iteration 129, loss = 0.09097886\n",
      "Iteration 130, loss = 0.09037797\n",
      "Iteration 131, loss = 0.08977646\n",
      "Iteration 132, loss = 0.08910921\n",
      "Iteration 133, loss = 0.08860848\n",
      "Iteration 134, loss = 0.08808370\n",
      "Iteration 135, loss = 0.08752883\n",
      "Iteration 136, loss = 0.08710850\n",
      "Iteration 137, loss = 0.08664591\n",
      "Iteration 138, loss = 0.08618415\n",
      "Iteration 139, loss = 0.08582216\n",
      "Iteration 140, loss = 0.08541482\n",
      "Iteration 141, loss = 0.08502568\n",
      "Iteration 142, loss = 0.08470809\n",
      "Iteration 143, loss = 0.08435198\n",
      "Iteration 144, loss = 0.08401785\n",
      "Iteration 145, loss = 0.08373603\n",
      "Iteration 146, loss = 0.08342548\n",
      "Iteration 147, loss = 0.08313255\n",
      "Iteration 148, loss = 0.08288111\n",
      "Iteration 149, loss = 0.08261057\n",
      "Iteration 150, loss = 0.08234868\n",
      "Iteration 151, loss = 0.08212226\n",
      "Iteration 152, loss = 0.08188665\n",
      "Iteration 153, loss = 0.08165065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 154, loss = 0.08144330\n",
      "Iteration 155, loss = 0.08123722\n",
      "Iteration 156, loss = 0.08102399\n",
      "Iteration 157, loss = 0.08082976\n",
      "Iteration 158, loss = 0.08064640\n",
      "Iteration 159, loss = 0.08045628\n",
      "Iteration 160, loss = 0.08027293\n",
      "Iteration 161, loss = 0.08010461\n",
      "Iteration 162, loss = 0.07993691\n",
      "Iteration 163, loss = 0.07976737\n",
      "Iteration 164, loss = 0.07960730\n",
      "Iteration 165, loss = 0.07945546\n",
      "Iteration 166, loss = 0.07930504\n",
      "Iteration 167, loss = 0.07915508\n",
      "Iteration 168, loss = 0.07901203\n",
      "Iteration 169, loss = 0.07887424\n",
      "Iteration 170, loss = 0.07873635\n",
      "Iteration 171, loss = 0.07859894\n",
      "Iteration 172, loss = 0.07846616\n",
      "Iteration 173, loss = 0.07833800\n",
      "Iteration 174, loss = 0.07821129\n",
      "Iteration 175, loss = 0.07808498\n",
      "Iteration 176, loss = 0.07796254\n",
      "Iteration 177, loss = 0.07784383\n",
      "Iteration 178, loss = 0.07772807\n",
      "Iteration 179, loss = 0.07761357\n",
      "Iteration 180, loss = 0.07749990\n",
      "Iteration 181, loss = 0.07738798\n",
      "Iteration 182, loss = 0.07727860\n",
      "Iteration 183, loss = 0.07717162\n",
      "Iteration 184, loss = 0.07706630\n",
      "Iteration 185, loss = 0.07696217\n",
      "Iteration 186, loss = 0.07685909\n",
      "Iteration 187, loss = 0.07675855\n",
      "Iteration 188, loss = 0.07665943\n",
      "Iteration 189, loss = 0.07656180\n",
      "Iteration 190, loss = 0.07646552\n",
      "Iteration 191, loss = 0.07637035\n",
      "Iteration 192, loss = 0.07627651\n",
      "Iteration 193, loss = 0.07618446\n",
      "Iteration 194, loss = 0.07609349\n",
      "Iteration 195, loss = 0.07600354\n",
      "Iteration 196, loss = 0.07591467\n",
      "Iteration 197, loss = 0.07582685\n",
      "Iteration 198, loss = 0.07574010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.03891745\n",
      "Iteration 3, loss = 0.83917146\n",
      "Iteration 4, loss = 0.77333751\n",
      "Iteration 5, loss = 0.75961469\n",
      "Iteration 6, loss = 0.76529405\n",
      "Iteration 7, loss = 0.74912679\n",
      "Iteration 8, loss = 0.71361679\n",
      "Iteration 9, loss = 0.67372739\n",
      "Iteration 10, loss = 0.63932786\n",
      "Iteration 11, loss = 0.61466974\n",
      "Iteration 12, loss = 0.59880927\n",
      "Iteration 13, loss = 0.58701298\n",
      "Iteration 14, loss = 0.57505826\n",
      "Iteration 15, loss = 0.56122584\n",
      "Iteration 16, loss = 0.54632944\n",
      "Iteration 17, loss = 0.53190532\n",
      "Iteration 18, loss = 0.51920328\n",
      "Iteration 19, loss = 0.50846125\n",
      "Iteration 20, loss = 0.49926207\n",
      "Iteration 21, loss = 0.49086139\n",
      "Iteration 22, loss = 0.48286064\n",
      "Iteration 23, loss = 0.47508968\n",
      "Iteration 24, loss = 0.46763812\n",
      "Iteration 25, loss = 0.46064106\n",
      "Iteration 26, loss = 0.45413540\n",
      "Iteration 27, loss = 0.44809128\n",
      "Iteration 28, loss = 0.44235274\n",
      "Iteration 29, loss = 0.43684694\n",
      "Iteration 30, loss = 0.43157091\n",
      "Iteration 31, loss = 0.42652390\n",
      "Iteration 32, loss = 0.42165811\n",
      "Iteration 33, loss = 0.41692826\n",
      "Iteration 34, loss = 0.41236757\n",
      "Iteration 35, loss = 0.40794393\n",
      "Iteration 36, loss = 0.40364881\n",
      "Iteration 37, loss = 0.39945252\n",
      "Iteration 38, loss = 0.39534733\n",
      "Iteration 39, loss = 0.39132530\n",
      "Iteration 40, loss = 0.38737844\n",
      "Iteration 41, loss = 0.38349915\n",
      "Iteration 42, loss = 0.37968068\n",
      "Iteration 43, loss = 0.37591729\n",
      "Iteration 44, loss = 0.37220595\n",
      "Iteration 45, loss = 0.36854164\n",
      "Iteration 46, loss = 0.36492131\n",
      "Iteration 47, loss = 0.36134789\n",
      "Iteration 48, loss = 0.35782417\n",
      "Iteration 49, loss = 0.35433920\n",
      "Iteration 50, loss = 0.35089174\n",
      "Iteration 51, loss = 0.34748078\n",
      "Iteration 52, loss = 0.34412020\n",
      "Iteration 53, loss = 0.34079359\n",
      "Iteration 54, loss = 0.33749535\n",
      "Iteration 55, loss = 0.33423015\n",
      "Iteration 56, loss = 0.33099723\n",
      "Iteration 57, loss = 0.32779647\n",
      "Iteration 58, loss = 0.32463144\n",
      "Iteration 59, loss = 0.32150157\n",
      "Iteration 60, loss = 0.31840435\n",
      "Iteration 61, loss = 0.31533979\n",
      "Iteration 62, loss = 0.31230457\n",
      "Iteration 63, loss = 0.30929801\n",
      "Iteration 64, loss = 0.30632470\n",
      "Iteration 65, loss = 0.30338255\n",
      "Iteration 66, loss = 0.30046474\n",
      "Iteration 67, loss = 0.29758048\n",
      "Iteration 68, loss = 0.29473424\n",
      "Iteration 69, loss = 0.29192642\n",
      "Iteration 70, loss = 0.28915876\n",
      "Iteration 71, loss = 0.28642830\n",
      "Iteration 72, loss = 0.28374344\n",
      "Iteration 73, loss = 0.28109322\n",
      "Iteration 74, loss = 0.27848074\n",
      "Iteration 75, loss = 0.27591037\n",
      "Iteration 76, loss = 0.27337569\n",
      "Iteration 77, loss = 0.27087427\n",
      "Iteration 78, loss = 0.26841261\n",
      "Iteration 79, loss = 0.26598397\n",
      "Iteration 80, loss = 0.26358900\n",
      "Iteration 81, loss = 0.26123341\n",
      "Iteration 82, loss = 0.25891471\n",
      "Iteration 83, loss = 0.25662826\n",
      "Iteration 84, loss = 0.25437260\n",
      "Iteration 85, loss = 0.25215167\n",
      "Iteration 86, loss = 0.24996642\n",
      "Iteration 87, loss = 0.24781984\n",
      "Iteration 88, loss = 0.24571169\n",
      "Iteration 89, loss = 0.24364013\n",
      "Iteration 90, loss = 0.24160214\n",
      "Iteration 91, loss = 0.23959742\n",
      "Iteration 92, loss = 0.23762726\n",
      "Iteration 93, loss = 0.23569108\n",
      "Iteration 94, loss = 0.23378958\n",
      "Iteration 95, loss = 0.23191996\n",
      "Iteration 96, loss = 0.23008179\n",
      "Iteration 97, loss = 0.22827425\n",
      "Iteration 98, loss = 0.22649755\n",
      "Iteration 99, loss = 0.22475107\n",
      "Iteration 100, loss = 0.22303443\n",
      "Iteration 101, loss = 0.22134748\n",
      "Iteration 102, loss = 0.21969003\n",
      "Iteration 103, loss = 0.21806215\n",
      "Iteration 104, loss = 0.21646346\n",
      "Iteration 105, loss = 0.21489288\n",
      "Iteration 106, loss = 0.21334989\n",
      "Iteration 107, loss = 0.21183391\n",
      "Iteration 108, loss = 0.21034472\n",
      "Iteration 109, loss = 0.20888142\n",
      "Iteration 110, loss = 0.20744387\n",
      "Iteration 111, loss = 0.20603161\n",
      "Iteration 112, loss = 0.20464421\n",
      "Iteration 113, loss = 0.20328108\n",
      "Iteration 114, loss = 0.20194197\n",
      "Iteration 115, loss = 0.20062634\n",
      "Iteration 116, loss = 0.19933398\n",
      "Iteration 117, loss = 0.19806395\n",
      "Iteration 118, loss = 0.19681627\n",
      "Iteration 119, loss = 0.19559050\n",
      "Iteration 120, loss = 0.19438842\n",
      "Iteration 121, loss = 0.19320753\n",
      "Iteration 122, loss = 0.19204727\n",
      "Iteration 123, loss = 0.19090715\n",
      "Iteration 124, loss = 0.18978674\n",
      "Iteration 125, loss = 0.18868569\n",
      "Iteration 126, loss = 0.18760361\n",
      "Iteration 127, loss = 0.18654010\n",
      "Iteration 128, loss = 0.18549477\n",
      "Iteration 129, loss = 0.18446725\n",
      "Iteration 130, loss = 0.18345718\n",
      "Iteration 131, loss = 0.18246422\n",
      "Iteration 132, loss = 0.18148796\n",
      "Iteration 133, loss = 0.18052810\n",
      "Iteration 134, loss = 0.17958430\n",
      "Iteration 135, loss = 0.17865620\n",
      "Iteration 136, loss = 0.17774348\n",
      "Iteration 137, loss = 0.17684582\n",
      "Iteration 138, loss = 0.17596295\n",
      "Iteration 139, loss = 0.17509445\n",
      "Iteration 140, loss = 0.17424010\n",
      "Iteration 141, loss = 0.17339960\n",
      "Iteration 142, loss = 0.17257266\n",
      "Iteration 143, loss = 0.17175897\n",
      "Iteration 144, loss = 0.17095857\n",
      "Iteration 145, loss = 0.17017081\n",
      "Iteration 146, loss = 0.16939580\n",
      "Iteration 147, loss = 0.16863346\n",
      "Iteration 148, loss = 0.16788340\n",
      "Iteration 149, loss = 0.16714490\n",
      "Iteration 150, loss = 0.16641768\n",
      "Iteration 151, loss = 0.16570169\n",
      "Iteration 152, loss = 0.16499692\n",
      "Iteration 153, loss = 0.16430280\n",
      "Iteration 154, loss = 0.16361948\n",
      "Iteration 155, loss = 0.16294669\n",
      "Iteration 156, loss = 0.16228398\n",
      "Iteration 157, loss = 0.16163128\n",
      "Iteration 158, loss = 0.16098823\n",
      "Iteration 159, loss = 0.16035489\n",
      "Iteration 160, loss = 0.15973095\n",
      "Iteration 161, loss = 0.15911604\n",
      "Iteration 162, loss = 0.15851033\n",
      "Iteration 163, loss = 0.15791330\n",
      "Iteration 164, loss = 0.15732504\n",
      "Iteration 165, loss = 0.15674540\n",
      "Iteration 166, loss = 0.15617435\n",
      "Iteration 167, loss = 0.15561125\n",
      "Iteration 168, loss = 0.15505621\n",
      "Iteration 169, loss = 0.15450912\n",
      "Iteration 170, loss = 0.15396965\n",
      "Iteration 171, loss = 0.15343767\n",
      "Iteration 172, loss = 0.15291310\n",
      "Iteration 173, loss = 0.15239579\n",
      "Iteration 174, loss = 0.15188557\n",
      "Iteration 175, loss = 0.15138233\n",
      "Iteration 176, loss = 0.15088593\n",
      "Iteration 177, loss = 0.15039623\n",
      "Iteration 178, loss = 0.14991310\n",
      "Iteration 179, loss = 0.14943645\n",
      "Iteration 180, loss = 0.14896610\n",
      "Iteration 181, loss = 0.14850196\n",
      "Iteration 182, loss = 0.14804396\n",
      "Iteration 183, loss = 0.14759188\n",
      "Iteration 184, loss = 0.14714570\n",
      "Iteration 185, loss = 0.14670528\n",
      "Iteration 186, loss = 0.14627050\n",
      "Iteration 187, loss = 0.14584126\n",
      "Iteration 188, loss = 0.14541749\n",
      "Iteration 189, loss = 0.14499904\n",
      "Iteration 190, loss = 0.14458584\n",
      "Iteration 191, loss = 0.14417782\n",
      "Iteration 192, loss = 0.14377484\n",
      "Iteration 193, loss = 0.14337683\n",
      "Iteration 194, loss = 0.14298369\n",
      "Iteration 195, loss = 0.14259536\n",
      "Iteration 196, loss = 0.14221173\n",
      "Iteration 197, loss = 0.14183272\n",
      "Iteration 198, loss = 0.14145828\n",
      "Iteration 199, loss = 0.14108826\n",
      "Iteration 200, loss = 0.14072266\n",
      "Iteration 201, loss = 0.14036137\n",
      "Iteration 202, loss = 0.14000430\n",
      "Iteration 203, loss = 0.13965140\n",
      "Iteration 204, loss = 0.13930264\n",
      "Iteration 205, loss = 0.13895799\n",
      "Iteration 206, loss = 0.13861725\n",
      "Iteration 207, loss = 0.13828037\n",
      "Iteration 208, loss = 0.13794739\n",
      "Iteration 209, loss = 0.13761816\n",
      "Iteration 210, loss = 0.13729261\n",
      "Iteration 211, loss = 0.13697076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 212, loss = 0.13665244\n",
      "Iteration 213, loss = 0.13633774\n",
      "Iteration 214, loss = 0.13602641\n",
      "Iteration 215, loss = 0.13571856\n",
      "Iteration 216, loss = 0.13541404\n",
      "Iteration 217, loss = 0.13511284\n",
      "Iteration 218, loss = 0.13481493\n",
      "Iteration 219, loss = 0.13452017\n",
      "Iteration 220, loss = 0.13422862\n",
      "Iteration 221, loss = 0.13394018\n",
      "Iteration 222, loss = 0.13365487\n",
      "Iteration 223, loss = 0.13337256\n",
      "Iteration 224, loss = 0.13309324\n",
      "Iteration 225, loss = 0.13281684\n",
      "Iteration 226, loss = 0.13254335\n",
      "Iteration 227, loss = 0.13227266\n",
      "Iteration 228, loss = 0.13200474\n",
      "Iteration 229, loss = 0.13173959\n",
      "Iteration 230, loss = 0.13147710\n",
      "Iteration 231, loss = 0.13121728\n",
      "Iteration 232, loss = 0.13096006\n",
      "Iteration 233, loss = 0.13070543\n",
      "Iteration 234, loss = 0.13045337\n",
      "Iteration 235, loss = 0.13020377\n",
      "Iteration 236, loss = 0.12995665\n",
      "Iteration 237, loss = 0.12971197\n",
      "Iteration 238, loss = 0.12946967\n",
      "Iteration 239, loss = 0.12922977\n",
      "Iteration 240, loss = 0.12899216\n",
      "Iteration 241, loss = 0.12875688\n",
      "Iteration 242, loss = 0.12852388\n",
      "Iteration 243, loss = 0.12829310\n",
      "Iteration 244, loss = 0.12806450\n",
      "Iteration 245, loss = 0.12783811\n",
      "Iteration 246, loss = 0.12761381\n",
      "Iteration 247, loss = 0.12739165\n",
      "Iteration 248, loss = 0.12717158\n",
      "Iteration 249, loss = 0.12695354\n",
      "Iteration 250, loss = 0.12673751\n",
      "Iteration 251, loss = 0.12652348\n",
      "Iteration 252, loss = 0.12631146\n",
      "Iteration 253, loss = 0.12610131\n",
      "Iteration 254, loss = 0.12589313\n",
      "Iteration 255, loss = 0.12568683\n",
      "Iteration 256, loss = 0.12548238\n",
      "Iteration 257, loss = 0.12527977\n",
      "Iteration 258, loss = 0.12507897\n",
      "Iteration 259, loss = 0.12487997\n",
      "Iteration 260, loss = 0.12468274\n",
      "Iteration 261, loss = 0.12448725\n",
      "Iteration 262, loss = 0.12429348\n",
      "Iteration 263, loss = 0.12410140\n",
      "Iteration 264, loss = 0.12391103\n",
      "Iteration 265, loss = 0.12372229\n",
      "Iteration 266, loss = 0.12353518\n",
      "Iteration 267, loss = 0.12334968\n",
      "Iteration 268, loss = 0.12316579\n",
      "Iteration 269, loss = 0.12298346\n",
      "Iteration 270, loss = 0.12280268\n",
      "Iteration 271, loss = 0.12262346\n",
      "Iteration 272, loss = 0.12244570\n",
      "Iteration 273, loss = 0.12226946\n",
      "Iteration 274, loss = 0.12209478\n",
      "Iteration 275, loss = 0.12192166\n",
      "Iteration 276, loss = 0.12175002\n",
      "Iteration 277, loss = 0.12157979\n",
      "Iteration 278, loss = 0.12141098\n",
      "Iteration 279, loss = 0.12124359\n",
      "Iteration 280, loss = 0.12107752\n",
      "Iteration 281, loss = 0.12091283\n",
      "Iteration 282, loss = 0.12074947\n",
      "Iteration 283, loss = 0.12058745\n",
      "Iteration 284, loss = 0.12042673\n",
      "Iteration 285, loss = 0.12026730\n",
      "Iteration 286, loss = 0.12010913\n",
      "Iteration 287, loss = 0.11995223\n",
      "Iteration 288, loss = 0.11979657\n",
      "Iteration 289, loss = 0.11964215\n",
      "Iteration 290, loss = 0.11948893\n",
      "Iteration 291, loss = 0.11933692\n",
      "Iteration 292, loss = 0.11918608\n",
      "Iteration 293, loss = 0.11903642\n",
      "Iteration 294, loss = 0.11888792\n",
      "Iteration 295, loss = 0.11874058\n",
      "Iteration 296, loss = 0.11859435\n",
      "Iteration 297, loss = 0.11844924\n",
      "Iteration 298, loss = 0.11830523\n",
      "Iteration 299, loss = 0.11816232\n",
      "Iteration 300, loss = 0.11802048\n",
      "Iteration 301, loss = 0.11787971\n",
      "Iteration 302, loss = 0.11774003\n",
      "Iteration 303, loss = 0.11760133\n",
      "Iteration 304, loss = 0.11746370\n",
      "Iteration 305, loss = 0.11732717\n",
      "Iteration 306, loss = 0.11719163\n",
      "Iteration 307, loss = 0.11705707\n",
      "Iteration 308, loss = 0.11692347\n",
      "Iteration 309, loss = 0.11679084\n",
      "Iteration 310, loss = 0.11665914\n",
      "Iteration 311, loss = 0.11652848\n",
      "Iteration 312, loss = 0.11639873\n",
      "Iteration 313, loss = 0.11626992\n",
      "Iteration 314, loss = 0.11614203\n",
      "Iteration 315, loss = 0.11601505\n",
      "Iteration 316, loss = 0.11588896\n",
      "Iteration 317, loss = 0.11576377\n",
      "Iteration 318, loss = 0.11563946\n",
      "Iteration 319, loss = 0.11551604\n",
      "Iteration 320, loss = 0.11539347\n",
      "Iteration 321, loss = 0.11527181\n",
      "Iteration 322, loss = 0.11515093\n",
      "Iteration 323, loss = 0.11503093\n",
      "Iteration 324, loss = 0.11491174\n",
      "Iteration 325, loss = 0.11479336\n",
      "Iteration 326, loss = 0.11467581\n",
      "Iteration 327, loss = 0.11455907\n",
      "Iteration 328, loss = 0.11444311\n",
      "Iteration 329, loss = 0.11432794\n",
      "Iteration 330, loss = 0.11421357\n",
      "Iteration 331, loss = 0.11409995\n",
      "Iteration 332, loss = 0.11398711\n",
      "Iteration 333, loss = 0.11387507\n",
      "Iteration 334, loss = 0.11376378\n",
      "Iteration 335, loss = 0.11365322\n",
      "Iteration 336, loss = 0.11354340\n",
      "Iteration 337, loss = 0.11343433\n",
      "Iteration 338, loss = 0.11332595\n",
      "Iteration 339, loss = 0.11321829\n",
      "Iteration 340, loss = 0.11311135\n",
      "Iteration 341, loss = 0.11300510\n",
      "Iteration 342, loss = 0.11289954\n",
      "Iteration 343, loss = 0.11279466\n",
      "Iteration 344, loss = 0.11269047\n",
      "Iteration 345, loss = 0.11258694\n",
      "Iteration 346, loss = 0.11248407\n",
      "Iteration 347, loss = 0.11238188\n",
      "Iteration 348, loss = 0.11228032\n",
      "Iteration 349, loss = 0.11217940\n",
      "Iteration 350, loss = 0.11207915\n",
      "Iteration 351, loss = 0.11197950\n",
      "Iteration 352, loss = 0.11188048\n",
      "Iteration 353, loss = 0.11178211\n",
      "Iteration 354, loss = 0.11168433\n",
      "Iteration 355, loss = 0.11158715\n",
      "Iteration 356, loss = 0.11149059\n",
      "Iteration 357, loss = 0.11139463\n",
      "Iteration 358, loss = 0.11129925\n",
      "Iteration 359, loss = 0.11120445\n",
      "Iteration 360, loss = 0.11111025\n",
      "Iteration 361, loss = 0.11101660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.04594581\n",
      "Iteration 3, loss = 0.84704508\n",
      "Iteration 4, loss = 0.78052835\n",
      "Iteration 5, loss = 0.76511094\n",
      "Iteration 6, loss = 0.76931331\n",
      "Iteration 7, loss = 0.75294903\n",
      "Iteration 8, loss = 0.71688566\n",
      "Iteration 9, loss = 0.67641070\n",
      "Iteration 10, loss = 0.64163133\n",
      "Iteration 11, loss = 0.61706966\n",
      "Iteration 12, loss = 0.60116979\n",
      "Iteration 13, loss = 0.58899206\n",
      "Iteration 14, loss = 0.57633866\n",
      "Iteration 15, loss = 0.56170033\n",
      "Iteration 16, loss = 0.54626608\n",
      "Iteration 17, loss = 0.53145672\n",
      "Iteration 18, loss = 0.51840700\n",
      "Iteration 19, loss = 0.50736574\n",
      "Iteration 20, loss = 0.49781331\n",
      "Iteration 21, loss = 0.48898704\n",
      "Iteration 22, loss = 0.48052493\n",
      "Iteration 23, loss = 0.47230650\n",
      "Iteration 24, loss = 0.46443305\n",
      "Iteration 25, loss = 0.45703472\n",
      "Iteration 26, loss = 0.45021803\n",
      "Iteration 27, loss = 0.44389943\n",
      "Iteration 28, loss = 0.43792114\n",
      "Iteration 29, loss = 0.43215424\n",
      "Iteration 30, loss = 0.42657045\n",
      "Iteration 31, loss = 0.42115218\n",
      "Iteration 32, loss = 0.41588954\n",
      "Iteration 33, loss = 0.41078009\n",
      "Iteration 34, loss = 0.40587526\n",
      "Iteration 35, loss = 0.40114685\n",
      "Iteration 36, loss = 0.39653032\n",
      "Iteration 37, loss = 0.39201367\n",
      "Iteration 38, loss = 0.38761191\n",
      "Iteration 39, loss = 0.38330607\n",
      "Iteration 40, loss = 0.37907936\n",
      "Iteration 41, loss = 0.37492529\n",
      "Iteration 42, loss = 0.37083812\n",
      "Iteration 43, loss = 0.36681303\n",
      "Iteration 44, loss = 0.36284604\n",
      "Iteration 45, loss = 0.35893384\n",
      "Iteration 46, loss = 0.35507381\n",
      "Iteration 47, loss = 0.35129475\n",
      "Iteration 48, loss = 0.34756304\n",
      "Iteration 49, loss = 0.34387664\n",
      "Iteration 50, loss = 0.34023434\n",
      "Iteration 51, loss = 0.33663556\n",
      "Iteration 52, loss = 0.33309985\n",
      "Iteration 53, loss = 0.32959507\n",
      "Iteration 54, loss = 0.32611957\n",
      "Iteration 55, loss = 0.32267664\n",
      "Iteration 56, loss = 0.31926866\n",
      "Iteration 57, loss = 0.31589837\n",
      "Iteration 58, loss = 0.31257134\n",
      "Iteration 59, loss = 0.30928601\n",
      "Iteration 60, loss = 0.30604181\n",
      "Iteration 61, loss = 0.30283743\n",
      "Iteration 62, loss = 0.29967191\n",
      "Iteration 63, loss = 0.29654657\n",
      "Iteration 64, loss = 0.29345835\n",
      "Iteration 65, loss = 0.29040524\n",
      "Iteration 66, loss = 0.28739180\n",
      "Iteration 67, loss = 0.28441962\n",
      "Iteration 68, loss = 0.28148479\n",
      "Iteration 69, loss = 0.27858965\n",
      "Iteration 70, loss = 0.27573846\n",
      "Iteration 71, loss = 0.27292902\n",
      "Iteration 72, loss = 0.27016339\n",
      "Iteration 73, loss = 0.26744336\n",
      "Iteration 74, loss = 0.26476506\n",
      "Iteration 75, loss = 0.26213123\n",
      "Iteration 76, loss = 0.25953777\n",
      "Iteration 77, loss = 0.25699003\n",
      "Iteration 78, loss = 0.25448267\n",
      "Iteration 79, loss = 0.25201164\n",
      "Iteration 80, loss = 0.24958099\n",
      "Iteration 81, loss = 0.24719064\n",
      "Iteration 82, loss = 0.24484983\n",
      "Iteration 83, loss = 0.24254778\n",
      "Iteration 84, loss = 0.24028660\n",
      "Iteration 85, loss = 0.23806585\n",
      "Iteration 86, loss = 0.23588872\n",
      "Iteration 87, loss = 0.23375429\n",
      "Iteration 88, loss = 0.23165597\n",
      "Iteration 89, loss = 0.22959658\n",
      "Iteration 90, loss = 0.22757736\n",
      "Iteration 91, loss = 0.22559697\n",
      "Iteration 92, loss = 0.22365325\n",
      "Iteration 93, loss = 0.22174461\n",
      "Iteration 94, loss = 0.21987137\n",
      "Iteration 95, loss = 0.21803285\n",
      "Iteration 96, loss = 0.21622737\n",
      "Iteration 97, loss = 0.21445498\n",
      "Iteration 98, loss = 0.21271524\n",
      "Iteration 99, loss = 0.21100760\n",
      "Iteration 100, loss = 0.20933273\n",
      "Iteration 101, loss = 0.20768818\n",
      "Iteration 102, loss = 0.20607387\n",
      "Iteration 103, loss = 0.20448924\n",
      "Iteration 104, loss = 0.20293374\n",
      "Iteration 105, loss = 0.20140697\n",
      "Iteration 106, loss = 0.19990962\n",
      "Iteration 107, loss = 0.19844023\n",
      "Iteration 108, loss = 0.19699811\n",
      "Iteration 109, loss = 0.19558263\n",
      "Iteration 110, loss = 0.19419336\n",
      "Iteration 111, loss = 0.19282983\n",
      "Iteration 112, loss = 0.19149227\n",
      "Iteration 113, loss = 0.19017936\n",
      "Iteration 114, loss = 0.18889071\n",
      "Iteration 115, loss = 0.18762584\n",
      "Iteration 116, loss = 0.18638453\n",
      "Iteration 117, loss = 0.18516599\n",
      "Iteration 118, loss = 0.18396976\n",
      "Iteration 119, loss = 0.18279537\n",
      "Iteration 120, loss = 0.18164235\n",
      "Iteration 121, loss = 0.18051027\n",
      "Iteration 122, loss = 0.17939871\n",
      "Iteration 123, loss = 0.17830730\n",
      "Iteration 124, loss = 0.17723548\n",
      "Iteration 125, loss = 0.17618294\n",
      "Iteration 126, loss = 0.17514912\n",
      "Iteration 127, loss = 0.17413366\n",
      "Iteration 128, loss = 0.17313619\n",
      "Iteration 129, loss = 0.17215768\n",
      "Iteration 130, loss = 0.17119729\n",
      "Iteration 131, loss = 0.17025468\n",
      "Iteration 132, loss = 0.16932860\n",
      "Iteration 133, loss = 0.16841827\n",
      "Iteration 134, loss = 0.16752344\n",
      "Iteration 135, loss = 0.16664403\n",
      "Iteration 136, loss = 0.16577963\n",
      "Iteration 137, loss = 0.16492992\n",
      "Iteration 138, loss = 0.16409458\n",
      "Iteration 139, loss = 0.16327334\n",
      "Iteration 140, loss = 0.16246597\n",
      "Iteration 141, loss = 0.16167214\n",
      "Iteration 142, loss = 0.16089157\n",
      "Iteration 143, loss = 0.16012425\n",
      "Iteration 144, loss = 0.15936974\n",
      "Iteration 145, loss = 0.15862777\n",
      "Iteration 146, loss = 0.15789788\n",
      "Iteration 147, loss = 0.15717975\n",
      "Iteration 148, loss = 0.15647317\n",
      "Iteration 149, loss = 0.15577789\n",
      "Iteration 150, loss = 0.15509368\n",
      "Iteration 151, loss = 0.15442035\n",
      "Iteration 152, loss = 0.15375784\n",
      "Iteration 153, loss = 0.15310610\n",
      "Iteration 154, loss = 0.15246454\n",
      "Iteration 155, loss = 0.15183300\n",
      "Iteration 156, loss = 0.15121120\n",
      "Iteration 157, loss = 0.15059894\n",
      "Iteration 158, loss = 0.14999603\n",
      "Iteration 159, loss = 0.14940229\n",
      "Iteration 160, loss = 0.14881751\n",
      "Iteration 161, loss = 0.14824160\n",
      "Iteration 162, loss = 0.14767429\n",
      "Iteration 163, loss = 0.14711541\n",
      "Iteration 164, loss = 0.14656478\n",
      "Iteration 165, loss = 0.14602222\n",
      "Iteration 166, loss = 0.14548757\n",
      "Iteration 167, loss = 0.14496069\n",
      "Iteration 168, loss = 0.14444141\n",
      "Iteration 169, loss = 0.14392957\n",
      "Iteration 170, loss = 0.14342504\n",
      "Iteration 171, loss = 0.14292766\n",
      "Iteration 172, loss = 0.14243729\n",
      "Iteration 173, loss = 0.14195380\n",
      "Iteration 174, loss = 0.14147705\n",
      "Iteration 175, loss = 0.14100690\n",
      "Iteration 176, loss = 0.14054323\n",
      "Iteration 177, loss = 0.14008592\n",
      "Iteration 178, loss = 0.13963483\n",
      "Iteration 179, loss = 0.13918986\n",
      "Iteration 180, loss = 0.13875085\n",
      "Iteration 181, loss = 0.13831773\n",
      "Iteration 182, loss = 0.13789038\n",
      "Iteration 183, loss = 0.13746867\n",
      "Iteration 184, loss = 0.13705251\n",
      "Iteration 185, loss = 0.13664181\n",
      "Iteration 186, loss = 0.13623644\n",
      "Iteration 187, loss = 0.13583632\n",
      "Iteration 188, loss = 0.13544133\n",
      "Iteration 189, loss = 0.13505141\n",
      "Iteration 190, loss = 0.13466643\n",
      "Iteration 191, loss = 0.13428632\n",
      "Iteration 192, loss = 0.13391100\n",
      "Iteration 193, loss = 0.13354037\n",
      "Iteration 194, loss = 0.13317434\n",
      "Iteration 195, loss = 0.13281284\n",
      "Iteration 196, loss = 0.13245579\n",
      "Iteration 197, loss = 0.13210310\n",
      "Iteration 198, loss = 0.13175470\n",
      "Iteration 199, loss = 0.13141052\n",
      "Iteration 200, loss = 0.13107048\n",
      "Iteration 201, loss = 0.13073465\n",
      "Iteration 202, loss = 0.13040291\n",
      "Iteration 203, loss = 0.13007512\n",
      "Iteration 204, loss = 0.12975132\n",
      "Iteration 205, loss = 0.12943133\n",
      "Iteration 206, loss = 0.12911509\n",
      "Iteration 207, loss = 0.12880252\n",
      "Iteration 208, loss = 0.12849357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 209, loss = 0.12818817\n",
      "Iteration 210, loss = 0.12788627\n",
      "Iteration 211, loss = 0.12758779\n",
      "Iteration 212, loss = 0.12729269\n",
      "Iteration 213, loss = 0.12700091\n",
      "Iteration 214, loss = 0.12671240\n",
      "Iteration 215, loss = 0.12642709\n",
      "Iteration 216, loss = 0.12614495\n",
      "Iteration 217, loss = 0.12586591\n",
      "Iteration 218, loss = 0.12558992\n",
      "Iteration 219, loss = 0.12531695\n",
      "Iteration 220, loss = 0.12504693\n",
      "Iteration 221, loss = 0.12477982\n",
      "Iteration 222, loss = 0.12451558\n",
      "Iteration 223, loss = 0.12425415\n",
      "Iteration 224, loss = 0.12399550\n",
      "Iteration 225, loss = 0.12373959\n",
      "Iteration 226, loss = 0.12348635\n",
      "Iteration 227, loss = 0.12323577\n",
      "Iteration 228, loss = 0.12298779\n",
      "Iteration 229, loss = 0.12274238\n",
      "Iteration 230, loss = 0.12249954\n",
      "Iteration 231, loss = 0.12225928\n",
      "Iteration 232, loss = 0.12202164\n",
      "Iteration 233, loss = 0.12178642\n",
      "Iteration 234, loss = 0.12155358\n",
      "Iteration 235, loss = 0.12132308\n",
      "Iteration 236, loss = 0.12109488\n",
      "Iteration 237, loss = 0.12086896\n",
      "Iteration 238, loss = 0.12064533\n",
      "Iteration 239, loss = 0.12042391\n",
      "Iteration 240, loss = 0.12020466\n",
      "Iteration 241, loss = 0.11998756\n",
      "Iteration 242, loss = 0.11977258\n",
      "Iteration 243, loss = 0.11955968\n",
      "Iteration 244, loss = 0.11934884\n",
      "Iteration 245, loss = 0.11914003\n",
      "Iteration 246, loss = 0.11893321\n",
      "Iteration 247, loss = 0.11872837\n",
      "Iteration 248, loss = 0.11852548\n",
      "Iteration 249, loss = 0.11832450\n",
      "Iteration 250, loss = 0.11812541\n",
      "Iteration 251, loss = 0.11792819\n",
      "Iteration 252, loss = 0.11773286\n",
      "Iteration 253, loss = 0.11753941\n",
      "Iteration 254, loss = 0.11734773\n",
      "Iteration 255, loss = 0.11715780\n",
      "Iteration 256, loss = 0.11696961\n",
      "Iteration 257, loss = 0.11678312\n",
      "Iteration 258, loss = 0.11659831\n",
      "Iteration 259, loss = 0.11641517\n",
      "Iteration 260, loss = 0.11623367\n",
      "Iteration 261, loss = 0.11605383\n",
      "Iteration 262, loss = 0.11587560\n",
      "Iteration 263, loss = 0.11569894\n",
      "Iteration 264, loss = 0.11552383\n",
      "Iteration 265, loss = 0.11535025\n",
      "Iteration 266, loss = 0.11517823\n",
      "Iteration 267, loss = 0.11500769\n",
      "Iteration 268, loss = 0.11483862\n",
      "Iteration 269, loss = 0.11467102\n",
      "Iteration 270, loss = 0.11450486\n",
      "Iteration 271, loss = 0.11434013\n",
      "Iteration 272, loss = 0.11417680\n",
      "Iteration 273, loss = 0.11401486\n",
      "Iteration 274, loss = 0.11385429\n",
      "Iteration 275, loss = 0.11369506\n",
      "Iteration 276, loss = 0.11353718\n",
      "Iteration 277, loss = 0.11338060\n",
      "Iteration 278, loss = 0.11322535\n",
      "Iteration 279, loss = 0.11307136\n",
      "Iteration 280, loss = 0.11291864\n",
      "Iteration 281, loss = 0.11276719\n",
      "Iteration 282, loss = 0.11261696\n",
      "Iteration 283, loss = 0.11246797\n",
      "Iteration 284, loss = 0.11232016\n",
      "Iteration 285, loss = 0.11217358\n",
      "Iteration 286, loss = 0.11202815\n",
      "Iteration 287, loss = 0.11188390\n",
      "Iteration 288, loss = 0.11174080\n",
      "Iteration 289, loss = 0.11159883\n",
      "Iteration 290, loss = 0.11145798\n",
      "Iteration 291, loss = 0.11131825\n",
      "Iteration 292, loss = 0.11117961\n",
      "Iteration 293, loss = 0.11104206\n",
      "Iteration 294, loss = 0.11090558\n",
      "Iteration 295, loss = 0.11077016\n",
      "Iteration 296, loss = 0.11063578\n",
      "Iteration 297, loss = 0.11050244\n",
      "Iteration 298, loss = 0.11037012\n",
      "Iteration 299, loss = 0.11023881\n",
      "Iteration 300, loss = 0.11010850\n",
      "Iteration 301, loss = 0.10997918\n",
      "Iteration 302, loss = 0.10985083\n",
      "Iteration 303, loss = 0.10972345\n",
      "Iteration 304, loss = 0.10959703\n",
      "Iteration 305, loss = 0.10947154\n",
      "Iteration 306, loss = 0.10934700\n",
      "Iteration 307, loss = 0.10922337\n",
      "Iteration 308, loss = 0.10910066\n",
      "Iteration 309, loss = 0.10897884\n",
      "Iteration 310, loss = 0.10885793\n",
      "Iteration 311, loss = 0.10873789\n",
      "Iteration 312, loss = 0.10861873\n",
      "Iteration 313, loss = 0.10850043\n",
      "Iteration 314, loss = 0.10838299\n",
      "Iteration 315, loss = 0.10826638\n",
      "Iteration 316, loss = 0.10815062\n",
      "Iteration 317, loss = 0.10803569\n",
      "Iteration 318, loss = 0.10792157\n",
      "Iteration 319, loss = 0.10780826\n",
      "Iteration 320, loss = 0.10769574\n",
      "Iteration 321, loss = 0.10758403\n",
      "Iteration 322, loss = 0.10747310\n",
      "Iteration 323, loss = 0.10736294\n",
      "Iteration 324, loss = 0.10725354\n",
      "Iteration 325, loss = 0.10714491\n",
      "Iteration 326, loss = 0.10703704\n",
      "Iteration 327, loss = 0.10692990\n",
      "Iteration 328, loss = 0.10682350\n",
      "Iteration 329, loss = 0.10671783\n",
      "Iteration 330, loss = 0.10661287\n",
      "Iteration 331, loss = 0.10650865\n",
      "Iteration 332, loss = 0.10640511\n",
      "Iteration 333, loss = 0.10630227\n",
      "Iteration 334, loss = 0.10620013\n",
      "Iteration 335, loss = 0.10609868\n",
      "Iteration 336, loss = 0.10599790\n",
      "Iteration 337, loss = 0.10589780\n",
      "Iteration 338, loss = 0.10579836\n",
      "Iteration 339, loss = 0.10569957\n",
      "Iteration 340, loss = 0.10560144\n",
      "Iteration 341, loss = 0.10550395\n",
      "Iteration 342, loss = 0.10540711\n",
      "Iteration 343, loss = 0.10531089\n",
      "Iteration 344, loss = 0.10521530\n",
      "Iteration 345, loss = 0.10512033\n",
      "Iteration 346, loss = 0.10502598\n",
      "Iteration 347, loss = 0.10493223\n",
      "Iteration 348, loss = 0.10483908\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.03698609\n",
      "Iteration 3, loss = 0.83678868\n",
      "Iteration 4, loss = 0.76876249\n",
      "Iteration 5, loss = 0.75555596\n",
      "Iteration 6, loss = 0.76020450\n",
      "Iteration 7, loss = 0.74463866\n",
      "Iteration 8, loss = 0.70916466\n",
      "Iteration 9, loss = 0.66885924\n",
      "Iteration 10, loss = 0.63352907\n",
      "Iteration 11, loss = 0.60826978\n",
      "Iteration 12, loss = 0.59212185\n",
      "Iteration 13, loss = 0.58020821\n",
      "Iteration 14, loss = 0.56794042\n",
      "Iteration 15, loss = 0.55370861\n",
      "Iteration 16, loss = 0.53826978\n",
      "Iteration 17, loss = 0.52347568\n",
      "Iteration 18, loss = 0.51043793\n",
      "Iteration 19, loss = 0.49935048\n",
      "Iteration 20, loss = 0.48983062\n",
      "Iteration 21, loss = 0.48109183\n",
      "Iteration 22, loss = 0.47271596\n",
      "Iteration 23, loss = 0.46455809\n",
      "Iteration 24, loss = 0.45670749\n",
      "Iteration 25, loss = 0.44928101\n",
      "Iteration 26, loss = 0.44241858\n",
      "Iteration 27, loss = 0.43599578\n",
      "Iteration 28, loss = 0.42984201\n",
      "Iteration 29, loss = 0.42389910\n",
      "Iteration 30, loss = 0.41813139\n",
      "Iteration 31, loss = 0.41253548\n",
      "Iteration 32, loss = 0.40709318\n",
      "Iteration 33, loss = 0.40182855\n",
      "Iteration 34, loss = 0.39674783\n",
      "Iteration 35, loss = 0.39185340\n",
      "Iteration 36, loss = 0.38706150\n",
      "Iteration 37, loss = 0.38236341\n",
      "Iteration 38, loss = 0.37776334\n",
      "Iteration 39, loss = 0.37324906\n",
      "Iteration 40, loss = 0.36881014\n",
      "Iteration 41, loss = 0.36443967\n",
      "Iteration 42, loss = 0.36013179\n",
      "Iteration 43, loss = 0.35588190\n",
      "Iteration 44, loss = 0.35168643\n",
      "Iteration 45, loss = 0.34754249\n",
      "Iteration 46, loss = 0.34344875\n",
      "Iteration 47, loss = 0.33941093\n",
      "Iteration 48, loss = 0.33541951\n",
      "Iteration 49, loss = 0.33147116\n",
      "Iteration 50, loss = 0.32756519\n",
      "Iteration 51, loss = 0.32370119\n",
      "Iteration 52, loss = 0.31987858\n",
      "Iteration 53, loss = 0.31609529\n",
      "Iteration 54, loss = 0.31235612\n",
      "Iteration 55, loss = 0.30866139\n",
      "Iteration 56, loss = 0.30500944\n",
      "Iteration 57, loss = 0.30140014\n",
      "Iteration 58, loss = 0.29783353\n",
      "Iteration 59, loss = 0.29431534\n",
      "Iteration 60, loss = 0.29083823\n",
      "Iteration 61, loss = 0.28740417\n",
      "Iteration 62, loss = 0.28400940\n",
      "Iteration 63, loss = 0.28065307\n",
      "Iteration 64, loss = 0.27733915\n",
      "Iteration 65, loss = 0.27406934\n",
      "Iteration 66, loss = 0.27084108\n",
      "Iteration 67, loss = 0.26765823\n",
      "Iteration 68, loss = 0.26452262\n",
      "Iteration 69, loss = 0.26143624\n",
      "Iteration 70, loss = 0.25839964\n",
      "Iteration 71, loss = 0.25541237\n",
      "Iteration 72, loss = 0.25247404\n",
      "Iteration 73, loss = 0.24958884\n",
      "Iteration 74, loss = 0.24674956\n",
      "Iteration 75, loss = 0.24395503\n",
      "Iteration 76, loss = 0.24120769\n",
      "Iteration 77, loss = 0.23850764\n",
      "Iteration 78, loss = 0.23585078\n",
      "Iteration 79, loss = 0.23323739\n",
      "Iteration 80, loss = 0.23066517\n",
      "Iteration 81, loss = 0.22813666\n",
      "Iteration 82, loss = 0.22566156\n",
      "Iteration 83, loss = 0.22322585\n",
      "Iteration 84, loss = 0.22083288\n",
      "Iteration 85, loss = 0.21848784\n",
      "Iteration 86, loss = 0.21619810\n",
      "Iteration 87, loss = 0.21395247\n",
      "Iteration 88, loss = 0.21175095\n",
      "Iteration 89, loss = 0.20959597\n",
      "Iteration 90, loss = 0.20748646\n",
      "Iteration 91, loss = 0.20541768\n",
      "Iteration 92, loss = 0.20338993\n",
      "Iteration 93, loss = 0.20139963\n",
      "Iteration 94, loss = 0.19944772\n",
      "Iteration 95, loss = 0.19753411\n",
      "Iteration 96, loss = 0.19565739\n",
      "Iteration 97, loss = 0.19381697\n",
      "Iteration 98, loss = 0.19201226\n",
      "Iteration 99, loss = 0.19024287\n",
      "Iteration 100, loss = 0.18850819\n",
      "Iteration 101, loss = 0.18680756\n",
      "Iteration 102, loss = 0.18514032\n",
      "Iteration 103, loss = 0.18350593\n",
      "Iteration 104, loss = 0.18190349\n",
      "Iteration 105, loss = 0.18033247\n",
      "Iteration 106, loss = 0.17879239\n",
      "Iteration 107, loss = 0.17728277\n",
      "Iteration 108, loss = 0.17580537\n",
      "Iteration 109, loss = 0.17435749\n",
      "Iteration 110, loss = 0.17293884\n",
      "Iteration 111, loss = 0.17154788\n",
      "Iteration 112, loss = 0.17018380\n",
      "Iteration 113, loss = 0.16884695\n",
      "Iteration 114, loss = 0.16753590\n",
      "Iteration 115, loss = 0.16625076\n",
      "Iteration 116, loss = 0.16499111\n",
      "Iteration 117, loss = 0.16375622\n",
      "Iteration 118, loss = 0.16254506\n",
      "Iteration 119, loss = 0.16135945\n",
      "Iteration 120, loss = 0.16019510\n",
      "Iteration 121, loss = 0.15905356\n",
      "Iteration 122, loss = 0.15793404\n",
      "Iteration 123, loss = 0.15683517\n",
      "Iteration 124, loss = 0.15575688\n",
      "Iteration 125, loss = 0.15469924\n",
      "Iteration 126, loss = 0.15366130\n",
      "Iteration 127, loss = 0.15264300\n",
      "Iteration 128, loss = 0.15164364\n",
      "Iteration 129, loss = 0.15066272\n",
      "Iteration 130, loss = 0.14969995\n",
      "Iteration 131, loss = 0.14875464\n",
      "Iteration 132, loss = 0.14782654\n",
      "Iteration 133, loss = 0.14691529\n",
      "Iteration 134, loss = 0.14602049\n",
      "Iteration 135, loss = 0.14514191\n",
      "Iteration 136, loss = 0.14427892\n",
      "Iteration 137, loss = 0.14343117\n",
      "Iteration 138, loss = 0.14259855\n",
      "Iteration 139, loss = 0.14178038\n",
      "Iteration 140, loss = 0.14097661\n",
      "Iteration 141, loss = 0.14018673\n",
      "Iteration 142, loss = 0.13941062\n",
      "Iteration 143, loss = 0.13864762\n",
      "Iteration 144, loss = 0.13789794\n",
      "Iteration 145, loss = 0.13716064\n",
      "Iteration 146, loss = 0.13643604\n",
      "Iteration 147, loss = 0.13572339\n",
      "Iteration 148, loss = 0.13502282\n",
      "Iteration 149, loss = 0.13433369\n",
      "Iteration 150, loss = 0.13365595\n",
      "Iteration 151, loss = 0.13298940\n",
      "Iteration 152, loss = 0.13233356\n",
      "Iteration 153, loss = 0.13168835\n",
      "Iteration 154, loss = 0.13105354\n",
      "Iteration 155, loss = 0.13042893\n",
      "Iteration 156, loss = 0.12981414\n",
      "Iteration 157, loss = 0.12920906\n",
      "Iteration 158, loss = 0.12861347\n",
      "Iteration 159, loss = 0.12802720\n",
      "Iteration 160, loss = 0.12744999\n",
      "Iteration 161, loss = 0.12688172\n",
      "Iteration 162, loss = 0.12632208\n",
      "Iteration 163, loss = 0.12577096\n",
      "Iteration 164, loss = 0.12522817\n",
      "Iteration 165, loss = 0.12469364\n",
      "Iteration 166, loss = 0.12416708\n",
      "Iteration 167, loss = 0.12364835\n",
      "Iteration 168, loss = 0.12313725\n",
      "Iteration 169, loss = 0.12263366\n",
      "Iteration 170, loss = 0.12213740\n",
      "Iteration 171, loss = 0.12164833\n",
      "Iteration 172, loss = 0.12116631\n",
      "Iteration 173, loss = 0.12069119\n",
      "Iteration 174, loss = 0.12022282\n",
      "Iteration 175, loss = 0.11976109\n",
      "Iteration 176, loss = 0.11930588\n",
      "Iteration 177, loss = 0.11885699\n",
      "Iteration 178, loss = 0.11841434\n",
      "Iteration 179, loss = 0.11797782\n",
      "Iteration 180, loss = 0.11754726\n",
      "Iteration 181, loss = 0.11712259\n",
      "Iteration 182, loss = 0.11670368\n",
      "Iteration 183, loss = 0.11629040\n",
      "Iteration 184, loss = 0.11588266\n",
      "Iteration 185, loss = 0.11548050\n",
      "Iteration 186, loss = 0.11508372\n",
      "Iteration 187, loss = 0.11469217\n",
      "Iteration 188, loss = 0.11430574\n",
      "Iteration 189, loss = 0.11392433\n",
      "Iteration 190, loss = 0.11354784\n",
      "Iteration 191, loss = 0.11317620\n",
      "Iteration 192, loss = 0.11280929\n",
      "Iteration 193, loss = 0.11244703\n",
      "Iteration 194, loss = 0.11208936\n",
      "Iteration 195, loss = 0.11173616\n",
      "Iteration 196, loss = 0.11138736\n",
      "Iteration 197, loss = 0.11104290\n",
      "Iteration 198, loss = 0.11070267\n",
      "Iteration 199, loss = 0.11036662\n",
      "Iteration 200, loss = 0.11003465\n",
      "Iteration 201, loss = 0.10970672\n",
      "Iteration 202, loss = 0.10938274\n",
      "Iteration 203, loss = 0.10906265\n",
      "Iteration 204, loss = 0.10874637\n",
      "Iteration 205, loss = 0.10843383\n",
      "Iteration 206, loss = 0.10812498\n",
      "Iteration 207, loss = 0.10781975\n",
      "Iteration 208, loss = 0.10751808\n",
      "Iteration 209, loss = 0.10721989\n",
      "Iteration 210, loss = 0.10692516\n",
      "Iteration 211, loss = 0.10663379\n",
      "Iteration 212, loss = 0.10634575\n",
      "Iteration 213, loss = 0.10606097\n",
      "Iteration 214, loss = 0.10577941\n",
      "Iteration 215, loss = 0.10550100\n",
      "Iteration 216, loss = 0.10522570\n",
      "Iteration 217, loss = 0.10495346\n",
      "Iteration 218, loss = 0.10468422\n",
      "Iteration 219, loss = 0.10441800\n",
      "Iteration 220, loss = 0.10415467\n",
      "Iteration 221, loss = 0.10389419\n",
      "Iteration 222, loss = 0.10363652\n",
      "Iteration 223, loss = 0.10338161\n",
      "Iteration 224, loss = 0.10312956\n",
      "Iteration 225, loss = 0.10288018\n",
      "Iteration 226, loss = 0.10263344\n",
      "Iteration 227, loss = 0.10238930\n",
      "Iteration 228, loss = 0.10214789\n",
      "Iteration 229, loss = 0.10190908\n",
      "Iteration 230, loss = 0.10167276\n",
      "Iteration 231, loss = 0.10143889\n",
      "Iteration 232, loss = 0.10120744\n",
      "Iteration 233, loss = 0.10097836\n",
      "Iteration 234, loss = 0.10075165\n",
      "Iteration 235, loss = 0.10052724\n",
      "Iteration 236, loss = 0.10030510\n",
      "Iteration 237, loss = 0.10008519\n",
      "Iteration 238, loss = 0.09986749\n",
      "Iteration 239, loss = 0.09965196\n",
      "Iteration 240, loss = 0.09943857\n",
      "Iteration 241, loss = 0.09922728\n",
      "Iteration 242, loss = 0.09901807\n",
      "Iteration 243, loss = 0.09881091\n",
      "Iteration 244, loss = 0.09860575\n",
      "Iteration 245, loss = 0.09840259\n",
      "Iteration 246, loss = 0.09820139\n",
      "Iteration 247, loss = 0.09800210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 248, loss = 0.09780472\n",
      "Iteration 249, loss = 0.09760923\n",
      "Iteration 250, loss = 0.09741556\n",
      "Iteration 251, loss = 0.09722373\n",
      "Iteration 252, loss = 0.09703369\n",
      "Iteration 253, loss = 0.09684542\n",
      "Iteration 254, loss = 0.09665890\n",
      "Iteration 255, loss = 0.09647410\n",
      "Iteration 256, loss = 0.09629100\n",
      "Iteration 257, loss = 0.09610957\n",
      "Iteration 258, loss = 0.09592979\n",
      "Iteration 259, loss = 0.09575166\n",
      "Iteration 260, loss = 0.09557511\n",
      "Iteration 261, loss = 0.09540016\n",
      "Iteration 262, loss = 0.09522678\n",
      "Iteration 263, loss = 0.09505493\n",
      "Iteration 264, loss = 0.09488475\n",
      "Iteration 265, loss = 0.09471605\n",
      "Iteration 266, loss = 0.09454885\n",
      "Iteration 267, loss = 0.09438312\n",
      "Iteration 268, loss = 0.09421884\n",
      "Iteration 269, loss = 0.09405598\n",
      "Iteration 270, loss = 0.09389453\n",
      "Iteration 271, loss = 0.09373447\n",
      "Iteration 272, loss = 0.09357579\n",
      "Iteration 273, loss = 0.09341846\n",
      "Iteration 274, loss = 0.09326246\n",
      "Iteration 275, loss = 0.09310779\n",
      "Iteration 276, loss = 0.09295442\n",
      "Iteration 277, loss = 0.09280235\n",
      "Iteration 278, loss = 0.09265155\n",
      "Iteration 279, loss = 0.09250200\n",
      "Iteration 280, loss = 0.09235370\n",
      "Iteration 281, loss = 0.09220662\n",
      "Iteration 282, loss = 0.09206075\n",
      "Iteration 283, loss = 0.09191608\n",
      "Iteration 284, loss = 0.09177259\n",
      "Iteration 285, loss = 0.09163026\n",
      "Iteration 286, loss = 0.09148909\n",
      "Iteration 287, loss = 0.09134906\n",
      "Iteration 288, loss = 0.09121015\n",
      "Iteration 289, loss = 0.09107236\n",
      "Iteration 290, loss = 0.09093566\n",
      "Iteration 291, loss = 0.09080005\n",
      "Iteration 292, loss = 0.09066550\n",
      "Iteration 293, loss = 0.09053202\n",
      "Iteration 294, loss = 0.09039959\n",
      "Iteration 295, loss = 0.09026818\n",
      "Iteration 296, loss = 0.09013780\n",
      "Iteration 297, loss = 0.09000844\n",
      "Iteration 298, loss = 0.08988007\n",
      "Iteration 299, loss = 0.08975268\n",
      "Iteration 300, loss = 0.08962627\n",
      "Iteration 301, loss = 0.08950083\n",
      "Iteration 302, loss = 0.08937634\n",
      "Iteration 303, loss = 0.08925279\n",
      "Iteration 304, loss = 0.08913017\n",
      "Iteration 305, loss = 0.08900848\n",
      "Iteration 306, loss = 0.08888769\n",
      "Iteration 307, loss = 0.08876781\n",
      "Iteration 308, loss = 0.08864881\n",
      "Iteration 309, loss = 0.08853070\n",
      "Iteration 310, loss = 0.08841345\n",
      "Iteration 311, loss = 0.08829707\n",
      "Iteration 312, loss = 0.08818154\n",
      "Iteration 313, loss = 0.08806685\n",
      "Iteration 314, loss = 0.08795299\n",
      "Iteration 315, loss = 0.08783995\n",
      "Iteration 316, loss = 0.08772775\n",
      "Iteration 317, loss = 0.08761638\n",
      "Iteration 318, loss = 0.08750579\n",
      "Iteration 319, loss = 0.08739598\n",
      "Iteration 320, loss = 0.08728696\n",
      "Iteration 321, loss = 0.08717871\n",
      "Iteration 322, loss = 0.08707121\n",
      "Iteration 323, loss = 0.08696450\n",
      "Iteration 324, loss = 0.08685851\n",
      "Iteration 325, loss = 0.08675326\n",
      "Iteration 326, loss = 0.08664875\n",
      "Iteration 327, loss = 0.08654496\n",
      "Iteration 328, loss = 0.08644190\n",
      "Iteration 329, loss = 0.08633953\n",
      "Iteration 330, loss = 0.08623787\n",
      "Iteration 331, loss = 0.08613693\n",
      "Iteration 332, loss = 0.08603670\n",
      "Iteration 333, loss = 0.08593715\n",
      "Iteration 334, loss = 0.08583828\n",
      "Iteration 335, loss = 0.08574010\n",
      "Iteration 336, loss = 0.08564256\n",
      "Iteration 337, loss = 0.08554567\n",
      "Iteration 338, loss = 0.08544945\n",
      "Iteration 339, loss = 0.08535386\n",
      "Iteration 340, loss = 0.08525889\n",
      "Iteration 341, loss = 0.08516458\n",
      "Iteration 342, loss = 0.08507087\n",
      "Iteration 343, loss = 0.08497777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.03698554\n",
      "Iteration 3, loss = 0.83797852\n",
      "Iteration 4, loss = 0.77150908\n",
      "Iteration 5, loss = 0.75843887\n",
      "Iteration 6, loss = 0.76303549\n",
      "Iteration 7, loss = 0.74651920\n",
      "Iteration 8, loss = 0.71049224\n",
      "Iteration 9, loss = 0.67024327\n",
      "Iteration 10, loss = 0.63537479\n",
      "Iteration 11, loss = 0.61091170\n",
      "Iteration 12, loss = 0.59488892\n",
      "Iteration 13, loss = 0.58263562\n",
      "Iteration 14, loss = 0.56994528\n",
      "Iteration 15, loss = 0.55540474\n",
      "Iteration 16, loss = 0.53999877\n",
      "Iteration 17, loss = 0.52511571\n",
      "Iteration 18, loss = 0.51193797\n",
      "Iteration 19, loss = 0.50077064\n",
      "Iteration 20, loss = 0.49106360\n",
      "Iteration 21, loss = 0.48210900\n",
      "Iteration 22, loss = 0.47348623\n",
      "Iteration 23, loss = 0.46509036\n",
      "Iteration 24, loss = 0.45704810\n",
      "Iteration 25, loss = 0.44946481\n",
      "Iteration 26, loss = 0.44238929\n",
      "Iteration 27, loss = 0.43585047\n",
      "Iteration 28, loss = 0.42961737\n",
      "Iteration 29, loss = 0.42357942\n",
      "Iteration 30, loss = 0.41767826\n",
      "Iteration 31, loss = 0.41192492\n",
      "Iteration 32, loss = 0.40633418\n",
      "Iteration 33, loss = 0.40089893\n",
      "Iteration 34, loss = 0.39566768\n",
      "Iteration 35, loss = 0.39066703\n",
      "Iteration 36, loss = 0.38577688\n",
      "Iteration 37, loss = 0.38099908\n",
      "Iteration 38, loss = 0.37633013\n",
      "Iteration 39, loss = 0.37174584\n",
      "Iteration 40, loss = 0.36723980\n",
      "Iteration 41, loss = 0.36280559\n",
      "Iteration 42, loss = 0.35843794\n",
      "Iteration 43, loss = 0.35413155\n",
      "Iteration 44, loss = 0.34988305\n",
      "Iteration 45, loss = 0.34568961\n",
      "Iteration 46, loss = 0.34154895\n",
      "Iteration 47, loss = 0.33745893\n",
      "Iteration 48, loss = 0.33341811\n",
      "Iteration 49, loss = 0.32942501\n",
      "Iteration 50, loss = 0.32547305\n",
      "Iteration 51, loss = 0.32156466\n",
      "Iteration 52, loss = 0.31770228\n",
      "Iteration 53, loss = 0.31388582\n",
      "Iteration 54, loss = 0.31011488\n",
      "Iteration 55, loss = 0.30638786\n",
      "Iteration 56, loss = 0.30270282\n",
      "Iteration 57, loss = 0.29906627\n",
      "Iteration 58, loss = 0.29547033\n",
      "Iteration 59, loss = 0.29191606\n",
      "Iteration 60, loss = 0.28839803\n",
      "Iteration 61, loss = 0.28491618\n",
      "Iteration 62, loss = 0.28148366\n",
      "Iteration 63, loss = 0.27809760\n",
      "Iteration 64, loss = 0.27475780\n",
      "Iteration 65, loss = 0.27147311\n",
      "Iteration 66, loss = 0.26824528\n",
      "Iteration 67, loss = 0.26507324\n",
      "Iteration 68, loss = 0.26195062\n",
      "Iteration 69, loss = 0.25887561\n",
      "Iteration 70, loss = 0.25585194\n",
      "Iteration 71, loss = 0.25287730\n",
      "Iteration 72, loss = 0.24995138\n",
      "Iteration 73, loss = 0.24707056\n",
      "Iteration 74, loss = 0.24423136\n",
      "Iteration 75, loss = 0.24143886\n",
      "Iteration 76, loss = 0.23870235\n",
      "Iteration 77, loss = 0.23602342\n",
      "Iteration 78, loss = 0.23339525\n",
      "Iteration 79, loss = 0.23081296\n",
      "Iteration 80, loss = 0.22827435\n",
      "Iteration 81, loss = 0.22578141\n",
      "Iteration 82, loss = 0.22333312\n",
      "Iteration 83, loss = 0.22092779\n",
      "Iteration 84, loss = 0.21856527\n",
      "Iteration 85, loss = 0.21624497\n",
      "Iteration 86, loss = 0.21396660\n",
      "Iteration 87, loss = 0.21172958\n",
      "Iteration 88, loss = 0.20953426\n",
      "Iteration 89, loss = 0.20738031\n",
      "Iteration 90, loss = 0.20526618\n",
      "Iteration 91, loss = 0.20319136\n",
      "Iteration 92, loss = 0.20115632\n",
      "Iteration 93, loss = 0.19915970\n",
      "Iteration 94, loss = 0.19720008\n",
      "Iteration 95, loss = 0.19527773\n",
      "Iteration 96, loss = 0.19339188\n",
      "Iteration 97, loss = 0.19154204\n",
      "Iteration 98, loss = 0.18972862\n",
      "Iteration 99, loss = 0.18794963\n",
      "Iteration 100, loss = 0.18620451\n",
      "Iteration 101, loss = 0.18449304\n",
      "Iteration 102, loss = 0.18281415\n",
      "Iteration 103, loss = 0.18116757\n",
      "Iteration 104, loss = 0.17955204\n",
      "Iteration 105, loss = 0.17796705\n",
      "Iteration 106, loss = 0.17641203\n",
      "Iteration 107, loss = 0.17488656\n",
      "Iteration 108, loss = 0.17339023\n",
      "Iteration 109, loss = 0.17192215\n",
      "Iteration 110, loss = 0.17048173\n",
      "Iteration 111, loss = 0.16906846\n",
      "Iteration 112, loss = 0.16768157\n",
      "Iteration 113, loss = 0.16632049\n",
      "Iteration 114, loss = 0.16498472\n",
      "Iteration 115, loss = 0.16367386\n",
      "Iteration 116, loss = 0.16238713\n",
      "Iteration 117, loss = 0.16112419\n",
      "Iteration 118, loss = 0.15988424\n",
      "Iteration 119, loss = 0.15866799\n",
      "Iteration 120, loss = 0.15747386\n",
      "Iteration 121, loss = 0.15630132\n",
      "Iteration 122, loss = 0.15515003\n",
      "Iteration 123, loss = 0.15401929\n",
      "Iteration 124, loss = 0.15290888\n",
      "Iteration 125, loss = 0.15181812\n",
      "Iteration 126, loss = 0.15074670\n",
      "Iteration 127, loss = 0.14969414\n",
      "Iteration 128, loss = 0.14866023\n",
      "Iteration 129, loss = 0.14764540\n",
      "Iteration 130, loss = 0.14664838\n",
      "Iteration 131, loss = 0.14566858\n",
      "Iteration 132, loss = 0.14470571\n",
      "Iteration 133, loss = 0.14375921\n",
      "Iteration 134, loss = 0.14282897\n",
      "Iteration 135, loss = 0.14191436\n",
      "Iteration 136, loss = 0.14101527\n",
      "Iteration 137, loss = 0.14013112\n",
      "Iteration 138, loss = 0.13926176\n",
      "Iteration 139, loss = 0.13840674\n",
      "Iteration 140, loss = 0.13756575\n",
      "Iteration 141, loss = 0.13673858\n",
      "Iteration 142, loss = 0.13592524\n",
      "Iteration 143, loss = 0.13512518\n",
      "Iteration 144, loss = 0.13433800\n",
      "Iteration 145, loss = 0.13356341\n",
      "Iteration 146, loss = 0.13280111\n",
      "Iteration 147, loss = 0.13205081\n",
      "Iteration 148, loss = 0.13131228\n",
      "Iteration 149, loss = 0.13058526\n",
      "Iteration 150, loss = 0.12986951\n",
      "Iteration 151, loss = 0.12916476\n",
      "Iteration 152, loss = 0.12847079\n",
      "Iteration 153, loss = 0.12778737\n",
      "Iteration 154, loss = 0.12711430\n",
      "Iteration 155, loss = 0.12645137\n",
      "Iteration 156, loss = 0.12579841\n",
      "Iteration 157, loss = 0.12515512\n",
      "Iteration 158, loss = 0.12452129\n",
      "Iteration 159, loss = 0.12389675\n",
      "Iteration 160, loss = 0.12328121\n",
      "Iteration 161, loss = 0.12267459\n",
      "Iteration 162, loss = 0.12207671\n",
      "Iteration 163, loss = 0.12148744\n",
      "Iteration 164, loss = 0.12090650\n",
      "Iteration 165, loss = 0.12033421\n",
      "Iteration 166, loss = 0.11977037\n",
      "Iteration 167, loss = 0.11921448\n",
      "Iteration 168, loss = 0.11866633\n",
      "Iteration 169, loss = 0.11812581\n",
      "Iteration 170, loss = 0.11759272\n",
      "Iteration 171, loss = 0.11706692\n",
      "Iteration 172, loss = 0.11654826\n",
      "Iteration 173, loss = 0.11603661\n",
      "Iteration 174, loss = 0.11553184\n",
      "Iteration 175, loss = 0.11503384\n",
      "Iteration 176, loss = 0.11454254\n",
      "Iteration 177, loss = 0.11405761\n",
      "Iteration 178, loss = 0.11357907\n",
      "Iteration 179, loss = 0.11310675\n",
      "Iteration 180, loss = 0.11264062\n",
      "Iteration 181, loss = 0.11218043\n",
      "Iteration 182, loss = 0.11172615\n",
      "Iteration 183, loss = 0.11127769\n",
      "Iteration 184, loss = 0.11083487\n",
      "Iteration 185, loss = 0.11039765\n",
      "Iteration 186, loss = 0.10996590\n",
      "Iteration 187, loss = 0.10953950\n",
      "Iteration 188, loss = 0.10911837\n",
      "Iteration 189, loss = 0.10870246\n",
      "Iteration 190, loss = 0.10829158\n",
      "Iteration 191, loss = 0.10788572\n",
      "Iteration 192, loss = 0.10748475\n",
      "Iteration 193, loss = 0.10708858\n",
      "Iteration 194, loss = 0.10669719\n",
      "Iteration 195, loss = 0.10631048\n",
      "Iteration 196, loss = 0.10592833\n",
      "Iteration 197, loss = 0.10555069\n",
      "Iteration 198, loss = 0.10517754\n",
      "Iteration 199, loss = 0.10480870\n",
      "Iteration 200, loss = 0.10444410\n",
      "Iteration 201, loss = 0.10408372\n",
      "Iteration 202, loss = 0.10372747\n",
      "Iteration 203, loss = 0.10337529\n",
      "Iteration 204, loss = 0.10302707\n",
      "Iteration 205, loss = 0.10268273\n",
      "Iteration 206, loss = 0.10234233\n",
      "Iteration 207, loss = 0.10200566\n",
      "Iteration 208, loss = 0.10167272\n",
      "Iteration 209, loss = 0.10134345\n",
      "Iteration 210, loss = 0.10101782\n",
      "Iteration 211, loss = 0.10069591\n",
      "Iteration 212, loss = 0.10037749\n",
      "Iteration 213, loss = 0.10006257\n",
      "Iteration 214, loss = 0.09975099\n",
      "Iteration 215, loss = 0.09944283\n",
      "Iteration 216, loss = 0.09913787\n",
      "Iteration 217, loss = 0.09883617\n",
      "Iteration 218, loss = 0.09853762\n",
      "Iteration 219, loss = 0.09824224\n",
      "Iteration 220, loss = 0.09794989\n",
      "Iteration 221, loss = 0.09766064\n",
      "Iteration 222, loss = 0.09737431\n",
      "Iteration 223, loss = 0.09709096\n",
      "Iteration 224, loss = 0.09681047\n",
      "Iteration 225, loss = 0.09653286\n",
      "Iteration 226, loss = 0.09625804\n",
      "Iteration 227, loss = 0.09598604\n",
      "Iteration 228, loss = 0.09571677\n",
      "Iteration 229, loss = 0.09545018\n",
      "Iteration 230, loss = 0.09518620\n",
      "Iteration 231, loss = 0.09492489\n",
      "Iteration 232, loss = 0.09466607\n",
      "Iteration 233, loss = 0.09440988\n",
      "Iteration 234, loss = 0.09415608\n",
      "Iteration 235, loss = 0.09390481\n",
      "Iteration 236, loss = 0.09365593\n",
      "Iteration 237, loss = 0.09340940\n",
      "Iteration 238, loss = 0.09316530\n",
      "Iteration 239, loss = 0.09292346\n",
      "Iteration 240, loss = 0.09268395\n",
      "Iteration 241, loss = 0.09244665\n",
      "Iteration 242, loss = 0.09221161\n",
      "Iteration 243, loss = 0.09197875\n",
      "Iteration 244, loss = 0.09174803\n",
      "Iteration 245, loss = 0.09151957\n",
      "Iteration 246, loss = 0.09129326\n",
      "Iteration 247, loss = 0.09106903\n",
      "Iteration 248, loss = 0.09084689\n",
      "Iteration 249, loss = 0.09062676\n",
      "Iteration 250, loss = 0.09040861\n",
      "Iteration 251, loss = 0.09019249\n",
      "Iteration 252, loss = 0.08997833\n",
      "Iteration 253, loss = 0.08976607\n",
      "Iteration 254, loss = 0.08955570\n",
      "Iteration 255, loss = 0.08934721\n",
      "Iteration 256, loss = 0.08914054\n",
      "Iteration 257, loss = 0.08893568\n",
      "Iteration 258, loss = 0.08873261\n",
      "Iteration 259, loss = 0.08853134\n",
      "Iteration 260, loss = 0.08833179\n",
      "Iteration 261, loss = 0.08813397\n",
      "Iteration 262, loss = 0.08793785\n",
      "Iteration 263, loss = 0.08774344\n",
      "Iteration 264, loss = 0.08755067\n",
      "Iteration 265, loss = 0.08735954\n",
      "Iteration 266, loss = 0.08717006\n",
      "Iteration 267, loss = 0.08698213\n",
      "Iteration 268, loss = 0.08679588\n",
      "Iteration 269, loss = 0.08661108\n",
      "Iteration 270, loss = 0.08642786\n",
      "Iteration 271, loss = 0.08624615\n",
      "Iteration 272, loss = 0.08606598\n",
      "Iteration 273, loss = 0.08588725\n",
      "Iteration 274, loss = 0.08571001\n",
      "Iteration 275, loss = 0.08553421\n",
      "Iteration 276, loss = 0.08535983\n",
      "Iteration 277, loss = 0.08518687\n",
      "Iteration 278, loss = 0.08501532\n",
      "Iteration 279, loss = 0.08484514\n",
      "Iteration 280, loss = 0.08467632\n",
      "Iteration 281, loss = 0.08450885\n",
      "Iteration 282, loss = 0.08434271\n",
      "Iteration 283, loss = 0.08417787\n",
      "Iteration 284, loss = 0.08401434\n",
      "Iteration 285, loss = 0.08385209\n",
      "Iteration 286, loss = 0.08369114\n",
      "Iteration 287, loss = 0.08353140\n",
      "Iteration 288, loss = 0.08337292\n",
      "Iteration 289, loss = 0.08321566\n",
      "Iteration 290, loss = 0.08305962\n",
      "Iteration 291, loss = 0.08290477\n",
      "Iteration 292, loss = 0.08275111\n",
      "Iteration 293, loss = 0.08259862\n",
      "Iteration 294, loss = 0.08244729\n",
      "Iteration 295, loss = 0.08229710\n",
      "Iteration 296, loss = 0.08214805\n",
      "Iteration 297, loss = 0.08200011\n",
      "Iteration 298, loss = 0.08185328\n",
      "Iteration 299, loss = 0.08170755\n",
      "Iteration 300, loss = 0.08156289\n",
      "Iteration 301, loss = 0.08141930\n",
      "Iteration 302, loss = 0.08127677\n",
      "Iteration 303, loss = 0.08113529\n",
      "Iteration 304, loss = 0.08099484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 305, loss = 0.08085541\n",
      "Iteration 306, loss = 0.08071700\n",
      "Iteration 307, loss = 0.08057959\n",
      "Iteration 308, loss = 0.08044317\n",
      "Iteration 309, loss = 0.08030772\n",
      "Iteration 310, loss = 0.08017325\n",
      "Iteration 311, loss = 0.08003974\n",
      "Iteration 312, loss = 0.07990718\n",
      "Iteration 313, loss = 0.07977556\n",
      "Iteration 314, loss = 0.07964487\n",
      "Iteration 315, loss = 0.07951509\n",
      "Iteration 316, loss = 0.07938623\n",
      "Iteration 317, loss = 0.07925826\n",
      "Iteration 318, loss = 0.07913119\n",
      "Iteration 319, loss = 0.07900500\n",
      "Iteration 320, loss = 0.07887969\n",
      "Iteration 321, loss = 0.07875524\n",
      "Iteration 322, loss = 0.07863165\n",
      "Iteration 323, loss = 0.07850890\n",
      "Iteration 324, loss = 0.07838699\n",
      "Iteration 325, loss = 0.07826591\n",
      "Iteration 326, loss = 0.07814565\n",
      "Iteration 327, loss = 0.07802621\n",
      "Iteration 328, loss = 0.07790757\n",
      "Iteration 329, loss = 0.07778972\n",
      "Iteration 330, loss = 0.07767267\n",
      "Iteration 331, loss = 0.07755639\n",
      "Iteration 332, loss = 0.07744089\n",
      "Iteration 333, loss = 0.07732615\n",
      "Iteration 334, loss = 0.07721217\n",
      "Iteration 335, loss = 0.07709894\n",
      "Iteration 336, loss = 0.07698645\n",
      "Iteration 337, loss = 0.07687470\n",
      "Iteration 338, loss = 0.07676368\n",
      "Iteration 339, loss = 0.07665338\n",
      "Iteration 340, loss = 0.07654379\n",
      "Iteration 341, loss = 0.07643491\n",
      "Iteration 342, loss = 0.07632674\n",
      "Iteration 343, loss = 0.07621925\n",
      "Iteration 344, loss = 0.07611246\n",
      "Iteration 345, loss = 0.07600635\n",
      "Iteration 346, loss = 0.07590091\n",
      "Iteration 347, loss = 0.07579615\n",
      "Iteration 348, loss = 0.07569204\n",
      "Iteration 349, loss = 0.07558859\n",
      "Iteration 350, loss = 0.07548579\n",
      "Iteration 351, loss = 0.07538363\n",
      "Iteration 352, loss = 0.07528211\n",
      "Iteration 353, loss = 0.07518122\n",
      "Iteration 354, loss = 0.07508095\n",
      "Iteration 355, loss = 0.07498132\n",
      "Iteration 356, loss = 0.07488232\n",
      "Iteration 357, loss = 0.07478393\n",
      "Iteration 358, loss = 0.07468615\n",
      "Iteration 359, loss = 0.07458896\n",
      "Iteration 360, loss = 0.07449236\n",
      "Iteration 361, loss = 0.07439636\n",
      "Iteration 362, loss = 0.07430093\n",
      "Iteration 363, loss = 0.07420608\n",
      "Iteration 364, loss = 0.07411181\n",
      "Iteration 365, loss = 0.07401810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.03402933\n",
      "Iteration 3, loss = 0.83141386\n",
      "Iteration 4, loss = 0.76622790\n",
      "Iteration 5, loss = 0.75630545\n",
      "Iteration 6, loss = 0.76115373\n",
      "Iteration 7, loss = 0.74386663\n",
      "Iteration 8, loss = 0.70644493\n",
      "Iteration 9, loss = 0.66535736\n",
      "Iteration 10, loss = 0.63035475\n",
      "Iteration 11, loss = 0.60626673\n",
      "Iteration 12, loss = 0.59095265\n",
      "Iteration 13, loss = 0.57916585\n",
      "Iteration 14, loss = 0.56682602\n",
      "Iteration 15, loss = 0.55254506\n",
      "Iteration 16, loss = 0.53728625\n",
      "Iteration 17, loss = 0.52267212\n",
      "Iteration 18, loss = 0.50990656\n",
      "Iteration 19, loss = 0.49918605\n",
      "Iteration 20, loss = 0.48989954\n",
      "Iteration 21, loss = 0.48133570\n",
      "Iteration 22, loss = 0.47313011\n",
      "Iteration 23, loss = 0.46516567\n",
      "Iteration 24, loss = 0.45754044\n",
      "Iteration 25, loss = 0.45033470\n",
      "Iteration 26, loss = 0.44362695\n",
      "Iteration 27, loss = 0.43736216\n",
      "Iteration 28, loss = 0.43147229\n",
      "Iteration 29, loss = 0.42577095\n",
      "Iteration 30, loss = 0.42020625\n",
      "Iteration 31, loss = 0.41478350\n",
      "Iteration 32, loss = 0.40955710\n",
      "Iteration 33, loss = 0.40453280\n",
      "Iteration 34, loss = 0.39965984\n",
      "Iteration 35, loss = 0.39490702\n",
      "Iteration 36, loss = 0.39032585\n",
      "Iteration 37, loss = 0.38586619\n",
      "Iteration 38, loss = 0.38149576\n",
      "Iteration 39, loss = 0.37721091\n",
      "Iteration 40, loss = 0.37300558\n",
      "Iteration 41, loss = 0.36887278\n",
      "Iteration 42, loss = 0.36480667\n",
      "Iteration 43, loss = 0.36081075\n",
      "Iteration 44, loss = 0.35687243\n",
      "Iteration 45, loss = 0.35299440\n",
      "Iteration 46, loss = 0.34916909\n",
      "Iteration 47, loss = 0.34539207\n",
      "Iteration 48, loss = 0.34166286\n",
      "Iteration 49, loss = 0.33797659\n",
      "Iteration 50, loss = 0.33433178\n",
      "Iteration 51, loss = 0.33073118\n",
      "Iteration 52, loss = 0.32717436\n",
      "Iteration 53, loss = 0.32366103\n",
      "Iteration 54, loss = 0.32019165\n",
      "Iteration 55, loss = 0.31676386\n",
      "Iteration 56, loss = 0.31337811\n",
      "Iteration 57, loss = 0.31003699\n",
      "Iteration 58, loss = 0.30673252\n",
      "Iteration 59, loss = 0.30347030\n",
      "Iteration 60, loss = 0.30024919\n",
      "Iteration 61, loss = 0.29706497\n",
      "Iteration 62, loss = 0.29392421\n",
      "Iteration 63, loss = 0.29082979\n",
      "Iteration 64, loss = 0.28778355\n",
      "Iteration 65, loss = 0.28478474\n",
      "Iteration 66, loss = 0.28183421\n",
      "Iteration 67, loss = 0.27893058\n",
      "Iteration 68, loss = 0.27607126\n",
      "Iteration 69, loss = 0.27325937\n",
      "Iteration 70, loss = 0.27049055\n",
      "Iteration 71, loss = 0.26776185\n",
      "Iteration 72, loss = 0.26507818\n",
      "Iteration 73, loss = 0.26243580\n",
      "Iteration 74, loss = 0.25984160\n",
      "Iteration 75, loss = 0.25728924\n",
      "Iteration 76, loss = 0.25478112\n",
      "Iteration 77, loss = 0.25231708\n",
      "Iteration 78, loss = 0.24989877\n",
      "Iteration 79, loss = 0.24752696\n",
      "Iteration 80, loss = 0.24519679\n",
      "Iteration 81, loss = 0.24290930\n",
      "Iteration 82, loss = 0.24066682\n",
      "Iteration 83, loss = 0.23846769\n",
      "Iteration 84, loss = 0.23631190\n",
      "Iteration 85, loss = 0.23419498\n",
      "Iteration 86, loss = 0.23211947\n",
      "Iteration 87, loss = 0.23008181\n",
      "Iteration 88, loss = 0.22808270\n",
      "Iteration 89, loss = 0.22612141\n",
      "Iteration 90, loss = 0.22419693\n",
      "Iteration 91, loss = 0.22230916\n",
      "Iteration 92, loss = 0.22045763\n",
      "Iteration 93, loss = 0.21864168\n",
      "Iteration 94, loss = 0.21686145\n",
      "Iteration 95, loss = 0.21511540\n",
      "Iteration 96, loss = 0.21340342\n",
      "Iteration 97, loss = 0.21172449\n",
      "Iteration 98, loss = 0.21007890\n",
      "Iteration 99, loss = 0.20846560\n",
      "Iteration 100, loss = 0.20688352\n",
      "Iteration 101, loss = 0.20533255\n",
      "Iteration 102, loss = 0.20381257\n",
      "Iteration 103, loss = 0.20232190\n",
      "Iteration 104, loss = 0.20086074\n",
      "Iteration 105, loss = 0.19942783\n",
      "Iteration 106, loss = 0.19802304\n",
      "Iteration 107, loss = 0.19664552\n",
      "Iteration 108, loss = 0.19529484\n",
      "Iteration 109, loss = 0.19397039\n",
      "Iteration 110, loss = 0.19267169\n",
      "Iteration 111, loss = 0.19139812\n",
      "Iteration 112, loss = 0.19014910\n",
      "Iteration 113, loss = 0.18892418\n",
      "Iteration 114, loss = 0.18772268\n",
      "Iteration 115, loss = 0.18654433\n",
      "Iteration 116, loss = 0.18538827\n",
      "Iteration 117, loss = 0.18425460\n",
      "Iteration 118, loss = 0.18314216\n",
      "Iteration 119, loss = 0.18205099\n",
      "Iteration 120, loss = 0.18098017\n",
      "Iteration 121, loss = 0.17992957\n",
      "Iteration 122, loss = 0.17889824\n",
      "Iteration 123, loss = 0.17788630\n",
      "Iteration 124, loss = 0.17689301\n",
      "Iteration 125, loss = 0.17591821\n",
      "Iteration 126, loss = 0.17496113\n",
      "Iteration 127, loss = 0.17402152\n",
      "Iteration 128, loss = 0.17309904\n",
      "Iteration 129, loss = 0.17219318\n",
      "Iteration 130, loss = 0.17130394\n",
      "Iteration 131, loss = 0.17043051\n",
      "Iteration 132, loss = 0.16957295\n",
      "Iteration 133, loss = 0.16873047\n",
      "Iteration 134, loss = 0.16790290\n",
      "Iteration 135, loss = 0.16708997\n",
      "Iteration 136, loss = 0.16629117\n",
      "Iteration 137, loss = 0.16550629\n",
      "Iteration 138, loss = 0.16473491\n",
      "Iteration 139, loss = 0.16397699\n",
      "Iteration 140, loss = 0.16323199\n",
      "Iteration 141, loss = 0.16249973\n",
      "Iteration 142, loss = 0.16177994\n",
      "Iteration 143, loss = 0.16107223\n",
      "Iteration 144, loss = 0.16037647\n",
      "Iteration 145, loss = 0.15969226\n",
      "Iteration 146, loss = 0.15901939\n",
      "Iteration 147, loss = 0.15835774\n",
      "Iteration 148, loss = 0.15770684\n",
      "Iteration 149, loss = 0.15706663\n",
      "Iteration 150, loss = 0.15643688\n",
      "Iteration 151, loss = 0.15581722\n",
      "Iteration 152, loss = 0.15520754\n",
      "Iteration 153, loss = 0.15460761\n",
      "Iteration 154, loss = 0.15401718\n",
      "Iteration 155, loss = 0.15343609\n",
      "Iteration 156, loss = 0.15286418\n",
      "Iteration 157, loss = 0.15230116\n",
      "Iteration 158, loss = 0.15174691\n",
      "Iteration 159, loss = 0.15120160\n",
      "Iteration 160, loss = 0.15066543\n",
      "Iteration 161, loss = 0.15013762\n",
      "Iteration 162, loss = 0.14961789\n",
      "Iteration 163, loss = 0.14910603\n",
      "Iteration 164, loss = 0.14860188\n",
      "Iteration 165, loss = 0.14810529\n",
      "Iteration 166, loss = 0.14761612\n",
      "Iteration 167, loss = 0.14713420\n",
      "Iteration 168, loss = 0.14665937\n",
      "Iteration 169, loss = 0.14619150\n",
      "Iteration 170, loss = 0.14573043\n",
      "Iteration 171, loss = 0.14527603\n",
      "Iteration 172, loss = 0.14482818\n",
      "Iteration 173, loss = 0.14438672\n",
      "Iteration 174, loss = 0.14395154\n",
      "Iteration 175, loss = 0.14352251\n",
      "Iteration 176, loss = 0.14309950\n",
      "Iteration 177, loss = 0.14268240\n",
      "Iteration 178, loss = 0.14227109\n",
      "Iteration 179, loss = 0.14186546\n",
      "Iteration 180, loss = 0.14146539\n",
      "Iteration 181, loss = 0.14107078\n",
      "Iteration 182, loss = 0.14068152\n",
      "Iteration 183, loss = 0.14029750\n",
      "Iteration 184, loss = 0.13991864\n",
      "Iteration 185, loss = 0.13954507\n",
      "Iteration 186, loss = 0.13917654\n",
      "Iteration 187, loss = 0.13881292\n",
      "Iteration 188, loss = 0.13845411\n",
      "Iteration 189, loss = 0.13809997\n",
      "Iteration 190, loss = 0.13775058\n",
      "Iteration 191, loss = 0.13740553\n",
      "Iteration 192, loss = 0.13706488\n",
      "Iteration 193, loss = 0.13672859\n",
      "Iteration 194, loss = 0.13639654\n",
      "Iteration 195, loss = 0.13606873\n",
      "Iteration 196, loss = 0.13574504\n",
      "Iteration 197, loss = 0.13542544\n",
      "Iteration 198, loss = 0.13510981\n",
      "Iteration 199, loss = 0.13479807\n",
      "Iteration 200, loss = 0.13449018\n",
      "Iteration 201, loss = 0.13418605\n",
      "Iteration 202, loss = 0.13388562\n",
      "Iteration 203, loss = 0.13358881\n",
      "Iteration 204, loss = 0.13329560\n",
      "Iteration 205, loss = 0.13300592\n",
      "Iteration 206, loss = 0.13271966\n",
      "Iteration 207, loss = 0.13243677\n",
      "Iteration 208, loss = 0.13215725\n",
      "Iteration 209, loss = 0.13188099\n",
      "Iteration 210, loss = 0.13160791\n",
      "Iteration 211, loss = 0.13133809\n",
      "Iteration 212, loss = 0.13107128\n",
      "Iteration 213, loss = 0.13080769\n",
      "Iteration 214, loss = 0.13054717\n",
      "Iteration 215, loss = 0.13028955\n",
      "Iteration 216, loss = 0.13003490\n",
      "Iteration 217, loss = 0.12978305\n",
      "Iteration 218, loss = 0.12953407\n",
      "Iteration 219, loss = 0.12928785\n",
      "Iteration 220, loss = 0.12904433\n",
      "Iteration 221, loss = 0.12880350\n",
      "Iteration 222, loss = 0.12856535\n",
      "Iteration 223, loss = 0.12832973\n",
      "Iteration 224, loss = 0.12809669\n",
      "Iteration 225, loss = 0.12786614\n",
      "Iteration 226, loss = 0.12763811\n",
      "Iteration 227, loss = 0.12741246\n",
      "Iteration 228, loss = 0.12718921\n",
      "Iteration 229, loss = 0.12696832\n",
      "Iteration 230, loss = 0.12674975\n",
      "Iteration 231, loss = 0.12653348\n",
      "Iteration 232, loss = 0.12631943\n",
      "Iteration 233, loss = 0.12610760\n",
      "Iteration 234, loss = 0.12589794\n",
      "Iteration 235, loss = 0.12569043\n",
      "Iteration 236, loss = 0.12548503\n",
      "Iteration 237, loss = 0.12528171\n",
      "Iteration 238, loss = 0.12508044\n",
      "Iteration 239, loss = 0.12488118\n",
      "Iteration 240, loss = 0.12468392\n",
      "Iteration 241, loss = 0.12448860\n",
      "Iteration 242, loss = 0.12429522\n",
      "Iteration 243, loss = 0.12410373\n",
      "Iteration 244, loss = 0.12391413\n",
      "Iteration 245, loss = 0.12372635\n",
      "Iteration 246, loss = 0.12354040\n",
      "Iteration 247, loss = 0.12335624\n",
      "Iteration 248, loss = 0.12317384\n",
      "Iteration 249, loss = 0.12299317\n",
      "Iteration 250, loss = 0.12281422\n",
      "Iteration 251, loss = 0.12263696\n",
      "Iteration 252, loss = 0.12246137\n",
      "Iteration 253, loss = 0.12228740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.12211507\n",
      "Iteration 255, loss = 0.12194432\n",
      "Iteration 256, loss = 0.12177514\n",
      "Iteration 257, loss = 0.12160752\n",
      "Iteration 258, loss = 0.12144142\n",
      "Iteration 259, loss = 0.12127683\n",
      "Iteration 260, loss = 0.12111372\n",
      "Iteration 261, loss = 0.12095208\n",
      "Iteration 262, loss = 0.12079188\n",
      "Iteration 263, loss = 0.12063314\n",
      "Iteration 264, loss = 0.12047586\n",
      "Iteration 265, loss = 0.12031996\n",
      "Iteration 266, loss = 0.12016543\n",
      "Iteration 267, loss = 0.12001225\n",
      "Iteration 268, loss = 0.11986040\n",
      "Iteration 269, loss = 0.11970986\n",
      "Iteration 270, loss = 0.11956062\n",
      "Iteration 271, loss = 0.11941266\n",
      "Iteration 272, loss = 0.11926596\n",
      "Iteration 273, loss = 0.11912050\n",
      "Iteration 274, loss = 0.11897628\n",
      "Iteration 275, loss = 0.11883326\n",
      "Iteration 276, loss = 0.11869144\n",
      "Iteration 277, loss = 0.11855080\n",
      "Iteration 278, loss = 0.11841133\n",
      "Iteration 279, loss = 0.11827302\n",
      "Iteration 280, loss = 0.11813586\n",
      "Iteration 281, loss = 0.11799982\n",
      "Iteration 282, loss = 0.11786488\n",
      "Iteration 283, loss = 0.11773104\n",
      "Iteration 284, loss = 0.11759827\n",
      "Iteration 285, loss = 0.11746656\n",
      "Iteration 286, loss = 0.11733592\n",
      "Iteration 287, loss = 0.11720630\n",
      "Iteration 288, loss = 0.11707772\n",
      "Iteration 289, loss = 0.11695015\n",
      "Iteration 290, loss = 0.11682359\n",
      "Iteration 291, loss = 0.11669802\n",
      "Iteration 292, loss = 0.11657343\n",
      "Iteration 293, loss = 0.11644979\n",
      "Iteration 294, loss = 0.11632712\n",
      "Iteration 295, loss = 0.11620539\n",
      "Iteration 296, loss = 0.11608460\n",
      "Iteration 297, loss = 0.11596472\n",
      "Iteration 298, loss = 0.11584575\n",
      "Iteration 299, loss = 0.11572768\n",
      "Iteration 300, loss = 0.11561050\n",
      "Iteration 301, loss = 0.11549420\n",
      "Iteration 302, loss = 0.11537876\n",
      "Iteration 303, loss = 0.11526418\n",
      "Iteration 304, loss = 0.11515045\n",
      "Iteration 305, loss = 0.11503756\n",
      "Iteration 306, loss = 0.11492549\n",
      "Iteration 307, loss = 0.11481425\n",
      "Iteration 308, loss = 0.11470381\n",
      "Iteration 309, loss = 0.11459418\n",
      "Iteration 310, loss = 0.11448533\n",
      "Iteration 311, loss = 0.11437727\n",
      "Iteration 312, loss = 0.11426998\n",
      "Iteration 313, loss = 0.11416345\n",
      "Iteration 314, loss = 0.11405768\n",
      "Iteration 315, loss = 0.11395266\n",
      "Iteration 316, loss = 0.11384837\n",
      "Iteration 317, loss = 0.11374482\n",
      "Iteration 318, loss = 0.11364199\n",
      "Iteration 319, loss = 0.11353987\n",
      "Iteration 320, loss = 0.11343846\n",
      "Iteration 321, loss = 0.11333775\n",
      "Iteration 322, loss = 0.11323773\n",
      "Iteration 323, loss = 0.11313840\n",
      "Iteration 324, loss = 0.11303975\n",
      "Iteration 325, loss = 0.11294177\n",
      "Iteration 326, loss = 0.11284445\n",
      "Iteration 327, loss = 0.11274782\n",
      "Iteration 328, loss = 0.11265187\n",
      "Iteration 329, loss = 0.11255657\n",
      "Iteration 330, loss = 0.11246190\n",
      "Iteration 331, loss = 0.11236787\n",
      "Iteration 332, loss = 0.11227445\n",
      "Iteration 333, loss = 0.11218165\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.33009489\n",
      "Iteration 3, loss = 1.30346572\n",
      "Iteration 4, loss = 1.27732700\n",
      "Iteration 5, loss = 1.25170132\n",
      "Iteration 6, loss = 1.22666135\n",
      "Iteration 7, loss = 1.20223979\n",
      "Iteration 8, loss = 1.17843107\n",
      "Iteration 9, loss = 1.15515594\n",
      "Iteration 10, loss = 1.13244415\n",
      "Iteration 11, loss = 1.11038284\n",
      "Iteration 12, loss = 1.08898304\n",
      "Iteration 13, loss = 1.06829344\n",
      "Iteration 14, loss = 1.04828722\n",
      "Iteration 15, loss = 1.02900234\n",
      "Iteration 16, loss = 1.01043707\n",
      "Iteration 17, loss = 0.99260728\n",
      "Iteration 18, loss = 0.97547184\n",
      "Iteration 19, loss = 0.95903481\n",
      "Iteration 20, loss = 0.94330243\n",
      "Iteration 21, loss = 0.92826029\n",
      "Iteration 22, loss = 0.91390221\n",
      "Iteration 23, loss = 0.90022123\n",
      "Iteration 24, loss = 0.88724220\n",
      "Iteration 25, loss = 0.87494733\n",
      "Iteration 26, loss = 0.86335129\n",
      "Iteration 27, loss = 0.85238961\n",
      "Iteration 28, loss = 0.84207495\n",
      "Iteration 29, loss = 0.83235934\n",
      "Iteration 30, loss = 0.82323805\n",
      "Iteration 31, loss = 0.81468567\n",
      "Iteration 32, loss = 0.80664261\n",
      "Iteration 33, loss = 0.79911997\n",
      "Iteration 34, loss = 0.79212227\n",
      "Iteration 35, loss = 0.78562358\n",
      "Iteration 36, loss = 0.77959123\n",
      "Iteration 37, loss = 0.77398808\n",
      "Iteration 38, loss = 0.76879598\n",
      "Iteration 39, loss = 0.76395967\n",
      "Iteration 40, loss = 0.75951701\n",
      "Iteration 41, loss = 0.75548287\n",
      "Iteration 42, loss = 0.75171875\n",
      "Iteration 43, loss = 0.74823026\n",
      "Iteration 44, loss = 0.74491026\n",
      "Iteration 45, loss = 0.74187379\n",
      "Iteration 46, loss = 0.73901306\n",
      "Iteration 47, loss = 0.73623077\n",
      "Iteration 48, loss = 0.73352461\n",
      "Iteration 49, loss = 0.73089149\n",
      "Iteration 50, loss = 0.72830036\n",
      "Iteration 51, loss = 0.72573704\n",
      "Iteration 52, loss = 0.72319944\n",
      "Iteration 53, loss = 0.72068324\n",
      "Iteration 54, loss = 0.71814857\n",
      "Iteration 55, loss = 0.71559897\n",
      "Iteration 56, loss = 0.71303736\n",
      "Iteration 57, loss = 0.71047106\n",
      "Iteration 58, loss = 0.70789241\n",
      "Iteration 59, loss = 0.70529200\n",
      "Iteration 60, loss = 0.70265787\n",
      "Iteration 61, loss = 0.70002010\n",
      "Iteration 62, loss = 0.69736787\n",
      "Iteration 63, loss = 0.69469631\n",
      "Iteration 64, loss = 0.69199054\n",
      "Iteration 65, loss = 0.68921282\n",
      "Iteration 66, loss = 0.68634574\n",
      "Iteration 67, loss = 0.68343410\n",
      "Iteration 68, loss = 0.68052875\n",
      "Iteration 69, loss = 0.67763173\n",
      "Iteration 70, loss = 0.67471576\n",
      "Iteration 71, loss = 0.67182971\n",
      "Iteration 72, loss = 0.66895361\n",
      "Iteration 73, loss = 0.66611459\n",
      "Iteration 74, loss = 0.66335415\n",
      "Iteration 75, loss = 0.66064013\n",
      "Iteration 76, loss = 0.65800497\n",
      "Iteration 77, loss = 0.65538176\n",
      "Iteration 78, loss = 0.65278038\n",
      "Iteration 79, loss = 0.65017749\n",
      "Iteration 80, loss = 0.64759967\n",
      "Iteration 81, loss = 0.64506119\n",
      "Iteration 82, loss = 0.64251046\n",
      "Iteration 83, loss = 0.63996469\n",
      "Iteration 84, loss = 0.63741479\n",
      "Iteration 85, loss = 0.63488393\n",
      "Iteration 86, loss = 0.63236299\n",
      "Iteration 87, loss = 0.62983461\n",
      "Iteration 88, loss = 0.62730932\n",
      "Iteration 89, loss = 0.62479435\n",
      "Iteration 90, loss = 0.62228398\n",
      "Iteration 91, loss = 0.61978091\n",
      "Iteration 92, loss = 0.61728805\n",
      "Iteration 93, loss = 0.61480916\n",
      "Iteration 94, loss = 0.61234249\n",
      "Iteration 95, loss = 0.60988870\n",
      "Iteration 96, loss = 0.60743178\n",
      "Iteration 97, loss = 0.60498672\n",
      "Iteration 98, loss = 0.60255649\n",
      "Iteration 99, loss = 0.60014195\n",
      "Iteration 100, loss = 0.59774445\n",
      "Iteration 101, loss = 0.59536417\n",
      "Iteration 102, loss = 0.59300252\n",
      "Iteration 103, loss = 0.59065932\n",
      "Iteration 104, loss = 0.58833453\n",
      "Iteration 105, loss = 0.58602914\n",
      "Iteration 106, loss = 0.58374401\n",
      "Iteration 107, loss = 0.58147943\n",
      "Iteration 108, loss = 0.57923365\n",
      "Iteration 109, loss = 0.57700610\n",
      "Iteration 110, loss = 0.57479705\n",
      "Iteration 111, loss = 0.57260755\n",
      "Iteration 112, loss = 0.57043683\n",
      "Iteration 113, loss = 0.56828449\n",
      "Iteration 114, loss = 0.56615035\n",
      "Iteration 115, loss = 0.56407985\n",
      "Iteration 116, loss = 0.56216624\n",
      "Iteration 117, loss = 0.56034768\n",
      "Iteration 118, loss = 0.55855105\n",
      "Iteration 119, loss = 0.55682340\n",
      "Iteration 120, loss = 0.55521350\n",
      "Iteration 121, loss = 0.55363915\n",
      "Iteration 122, loss = 0.55212648\n",
      "Iteration 123, loss = 0.55062174\n",
      "Iteration 124, loss = 0.54910416\n",
      "Iteration 125, loss = 0.54756860\n",
      "Iteration 126, loss = 0.54600446\n",
      "Iteration 127, loss = 0.54441934\n",
      "Iteration 128, loss = 0.54282692\n",
      "Iteration 129, loss = 0.54124770\n",
      "Iteration 130, loss = 0.53968233\n",
      "Iteration 131, loss = 0.53814848\n",
      "Iteration 132, loss = 0.53663082\n",
      "Iteration 133, loss = 0.53515648\n",
      "Iteration 134, loss = 0.53370827\n",
      "Iteration 135, loss = 0.53228539\n",
      "Iteration 136, loss = 0.53089242\n",
      "Iteration 137, loss = 0.52951242\n",
      "Iteration 138, loss = 0.52814333\n",
      "Iteration 139, loss = 0.52678797\n",
      "Iteration 140, loss = 0.52545076\n",
      "Iteration 141, loss = 0.52411862\n",
      "Iteration 142, loss = 0.52279218\n",
      "Iteration 143, loss = 0.52147314\n",
      "Iteration 144, loss = 0.52016186\n",
      "Iteration 145, loss = 0.51885866\n",
      "Iteration 146, loss = 0.51756678\n",
      "Iteration 147, loss = 0.51628689\n",
      "Iteration 148, loss = 0.51501622\n",
      "Iteration 149, loss = 0.51375896\n",
      "Iteration 150, loss = 0.51251271\n",
      "Iteration 151, loss = 0.51127635\n",
      "Iteration 152, loss = 0.51005007\n",
      "Iteration 153, loss = 0.50883884\n",
      "Iteration 154, loss = 0.50763823\n",
      "Iteration 155, loss = 0.50644864\n",
      "Iteration 156, loss = 0.50527035\n",
      "Iteration 157, loss = 0.50410110\n",
      "Iteration 158, loss = 0.50293823\n",
      "Iteration 159, loss = 0.50178179\n",
      "Iteration 160, loss = 0.50063181\n",
      "Iteration 161, loss = 0.49948834\n",
      "Iteration 162, loss = 0.49835144\n",
      "Iteration 163, loss = 0.49722266\n",
      "Iteration 164, loss = 0.49610426\n",
      "Iteration 165, loss = 0.49499876\n",
      "Iteration 166, loss = 0.49390180\n",
      "Iteration 167, loss = 0.49281283\n",
      "Iteration 168, loss = 0.49172945\n",
      "Iteration 169, loss = 0.49065176\n",
      "Iteration 170, loss = 0.48957988\n",
      "Iteration 171, loss = 0.48851387\n",
      "Iteration 172, loss = 0.48745433\n",
      "Iteration 173, loss = 0.48640623\n",
      "Iteration 174, loss = 0.48536612\n",
      "Iteration 175, loss = 0.48433445\n",
      "Iteration 176, loss = 0.48331050\n",
      "Iteration 177, loss = 0.48229336\n",
      "Iteration 178, loss = 0.48128262\n",
      "Iteration 179, loss = 0.48027822\n",
      "Iteration 180, loss = 0.47928082\n",
      "Iteration 181, loss = 0.47828960\n",
      "Iteration 182, loss = 0.47730488\n",
      "Iteration 183, loss = 0.47632712\n",
      "Iteration 184, loss = 0.47535650\n",
      "Iteration 185, loss = 0.47439169\n",
      "Iteration 186, loss = 0.47343258\n",
      "Iteration 187, loss = 0.47247913\n",
      "Iteration 188, loss = 0.47153139\n",
      "Iteration 189, loss = 0.47058982\n",
      "Iteration 190, loss = 0.46965384\n",
      "Iteration 191, loss = 0.46872382\n",
      "Iteration 192, loss = 0.46779923\n",
      "Iteration 193, loss = 0.46688081\n",
      "Iteration 194, loss = 0.46596729\n",
      "Iteration 195, loss = 0.46505861\n",
      "Iteration 196, loss = 0.46415473\n",
      "Iteration 197, loss = 0.46325558\n",
      "Iteration 198, loss = 0.46236113\n",
      "Iteration 199, loss = 0.46147132\n",
      "Iteration 200, loss = 0.46058638\n",
      "Iteration 201, loss = 0.45970604\n",
      "Iteration 202, loss = 0.45882989\n",
      "Iteration 203, loss = 0.45795782\n",
      "Iteration 204, loss = 0.45709042\n",
      "Iteration 205, loss = 0.45622722\n",
      "Iteration 206, loss = 0.45536814\n",
      "Iteration 207, loss = 0.45451303\n",
      "Iteration 208, loss = 0.45366192\n",
      "Iteration 209, loss = 0.45281470\n",
      "Iteration 210, loss = 0.45197131\n",
      "Iteration 211, loss = 0.45113169\n",
      "Iteration 212, loss = 0.45029577\n",
      "Iteration 213, loss = 0.44946551\n",
      "Iteration 214, loss = 0.44863941\n",
      "Iteration 215, loss = 0.44781874\n",
      "Iteration 216, loss = 0.44700229\n",
      "Iteration 217, loss = 0.44618927\n",
      "Iteration 218, loss = 0.44538125\n",
      "Iteration 219, loss = 0.44457689\n",
      "Iteration 220, loss = 0.44377535\n",
      "Iteration 221, loss = 0.44297681\n",
      "Iteration 222, loss = 0.44218121\n",
      "Iteration 223, loss = 0.44138881\n",
      "Iteration 224, loss = 0.44060031\n",
      "Iteration 225, loss = 0.43981499\n",
      "Iteration 226, loss = 0.43903306\n",
      "Iteration 227, loss = 0.43825396\n",
      "Iteration 228, loss = 0.43747889\n",
      "Iteration 229, loss = 0.43670682\n",
      "Iteration 230, loss = 0.43593775\n",
      "Iteration 231, loss = 0.43517253\n",
      "Iteration 232, loss = 0.43441050\n",
      "Iteration 233, loss = 0.43365126\n",
      "Iteration 234, loss = 0.43289470\n",
      "Iteration 235, loss = 0.43214077\n",
      "Iteration 236, loss = 0.43138981\n",
      "Iteration 237, loss = 0.43064145\n",
      "Iteration 238, loss = 0.42989556\n",
      "Iteration 239, loss = 0.42915214\n",
      "Iteration 240, loss = 0.42841135\n",
      "Iteration 241, loss = 0.42767379\n",
      "Iteration 242, loss = 0.42693890\n",
      "Iteration 243, loss = 0.42620635\n",
      "Iteration 244, loss = 0.42547647\n",
      "Iteration 245, loss = 0.42474904\n",
      "Iteration 246, loss = 0.42402397\n",
      "Iteration 247, loss = 0.42330124\n",
      "Iteration 248, loss = 0.42258083\n",
      "Iteration 249, loss = 0.42186272\n",
      "Iteration 250, loss = 0.42114688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251, loss = 0.42043335\n",
      "Iteration 252, loss = 0.41972198\n",
      "Iteration 253, loss = 0.41901322\n",
      "Iteration 254, loss = 0.41830699\n",
      "Iteration 255, loss = 0.41760297\n",
      "Iteration 256, loss = 0.41690102\n",
      "Iteration 257, loss = 0.41620154\n",
      "Iteration 258, loss = 0.41550447\n",
      "Iteration 259, loss = 0.41480945\n",
      "Iteration 260, loss = 0.41411643\n",
      "Iteration 261, loss = 0.41342540\n",
      "Iteration 262, loss = 0.41273632\n",
      "Iteration 263, loss = 0.41204914\n",
      "Iteration 264, loss = 0.41136385\n",
      "Iteration 265, loss = 0.41068038\n",
      "Iteration 266, loss = 0.40999872\n",
      "Iteration 267, loss = 0.40931884\n",
      "Iteration 268, loss = 0.40864069\n",
      "Iteration 269, loss = 0.40796443\n",
      "Iteration 270, loss = 0.40729019\n",
      "Iteration 271, loss = 0.40661762\n",
      "Iteration 272, loss = 0.40594668\n",
      "Iteration 273, loss = 0.40527733\n",
      "Iteration 274, loss = 0.40460955\n",
      "Iteration 275, loss = 0.40394331\n",
      "Iteration 276, loss = 0.40327856\n",
      "Iteration 277, loss = 0.40261529\n",
      "Iteration 278, loss = 0.40195347\n",
      "Iteration 279, loss = 0.40129306\n",
      "Iteration 280, loss = 0.40063402\n",
      "Iteration 281, loss = 0.39997634\n",
      "Iteration 282, loss = 0.39931999\n",
      "Iteration 283, loss = 0.39866493\n",
      "Iteration 284, loss = 0.39801114\n",
      "Iteration 285, loss = 0.39735860\n",
      "Iteration 286, loss = 0.39670728\n",
      "Iteration 287, loss = 0.39605741\n",
      "Iteration 288, loss = 0.39540937\n",
      "Iteration 289, loss = 0.39476356\n",
      "Iteration 290, loss = 0.39411883\n",
      "Iteration 291, loss = 0.39347519\n",
      "Iteration 292, loss = 0.39283265\n",
      "Iteration 293, loss = 0.39219121\n",
      "Iteration 294, loss = 0.39155088\n",
      "Iteration 295, loss = 0.39091167\n",
      "Iteration 296, loss = 0.39027357\n",
      "Iteration 297, loss = 0.38963657\n",
      "Iteration 298, loss = 0.38900067\n",
      "Iteration 299, loss = 0.38836586\n",
      "Iteration 300, loss = 0.38773215\n",
      "Iteration 301, loss = 0.38709950\n",
      "Iteration 302, loss = 0.38646847\n",
      "Iteration 303, loss = 0.38583823\n",
      "Iteration 304, loss = 0.38520869\n",
      "Iteration 305, loss = 0.38457984\n",
      "Iteration 306, loss = 0.38395227\n",
      "Iteration 307, loss = 0.38332580\n",
      "Iteration 308, loss = 0.38270024\n",
      "Iteration 309, loss = 0.38207558\n",
      "Iteration 310, loss = 0.38145181\n",
      "Iteration 311, loss = 0.38082906\n",
      "Iteration 312, loss = 0.38020748\n",
      "Iteration 313, loss = 0.37958678\n",
      "Iteration 314, loss = 0.37896697\n",
      "Iteration 315, loss = 0.37834801\n",
      "Iteration 316, loss = 0.37772991\n",
      "Iteration 317, loss = 0.37711265\n",
      "Iteration 318, loss = 0.37649621\n",
      "Iteration 319, loss = 0.37588059\n",
      "Iteration 320, loss = 0.37526578\n",
      "Iteration 321, loss = 0.37465249\n",
      "Iteration 322, loss = 0.37403859\n",
      "Iteration 323, loss = 0.37342619\n",
      "Iteration 324, loss = 0.37281453\n",
      "Iteration 325, loss = 0.37220361\n",
      "Iteration 326, loss = 0.37159340\n",
      "Iteration 327, loss = 0.37098406\n",
      "Iteration 328, loss = 0.37037572\n",
      "Iteration 329, loss = 0.36976847\n",
      "Iteration 330, loss = 0.36916234\n",
      "Iteration 331, loss = 0.36855690\n",
      "Iteration 332, loss = 0.36795214\n",
      "Iteration 333, loss = 0.36734806\n",
      "Iteration 334, loss = 0.36674490\n",
      "Iteration 335, loss = 0.36614241\n",
      "Iteration 336, loss = 0.36554060\n",
      "Iteration 337, loss = 0.36493944\n",
      "Iteration 338, loss = 0.36433893\n",
      "Iteration 339, loss = 0.36373905\n",
      "Iteration 340, loss = 0.36313978\n",
      "Iteration 341, loss = 0.36254110\n",
      "Iteration 342, loss = 0.36194295\n",
      "Iteration 343, loss = 0.36134528\n",
      "Iteration 344, loss = 0.36074784\n",
      "Iteration 345, loss = 0.36014793\n",
      "Iteration 346, loss = 0.35952676\n",
      "Iteration 347, loss = 0.35886063\n",
      "Iteration 348, loss = 0.35816189\n",
      "Iteration 349, loss = 0.35754969\n",
      "Iteration 350, loss = 0.35695687\n",
      "Iteration 351, loss = 0.35632937\n",
      "Iteration 352, loss = 0.35567392\n",
      "Iteration 353, loss = 0.35499652\n",
      "Iteration 354, loss = 0.35430903\n",
      "Iteration 355, loss = 0.35364008\n",
      "Iteration 356, loss = 0.35303495\n",
      "Iteration 357, loss = 0.35234822\n",
      "Iteration 358, loss = 0.35164602\n",
      "Iteration 359, loss = 0.35098858\n",
      "Iteration 360, loss = 0.35032916\n",
      "Iteration 361, loss = 0.34965665\n",
      "Iteration 362, loss = 0.34896952\n",
      "Iteration 363, loss = 0.34826914\n",
      "Iteration 364, loss = 0.34756079\n",
      "Iteration 365, loss = 0.34685009\n",
      "Iteration 366, loss = 0.34614282\n",
      "Iteration 367, loss = 0.34544534\n",
      "Iteration 368, loss = 0.34472350\n",
      "Iteration 369, loss = 0.34399457\n",
      "Iteration 370, loss = 0.34327081\n",
      "Iteration 371, loss = 0.34254576\n",
      "Iteration 372, loss = 0.34181284\n",
      "Iteration 373, loss = 0.34107215\n",
      "Iteration 374, loss = 0.34032415\n",
      "Iteration 375, loss = 0.33957058\n",
      "Iteration 376, loss = 0.33881219\n",
      "Iteration 377, loss = 0.33805014\n",
      "Iteration 378, loss = 0.33728477\n",
      "Iteration 379, loss = 0.33651585\n",
      "Iteration 380, loss = 0.33574194\n",
      "Iteration 381, loss = 0.33496379\n",
      "Iteration 382, loss = 0.33418043\n",
      "Iteration 383, loss = 0.33339288\n",
      "Iteration 384, loss = 0.33260161\n",
      "Iteration 385, loss = 0.33180713\n",
      "Iteration 386, loss = 0.33100898\n",
      "Iteration 387, loss = 0.33020952\n",
      "Iteration 388, loss = 0.32940668\n",
      "Iteration 389, loss = 0.32859610\n",
      "Iteration 390, loss = 0.32778212\n",
      "Iteration 391, loss = 0.32696674\n",
      "Iteration 392, loss = 0.32614807\n",
      "Iteration 393, loss = 0.32532628\n",
      "Iteration 394, loss = 0.32450223\n",
      "Iteration 395, loss = 0.32367417\n",
      "Iteration 396, loss = 0.32284411\n",
      "Iteration 397, loss = 0.32201278\n",
      "Iteration 398, loss = 0.32117780\n",
      "Iteration 399, loss = 0.32034157\n",
      "Iteration 400, loss = 0.31950362\n",
      "Iteration 401, loss = 0.31866310\n",
      "Iteration 402, loss = 0.31782144\n",
      "Iteration 403, loss = 0.31697819\n",
      "Iteration 404, loss = 0.31613368\n",
      "Iteration 405, loss = 0.31528765\n",
      "Iteration 406, loss = 0.31444040\n",
      "Iteration 407, loss = 0.31359182\n",
      "Iteration 408, loss = 0.31274207\n",
      "Iteration 409, loss = 0.31189125\n",
      "Iteration 410, loss = 0.31104061\n",
      "Iteration 411, loss = 0.31018831\n",
      "Iteration 412, loss = 0.30933540\n",
      "Iteration 413, loss = 0.30848259\n",
      "Iteration 414, loss = 0.30762965\n",
      "Iteration 415, loss = 0.30677488\n",
      "Iteration 416, loss = 0.30592096\n",
      "Iteration 417, loss = 0.30506707\n",
      "Iteration 418, loss = 0.30421292\n",
      "Iteration 419, loss = 0.30335755\n",
      "Iteration 420, loss = 0.30251104\n",
      "Iteration 421, loss = 0.30165404\n",
      "Iteration 422, loss = 0.30079669\n",
      "Iteration 423, loss = 0.29994528\n",
      "Iteration 424, loss = 0.29909312\n",
      "Iteration 425, loss = 0.29823971\n",
      "Iteration 426, loss = 0.29738423\n",
      "Iteration 427, loss = 0.29653127\n",
      "Iteration 428, loss = 0.29567854\n",
      "Iteration 429, loss = 0.29482519\n",
      "Iteration 430, loss = 0.29396881\n",
      "Iteration 431, loss = 0.29311491\n",
      "Iteration 432, loss = 0.29226247\n",
      "Iteration 433, loss = 0.29140748\n",
      "Iteration 434, loss = 0.29055387\n",
      "Iteration 435, loss = 0.28969856\n",
      "Iteration 436, loss = 0.28884479\n",
      "Iteration 437, loss = 0.28799082\n",
      "Iteration 438, loss = 0.28713687\n",
      "Iteration 439, loss = 0.28628398\n",
      "Iteration 440, loss = 0.28543044\n",
      "Iteration 441, loss = 0.28458193\n",
      "Iteration 442, loss = 0.28372714\n",
      "Iteration 443, loss = 0.28287874\n",
      "Iteration 444, loss = 0.28202863\n",
      "Iteration 445, loss = 0.28117811\n",
      "Iteration 446, loss = 0.28033117\n",
      "Iteration 447, loss = 0.27948360\n",
      "Iteration 448, loss = 0.27863481\n",
      "Iteration 449, loss = 0.27778790\n",
      "Iteration 450, loss = 0.27694260\n",
      "Iteration 451, loss = 0.27609745\n",
      "Iteration 452, loss = 0.27525276\n",
      "Iteration 453, loss = 0.27441039\n",
      "Iteration 454, loss = 0.27357065\n",
      "Iteration 455, loss = 0.27272916\n",
      "Iteration 456, loss = 0.27189190\n",
      "Iteration 457, loss = 0.27105505\n",
      "Iteration 458, loss = 0.27021909\n",
      "Iteration 459, loss = 0.26938546\n",
      "Iteration 460, loss = 0.26855319\n",
      "Iteration 461, loss = 0.26772289\n",
      "Iteration 462, loss = 0.26689443\n",
      "Iteration 463, loss = 0.26606829\n",
      "Iteration 464, loss = 0.26524333\n",
      "Iteration 465, loss = 0.26442052\n",
      "Iteration 466, loss = 0.26359960\n",
      "Iteration 467, loss = 0.26278063\n",
      "Iteration 468, loss = 0.26196365\n",
      "Iteration 469, loss = 0.26114884\n",
      "Iteration 470, loss = 0.26033598\n",
      "Iteration 471, loss = 0.25952543\n",
      "Iteration 472, loss = 0.25871718\n",
      "Iteration 473, loss = 0.25791229\n",
      "Iteration 474, loss = 0.25710851\n",
      "Iteration 475, loss = 0.25630805\n",
      "Iteration 476, loss = 0.25551007\n",
      "Iteration 477, loss = 0.25471497\n",
      "Iteration 478, loss = 0.25392250\n",
      "Iteration 479, loss = 0.25313308\n",
      "Iteration 480, loss = 0.25234647\n",
      "Iteration 481, loss = 0.25156254\n",
      "Iteration 482, loss = 0.25078135\n",
      "Iteration 483, loss = 0.25000291\n",
      "Iteration 484, loss = 0.24922991\n",
      "Iteration 485, loss = 0.24845782\n",
      "Iteration 486, loss = 0.24769139\n",
      "Iteration 487, loss = 0.24692769\n",
      "Iteration 488, loss = 0.24616680\n",
      "Iteration 489, loss = 0.24540879\n",
      "Iteration 490, loss = 0.24465374\n",
      "Iteration 491, loss = 0.24390172\n",
      "Iteration 492, loss = 0.24315279\n",
      "Iteration 493, loss = 0.24240702\n",
      "Iteration 494, loss = 0.24166463\n",
      "Iteration 495, loss = 0.24092573\n",
      "Iteration 496, loss = 0.24019064\n",
      "Iteration 497, loss = 0.23945876\n",
      "Iteration 498, loss = 0.23873007\n",
      "Iteration 499, loss = 0.23800461\n",
      "Iteration 500, loss = 0.23728240\n",
      "Iteration 501, loss = 0.23656347\n",
      "Iteration 502, loss = 0.23584790\n",
      "Iteration 503, loss = 0.23513571\n",
      "Iteration 504, loss = 0.23442688\n",
      "Iteration 505, loss = 0.23372167\n",
      "Iteration 506, loss = 0.23301991\n",
      "Iteration 507, loss = 0.23232169\n",
      "Iteration 508, loss = 0.23162685\n",
      "Iteration 509, loss = 0.23093536\n",
      "Iteration 510, loss = 0.23024731\n",
      "Iteration 511, loss = 0.22956263\n",
      "Iteration 512, loss = 0.22888131\n",
      "Iteration 513, loss = 0.22820334\n",
      "Iteration 514, loss = 0.22752871\n",
      "Iteration 515, loss = 0.22685742\n",
      "Iteration 516, loss = 0.22618946\n",
      "Iteration 517, loss = 0.22552481\n",
      "Iteration 518, loss = 0.22486366\n",
      "Iteration 519, loss = 0.22420550\n",
      "Iteration 520, loss = 0.22355131\n",
      "Iteration 521, loss = 0.22290050\n",
      "Iteration 522, loss = 0.22225290\n",
      "Iteration 523, loss = 0.22160851\n",
      "Iteration 524, loss = 0.22096734\n",
      "Iteration 525, loss = 0.22032941\n",
      "Iteration 526, loss = 0.21969470\n",
      "Iteration 527, loss = 0.21906322\n",
      "Iteration 528, loss = 0.21843496\n",
      "Iteration 529, loss = 0.21780994\n",
      "Iteration 530, loss = 0.21718836\n",
      "Iteration 531, loss = 0.21657011\n",
      "Iteration 532, loss = 0.21595497\n",
      "Iteration 533, loss = 0.21534293\n",
      "Iteration 534, loss = 0.21473415\n",
      "Iteration 535, loss = 0.21412854\n",
      "Iteration 536, loss = 0.21352635\n",
      "Iteration 537, loss = 0.21292734\n",
      "Iteration 538, loss = 0.21233160\n",
      "Iteration 539, loss = 0.21173907\n",
      "Iteration 540, loss = 0.21114963\n",
      "Iteration 541, loss = 0.21056328\n",
      "Iteration 542, loss = 0.20998119\n",
      "Iteration 543, loss = 0.20940118\n",
      "Iteration 544, loss = 0.20882414\n",
      "Iteration 545, loss = 0.20825114\n",
      "Iteration 546, loss = 0.20768110\n",
      "Iteration 547, loss = 0.20711460\n",
      "Iteration 548, loss = 0.20655048\n",
      "Iteration 549, loss = 0.20598942\n",
      "Iteration 550, loss = 0.20543118\n",
      "Iteration 551, loss = 0.20487623\n",
      "Iteration 552, loss = 0.20432451\n",
      "Iteration 553, loss = 0.20377522\n",
      "Iteration 554, loss = 0.20322870\n",
      "Iteration 555, loss = 0.20268679\n",
      "Iteration 556, loss = 0.20214685\n",
      "Iteration 557, loss = 0.20160917\n",
      "Iteration 558, loss = 0.20107610\n",
      "Iteration 559, loss = 0.20054535\n",
      "Iteration 560, loss = 0.20001642\n",
      "Iteration 561, loss = 0.19948985\n",
      "Iteration 562, loss = 0.19896819\n",
      "Iteration 563, loss = 0.19844853\n",
      "Iteration 564, loss = 0.19793057\n",
      "Iteration 565, loss = 0.19741669\n",
      "Iteration 566, loss = 0.19690567\n",
      "Iteration 567, loss = 0.19639721\n",
      "Iteration 568, loss = 0.19589197\n",
      "Iteration 569, loss = 0.19538934\n",
      "Iteration 570, loss = 0.19488935\n",
      "Iteration 571, loss = 0.19439174\n",
      "Iteration 572, loss = 0.19389791\n",
      "Iteration 573, loss = 0.19340658\n",
      "Iteration 574, loss = 0.19291660\n",
      "Iteration 575, loss = 0.19243004\n",
      "Iteration 576, loss = 0.19194610\n",
      "Iteration 577, loss = 0.19146511\n",
      "Iteration 578, loss = 0.19098655\n",
      "Iteration 579, loss = 0.19051061\n",
      "Iteration 580, loss = 0.19003777\n",
      "Iteration 581, loss = 0.18956706\n",
      "Iteration 582, loss = 0.18909862\n",
      "Iteration 583, loss = 0.18863284\n",
      "Iteration 584, loss = 0.18816992\n",
      "Iteration 585, loss = 0.18771026\n",
      "Iteration 586, loss = 0.18725163\n",
      "Iteration 587, loss = 0.18679602\n",
      "Iteration 588, loss = 0.18634323\n",
      "Iteration 589, loss = 0.18589313\n",
      "Iteration 590, loss = 0.18544505\n",
      "Iteration 591, loss = 0.18499902\n",
      "Iteration 592, loss = 0.18455533\n",
      "Iteration 593, loss = 0.18411412\n",
      "Iteration 594, loss = 0.18367669\n",
      "Iteration 595, loss = 0.18324046\n",
      "Iteration 596, loss = 0.18280589\n",
      "Iteration 597, loss = 0.18237467\n",
      "Iteration 598, loss = 0.18194596\n",
      "Iteration 599, loss = 0.18151938\n",
      "Iteration 600, loss = 0.18109490\n",
      "Iteration 601, loss = 0.18067254\n",
      "Iteration 602, loss = 0.18025236\n",
      "Iteration 603, loss = 0.17983441\n",
      "Iteration 604, loss = 0.17941869\n",
      "Iteration 605, loss = 0.17900523\n",
      "Iteration 606, loss = 0.17859494\n",
      "Iteration 607, loss = 0.17818623\n",
      "Iteration 608, loss = 0.17777884\n",
      "Iteration 609, loss = 0.17737463\n",
      "Iteration 610, loss = 0.17697267\n",
      "Iteration 611, loss = 0.17657277\n",
      "Iteration 612, loss = 0.17617492\n",
      "Iteration 613, loss = 0.17577909\n",
      "Iteration 614, loss = 0.17538530\n",
      "Iteration 615, loss = 0.17499355\n",
      "Iteration 616, loss = 0.17460394\n",
      "Iteration 617, loss = 0.17421623\n",
      "Iteration 618, loss = 0.17383060\n",
      "Iteration 619, loss = 0.17344684\n",
      "Iteration 620, loss = 0.17306508\n",
      "Iteration 621, loss = 0.17268547\n",
      "Iteration 622, loss = 0.17230775\n",
      "Iteration 623, loss = 0.17193226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 624, loss = 0.17155879\n",
      "Iteration 625, loss = 0.17118717\n",
      "Iteration 626, loss = 0.17081761\n",
      "Iteration 627, loss = 0.17044997\n",
      "Iteration 628, loss = 0.17008424\n",
      "Iteration 629, loss = 0.16972042\n",
      "Iteration 630, loss = 0.16935849\n",
      "Iteration 631, loss = 0.16899843\n",
      "Iteration 632, loss = 0.16864026\n",
      "Iteration 633, loss = 0.16828398\n",
      "Iteration 634, loss = 0.16792958\n",
      "Iteration 635, loss = 0.16757702\n",
      "Iteration 636, loss = 0.16722631\n",
      "Iteration 637, loss = 0.16687772\n",
      "Iteration 638, loss = 0.16653042\n",
      "Iteration 639, loss = 0.16618532\n",
      "Iteration 640, loss = 0.16584184\n",
      "Iteration 641, loss = 0.16550026\n",
      "Iteration 642, loss = 0.16516049\n",
      "Iteration 643, loss = 0.16482250\n",
      "Iteration 644, loss = 0.16448625\n",
      "Iteration 645, loss = 0.16415174\n",
      "Iteration 646, loss = 0.16381894\n",
      "Iteration 647, loss = 0.16348786\n",
      "Iteration 648, loss = 0.16315825\n",
      "Iteration 649, loss = 0.16283017\n",
      "Iteration 650, loss = 0.16250374\n",
      "Iteration 651, loss = 0.16217896\n",
      "Iteration 652, loss = 0.16185588\n",
      "Iteration 653, loss = 0.16153466\n",
      "Iteration 654, loss = 0.16121490\n",
      "Iteration 655, loss = 0.16089680\n",
      "Iteration 656, loss = 0.16058058\n",
      "Iteration 657, loss = 0.16026607\n",
      "Iteration 658, loss = 0.15995296\n",
      "Iteration 659, loss = 0.15964127\n",
      "Iteration 660, loss = 0.15933102\n",
      "Iteration 661, loss = 0.15902141\n",
      "Iteration 662, loss = 0.15871277\n",
      "Iteration 663, loss = 0.15840546\n",
      "Iteration 664, loss = 0.15809951\n",
      "Iteration 665, loss = 0.15779501\n",
      "Iteration 666, loss = 0.15749186\n",
      "Iteration 667, loss = 0.15718938\n",
      "Iteration 668, loss = 0.15688834\n",
      "Iteration 669, loss = 0.15658864\n",
      "Iteration 670, loss = 0.15629052\n",
      "Iteration 671, loss = 0.15599408\n",
      "Iteration 672, loss = 0.15569902\n",
      "Iteration 673, loss = 0.15540552\n",
      "Iteration 674, loss = 0.15511334\n",
      "Iteration 675, loss = 0.15482306\n",
      "Iteration 676, loss = 0.15453473\n",
      "Iteration 677, loss = 0.15424821\n",
      "Iteration 678, loss = 0.15396262\n",
      "Iteration 679, loss = 0.15367867\n",
      "Iteration 680, loss = 0.15339658\n",
      "Iteration 681, loss = 0.15311564\n",
      "Iteration 682, loss = 0.15283583\n",
      "Iteration 683, loss = 0.15255735\n",
      "Iteration 684, loss = 0.15228149\n",
      "Iteration 685, loss = 0.15200694\n",
      "Iteration 686, loss = 0.15173562\n",
      "Iteration 687, loss = 0.15146566\n",
      "Iteration 688, loss = 0.15119758\n",
      "Iteration 689, loss = 0.15093125\n",
      "Iteration 690, loss = 0.15066634\n",
      "Iteration 691, loss = 0.15040286\n",
      "Iteration 692, loss = 0.15014078\n",
      "Iteration 693, loss = 0.14988012\n",
      "Iteration 694, loss = 0.14962084\n",
      "Iteration 695, loss = 0.14936290\n",
      "Iteration 696, loss = 0.14910629\n",
      "Iteration 697, loss = 0.14885108\n",
      "Iteration 698, loss = 0.14859702\n",
      "Iteration 699, loss = 0.14834436\n",
      "Iteration 700, loss = 0.14809298\n",
      "Iteration 701, loss = 0.14784285\n",
      "Iteration 702, loss = 0.14759401\n",
      "Iteration 703, loss = 0.14734635\n",
      "Iteration 704, loss = 0.14709996\n",
      "Iteration 705, loss = 0.14685477\n",
      "Iteration 706, loss = 0.14661075\n",
      "Iteration 707, loss = 0.14636711\n",
      "Iteration 708, loss = 0.14612375\n",
      "Iteration 709, loss = 0.14588128\n",
      "Iteration 710, loss = 0.14563975\n",
      "Iteration 711, loss = 0.14539946\n",
      "Iteration 712, loss = 0.14516050\n",
      "Iteration 713, loss = 0.14492242\n",
      "Iteration 714, loss = 0.14468563\n",
      "Iteration 715, loss = 0.14444983\n",
      "Iteration 716, loss = 0.14421495\n",
      "Iteration 717, loss = 0.14398132\n",
      "Iteration 718, loss = 0.14374867\n",
      "Iteration 719, loss = 0.14351667\n",
      "Iteration 720, loss = 0.14328595\n",
      "Iteration 721, loss = 0.14305623\n",
      "Iteration 722, loss = 0.14282743\n",
      "Iteration 723, loss = 0.14259972\n",
      "Iteration 724, loss = 0.14237307\n",
      "Iteration 725, loss = 0.14214910\n",
      "Iteration 726, loss = 0.14192645\n",
      "Iteration 727, loss = 0.14170458\n",
      "Iteration 728, loss = 0.14148335\n",
      "Iteration 729, loss = 0.14126310\n",
      "Iteration 730, loss = 0.14104389\n",
      "Iteration 731, loss = 0.14082581\n",
      "Iteration 732, loss = 0.14060889\n",
      "Iteration 733, loss = 0.14039285\n",
      "Iteration 734, loss = 0.14017780\n",
      "Iteration 735, loss = 0.13996387\n",
      "Iteration 736, loss = 0.13975098\n",
      "Iteration 737, loss = 0.13953914\n",
      "Iteration 738, loss = 0.13932827\n",
      "Iteration 739, loss = 0.13911861\n",
      "Iteration 740, loss = 0.13891072\n",
      "Iteration 741, loss = 0.13870385\n",
      "Iteration 742, loss = 0.13849797\n",
      "Iteration 743, loss = 0.13829309\n",
      "Iteration 744, loss = 0.13808891\n",
      "Iteration 745, loss = 0.13788560\n",
      "Iteration 746, loss = 0.13768305\n",
      "Iteration 747, loss = 0.13748174\n",
      "Iteration 748, loss = 0.13728124\n",
      "Iteration 749, loss = 0.13708181\n",
      "Iteration 750, loss = 0.13688325\n",
      "Iteration 751, loss = 0.13668601\n",
      "Iteration 752, loss = 0.13649009\n",
      "Iteration 753, loss = 0.13629519\n",
      "Iteration 754, loss = 0.13610114\n",
      "Iteration 755, loss = 0.13590796\n",
      "Iteration 756, loss = 0.13571562\n",
      "Iteration 757, loss = 0.13552433\n",
      "Iteration 758, loss = 0.13533378\n",
      "Iteration 759, loss = 0.13514422\n",
      "Iteration 760, loss = 0.13495558\n",
      "Iteration 761, loss = 0.13476781\n",
      "Iteration 762, loss = 0.13458095\n",
      "Iteration 763, loss = 0.13439504\n",
      "Iteration 764, loss = 0.13420981\n",
      "Iteration 765, loss = 0.13402555\n",
      "Iteration 766, loss = 0.13384208\n",
      "Iteration 767, loss = 0.13365925\n",
      "Iteration 768, loss = 0.13347721\n",
      "Iteration 769, loss = 0.13329603\n",
      "Iteration 770, loss = 0.13311582\n",
      "Iteration 771, loss = 0.13293605\n",
      "Iteration 772, loss = 0.13275729\n",
      "Iteration 773, loss = 0.13257934\n",
      "Iteration 774, loss = 0.13240210\n",
      "Iteration 775, loss = 0.13222557\n",
      "Iteration 776, loss = 0.13204985\n",
      "Iteration 777, loss = 0.13187498\n",
      "Iteration 778, loss = 0.13170088\n",
      "Iteration 779, loss = 0.13152752\n",
      "Iteration 780, loss = 0.13135496\n",
      "Iteration 781, loss = 0.13118312\n",
      "Iteration 782, loss = 0.13101200\n",
      "Iteration 783, loss = 0.13084160\n",
      "Iteration 784, loss = 0.13067197\n",
      "Iteration 785, loss = 0.13050307\n",
      "Iteration 786, loss = 0.13033496\n",
      "Iteration 787, loss = 0.13016748\n",
      "Iteration 788, loss = 0.13000076\n",
      "Iteration 789, loss = 0.12983471\n",
      "Iteration 790, loss = 0.12966939\n",
      "Iteration 791, loss = 0.12950471\n",
      "Iteration 792, loss = 0.12934085\n",
      "Iteration 793, loss = 0.12917752\n",
      "Iteration 794, loss = 0.12901517\n",
      "Iteration 795, loss = 0.12885332\n",
      "Iteration 796, loss = 0.12869194\n",
      "Iteration 797, loss = 0.12853139\n",
      "Iteration 798, loss = 0.12837159\n",
      "Iteration 799, loss = 0.12821239\n",
      "Iteration 800, loss = 0.12805399\n",
      "Iteration 801, loss = 0.12789612\n",
      "Iteration 802, loss = 0.12773881\n",
      "Iteration 803, loss = 0.12758225\n",
      "Iteration 804, loss = 0.12742636\n",
      "Iteration 805, loss = 0.12727109\n",
      "Iteration 806, loss = 0.12711647\n",
      "Iteration 807, loss = 0.12696245\n",
      "Iteration 808, loss = 0.12680918\n",
      "Iteration 809, loss = 0.12665649\n",
      "Iteration 810, loss = 0.12650426\n",
      "Iteration 811, loss = 0.12635306\n",
      "Iteration 812, loss = 0.12620223\n",
      "Iteration 813, loss = 0.12605176\n",
      "Iteration 814, loss = 0.12590232\n",
      "Iteration 815, loss = 0.12575343\n",
      "Iteration 816, loss = 0.12560477\n",
      "Iteration 817, loss = 0.12545708\n",
      "Iteration 818, loss = 0.12531010\n",
      "Iteration 819, loss = 0.12516345\n",
      "Iteration 820, loss = 0.12501729\n",
      "Iteration 821, loss = 0.12487203\n",
      "Iteration 822, loss = 0.12472734\n",
      "Iteration 823, loss = 0.12458291\n",
      "Iteration 824, loss = 0.12443932\n",
      "Iteration 825, loss = 0.12429628\n",
      "Iteration 826, loss = 0.12415374\n",
      "Iteration 827, loss = 0.12401183\n",
      "Iteration 828, loss = 0.12387047\n",
      "Iteration 829, loss = 0.12372976\n",
      "Iteration 830, loss = 0.12358962\n",
      "Iteration 831, loss = 0.12344996\n",
      "Iteration 832, loss = 0.12331098\n",
      "Iteration 833, loss = 0.12317244\n",
      "Iteration 834, loss = 0.12303461\n",
      "Iteration 835, loss = 0.12289714\n",
      "Iteration 836, loss = 0.12276038\n",
      "Iteration 837, loss = 0.12262417\n",
      "Iteration 838, loss = 0.12248840\n",
      "Iteration 839, loss = 0.12235315\n",
      "Iteration 840, loss = 0.12221872\n",
      "Iteration 841, loss = 0.12208463\n",
      "Iteration 842, loss = 0.12195098\n",
      "Iteration 843, loss = 0.12181802\n",
      "Iteration 844, loss = 0.12168544\n",
      "Iteration 845, loss = 0.12155336\n",
      "Iteration 846, loss = 0.12142193\n",
      "Iteration 847, loss = 0.12129103\n",
      "Iteration 848, loss = 0.12116081\n",
      "Iteration 849, loss = 0.12103070\n",
      "Iteration 850, loss = 0.12090131\n",
      "Iteration 851, loss = 0.12077253\n",
      "Iteration 852, loss = 0.12064413\n",
      "Iteration 853, loss = 0.12051633\n",
      "Iteration 854, loss = 0.12038889\n",
      "Iteration 855, loss = 0.12026214\n",
      "Iteration 856, loss = 0.12013585\n",
      "Iteration 857, loss = 0.12001011\n",
      "Iteration 858, loss = 0.11988474\n",
      "Iteration 859, loss = 0.11975970\n",
      "Iteration 860, loss = 0.11963530\n",
      "Iteration 861, loss = 0.11951135\n",
      "Iteration 862, loss = 0.11938803\n",
      "Iteration 863, loss = 0.11926509\n",
      "Iteration 864, loss = 0.11914236\n",
      "Iteration 865, loss = 0.11902028\n",
      "Iteration 866, loss = 0.11889872\n",
      "Iteration 867, loss = 0.11877755\n",
      "Iteration 868, loss = 0.11865661\n",
      "Iteration 869, loss = 0.11853612\n",
      "Iteration 870, loss = 0.11841604\n",
      "Iteration 871, loss = 0.11829641\n",
      "Iteration 872, loss = 0.11817734\n",
      "Iteration 873, loss = 0.11805852\n",
      "Iteration 874, loss = 0.11794036\n",
      "Iteration 875, loss = 0.11782262\n",
      "Iteration 876, loss = 0.11770548\n",
      "Iteration 877, loss = 0.11758854\n",
      "Iteration 878, loss = 0.11747181\n",
      "Iteration 879, loss = 0.11735593\n",
      "Iteration 880, loss = 0.11724047\n",
      "Iteration 881, loss = 0.11712527\n",
      "Iteration 882, loss = 0.11701034\n",
      "Iteration 883, loss = 0.11689594\n",
      "Iteration 884, loss = 0.11678212\n",
      "Iteration 885, loss = 0.11666863\n",
      "Iteration 886, loss = 0.11655546\n",
      "Iteration 887, loss = 0.11644290\n",
      "Iteration 888, loss = 0.11633097\n",
      "Iteration 889, loss = 0.11621934\n",
      "Iteration 890, loss = 0.11610826\n",
      "Iteration 891, loss = 0.11599754\n",
      "Iteration 892, loss = 0.11588719\n",
      "Iteration 893, loss = 0.11577773\n",
      "Iteration 894, loss = 0.11566878\n",
      "Iteration 895, loss = 0.11556012\n",
      "Iteration 896, loss = 0.11545176\n",
      "Iteration 897, loss = 0.11534384\n",
      "Iteration 898, loss = 0.11523650\n",
      "Iteration 899, loss = 0.11512950\n",
      "Iteration 900, loss = 0.11502284\n",
      "Iteration 901, loss = 0.11491665\n",
      "Iteration 902, loss = 0.11481096\n",
      "Iteration 903, loss = 0.11470551\n",
      "Iteration 904, loss = 0.11460052\n",
      "Iteration 905, loss = 0.11449596\n",
      "Iteration 906, loss = 0.11439178\n",
      "Iteration 907, loss = 0.11428795\n",
      "Iteration 908, loss = 0.11418454\n",
      "Iteration 909, loss = 0.11408153\n",
      "Iteration 910, loss = 0.11397888\n",
      "Iteration 911, loss = 0.11387658\n",
      "Iteration 912, loss = 0.11377463\n",
      "Iteration 913, loss = 0.11367311\n",
      "Iteration 914, loss = 0.11357194\n",
      "Iteration 915, loss = 0.11347115\n",
      "Iteration 916, loss = 0.11337073\n",
      "Iteration 917, loss = 0.11327068\n",
      "Iteration 918, loss = 0.11317098\n",
      "Iteration 919, loss = 0.11307163\n",
      "Iteration 920, loss = 0.11297279\n",
      "Iteration 921, loss = 0.11287415\n",
      "Iteration 922, loss = 0.11277588\n",
      "Iteration 923, loss = 0.11267805\n",
      "Iteration 924, loss = 0.11258054\n",
      "Iteration 925, loss = 0.11248337\n",
      "Iteration 926, loss = 0.11238653\n",
      "Iteration 927, loss = 0.11229002\n",
      "Iteration 928, loss = 0.11219385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.33447200\n",
      "Iteration 3, loss = 1.30795436\n",
      "Iteration 4, loss = 1.28193138\n",
      "Iteration 5, loss = 1.25642138\n",
      "Iteration 6, loss = 1.23147408\n",
      "Iteration 7, loss = 1.20713579\n",
      "Iteration 8, loss = 1.18341619\n",
      "Iteration 9, loss = 1.16024621\n",
      "Iteration 10, loss = 1.13764362\n",
      "Iteration 11, loss = 1.11568987\n",
      "Iteration 12, loss = 1.09439915\n",
      "Iteration 13, loss = 1.07380354\n",
      "Iteration 14, loss = 1.05389406\n",
      "Iteration 15, loss = 1.03469235\n",
      "Iteration 16, loss = 1.01617501\n",
      "Iteration 17, loss = 0.99837495\n",
      "Iteration 18, loss = 0.98126395\n",
      "Iteration 19, loss = 0.96486348\n",
      "Iteration 20, loss = 0.94916245\n",
      "Iteration 21, loss = 0.93416830\n",
      "Iteration 22, loss = 0.91986348\n",
      "Iteration 23, loss = 0.90622404\n",
      "Iteration 24, loss = 0.89327890\n",
      "Iteration 25, loss = 0.88099813\n",
      "Iteration 26, loss = 0.86939625\n",
      "Iteration 27, loss = 0.85841180\n",
      "Iteration 28, loss = 0.84804436\n",
      "Iteration 29, loss = 0.83825873\n",
      "Iteration 30, loss = 0.82906359\n",
      "Iteration 31, loss = 0.82040915\n",
      "Iteration 32, loss = 0.81225520\n",
      "Iteration 33, loss = 0.80460337\n",
      "Iteration 34, loss = 0.79747052\n",
      "Iteration 35, loss = 0.79082224\n",
      "Iteration 36, loss = 0.78464901\n",
      "Iteration 37, loss = 0.77892238\n",
      "Iteration 38, loss = 0.77360873\n",
      "Iteration 39, loss = 0.76866538\n",
      "Iteration 40, loss = 0.76412003\n",
      "Iteration 41, loss = 0.75996739\n",
      "Iteration 42, loss = 0.75611761\n",
      "Iteration 43, loss = 0.75255812\n",
      "Iteration 44, loss = 0.74918366\n",
      "Iteration 45, loss = 0.74607968\n",
      "Iteration 46, loss = 0.74309653\n",
      "Iteration 47, loss = 0.74022581\n",
      "Iteration 48, loss = 0.73747102\n",
      "Iteration 49, loss = 0.73477969\n",
      "Iteration 50, loss = 0.73213564\n",
      "Iteration 51, loss = 0.72953012\n",
      "Iteration 52, loss = 0.72695714\n",
      "Iteration 53, loss = 0.72441710\n",
      "Iteration 54, loss = 0.72186197\n",
      "Iteration 55, loss = 0.71927948\n",
      "Iteration 56, loss = 0.71669157\n",
      "Iteration 57, loss = 0.71409428\n",
      "Iteration 58, loss = 0.71146377\n",
      "Iteration 59, loss = 0.70880643\n",
      "Iteration 60, loss = 0.70613214\n",
      "Iteration 61, loss = 0.70344037\n",
      "Iteration 62, loss = 0.70071234\n",
      "Iteration 63, loss = 0.69794292\n",
      "Iteration 64, loss = 0.69512198\n",
      "Iteration 65, loss = 0.69222778\n",
      "Iteration 66, loss = 0.68924679\n",
      "Iteration 67, loss = 0.68624343\n",
      "Iteration 68, loss = 0.68326737\n",
      "Iteration 69, loss = 0.68029540\n",
      "Iteration 70, loss = 0.67731169\n",
      "Iteration 71, loss = 0.67438715\n",
      "Iteration 72, loss = 0.67149582\n",
      "Iteration 73, loss = 0.66866938\n",
      "Iteration 74, loss = 0.66589594\n",
      "Iteration 75, loss = 0.66318258\n",
      "Iteration 76, loss = 0.66053223\n",
      "Iteration 77, loss = 0.65793088\n",
      "Iteration 78, loss = 0.65535284\n",
      "Iteration 79, loss = 0.65279857\n",
      "Iteration 80, loss = 0.65025721\n",
      "Iteration 81, loss = 0.64772911\n",
      "Iteration 82, loss = 0.64516857\n",
      "Iteration 83, loss = 0.64260824\n",
      "Iteration 84, loss = 0.64003171\n",
      "Iteration 85, loss = 0.63747702\n",
      "Iteration 86, loss = 0.63492885\n",
      "Iteration 87, loss = 0.63236022\n",
      "Iteration 88, loss = 0.62979068\n",
      "Iteration 89, loss = 0.62722921\n",
      "Iteration 90, loss = 0.62467067\n",
      "Iteration 91, loss = 0.62211754\n",
      "Iteration 92, loss = 0.61957237\n",
      "Iteration 93, loss = 0.61703866\n",
      "Iteration 94, loss = 0.61451745\n",
      "Iteration 95, loss = 0.61200915\n",
      "Iteration 96, loss = 0.60950100\n",
      "Iteration 97, loss = 0.60700233\n",
      "Iteration 98, loss = 0.60451958\n",
      "Iteration 99, loss = 0.60205289\n",
      "Iteration 100, loss = 0.59960329\n",
      "Iteration 101, loss = 0.59717148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 102, loss = 0.59475700\n",
      "Iteration 103, loss = 0.59236018\n",
      "Iteration 104, loss = 0.58998211\n",
      "Iteration 105, loss = 0.58762505\n",
      "Iteration 106, loss = 0.58528623\n",
      "Iteration 107, loss = 0.58296513\n",
      "Iteration 108, loss = 0.58066227\n",
      "Iteration 109, loss = 0.57837872\n",
      "Iteration 110, loss = 0.57611350\n",
      "Iteration 111, loss = 0.57386632\n",
      "Iteration 112, loss = 0.57163718\n",
      "Iteration 113, loss = 0.56942692\n",
      "Iteration 114, loss = 0.56723503\n",
      "Iteration 115, loss = 0.56509413\n",
      "Iteration 116, loss = 0.56305885\n",
      "Iteration 117, loss = 0.56113317\n",
      "Iteration 118, loss = 0.55925270\n",
      "Iteration 119, loss = 0.55748573\n",
      "Iteration 120, loss = 0.55585400\n",
      "Iteration 121, loss = 0.55426275\n",
      "Iteration 122, loss = 0.55271463\n",
      "Iteration 123, loss = 0.55117376\n",
      "Iteration 124, loss = 0.54961953\n",
      "Iteration 125, loss = 0.54804535\n",
      "Iteration 126, loss = 0.54644869\n",
      "Iteration 127, loss = 0.54483199\n",
      "Iteration 128, loss = 0.54320779\n",
      "Iteration 129, loss = 0.54158939\n",
      "Iteration 130, loss = 0.53998358\n",
      "Iteration 131, loss = 0.53840264\n",
      "Iteration 132, loss = 0.53685516\n",
      "Iteration 133, loss = 0.53532818\n",
      "Iteration 134, loss = 0.53383658\n",
      "Iteration 135, loss = 0.53236583\n",
      "Iteration 136, loss = 0.53091914\n",
      "Iteration 137, loss = 0.52949501\n",
      "Iteration 138, loss = 0.52808652\n",
      "Iteration 139, loss = 0.52669624\n",
      "Iteration 140, loss = 0.52533035\n",
      "Iteration 141, loss = 0.52397166\n",
      "Iteration 142, loss = 0.52261779\n",
      "Iteration 143, loss = 0.52126821\n",
      "Iteration 144, loss = 0.51992347\n",
      "Iteration 145, loss = 0.51858408\n",
      "Iteration 146, loss = 0.51725054\n",
      "Iteration 147, loss = 0.51592811\n",
      "Iteration 148, loss = 0.51461549\n",
      "Iteration 149, loss = 0.51332120\n",
      "Iteration 150, loss = 0.51204081\n",
      "Iteration 151, loss = 0.51077237\n",
      "Iteration 152, loss = 0.50951898\n",
      "Iteration 153, loss = 0.50827432\n",
      "Iteration 154, loss = 0.50703825\n",
      "Iteration 155, loss = 0.50581065\n",
      "Iteration 156, loss = 0.50459198\n",
      "Iteration 157, loss = 0.50338457\n",
      "Iteration 158, loss = 0.50218592\n",
      "Iteration 159, loss = 0.50099548\n",
      "Iteration 160, loss = 0.49981312\n",
      "Iteration 161, loss = 0.49863874\n",
      "Iteration 162, loss = 0.49747222\n",
      "Iteration 163, loss = 0.49631345\n",
      "Iteration 164, loss = 0.49516232\n",
      "Iteration 165, loss = 0.49401872\n",
      "Iteration 166, loss = 0.49288309\n",
      "Iteration 167, loss = 0.49175984\n",
      "Iteration 168, loss = 0.49064493\n",
      "Iteration 169, loss = 0.48953946\n",
      "Iteration 170, loss = 0.48844287\n",
      "Iteration 171, loss = 0.48735379\n",
      "Iteration 172, loss = 0.48627208\n",
      "Iteration 173, loss = 0.48519743\n",
      "Iteration 174, loss = 0.48412903\n",
      "Iteration 175, loss = 0.48306865\n",
      "Iteration 176, loss = 0.48201586\n",
      "Iteration 177, loss = 0.48097152\n",
      "Iteration 178, loss = 0.47993506\n",
      "Iteration 179, loss = 0.47890547\n",
      "Iteration 180, loss = 0.47788157\n",
      "Iteration 181, loss = 0.47686327\n",
      "Iteration 182, loss = 0.47585045\n",
      "Iteration 183, loss = 0.47484305\n",
      "Iteration 184, loss = 0.47384373\n",
      "Iteration 185, loss = 0.47285321\n",
      "Iteration 186, loss = 0.47186817\n",
      "Iteration 187, loss = 0.47088840\n",
      "Iteration 188, loss = 0.46991397\n",
      "Iteration 189, loss = 0.46894533\n",
      "Iteration 190, loss = 0.46798189\n",
      "Iteration 191, loss = 0.46702514\n",
      "Iteration 192, loss = 0.46607873\n",
      "Iteration 193, loss = 0.46513811\n",
      "Iteration 194, loss = 0.46420261\n",
      "Iteration 195, loss = 0.46327216\n",
      "Iteration 196, loss = 0.46234670\n",
      "Iteration 197, loss = 0.46142617\n",
      "Iteration 198, loss = 0.46051050\n",
      "Iteration 199, loss = 0.45959965\n",
      "Iteration 200, loss = 0.45869374\n",
      "Iteration 201, loss = 0.45779338\n",
      "Iteration 202, loss = 0.45689772\n",
      "Iteration 203, loss = 0.45600667\n",
      "Iteration 204, loss = 0.45512018\n",
      "Iteration 205, loss = 0.45423820\n",
      "Iteration 206, loss = 0.45336067\n",
      "Iteration 207, loss = 0.45248755\n",
      "Iteration 208, loss = 0.45161878\n",
      "Iteration 209, loss = 0.45075455\n",
      "Iteration 210, loss = 0.44989440\n",
      "Iteration 211, loss = 0.44903807\n",
      "Iteration 212, loss = 0.44818609\n",
      "Iteration 213, loss = 0.44733805\n",
      "Iteration 214, loss = 0.44649390\n",
      "Iteration 215, loss = 0.44565424\n",
      "Iteration 216, loss = 0.44481895\n",
      "Iteration 217, loss = 0.44398741\n",
      "Iteration 218, loss = 0.44315958\n",
      "Iteration 219, loss = 0.44233540\n",
      "Iteration 220, loss = 0.44151485\n",
      "Iteration 221, loss = 0.44069786\n",
      "Iteration 222, loss = 0.43988440\n",
      "Iteration 223, loss = 0.43907440\n",
      "Iteration 224, loss = 0.43826783\n",
      "Iteration 225, loss = 0.43746462\n",
      "Iteration 226, loss = 0.43666471\n",
      "Iteration 227, loss = 0.43586806\n",
      "Iteration 228, loss = 0.43507461\n",
      "Iteration 229, loss = 0.43428430\n",
      "Iteration 230, loss = 0.43349712\n",
      "Iteration 231, loss = 0.43271299\n",
      "Iteration 232, loss = 0.43193183\n",
      "Iteration 233, loss = 0.43115360\n",
      "Iteration 234, loss = 0.43037825\n",
      "Iteration 235, loss = 0.42960578\n",
      "Iteration 236, loss = 0.42883595\n",
      "Iteration 237, loss = 0.42806889\n",
      "Iteration 238, loss = 0.42730450\n",
      "Iteration 239, loss = 0.42654271\n",
      "Iteration 240, loss = 0.42578348\n",
      "Iteration 241, loss = 0.42502676\n",
      "Iteration 242, loss = 0.42427249\n",
      "Iteration 243, loss = 0.42352063\n",
      "Iteration 244, loss = 0.42277113\n",
      "Iteration 245, loss = 0.42202394\n",
      "Iteration 246, loss = 0.42128018\n",
      "Iteration 247, loss = 0.42053922\n",
      "Iteration 248, loss = 0.41980095\n",
      "Iteration 249, loss = 0.41906502\n",
      "Iteration 250, loss = 0.41833122\n",
      "Iteration 251, loss = 0.41759957\n",
      "Iteration 252, loss = 0.41687004\n",
      "Iteration 253, loss = 0.41614338\n",
      "Iteration 254, loss = 0.41541885\n",
      "Iteration 255, loss = 0.41469643\n",
      "Iteration 256, loss = 0.41397611\n",
      "Iteration 257, loss = 0.41325786\n",
      "Iteration 258, loss = 0.41254165\n",
      "Iteration 259, loss = 0.41182746\n",
      "Iteration 260, loss = 0.41111527\n",
      "Iteration 261, loss = 0.41040504\n",
      "Iteration 262, loss = 0.40969766\n",
      "Iteration 263, loss = 0.40899250\n",
      "Iteration 264, loss = 0.40828928\n",
      "Iteration 265, loss = 0.40758764\n",
      "Iteration 266, loss = 0.40688798\n",
      "Iteration 267, loss = 0.40619009\n",
      "Iteration 268, loss = 0.40549397\n",
      "Iteration 269, loss = 0.40479960\n",
      "Iteration 270, loss = 0.40410695\n",
      "Iteration 271, loss = 0.40341600\n",
      "Iteration 272, loss = 0.40272672\n",
      "Iteration 273, loss = 0.40203908\n",
      "Iteration 274, loss = 0.40135305\n",
      "Iteration 275, loss = 0.40066862\n",
      "Iteration 276, loss = 0.39998571\n",
      "Iteration 277, loss = 0.39930434\n",
      "Iteration 278, loss = 0.39862512\n",
      "Iteration 279, loss = 0.39794745\n",
      "Iteration 280, loss = 0.39727126\n",
      "Iteration 281, loss = 0.39659652\n",
      "Iteration 282, loss = 0.39592320\n",
      "Iteration 283, loss = 0.39525127\n",
      "Iteration 284, loss = 0.39458071\n",
      "Iteration 285, loss = 0.39391147\n",
      "Iteration 286, loss = 0.39324384\n",
      "Iteration 287, loss = 0.39257777\n",
      "Iteration 288, loss = 0.39191289\n",
      "Iteration 289, loss = 0.39124929\n",
      "Iteration 290, loss = 0.39058739\n",
      "Iteration 291, loss = 0.38992698\n",
      "Iteration 292, loss = 0.38926780\n",
      "Iteration 293, loss = 0.38860985\n",
      "Iteration 294, loss = 0.38795308\n",
      "Iteration 295, loss = 0.38729752\n",
      "Iteration 296, loss = 0.38664318\n",
      "Iteration 297, loss = 0.38599019\n",
      "Iteration 298, loss = 0.38533819\n",
      "Iteration 299, loss = 0.38468738\n",
      "Iteration 300, loss = 0.38403819\n",
      "Iteration 301, loss = 0.38339014\n",
      "Iteration 302, loss = 0.38274315\n",
      "Iteration 303, loss = 0.38209720\n",
      "Iteration 304, loss = 0.38145227\n",
      "Iteration 305, loss = 0.38080835\n",
      "Iteration 306, loss = 0.38016541\n",
      "Iteration 307, loss = 0.37952347\n",
      "Iteration 308, loss = 0.37888248\n",
      "Iteration 309, loss = 0.37824244\n",
      "Iteration 310, loss = 0.37760355\n",
      "Iteration 311, loss = 0.37696581\n",
      "Iteration 312, loss = 0.37632941\n",
      "Iteration 313, loss = 0.37569431\n",
      "Iteration 314, loss = 0.37505991\n",
      "Iteration 315, loss = 0.37442684\n",
      "Iteration 316, loss = 0.37379609\n",
      "Iteration 317, loss = 0.37316609\n",
      "Iteration 318, loss = 0.37253686\n",
      "Iteration 319, loss = 0.37190845\n",
      "Iteration 320, loss = 0.37128085\n",
      "Iteration 321, loss = 0.37065410\n",
      "Iteration 322, loss = 0.37002821\n",
      "Iteration 323, loss = 0.36940318\n",
      "Iteration 324, loss = 0.36877903\n",
      "Iteration 325, loss = 0.36815576\n",
      "Iteration 326, loss = 0.36753338\n",
      "Iteration 327, loss = 0.36691188\n",
      "Iteration 328, loss = 0.36629126\n",
      "Iteration 329, loss = 0.36567152\n",
      "Iteration 330, loss = 0.36505266\n",
      "Iteration 331, loss = 0.36443465\n",
      "Iteration 332, loss = 0.36381754\n",
      "Iteration 333, loss = 0.36320121\n",
      "Iteration 334, loss = 0.36258576\n",
      "Iteration 335, loss = 0.36197112\n",
      "Iteration 336, loss = 0.36135730\n",
      "Iteration 337, loss = 0.36074427\n",
      "Iteration 338, loss = 0.36013203\n",
      "Iteration 339, loss = 0.35952055\n",
      "Iteration 340, loss = 0.35890984\n",
      "Iteration 341, loss = 0.35829986\n",
      "Iteration 342, loss = 0.35769071\n",
      "Iteration 343, loss = 0.35708230\n",
      "Iteration 344, loss = 0.35647462\n",
      "Iteration 345, loss = 0.35586763\n",
      "Iteration 346, loss = 0.35526134\n",
      "Iteration 347, loss = 0.35465593\n",
      "Iteration 348, loss = 0.35405120\n",
      "Iteration 349, loss = 0.35344715\n",
      "Iteration 350, loss = 0.35284376\n",
      "Iteration 351, loss = 0.35224102\n",
      "Iteration 352, loss = 0.35163892\n",
      "Iteration 353, loss = 0.35103745\n",
      "Iteration 354, loss = 0.35043660\n",
      "Iteration 355, loss = 0.34983638\n",
      "Iteration 356, loss = 0.34923674\n",
      "Iteration 357, loss = 0.34863771\n",
      "Iteration 358, loss = 0.34803928\n",
      "Iteration 359, loss = 0.34744165\n",
      "Iteration 360, loss = 0.34684471\n",
      "Iteration 361, loss = 0.34624837\n",
      "Iteration 362, loss = 0.34565262\n",
      "Iteration 363, loss = 0.34505745\n",
      "Iteration 364, loss = 0.34446380\n",
      "Iteration 365, loss = 0.34387075\n",
      "Iteration 366, loss = 0.34327831\n",
      "Iteration 367, loss = 0.34268647\n",
      "Iteration 368, loss = 0.34209524\n",
      "Iteration 369, loss = 0.34150465\n",
      "Iteration 370, loss = 0.34091558\n",
      "Iteration 371, loss = 0.34032899\n",
      "Iteration 372, loss = 0.33974282\n",
      "Iteration 373, loss = 0.33915730\n",
      "Iteration 374, loss = 0.33857223\n",
      "Iteration 375, loss = 0.33798754\n",
      "Iteration 376, loss = 0.33740300\n",
      "Iteration 377, loss = 0.33681657\n",
      "Iteration 378, loss = 0.33621648\n",
      "Iteration 379, loss = 0.33558812\n",
      "Iteration 380, loss = 0.33497346\n",
      "Iteration 381, loss = 0.33437179\n",
      "Iteration 382, loss = 0.33374116\n",
      "Iteration 383, loss = 0.33311045\n",
      "Iteration 384, loss = 0.33249698\n",
      "Iteration 385, loss = 0.33185214\n",
      "Iteration 386, loss = 0.33122089\n",
      "Iteration 387, loss = 0.33058223\n",
      "Iteration 388, loss = 0.32993207\n",
      "Iteration 389, loss = 0.32927535\n",
      "Iteration 390, loss = 0.32861990\n",
      "Iteration 391, loss = 0.32795465\n",
      "Iteration 392, loss = 0.32727893\n",
      "Iteration 393, loss = 0.32660456\n",
      "Iteration 394, loss = 0.32591975\n",
      "Iteration 395, loss = 0.32522325\n",
      "Iteration 396, loss = 0.32451862\n",
      "Iteration 397, loss = 0.32381547\n",
      "Iteration 398, loss = 0.32310121\n",
      "Iteration 399, loss = 0.32238531\n",
      "Iteration 400, loss = 0.32166090\n",
      "Iteration 401, loss = 0.32092778\n",
      "Iteration 402, loss = 0.32019174\n",
      "Iteration 403, loss = 0.31945078\n",
      "Iteration 404, loss = 0.31870277\n",
      "Iteration 405, loss = 0.31794784\n",
      "Iteration 406, loss = 0.31718659\n",
      "Iteration 407, loss = 0.31641881\n",
      "Iteration 408, loss = 0.31564660\n",
      "Iteration 409, loss = 0.31486856\n",
      "Iteration 410, loss = 0.31408417\n",
      "Iteration 411, loss = 0.31329358\n",
      "Iteration 412, loss = 0.31249698\n",
      "Iteration 413, loss = 0.31169580\n",
      "Iteration 414, loss = 0.31089137\n",
      "Iteration 415, loss = 0.31008095\n",
      "Iteration 416, loss = 0.30926671\n",
      "Iteration 417, loss = 0.30844575\n",
      "Iteration 418, loss = 0.30763099\n",
      "Iteration 419, loss = 0.30680035\n",
      "Iteration 420, loss = 0.30596627\n",
      "Iteration 421, loss = 0.30513521\n",
      "Iteration 422, loss = 0.30429647\n",
      "Iteration 423, loss = 0.30345035\n",
      "Iteration 424, loss = 0.30260547\n",
      "Iteration 425, loss = 0.30175748\n",
      "Iteration 426, loss = 0.30090619\n",
      "Iteration 427, loss = 0.30005150\n",
      "Iteration 428, loss = 0.29919492\n",
      "Iteration 429, loss = 0.29833572\n",
      "Iteration 430, loss = 0.29747340\n",
      "Iteration 431, loss = 0.29660883\n",
      "Iteration 432, loss = 0.29574119\n",
      "Iteration 433, loss = 0.29487049\n",
      "Iteration 434, loss = 0.29399786\n",
      "Iteration 435, loss = 0.29312309\n",
      "Iteration 436, loss = 0.29224677\n",
      "Iteration 437, loss = 0.29136582\n",
      "Iteration 438, loss = 0.29048331\n",
      "Iteration 439, loss = 0.28959934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 440, loss = 0.28871359\n",
      "Iteration 441, loss = 0.28782887\n",
      "Iteration 442, loss = 0.28694210\n",
      "Iteration 443, loss = 0.28605179\n",
      "Iteration 444, loss = 0.28516046\n",
      "Iteration 445, loss = 0.28426960\n",
      "Iteration 446, loss = 0.28337876\n",
      "Iteration 447, loss = 0.28248755\n",
      "Iteration 448, loss = 0.28159396\n",
      "Iteration 449, loss = 0.28069930\n",
      "Iteration 450, loss = 0.27980332\n",
      "Iteration 451, loss = 0.27890781\n",
      "Iteration 452, loss = 0.27801289\n",
      "Iteration 453, loss = 0.27711702\n",
      "Iteration 454, loss = 0.27622151\n",
      "Iteration 455, loss = 0.27532595\n",
      "Iteration 456, loss = 0.27443473\n",
      "Iteration 457, loss = 0.27353984\n",
      "Iteration 458, loss = 0.27264123\n",
      "Iteration 459, loss = 0.27175190\n",
      "Iteration 460, loss = 0.27086091\n",
      "Iteration 461, loss = 0.26996705\n",
      "Iteration 462, loss = 0.26908044\n",
      "Iteration 463, loss = 0.26819343\n",
      "Iteration 464, loss = 0.26730559\n",
      "Iteration 465, loss = 0.26642271\n",
      "Iteration 466, loss = 0.26553855\n",
      "Iteration 467, loss = 0.26465323\n",
      "Iteration 468, loss = 0.26377244\n",
      "Iteration 469, loss = 0.26289283\n",
      "Iteration 470, loss = 0.26201616\n",
      "Iteration 471, loss = 0.26114034\n",
      "Iteration 472, loss = 0.26026731\n",
      "Iteration 473, loss = 0.25939651\n",
      "Iteration 474, loss = 0.25852755\n",
      "Iteration 475, loss = 0.25766063\n",
      "Iteration 476, loss = 0.25679580\n",
      "Iteration 477, loss = 0.25593321\n",
      "Iteration 478, loss = 0.25507282\n",
      "Iteration 479, loss = 0.25421522\n",
      "Iteration 480, loss = 0.25336050\n",
      "Iteration 481, loss = 0.25250799\n",
      "Iteration 482, loss = 0.25165851\n",
      "Iteration 483, loss = 0.25081098\n",
      "Iteration 484, loss = 0.24996622\n",
      "Iteration 485, loss = 0.24912499\n",
      "Iteration 486, loss = 0.24828863\n",
      "Iteration 487, loss = 0.24745424\n",
      "Iteration 488, loss = 0.24662354\n",
      "Iteration 489, loss = 0.24579563\n",
      "Iteration 490, loss = 0.24497078\n",
      "Iteration 491, loss = 0.24414881\n",
      "Iteration 492, loss = 0.24333348\n",
      "Iteration 493, loss = 0.24251604\n",
      "Iteration 494, loss = 0.24170519\n",
      "Iteration 495, loss = 0.24089726\n",
      "Iteration 496, loss = 0.24009245\n",
      "Iteration 497, loss = 0.23929159\n",
      "Iteration 498, loss = 0.23849306\n",
      "Iteration 499, loss = 0.23769854\n",
      "Iteration 500, loss = 0.23690722\n",
      "Iteration 501, loss = 0.23611980\n",
      "Iteration 502, loss = 0.23533560\n",
      "Iteration 503, loss = 0.23455510\n",
      "Iteration 504, loss = 0.23377780\n",
      "Iteration 505, loss = 0.23300622\n",
      "Iteration 506, loss = 0.23223551\n",
      "Iteration 507, loss = 0.23147116\n",
      "Iteration 508, loss = 0.23071006\n",
      "Iteration 509, loss = 0.22995154\n",
      "Iteration 510, loss = 0.22919578\n",
      "Iteration 511, loss = 0.22844335\n",
      "Iteration 512, loss = 0.22769841\n",
      "Iteration 513, loss = 0.22695445\n",
      "Iteration 514, loss = 0.22621198\n",
      "Iteration 515, loss = 0.22547653\n",
      "Iteration 516, loss = 0.22474536\n",
      "Iteration 517, loss = 0.22401639\n",
      "Iteration 518, loss = 0.22328979\n",
      "Iteration 519, loss = 0.22256627\n",
      "Iteration 520, loss = 0.22184790\n",
      "Iteration 521, loss = 0.22113399\n",
      "Iteration 522, loss = 0.22042144\n",
      "Iteration 523, loss = 0.21971541\n",
      "Iteration 524, loss = 0.21901343\n",
      "Iteration 525, loss = 0.21831385\n",
      "Iteration 526, loss = 0.21761671\n",
      "Iteration 527, loss = 0.21692291\n",
      "Iteration 528, loss = 0.21623485\n",
      "Iteration 529, loss = 0.21554987\n",
      "Iteration 530, loss = 0.21486691\n",
      "Iteration 531, loss = 0.21419057\n",
      "Iteration 532, loss = 0.21351698\n",
      "Iteration 533, loss = 0.21284555\n",
      "Iteration 534, loss = 0.21217675\n",
      "Iteration 535, loss = 0.21151247\n",
      "Iteration 536, loss = 0.21085327\n",
      "Iteration 537, loss = 0.21019601\n",
      "Iteration 538, loss = 0.20954405\n",
      "Iteration 539, loss = 0.20889468\n",
      "Iteration 540, loss = 0.20824790\n",
      "Iteration 541, loss = 0.20760423\n",
      "Iteration 542, loss = 0.20696838\n",
      "Iteration 543, loss = 0.20633062\n",
      "Iteration 544, loss = 0.20569952\n",
      "Iteration 545, loss = 0.20507370\n",
      "Iteration 546, loss = 0.20444941\n",
      "Iteration 547, loss = 0.20382662\n",
      "Iteration 548, loss = 0.20320797\n",
      "Iteration 549, loss = 0.20259652\n",
      "Iteration 550, loss = 0.20198480\n",
      "Iteration 551, loss = 0.20137551\n",
      "Iteration 552, loss = 0.20077377\n",
      "Iteration 553, loss = 0.20017430\n",
      "Iteration 554, loss = 0.19957683\n",
      "Iteration 555, loss = 0.19898204\n",
      "Iteration 556, loss = 0.19839594\n",
      "Iteration 557, loss = 0.19780795\n",
      "Iteration 558, loss = 0.19722205\n",
      "Iteration 559, loss = 0.19664324\n",
      "Iteration 560, loss = 0.19606628\n",
      "Iteration 561, loss = 0.19549124\n",
      "Iteration 562, loss = 0.19491990\n",
      "Iteration 563, loss = 0.19435505\n",
      "Iteration 564, loss = 0.19378921\n",
      "Iteration 565, loss = 0.19323016\n",
      "Iteration 566, loss = 0.19267454\n",
      "Iteration 567, loss = 0.19211974\n",
      "Iteration 568, loss = 0.19156688\n",
      "Iteration 569, loss = 0.19101952\n",
      "Iteration 570, loss = 0.19047493\n",
      "Iteration 571, loss = 0.18993144\n",
      "Iteration 572, loss = 0.18939384\n",
      "Iteration 573, loss = 0.18885789\n",
      "Iteration 574, loss = 0.18832596\n",
      "Iteration 575, loss = 0.18779640\n",
      "Iteration 576, loss = 0.18726982\n",
      "Iteration 577, loss = 0.18674677\n",
      "Iteration 578, loss = 0.18622666\n",
      "Iteration 579, loss = 0.18570913\n",
      "Iteration 580, loss = 0.18519426\n",
      "Iteration 581, loss = 0.18468325\n",
      "Iteration 582, loss = 0.18417506\n",
      "Iteration 583, loss = 0.18366898\n",
      "Iteration 584, loss = 0.18316555\n",
      "Iteration 585, loss = 0.18266579\n",
      "Iteration 586, loss = 0.18216868\n",
      "Iteration 587, loss = 0.18167415\n",
      "Iteration 588, loss = 0.18118306\n",
      "Iteration 589, loss = 0.18069434\n",
      "Iteration 590, loss = 0.18020794\n",
      "Iteration 591, loss = 0.17972489\n",
      "Iteration 592, loss = 0.17924441\n",
      "Iteration 593, loss = 0.17876657\n",
      "Iteration 594, loss = 0.17829170\n",
      "Iteration 595, loss = 0.17781951\n",
      "Iteration 596, loss = 0.17735054\n",
      "Iteration 597, loss = 0.17688330\n",
      "Iteration 598, loss = 0.17641851\n",
      "Iteration 599, loss = 0.17595700\n",
      "Iteration 600, loss = 0.17549807\n",
      "Iteration 601, loss = 0.17504176\n",
      "Iteration 602, loss = 0.17458793\n",
      "Iteration 603, loss = 0.17413658\n",
      "Iteration 604, loss = 0.17368769\n",
      "Iteration 605, loss = 0.17324129\n",
      "Iteration 606, loss = 0.17279802\n",
      "Iteration 607, loss = 0.17235731\n",
      "Iteration 608, loss = 0.17191832\n",
      "Iteration 609, loss = 0.17148273\n",
      "Iteration 610, loss = 0.17104931\n",
      "Iteration 611, loss = 0.17061822\n",
      "Iteration 612, loss = 0.17018943\n",
      "Iteration 613, loss = 0.16976299\n",
      "Iteration 614, loss = 0.16933976\n",
      "Iteration 615, loss = 0.16891764\n",
      "Iteration 616, loss = 0.16849933\n",
      "Iteration 617, loss = 0.16808296\n",
      "Iteration 618, loss = 0.16766892\n",
      "Iteration 619, loss = 0.16725634\n",
      "Iteration 620, loss = 0.16684750\n",
      "Iteration 621, loss = 0.16644085\n",
      "Iteration 622, loss = 0.16603496\n",
      "Iteration 623, loss = 0.16563248\n",
      "Iteration 624, loss = 0.16523282\n",
      "Iteration 625, loss = 0.16483484\n",
      "Iteration 626, loss = 0.16443769\n",
      "Iteration 627, loss = 0.16404385\n",
      "Iteration 628, loss = 0.16365278\n",
      "Iteration 629, loss = 0.16326252\n",
      "Iteration 630, loss = 0.16287532\n",
      "Iteration 631, loss = 0.16249125\n",
      "Iteration 632, loss = 0.16210806\n",
      "Iteration 633, loss = 0.16172688\n",
      "Iteration 634, loss = 0.16134771\n",
      "Iteration 635, loss = 0.16097227\n",
      "Iteration 636, loss = 0.16059814\n",
      "Iteration 637, loss = 0.16022493\n",
      "Iteration 638, loss = 0.15985472\n",
      "Iteration 639, loss = 0.15948719\n",
      "Iteration 640, loss = 0.15912102\n",
      "Iteration 641, loss = 0.15875621\n",
      "Iteration 642, loss = 0.15839368\n",
      "Iteration 643, loss = 0.15803395\n",
      "Iteration 644, loss = 0.15767550\n",
      "Iteration 645, loss = 0.15731931\n",
      "Iteration 646, loss = 0.15696543\n",
      "Iteration 647, loss = 0.15661306\n",
      "Iteration 648, loss = 0.15626262\n",
      "Iteration 649, loss = 0.15591425\n",
      "Iteration 650, loss = 0.15556739\n",
      "Iteration 651, loss = 0.15522262\n",
      "Iteration 652, loss = 0.15488001\n",
      "Iteration 653, loss = 0.15453889\n",
      "Iteration 654, loss = 0.15420037\n",
      "Iteration 655, loss = 0.15386265\n",
      "Iteration 656, loss = 0.15352666\n",
      "Iteration 657, loss = 0.15319299\n",
      "Iteration 658, loss = 0.15286092\n",
      "Iteration 659, loss = 0.15253018\n",
      "Iteration 660, loss = 0.15220159\n",
      "Iteration 661, loss = 0.15187444\n",
      "Iteration 662, loss = 0.15154904\n",
      "Iteration 663, loss = 0.15122540\n",
      "Iteration 664, loss = 0.15090350\n",
      "Iteration 665, loss = 0.15058352\n",
      "Iteration 666, loss = 0.15026495\n",
      "Iteration 667, loss = 0.14994789\n",
      "Iteration 668, loss = 0.14963137\n",
      "Iteration 669, loss = 0.14931652\n",
      "Iteration 670, loss = 0.14900263\n",
      "Iteration 671, loss = 0.14869084\n",
      "Iteration 672, loss = 0.14838052\n",
      "Iteration 673, loss = 0.14807141\n",
      "Iteration 674, loss = 0.14776364\n",
      "Iteration 675, loss = 0.14745781\n",
      "Iteration 676, loss = 0.14715350\n",
      "Iteration 677, loss = 0.14685065\n",
      "Iteration 678, loss = 0.14654958\n",
      "Iteration 679, loss = 0.14624998\n",
      "Iteration 680, loss = 0.14595182\n",
      "Iteration 681, loss = 0.14565521\n",
      "Iteration 682, loss = 0.14536069\n",
      "Iteration 683, loss = 0.14506800\n",
      "Iteration 684, loss = 0.14477712\n",
      "Iteration 685, loss = 0.14448778\n",
      "Iteration 686, loss = 0.14420003\n",
      "Iteration 687, loss = 0.14391385\n",
      "Iteration 688, loss = 0.14362923\n",
      "Iteration 689, loss = 0.14334550\n",
      "Iteration 690, loss = 0.14306304\n",
      "Iteration 691, loss = 0.14278157\n",
      "Iteration 692, loss = 0.14250303\n",
      "Iteration 693, loss = 0.14222672\n",
      "Iteration 694, loss = 0.14195199\n",
      "Iteration 695, loss = 0.14167920\n",
      "Iteration 696, loss = 0.14140809\n",
      "Iteration 697, loss = 0.14113854\n",
      "Iteration 698, loss = 0.14087056\n",
      "Iteration 699, loss = 0.14060407\n",
      "Iteration 700, loss = 0.14033905\n",
      "Iteration 701, loss = 0.14007549\n",
      "Iteration 702, loss = 0.13981338\n",
      "Iteration 703, loss = 0.13955272\n",
      "Iteration 704, loss = 0.13929336\n",
      "Iteration 705, loss = 0.13903527\n",
      "Iteration 706, loss = 0.13877855\n",
      "Iteration 707, loss = 0.13852223\n",
      "Iteration 708, loss = 0.13826700\n",
      "Iteration 709, loss = 0.13801224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 710, loss = 0.13775811\n",
      "Iteration 711, loss = 0.13750505\n",
      "Iteration 712, loss = 0.13725323\n",
      "Iteration 713, loss = 0.13700300\n",
      "Iteration 714, loss = 0.13675403\n",
      "Iteration 715, loss = 0.13650654\n",
      "Iteration 716, loss = 0.13626027\n",
      "Iteration 717, loss = 0.13601514\n",
      "Iteration 718, loss = 0.13577100\n",
      "Iteration 719, loss = 0.13552806\n",
      "Iteration 720, loss = 0.13528636\n",
      "Iteration 721, loss = 0.13504738\n",
      "Iteration 722, loss = 0.13481103\n",
      "Iteration 723, loss = 0.13457578\n",
      "Iteration 724, loss = 0.13434188\n",
      "Iteration 725, loss = 0.13410940\n",
      "Iteration 726, loss = 0.13387817\n",
      "Iteration 727, loss = 0.13364843\n",
      "Iteration 728, loss = 0.13342039\n",
      "Iteration 729, loss = 0.13319356\n",
      "Iteration 730, loss = 0.13296787\n",
      "Iteration 731, loss = 0.13274336\n",
      "Iteration 732, loss = 0.13252005\n",
      "Iteration 733, loss = 0.13229863\n",
      "Iteration 734, loss = 0.13207869\n",
      "Iteration 735, loss = 0.13185993\n",
      "Iteration 736, loss = 0.13164229\n",
      "Iteration 737, loss = 0.13142547\n",
      "Iteration 738, loss = 0.13120975\n",
      "Iteration 739, loss = 0.13099514\n",
      "Iteration 740, loss = 0.13078141\n",
      "Iteration 741, loss = 0.13056836\n",
      "Iteration 742, loss = 0.13035633\n",
      "Iteration 743, loss = 0.13014554\n",
      "Iteration 744, loss = 0.12993601\n",
      "Iteration 745, loss = 0.12972761\n",
      "Iteration 746, loss = 0.12952045\n",
      "Iteration 747, loss = 0.12931442\n",
      "Iteration 748, loss = 0.12910943\n",
      "Iteration 749, loss = 0.12890547\n",
      "Iteration 750, loss = 0.12870252\n",
      "Iteration 751, loss = 0.12850060\n",
      "Iteration 752, loss = 0.12829974\n",
      "Iteration 753, loss = 0.12810001\n",
      "Iteration 754, loss = 0.12790132\n",
      "Iteration 755, loss = 0.12770358\n",
      "Iteration 756, loss = 0.12750686\n",
      "Iteration 757, loss = 0.12731108\n",
      "Iteration 758, loss = 0.12711623\n",
      "Iteration 759, loss = 0.12692234\n",
      "Iteration 760, loss = 0.12672946\n",
      "Iteration 761, loss = 0.12653758\n",
      "Iteration 762, loss = 0.12634662\n",
      "Iteration 763, loss = 0.12615661\n",
      "Iteration 764, loss = 0.12596752\n",
      "Iteration 765, loss = 0.12577933\n",
      "Iteration 766, loss = 0.12559216\n",
      "Iteration 767, loss = 0.12540596\n",
      "Iteration 768, loss = 0.12522065\n",
      "Iteration 769, loss = 0.12503621\n",
      "Iteration 770, loss = 0.12485265\n",
      "Iteration 771, loss = 0.12466999\n",
      "Iteration 772, loss = 0.12448819\n",
      "Iteration 773, loss = 0.12430725\n",
      "Iteration 774, loss = 0.12412725\n",
      "Iteration 775, loss = 0.12394806\n",
      "Iteration 776, loss = 0.12376972\n",
      "Iteration 777, loss = 0.12359220\n",
      "Iteration 778, loss = 0.12341550\n",
      "Iteration 779, loss = 0.12323962\n",
      "Iteration 780, loss = 0.12306456\n",
      "Iteration 781, loss = 0.12289032\n",
      "Iteration 782, loss = 0.12271690\n",
      "Iteration 783, loss = 0.12254428\n",
      "Iteration 784, loss = 0.12237246\n",
      "Iteration 785, loss = 0.12220136\n",
      "Iteration 786, loss = 0.12203101\n",
      "Iteration 787, loss = 0.12186146\n",
      "Iteration 788, loss = 0.12169276\n",
      "Iteration 789, loss = 0.12152483\n",
      "Iteration 790, loss = 0.12135764\n",
      "Iteration 791, loss = 0.12119125\n",
      "Iteration 792, loss = 0.12102546\n",
      "Iteration 793, loss = 0.12086033\n",
      "Iteration 794, loss = 0.12069585\n",
      "Iteration 795, loss = 0.12053204\n",
      "Iteration 796, loss = 0.12036895\n",
      "Iteration 797, loss = 0.12020655\n",
      "Iteration 798, loss = 0.12004485\n",
      "Iteration 799, loss = 0.11988383\n",
      "Iteration 800, loss = 0.11972353\n",
      "Iteration 801, loss = 0.11956394\n",
      "Iteration 802, loss = 0.11940502\n",
      "Iteration 803, loss = 0.11924683\n",
      "Iteration 804, loss = 0.11908930\n",
      "Iteration 805, loss = 0.11893250\n",
      "Iteration 806, loss = 0.11877634\n",
      "Iteration 807, loss = 0.11862091\n",
      "Iteration 808, loss = 0.11846615\n",
      "Iteration 809, loss = 0.11831208\n",
      "Iteration 810, loss = 0.11815868\n",
      "Iteration 811, loss = 0.11800614\n",
      "Iteration 812, loss = 0.11785417\n",
      "Iteration 813, loss = 0.11770323\n",
      "Iteration 814, loss = 0.11755288\n",
      "Iteration 815, loss = 0.11740302\n",
      "Iteration 816, loss = 0.11725414\n",
      "Iteration 817, loss = 0.11710587\n",
      "Iteration 818, loss = 0.11695830\n",
      "Iteration 819, loss = 0.11681153\n",
      "Iteration 820, loss = 0.11666554\n",
      "Iteration 821, loss = 0.11652019\n",
      "Iteration 822, loss = 0.11637564\n",
      "Iteration 823, loss = 0.11623180\n",
      "Iteration 824, loss = 0.11608861\n",
      "Iteration 825, loss = 0.11594597\n",
      "Iteration 826, loss = 0.11580400\n",
      "Iteration 827, loss = 0.11566272\n",
      "Iteration 828, loss = 0.11552202\n",
      "Iteration 829, loss = 0.11538194\n",
      "Iteration 830, loss = 0.11524251\n",
      "Iteration 831, loss = 0.11510373\n",
      "Iteration 832, loss = 0.11496571\n",
      "Iteration 833, loss = 0.11482801\n",
      "Iteration 834, loss = 0.11469108\n",
      "Iteration 835, loss = 0.11455468\n",
      "Iteration 836, loss = 0.11441876\n",
      "Iteration 837, loss = 0.11428358\n",
      "Iteration 838, loss = 0.11414880\n",
      "Iteration 839, loss = 0.11401475\n",
      "Iteration 840, loss = 0.11388128\n",
      "Iteration 841, loss = 0.11374824\n",
      "Iteration 842, loss = 0.11361573\n",
      "Iteration 843, loss = 0.11348393\n",
      "Iteration 844, loss = 0.11335264\n",
      "Iteration 845, loss = 0.11322186\n",
      "Iteration 846, loss = 0.11309151\n",
      "Iteration 847, loss = 0.11296163\n",
      "Iteration 848, loss = 0.11283222\n",
      "Iteration 849, loss = 0.11270339\n",
      "Iteration 850, loss = 0.11257504\n",
      "Iteration 851, loss = 0.11244718\n",
      "Iteration 852, loss = 0.11231987\n",
      "Iteration 853, loss = 0.11219314\n",
      "Iteration 854, loss = 0.11206692\n",
      "Iteration 855, loss = 0.11194110\n",
      "Iteration 856, loss = 0.11181571\n",
      "Iteration 857, loss = 0.11169080\n",
      "Iteration 858, loss = 0.11156643\n",
      "Iteration 859, loss = 0.11144250\n",
      "Iteration 860, loss = 0.11131907\n",
      "Iteration 861, loss = 0.11119613\n",
      "Iteration 862, loss = 0.11107375\n",
      "Iteration 863, loss = 0.11095193\n",
      "Iteration 864, loss = 0.11083058\n",
      "Iteration 865, loss = 0.11070964\n",
      "Iteration 866, loss = 0.11058924\n",
      "Iteration 867, loss = 0.11046936\n",
      "Iteration 868, loss = 0.11035001\n",
      "Iteration 869, loss = 0.11023114\n",
      "Iteration 870, loss = 0.11011277\n",
      "Iteration 871, loss = 0.10999495\n",
      "Iteration 872, loss = 0.10987756\n",
      "Iteration 873, loss = 0.10976020\n",
      "Iteration 874, loss = 0.10964325\n",
      "Iteration 875, loss = 0.10952674\n",
      "Iteration 876, loss = 0.10941070\n",
      "Iteration 877, loss = 0.10929521\n",
      "Iteration 878, loss = 0.10918023\n",
      "Iteration 879, loss = 0.10906568\n",
      "Iteration 880, loss = 0.10895159\n",
      "Iteration 881, loss = 0.10883791\n",
      "Iteration 882, loss = 0.10872473\n",
      "Iteration 883, loss = 0.10861198\n",
      "Iteration 884, loss = 0.10849965\n",
      "Iteration 885, loss = 0.10838777\n",
      "Iteration 886, loss = 0.10827634\n",
      "Iteration 887, loss = 0.10816534\n",
      "Iteration 888, loss = 0.10805478\n",
      "Iteration 889, loss = 0.10794465\n",
      "Iteration 890, loss = 0.10783498\n",
      "Iteration 891, loss = 0.10772570\n",
      "Iteration 892, loss = 0.10761697\n",
      "Iteration 893, loss = 0.10750864\n",
      "Iteration 894, loss = 0.10740073\n",
      "Iteration 895, loss = 0.10729323\n",
      "Iteration 896, loss = 0.10718611\n",
      "Iteration 897, loss = 0.10707955\n",
      "Iteration 898, loss = 0.10697317\n",
      "Iteration 899, loss = 0.10686737\n",
      "Iteration 900, loss = 0.10676198\n",
      "Iteration 901, loss = 0.10665690\n",
      "Iteration 902, loss = 0.10655233\n",
      "Iteration 903, loss = 0.10644808\n",
      "Iteration 904, loss = 0.10634428\n",
      "Iteration 905, loss = 0.10624085\n",
      "Iteration 906, loss = 0.10613772\n",
      "Iteration 907, loss = 0.10603502\n",
      "Iteration 908, loss = 0.10593267\n",
      "Iteration 909, loss = 0.10583065\n",
      "Iteration 910, loss = 0.10572914\n",
      "Iteration 911, loss = 0.10562793\n",
      "Iteration 912, loss = 0.10552783\n",
      "Iteration 913, loss = 0.10542893\n",
      "Iteration 914, loss = 0.10533018\n",
      "Iteration 915, loss = 0.10523213\n",
      "Iteration 916, loss = 0.10513445\n",
      "Iteration 917, loss = 0.10503697\n",
      "Iteration 918, loss = 0.10493980\n",
      "Iteration 919, loss = 0.10484331\n",
      "Iteration 920, loss = 0.10474703\n",
      "Iteration 921, loss = 0.10465103\n",
      "Iteration 922, loss = 0.10455553\n",
      "Iteration 923, loss = 0.10446047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.32593075\n",
      "Iteration 3, loss = 1.29931942\n",
      "Iteration 4, loss = 1.27320119\n",
      "Iteration 5, loss = 1.24759210\n",
      "Iteration 6, loss = 1.22257009\n",
      "Iteration 7, loss = 1.19816746\n",
      "Iteration 8, loss = 1.17437665\n",
      "Iteration 9, loss = 1.15114735\n",
      "Iteration 10, loss = 1.12853993\n",
      "Iteration 11, loss = 1.10658865\n",
      "Iteration 12, loss = 1.08531855\n",
      "Iteration 13, loss = 1.06474805\n",
      "Iteration 14, loss = 1.04486888\n",
      "Iteration 15, loss = 1.02570883\n",
      "Iteration 16, loss = 1.00723940\n",
      "Iteration 17, loss = 0.98949685\n",
      "Iteration 18, loss = 0.97246602\n",
      "Iteration 19, loss = 0.95614953\n",
      "Iteration 20, loss = 0.94053662\n",
      "Iteration 21, loss = 0.92562381\n",
      "Iteration 22, loss = 0.91139502\n",
      "Iteration 23, loss = 0.89782857\n",
      "Iteration 24, loss = 0.88494378\n",
      "Iteration 25, loss = 0.87270108\n",
      "Iteration 26, loss = 0.86112299\n",
      "Iteration 27, loss = 0.85016694\n",
      "Iteration 28, loss = 0.83983702\n",
      "Iteration 29, loss = 0.83011009\n",
      "Iteration 30, loss = 0.82093505\n",
      "Iteration 31, loss = 0.81233203\n",
      "Iteration 32, loss = 0.80425081\n",
      "Iteration 33, loss = 0.79666166\n",
      "Iteration 34, loss = 0.78959179\n",
      "Iteration 35, loss = 0.78301744\n",
      "Iteration 36, loss = 0.77687764\n",
      "Iteration 37, loss = 0.77114880\n",
      "Iteration 38, loss = 0.76580676\n",
      "Iteration 39, loss = 0.76083203\n",
      "Iteration 40, loss = 0.75627977\n",
      "Iteration 41, loss = 0.75209367\n",
      "Iteration 42, loss = 0.74817328\n",
      "Iteration 43, loss = 0.74453853\n",
      "Iteration 44, loss = 0.74110838\n",
      "Iteration 45, loss = 0.73792378\n",
      "Iteration 46, loss = 0.73485162\n",
      "Iteration 47, loss = 0.73189856\n",
      "Iteration 48, loss = 0.72906016\n",
      "Iteration 49, loss = 0.72629502\n",
      "Iteration 50, loss = 0.72358639\n",
      "Iteration 51, loss = 0.72095585\n",
      "Iteration 52, loss = 0.71837734\n",
      "Iteration 53, loss = 0.71581030\n",
      "Iteration 54, loss = 0.71327262\n",
      "Iteration 55, loss = 0.71071676\n",
      "Iteration 56, loss = 0.70813082\n",
      "Iteration 57, loss = 0.70552613\n",
      "Iteration 58, loss = 0.70290798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59, loss = 0.70026682\n",
      "Iteration 60, loss = 0.69760516\n",
      "Iteration 61, loss = 0.69493166\n",
      "Iteration 62, loss = 0.69224621\n",
      "Iteration 63, loss = 0.68952912\n",
      "Iteration 64, loss = 0.68677833\n",
      "Iteration 65, loss = 0.68395838\n",
      "Iteration 66, loss = 0.68108072\n",
      "Iteration 67, loss = 0.67815100\n",
      "Iteration 68, loss = 0.67520185\n",
      "Iteration 69, loss = 0.67227842\n",
      "Iteration 70, loss = 0.66936551\n",
      "Iteration 71, loss = 0.66646278\n",
      "Iteration 72, loss = 0.66360708\n",
      "Iteration 73, loss = 0.66080377\n",
      "Iteration 74, loss = 0.65810407\n",
      "Iteration 75, loss = 0.65546919\n",
      "Iteration 76, loss = 0.65286365\n",
      "Iteration 77, loss = 0.65033282\n",
      "Iteration 78, loss = 0.64789028\n",
      "Iteration 79, loss = 0.64552162\n",
      "Iteration 80, loss = 0.64325066\n",
      "Iteration 81, loss = 0.64106114\n",
      "Iteration 82, loss = 0.63890186\n",
      "Iteration 83, loss = 0.63674905\n",
      "Iteration 84, loss = 0.63461162\n",
      "Iteration 85, loss = 0.63247505\n",
      "Iteration 86, loss = 0.63035348\n",
      "Iteration 87, loss = 0.62818321\n",
      "Iteration 88, loss = 0.62598479\n",
      "Iteration 89, loss = 0.62375693\n",
      "Iteration 90, loss = 0.62150605\n",
      "Iteration 91, loss = 0.61922333\n",
      "Iteration 92, loss = 0.61688852\n",
      "Iteration 93, loss = 0.61453929\n",
      "Iteration 94, loss = 0.61215732\n",
      "Iteration 95, loss = 0.60977194\n",
      "Iteration 96, loss = 0.60738889\n",
      "Iteration 97, loss = 0.60501233\n",
      "Iteration 98, loss = 0.60264326\n",
      "Iteration 99, loss = 0.60028198\n",
      "Iteration 100, loss = 0.59792980\n",
      "Iteration 101, loss = 0.59558838\n",
      "Iteration 102, loss = 0.59326073\n",
      "Iteration 103, loss = 0.59094478\n",
      "Iteration 104, loss = 0.58864083\n",
      "Iteration 105, loss = 0.58633271\n",
      "Iteration 106, loss = 0.58403490\n",
      "Iteration 107, loss = 0.58175037\n",
      "Iteration 108, loss = 0.57947805\n",
      "Iteration 109, loss = 0.57721789\n",
      "Iteration 110, loss = 0.57497019\n",
      "Iteration 111, loss = 0.57273572\n",
      "Iteration 112, loss = 0.57051446\n",
      "Iteration 113, loss = 0.56830610\n",
      "Iteration 114, loss = 0.56611087\n",
      "Iteration 115, loss = 0.56392899\n",
      "Iteration 116, loss = 0.56176167\n",
      "Iteration 117, loss = 0.55960867\n",
      "Iteration 118, loss = 0.55746999\n",
      "Iteration 119, loss = 0.55534588\n",
      "Iteration 120, loss = 0.55323658\n",
      "Iteration 121, loss = 0.55114723\n",
      "Iteration 122, loss = 0.54909611\n",
      "Iteration 123, loss = 0.54718864\n",
      "Iteration 124, loss = 0.54540364\n",
      "Iteration 125, loss = 0.54365888\n",
      "Iteration 126, loss = 0.54201989\n",
      "Iteration 127, loss = 0.54051379\n",
      "Iteration 128, loss = 0.53903374\n",
      "Iteration 129, loss = 0.53755068\n",
      "Iteration 130, loss = 0.53604042\n",
      "Iteration 131, loss = 0.53449880\n",
      "Iteration 132, loss = 0.53292889\n",
      "Iteration 133, loss = 0.53133963\n",
      "Iteration 134, loss = 0.52973754\n",
      "Iteration 135, loss = 0.52813980\n",
      "Iteration 136, loss = 0.52654584\n",
      "Iteration 137, loss = 0.52498489\n",
      "Iteration 138, loss = 0.52345631\n",
      "Iteration 139, loss = 0.52198161\n",
      "Iteration 140, loss = 0.52054481\n",
      "Iteration 141, loss = 0.51914087\n",
      "Iteration 142, loss = 0.51776337\n",
      "Iteration 143, loss = 0.51639269\n",
      "Iteration 144, loss = 0.51502649\n",
      "Iteration 145, loss = 0.51366501\n",
      "Iteration 146, loss = 0.51231064\n",
      "Iteration 147, loss = 0.51096342\n",
      "Iteration 148, loss = 0.50962280\n",
      "Iteration 149, loss = 0.50828814\n",
      "Iteration 150, loss = 0.50695979\n",
      "Iteration 151, loss = 0.50563812\n",
      "Iteration 152, loss = 0.50432345\n",
      "Iteration 153, loss = 0.50301609\n",
      "Iteration 154, loss = 0.50172364\n",
      "Iteration 155, loss = 0.50044530\n",
      "Iteration 156, loss = 0.49918864\n",
      "Iteration 157, loss = 0.49794677\n",
      "Iteration 158, loss = 0.49671643\n",
      "Iteration 159, loss = 0.49549393\n",
      "Iteration 160, loss = 0.49427785\n",
      "Iteration 161, loss = 0.49306658\n",
      "Iteration 162, loss = 0.49185942\n",
      "Iteration 163, loss = 0.49065778\n",
      "Iteration 164, loss = 0.48946232\n",
      "Iteration 165, loss = 0.48827801\n",
      "Iteration 166, loss = 0.48710488\n",
      "Iteration 167, loss = 0.48594789\n",
      "Iteration 168, loss = 0.48480281\n",
      "Iteration 169, loss = 0.48367068\n",
      "Iteration 170, loss = 0.48254482\n",
      "Iteration 171, loss = 0.48142520\n",
      "Iteration 172, loss = 0.48031295\n",
      "Iteration 173, loss = 0.47920900\n",
      "Iteration 174, loss = 0.47811124\n",
      "Iteration 175, loss = 0.47702001\n",
      "Iteration 176, loss = 0.47593632\n",
      "Iteration 177, loss = 0.47486285\n",
      "Iteration 178, loss = 0.47379835\n",
      "Iteration 179, loss = 0.47274320\n",
      "Iteration 180, loss = 0.47169319\n",
      "Iteration 181, loss = 0.47064718\n",
      "Iteration 182, loss = 0.46960530\n",
      "Iteration 183, loss = 0.46856795\n",
      "Iteration 184, loss = 0.46754656\n",
      "Iteration 185, loss = 0.46653040\n",
      "Iteration 186, loss = 0.46551866\n",
      "Iteration 187, loss = 0.46451154\n",
      "Iteration 188, loss = 0.46350896\n",
      "Iteration 189, loss = 0.46251119\n",
      "Iteration 190, loss = 0.46152122\n",
      "Iteration 191, loss = 0.46053837\n",
      "Iteration 192, loss = 0.45956100\n",
      "Iteration 193, loss = 0.45858952\n",
      "Iteration 194, loss = 0.45762252\n",
      "Iteration 195, loss = 0.45666068\n",
      "Iteration 196, loss = 0.45570446\n",
      "Iteration 197, loss = 0.45475325\n",
      "Iteration 198, loss = 0.45380694\n",
      "Iteration 199, loss = 0.45286545\n",
      "Iteration 200, loss = 0.45192871\n",
      "Iteration 201, loss = 0.45099664\n",
      "Iteration 202, loss = 0.45006917\n",
      "Iteration 203, loss = 0.44914670\n",
      "Iteration 204, loss = 0.44822930\n",
      "Iteration 205, loss = 0.44731621\n",
      "Iteration 206, loss = 0.44640742\n",
      "Iteration 207, loss = 0.44550282\n",
      "Iteration 208, loss = 0.44460244\n",
      "Iteration 209, loss = 0.44370619\n",
      "Iteration 210, loss = 0.44281402\n",
      "Iteration 211, loss = 0.44192588\n",
      "Iteration 212, loss = 0.44104172\n",
      "Iteration 213, loss = 0.44016148\n",
      "Iteration 214, loss = 0.43928509\n",
      "Iteration 215, loss = 0.43841286\n",
      "Iteration 216, loss = 0.43754429\n",
      "Iteration 217, loss = 0.43667909\n",
      "Iteration 218, loss = 0.43581732\n",
      "Iteration 219, loss = 0.43495943\n",
      "Iteration 220, loss = 0.43410496\n",
      "Iteration 221, loss = 0.43325394\n",
      "Iteration 222, loss = 0.43240677\n",
      "Iteration 223, loss = 0.43156257\n",
      "Iteration 224, loss = 0.43072133\n",
      "Iteration 225, loss = 0.42988461\n",
      "Iteration 226, loss = 0.42905200\n",
      "Iteration 227, loss = 0.42822199\n",
      "Iteration 228, loss = 0.42739539\n",
      "Iteration 229, loss = 0.42657244\n",
      "Iteration 230, loss = 0.42575253\n",
      "Iteration 231, loss = 0.42493561\n",
      "Iteration 232, loss = 0.42412160\n",
      "Iteration 233, loss = 0.42331045\n",
      "Iteration 234, loss = 0.42250211\n",
      "Iteration 235, loss = 0.42169650\n",
      "Iteration 236, loss = 0.42089358\n",
      "Iteration 237, loss = 0.42009330\n",
      "Iteration 238, loss = 0.41929578\n",
      "Iteration 239, loss = 0.41850134\n",
      "Iteration 240, loss = 0.41770960\n",
      "Iteration 241, loss = 0.41692022\n",
      "Iteration 242, loss = 0.41613338\n",
      "Iteration 243, loss = 0.41534908\n",
      "Iteration 244, loss = 0.41456832\n",
      "Iteration 245, loss = 0.41378975\n",
      "Iteration 246, loss = 0.41301346\n",
      "Iteration 247, loss = 0.41223959\n",
      "Iteration 248, loss = 0.41146793\n",
      "Iteration 249, loss = 0.41069861\n",
      "Iteration 250, loss = 0.40993145\n",
      "Iteration 251, loss = 0.40916648\n",
      "Iteration 252, loss = 0.40840375\n",
      "Iteration 253, loss = 0.40764306\n",
      "Iteration 254, loss = 0.40688492\n",
      "Iteration 255, loss = 0.40612905\n",
      "Iteration 256, loss = 0.40537523\n",
      "Iteration 257, loss = 0.40462353\n",
      "Iteration 258, loss = 0.40387376\n",
      "Iteration 259, loss = 0.40312591\n",
      "Iteration 260, loss = 0.40237995\n",
      "Iteration 261, loss = 0.40163595\n",
      "Iteration 262, loss = 0.40089368\n",
      "Iteration 263, loss = 0.40015330\n",
      "Iteration 264, loss = 0.39941470\n",
      "Iteration 265, loss = 0.39867788\n",
      "Iteration 266, loss = 0.39794272\n",
      "Iteration 267, loss = 0.39720929\n",
      "Iteration 268, loss = 0.39647751\n",
      "Iteration 269, loss = 0.39574734\n",
      "Iteration 270, loss = 0.39501922\n",
      "Iteration 271, loss = 0.39429290\n",
      "Iteration 272, loss = 0.39356817\n",
      "Iteration 273, loss = 0.39284509\n",
      "Iteration 274, loss = 0.39212395\n",
      "Iteration 275, loss = 0.39140434\n",
      "Iteration 276, loss = 0.39068622\n",
      "Iteration 277, loss = 0.38996958\n",
      "Iteration 278, loss = 0.38925440\n",
      "Iteration 279, loss = 0.38854063\n",
      "Iteration 280, loss = 0.38782823\n",
      "Iteration 281, loss = 0.38711755\n",
      "Iteration 282, loss = 0.38640836\n",
      "Iteration 283, loss = 0.38570051\n",
      "Iteration 284, loss = 0.38499396\n",
      "Iteration 285, loss = 0.38428868\n",
      "Iteration 286, loss = 0.38358467\n",
      "Iteration 287, loss = 0.38288187\n",
      "Iteration 288, loss = 0.38218028\n",
      "Iteration 289, loss = 0.38147987\n",
      "Iteration 290, loss = 0.38078062\n",
      "Iteration 291, loss = 0.38008249\n",
      "Iteration 292, loss = 0.37938547\n",
      "Iteration 293, loss = 0.37869148\n",
      "Iteration 294, loss = 0.37799860\n",
      "Iteration 295, loss = 0.37730645\n",
      "Iteration 296, loss = 0.37661522\n",
      "Iteration 297, loss = 0.37592497\n",
      "Iteration 298, loss = 0.37523561\n",
      "Iteration 299, loss = 0.37454731\n",
      "Iteration 300, loss = 0.37386018\n",
      "Iteration 301, loss = 0.37317514\n",
      "Iteration 302, loss = 0.37249240\n",
      "Iteration 303, loss = 0.37181058\n",
      "Iteration 304, loss = 0.37112968\n",
      "Iteration 305, loss = 0.37044971\n",
      "Iteration 306, loss = 0.36977068\n",
      "Iteration 307, loss = 0.36909259\n",
      "Iteration 308, loss = 0.36841545\n",
      "Iteration 309, loss = 0.36773926\n",
      "Iteration 310, loss = 0.36706406\n",
      "Iteration 311, loss = 0.36638983\n",
      "Iteration 312, loss = 0.36571660\n",
      "Iteration 313, loss = 0.36504436\n",
      "Iteration 314, loss = 0.36437312\n",
      "Iteration 315, loss = 0.36370288\n",
      "Iteration 316, loss = 0.36303363\n",
      "Iteration 317, loss = 0.36236592\n",
      "Iteration 318, loss = 0.36169888\n",
      "Iteration 319, loss = 0.36103241\n",
      "Iteration 320, loss = 0.36036670\n",
      "Iteration 321, loss = 0.35970297\n",
      "Iteration 322, loss = 0.35903874\n",
      "Iteration 323, loss = 0.35837608\n",
      "Iteration 324, loss = 0.35771424\n",
      "Iteration 325, loss = 0.35705327\n",
      "Iteration 326, loss = 0.35639314\n",
      "Iteration 327, loss = 0.35573382\n",
      "Iteration 328, loss = 0.35507529\n",
      "Iteration 329, loss = 0.35441756\n",
      "Iteration 330, loss = 0.35376061\n",
      "Iteration 331, loss = 0.35310443\n",
      "Iteration 332, loss = 0.35244902\n",
      "Iteration 333, loss = 0.35179436\n",
      "Iteration 334, loss = 0.35114044\n",
      "Iteration 335, loss = 0.35048758\n",
      "Iteration 336, loss = 0.34983535\n",
      "Iteration 337, loss = 0.34918375\n",
      "Iteration 338, loss = 0.34853297\n",
      "Iteration 339, loss = 0.34788294\n",
      "Iteration 340, loss = 0.34723366\n",
      "Iteration 341, loss = 0.34658531\n",
      "Iteration 342, loss = 0.34593762\n",
      "Iteration 343, loss = 0.34529058\n",
      "Iteration 344, loss = 0.34464419\n",
      "Iteration 345, loss = 0.34399842\n",
      "Iteration 346, loss = 0.34335343\n",
      "Iteration 347, loss = 0.34270911\n",
      "Iteration 348, loss = 0.34206540\n",
      "Iteration 349, loss = 0.34142229\n",
      "Iteration 350, loss = 0.34077978\n",
      "Iteration 351, loss = 0.34013811\n",
      "Iteration 352, loss = 0.33949732\n",
      "Iteration 353, loss = 0.33885711\n",
      "Iteration 354, loss = 0.33821758\n",
      "Iteration 355, loss = 0.33757885\n",
      "Iteration 356, loss = 0.33694072\n",
      "Iteration 357, loss = 0.33630333\n",
      "Iteration 358, loss = 0.33566820\n",
      "Iteration 359, loss = 0.33503439\n",
      "Iteration 360, loss = 0.33440090\n",
      "Iteration 361, loss = 0.33376782\n",
      "Iteration 362, loss = 0.33313517\n",
      "Iteration 363, loss = 0.33250298\n",
      "Iteration 364, loss = 0.33187130\n",
      "Iteration 365, loss = 0.33124013\n",
      "Iteration 366, loss = 0.33060953\n",
      "Iteration 367, loss = 0.32997951\n",
      "Iteration 368, loss = 0.32935007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 369, loss = 0.32872125\n",
      "Iteration 370, loss = 0.32809370\n",
      "Iteration 371, loss = 0.32746745\n",
      "Iteration 372, loss = 0.32684154\n",
      "Iteration 373, loss = 0.32621606\n",
      "Iteration 374, loss = 0.32559115\n",
      "Iteration 375, loss = 0.32496673\n",
      "Iteration 376, loss = 0.32434302\n",
      "Iteration 377, loss = 0.32372119\n",
      "Iteration 378, loss = 0.32310052\n",
      "Iteration 379, loss = 0.32248043\n",
      "Iteration 380, loss = 0.32186089\n",
      "Iteration 381, loss = 0.32124189\n",
      "Iteration 382, loss = 0.32062299\n",
      "Iteration 383, loss = 0.31999661\n",
      "Iteration 384, loss = 0.31933831\n",
      "Iteration 385, loss = 0.31863359\n",
      "Iteration 386, loss = 0.31797937\n",
      "Iteration 387, loss = 0.31736008\n",
      "Iteration 388, loss = 0.31671462\n",
      "Iteration 389, loss = 0.31604735\n",
      "Iteration 390, loss = 0.31536140\n",
      "Iteration 391, loss = 0.31466457\n",
      "Iteration 392, loss = 0.31398764\n",
      "Iteration 393, loss = 0.31332051\n",
      "Iteration 394, loss = 0.31261557\n",
      "Iteration 395, loss = 0.31193392\n",
      "Iteration 396, loss = 0.31124776\n",
      "Iteration 397, loss = 0.31054892\n",
      "Iteration 398, loss = 0.30983608\n",
      "Iteration 399, loss = 0.30911464\n",
      "Iteration 400, loss = 0.30839099\n",
      "Iteration 401, loss = 0.30767699\n",
      "Iteration 402, loss = 0.30694091\n",
      "Iteration 403, loss = 0.30619739\n",
      "Iteration 404, loss = 0.30545957\n",
      "Iteration 405, loss = 0.30471404\n",
      "Iteration 406, loss = 0.30395706\n",
      "Iteration 407, loss = 0.30318979\n",
      "Iteration 408, loss = 0.30241898\n",
      "Iteration 409, loss = 0.30164613\n",
      "Iteration 410, loss = 0.30086665\n",
      "Iteration 411, loss = 0.30008001\n",
      "Iteration 412, loss = 0.29928675\n",
      "Iteration 413, loss = 0.29848879\n",
      "Iteration 414, loss = 0.29768645\n",
      "Iteration 415, loss = 0.29687889\n",
      "Iteration 416, loss = 0.29606442\n",
      "Iteration 417, loss = 0.29524483\n",
      "Iteration 418, loss = 0.29442057\n",
      "Iteration 419, loss = 0.29359150\n",
      "Iteration 420, loss = 0.29275767\n",
      "Iteration 421, loss = 0.29191976\n",
      "Iteration 422, loss = 0.29107710\n",
      "Iteration 423, loss = 0.29023248\n",
      "Iteration 424, loss = 0.28938254\n",
      "Iteration 425, loss = 0.28852816\n",
      "Iteration 426, loss = 0.28767135\n",
      "Iteration 427, loss = 0.28681163\n",
      "Iteration 428, loss = 0.28594866\n",
      "Iteration 429, loss = 0.28508254\n",
      "Iteration 430, loss = 0.28421354\n",
      "Iteration 431, loss = 0.28334205\n",
      "Iteration 432, loss = 0.28246837\n",
      "Iteration 433, loss = 0.28159371\n",
      "Iteration 434, loss = 0.28071707\n",
      "Iteration 435, loss = 0.27983596\n",
      "Iteration 436, loss = 0.27895486\n",
      "Iteration 437, loss = 0.27807205\n",
      "Iteration 438, loss = 0.27718770\n",
      "Iteration 439, loss = 0.27630196\n",
      "Iteration 440, loss = 0.27541546\n",
      "Iteration 441, loss = 0.27452831\n",
      "Iteration 442, loss = 0.27363968\n",
      "Iteration 443, loss = 0.27275039\n",
      "Iteration 444, loss = 0.27186121\n",
      "Iteration 445, loss = 0.27097200\n",
      "Iteration 446, loss = 0.27008244\n",
      "Iteration 447, loss = 0.26919283\n",
      "Iteration 448, loss = 0.26830303\n",
      "Iteration 449, loss = 0.26741321\n",
      "Iteration 450, loss = 0.26652072\n",
      "Iteration 451, loss = 0.26563145\n",
      "Iteration 452, loss = 0.26474679\n",
      "Iteration 453, loss = 0.26385472\n",
      "Iteration 454, loss = 0.26296884\n",
      "Iteration 455, loss = 0.26208128\n",
      "Iteration 456, loss = 0.26119146\n",
      "Iteration 457, loss = 0.26030818\n",
      "Iteration 458, loss = 0.25941987\n",
      "Iteration 459, loss = 0.25853610\n",
      "Iteration 460, loss = 0.25765058\n",
      "Iteration 461, loss = 0.25676758\n",
      "Iteration 462, loss = 0.25588549\n",
      "Iteration 463, loss = 0.25500458\n",
      "Iteration 464, loss = 0.25412326\n",
      "Iteration 465, loss = 0.25324518\n",
      "Iteration 466, loss = 0.25236660\n",
      "Iteration 467, loss = 0.25148675\n",
      "Iteration 468, loss = 0.25060960\n",
      "Iteration 469, loss = 0.24973168\n",
      "Iteration 470, loss = 0.24885693\n",
      "Iteration 471, loss = 0.24798487\n",
      "Iteration 472, loss = 0.24711031\n",
      "Iteration 473, loss = 0.24623635\n",
      "Iteration 474, loss = 0.24536203\n",
      "Iteration 475, loss = 0.24449493\n",
      "Iteration 476, loss = 0.24362408\n",
      "Iteration 477, loss = 0.24275944\n",
      "Iteration 478, loss = 0.24189533\n",
      "Iteration 479, loss = 0.24103108\n",
      "Iteration 480, loss = 0.24016846\n",
      "Iteration 481, loss = 0.23930877\n",
      "Iteration 482, loss = 0.23845131\n",
      "Iteration 483, loss = 0.23759361\n",
      "Iteration 484, loss = 0.23673763\n",
      "Iteration 485, loss = 0.23588359\n",
      "Iteration 486, loss = 0.23503247\n",
      "Iteration 487, loss = 0.23418263\n",
      "Iteration 488, loss = 0.23333343\n",
      "Iteration 489, loss = 0.23248636\n",
      "Iteration 490, loss = 0.23164181\n",
      "Iteration 491, loss = 0.23079913\n",
      "Iteration 492, loss = 0.22995832\n",
      "Iteration 493, loss = 0.22911961\n",
      "Iteration 494, loss = 0.22828310\n",
      "Iteration 495, loss = 0.22744889\n",
      "Iteration 496, loss = 0.22661700\n",
      "Iteration 497, loss = 0.22578757\n",
      "Iteration 498, loss = 0.22496053\n",
      "Iteration 499, loss = 0.22413621\n",
      "Iteration 500, loss = 0.22331432\n",
      "Iteration 501, loss = 0.22249488\n",
      "Iteration 502, loss = 0.22167872\n",
      "Iteration 503, loss = 0.22086490\n",
      "Iteration 504, loss = 0.22005443\n",
      "Iteration 505, loss = 0.21924713\n",
      "Iteration 506, loss = 0.21844216\n",
      "Iteration 507, loss = 0.21763985\n",
      "Iteration 508, loss = 0.21684154\n",
      "Iteration 509, loss = 0.21604634\n",
      "Iteration 510, loss = 0.21525356\n",
      "Iteration 511, loss = 0.21446349\n",
      "Iteration 512, loss = 0.21367658\n",
      "Iteration 513, loss = 0.21289423\n",
      "Iteration 514, loss = 0.21211455\n",
      "Iteration 515, loss = 0.21133863\n",
      "Iteration 516, loss = 0.21056620\n",
      "Iteration 517, loss = 0.20979848\n",
      "Iteration 518, loss = 0.20903345\n",
      "Iteration 519, loss = 0.20827101\n",
      "Iteration 520, loss = 0.20751154\n",
      "Iteration 521, loss = 0.20675588\n",
      "Iteration 522, loss = 0.20600499\n",
      "Iteration 523, loss = 0.20525637\n",
      "Iteration 524, loss = 0.20451195\n",
      "Iteration 525, loss = 0.20377140\n",
      "Iteration 526, loss = 0.20303407\n",
      "Iteration 527, loss = 0.20229999\n",
      "Iteration 528, loss = 0.20156939\n",
      "Iteration 529, loss = 0.20084450\n",
      "Iteration 530, loss = 0.20012220\n",
      "Iteration 531, loss = 0.19940262\n",
      "Iteration 532, loss = 0.19868673\n",
      "Iteration 533, loss = 0.19797571\n",
      "Iteration 534, loss = 0.19726798\n",
      "Iteration 535, loss = 0.19656338\n",
      "Iteration 536, loss = 0.19586357\n",
      "Iteration 537, loss = 0.19516727\n",
      "Iteration 538, loss = 0.19447375\n",
      "Iteration 539, loss = 0.19378407\n",
      "Iteration 540, loss = 0.19309903\n",
      "Iteration 541, loss = 0.19241726\n",
      "Iteration 542, loss = 0.19173888\n",
      "Iteration 543, loss = 0.19106467\n",
      "Iteration 544, loss = 0.19039371\n",
      "Iteration 545, loss = 0.18972679\n",
      "Iteration 546, loss = 0.18906358\n",
      "Iteration 547, loss = 0.18840381\n",
      "Iteration 548, loss = 0.18774753\n",
      "Iteration 549, loss = 0.18709484\n",
      "Iteration 550, loss = 0.18644588\n",
      "Iteration 551, loss = 0.18580057\n",
      "Iteration 552, loss = 0.18515886\n",
      "Iteration 553, loss = 0.18452072\n",
      "Iteration 554, loss = 0.18388612\n",
      "Iteration 555, loss = 0.18325507\n",
      "Iteration 556, loss = 0.18262758\n",
      "Iteration 557, loss = 0.18200369\n",
      "Iteration 558, loss = 0.18138340\n",
      "Iteration 559, loss = 0.18076659\n",
      "Iteration 560, loss = 0.18015340\n",
      "Iteration 561, loss = 0.17954492\n",
      "Iteration 562, loss = 0.17893861\n",
      "Iteration 563, loss = 0.17833498\n",
      "Iteration 564, loss = 0.17773635\n",
      "Iteration 565, loss = 0.17714132\n",
      "Iteration 566, loss = 0.17654968\n",
      "Iteration 567, loss = 0.17596140\n",
      "Iteration 568, loss = 0.17537639\n",
      "Iteration 569, loss = 0.17479465\n",
      "Iteration 570, loss = 0.17421640\n",
      "Iteration 571, loss = 0.17364185\n",
      "Iteration 572, loss = 0.17307047\n",
      "Iteration 573, loss = 0.17250275\n",
      "Iteration 574, loss = 0.17193854\n",
      "Iteration 575, loss = 0.17137764\n",
      "Iteration 576, loss = 0.17081992\n",
      "Iteration 577, loss = 0.17026558\n",
      "Iteration 578, loss = 0.16971518\n",
      "Iteration 579, loss = 0.16916780\n",
      "Iteration 580, loss = 0.16862304\n",
      "Iteration 581, loss = 0.16808192\n",
      "Iteration 582, loss = 0.16754410\n",
      "Iteration 583, loss = 0.16700939\n",
      "Iteration 584, loss = 0.16647754\n",
      "Iteration 585, loss = 0.16594932\n",
      "Iteration 586, loss = 0.16542435\n",
      "Iteration 587, loss = 0.16490210\n",
      "Iteration 588, loss = 0.16438271\n",
      "Iteration 589, loss = 0.16386740\n",
      "Iteration 590, loss = 0.16335492\n",
      "Iteration 591, loss = 0.16284505\n",
      "Iteration 592, loss = 0.16233773\n",
      "Iteration 593, loss = 0.16183448\n",
      "Iteration 594, loss = 0.16133424\n",
      "Iteration 595, loss = 0.16083666\n",
      "Iteration 596, loss = 0.16034180\n",
      "Iteration 597, loss = 0.15984986\n",
      "Iteration 598, loss = 0.15936050\n",
      "Iteration 599, loss = 0.15887423\n",
      "Iteration 600, loss = 0.15839136\n",
      "Iteration 601, loss = 0.15791098\n",
      "Iteration 602, loss = 0.15743369\n",
      "Iteration 603, loss = 0.15695911\n",
      "Iteration 604, loss = 0.15648728\n",
      "Iteration 605, loss = 0.15601865\n",
      "Iteration 606, loss = 0.15555214\n",
      "Iteration 607, loss = 0.15508875\n",
      "Iteration 608, loss = 0.15462804\n",
      "Iteration 609, loss = 0.15417020\n",
      "Iteration 610, loss = 0.15371482\n",
      "Iteration 611, loss = 0.15326227\n",
      "Iteration 612, loss = 0.15281233\n",
      "Iteration 613, loss = 0.15236502\n",
      "Iteration 614, loss = 0.15192032\n",
      "Iteration 615, loss = 0.15147819\n",
      "Iteration 616, loss = 0.15103864\n",
      "Iteration 617, loss = 0.15060166\n",
      "Iteration 618, loss = 0.15016723\n",
      "Iteration 619, loss = 0.14973585\n",
      "Iteration 620, loss = 0.14930609\n",
      "Iteration 621, loss = 0.14887931\n",
      "Iteration 622, loss = 0.14845498\n",
      "Iteration 623, loss = 0.14803310\n",
      "Iteration 624, loss = 0.14761379\n",
      "Iteration 625, loss = 0.14719689\n",
      "Iteration 626, loss = 0.14678264\n",
      "Iteration 627, loss = 0.14637061\n",
      "Iteration 628, loss = 0.14596161\n",
      "Iteration 629, loss = 0.14555415\n",
      "Iteration 630, loss = 0.14514951\n",
      "Iteration 631, loss = 0.14474717\n",
      "Iteration 632, loss = 0.14434706\n",
      "Iteration 633, loss = 0.14394915\n",
      "Iteration 634, loss = 0.14355486\n",
      "Iteration 635, loss = 0.14316119\n",
      "Iteration 636, loss = 0.14277088\n",
      "Iteration 637, loss = 0.14238277\n",
      "Iteration 638, loss = 0.14199576\n",
      "Iteration 639, loss = 0.14161087\n",
      "Iteration 640, loss = 0.14122898\n",
      "Iteration 641, loss = 0.14084945\n",
      "Iteration 642, loss = 0.14047247\n",
      "Iteration 643, loss = 0.14009761\n",
      "Iteration 644, loss = 0.13972424\n",
      "Iteration 645, loss = 0.13935261\n",
      "Iteration 646, loss = 0.13898306\n",
      "Iteration 647, loss = 0.13861750\n",
      "Iteration 648, loss = 0.13825102\n",
      "Iteration 649, loss = 0.13788830\n",
      "Iteration 650, loss = 0.13752762\n",
      "Iteration 651, loss = 0.13716855\n",
      "Iteration 652, loss = 0.13681090\n",
      "Iteration 653, loss = 0.13645499\n",
      "Iteration 654, loss = 0.13610209\n",
      "Iteration 655, loss = 0.13575000\n",
      "Iteration 656, loss = 0.13540066\n",
      "Iteration 657, loss = 0.13505295\n",
      "Iteration 658, loss = 0.13470716\n",
      "Iteration 659, loss = 0.13436310\n",
      "Iteration 660, loss = 0.13402060\n",
      "Iteration 661, loss = 0.13367991\n",
      "Iteration 662, loss = 0.13334168\n",
      "Iteration 663, loss = 0.13300488\n",
      "Iteration 664, loss = 0.13267020\n",
      "Iteration 665, loss = 0.13233699\n",
      "Iteration 666, loss = 0.13200550\n",
      "Iteration 667, loss = 0.13167568\n",
      "Iteration 668, loss = 0.13134751\n",
      "Iteration 669, loss = 0.13102128\n",
      "Iteration 670, loss = 0.13069704\n",
      "Iteration 671, loss = 0.13037416\n",
      "Iteration 672, loss = 0.13005378\n",
      "Iteration 673, loss = 0.12973465\n",
      "Iteration 674, loss = 0.12941749\n",
      "Iteration 675, loss = 0.12910329\n",
      "Iteration 676, loss = 0.12878999\n",
      "Iteration 677, loss = 0.12847881\n",
      "Iteration 678, loss = 0.12817076\n",
      "Iteration 679, loss = 0.12786336\n",
      "Iteration 680, loss = 0.12755676\n",
      "Iteration 681, loss = 0.12725322\n",
      "Iteration 682, loss = 0.12695112\n",
      "Iteration 683, loss = 0.12665051\n",
      "Iteration 684, loss = 0.12635244\n",
      "Iteration 685, loss = 0.12605672\n",
      "Iteration 686, loss = 0.12576279\n",
      "Iteration 687, loss = 0.12547028\n",
      "Iteration 688, loss = 0.12517975\n",
      "Iteration 689, loss = 0.12489117\n",
      "Iteration 690, loss = 0.12460336\n",
      "Iteration 691, loss = 0.12431781\n",
      "Iteration 692, loss = 0.12403383\n",
      "Iteration 693, loss = 0.12375141\n",
      "Iteration 694, loss = 0.12347056\n",
      "Iteration 695, loss = 0.12319114\n",
      "Iteration 696, loss = 0.12291389\n",
      "Iteration 697, loss = 0.12263742\n",
      "Iteration 698, loss = 0.12236291\n",
      "Iteration 699, loss = 0.12209029\n",
      "Iteration 700, loss = 0.12181856\n",
      "Iteration 701, loss = 0.12154825\n",
      "Iteration 702, loss = 0.12127933\n",
      "Iteration 703, loss = 0.12101289\n",
      "Iteration 704, loss = 0.12074696\n",
      "Iteration 705, loss = 0.12048212\n",
      "Iteration 706, loss = 0.12021891\n",
      "Iteration 707, loss = 0.11995651\n",
      "Iteration 708, loss = 0.11969384\n",
      "Iteration 709, loss = 0.11943178\n",
      "Iteration 710, loss = 0.11917081\n",
      "Iteration 711, loss = 0.11891260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 712, loss = 0.11865480\n",
      "Iteration 713, loss = 0.11839714\n",
      "Iteration 714, loss = 0.11814153\n",
      "Iteration 715, loss = 0.11788791\n",
      "Iteration 716, loss = 0.11763478\n",
      "Iteration 717, loss = 0.11738213\n",
      "Iteration 718, loss = 0.11713060\n",
      "Iteration 719, loss = 0.11688114\n",
      "Iteration 720, loss = 0.11663303\n",
      "Iteration 721, loss = 0.11638678\n",
      "Iteration 722, loss = 0.11614292\n",
      "Iteration 723, loss = 0.11590035\n",
      "Iteration 724, loss = 0.11565850\n",
      "Iteration 725, loss = 0.11541747\n",
      "Iteration 726, loss = 0.11517840\n",
      "Iteration 727, loss = 0.11494139\n",
      "Iteration 728, loss = 0.11470526\n",
      "Iteration 729, loss = 0.11447069\n",
      "Iteration 730, loss = 0.11423772\n",
      "Iteration 731, loss = 0.11400576\n",
      "Iteration 732, loss = 0.11377542\n",
      "Iteration 733, loss = 0.11354638\n",
      "Iteration 734, loss = 0.11331821\n",
      "Iteration 735, loss = 0.11309093\n",
      "Iteration 736, loss = 0.11286468\n",
      "Iteration 737, loss = 0.11263932\n",
      "Iteration 738, loss = 0.11241618\n",
      "Iteration 739, loss = 0.11219383\n",
      "Iteration 740, loss = 0.11197369\n",
      "Iteration 741, loss = 0.11175532\n",
      "Iteration 742, loss = 0.11153792\n",
      "Iteration 743, loss = 0.11132159\n",
      "Iteration 744, loss = 0.11110681\n",
      "Iteration 745, loss = 0.11089350\n",
      "Iteration 746, loss = 0.11068129\n",
      "Iteration 747, loss = 0.11046996\n",
      "Iteration 748, loss = 0.11025942\n",
      "Iteration 749, loss = 0.11005029\n",
      "Iteration 750, loss = 0.10984230\n",
      "Iteration 751, loss = 0.10963535\n",
      "Iteration 752, loss = 0.10942964\n",
      "Iteration 753, loss = 0.10922508\n",
      "Iteration 754, loss = 0.10902170\n",
      "Iteration 755, loss = 0.10881923\n",
      "Iteration 756, loss = 0.10861790\n",
      "Iteration 757, loss = 0.10841746\n",
      "Iteration 758, loss = 0.10821835\n",
      "Iteration 759, loss = 0.10802045\n",
      "Iteration 760, loss = 0.10782360\n",
      "Iteration 761, loss = 0.10762787\n",
      "Iteration 762, loss = 0.10743301\n",
      "Iteration 763, loss = 0.10723911\n",
      "Iteration 764, loss = 0.10704606\n",
      "Iteration 765, loss = 0.10685391\n",
      "Iteration 766, loss = 0.10666367\n",
      "Iteration 767, loss = 0.10647279\n",
      "Iteration 768, loss = 0.10628388\n",
      "Iteration 769, loss = 0.10609560\n",
      "Iteration 770, loss = 0.10590827\n",
      "Iteration 771, loss = 0.10572188\n",
      "Iteration 772, loss = 0.10553620\n",
      "Iteration 773, loss = 0.10535166\n",
      "Iteration 774, loss = 0.10516808\n",
      "Iteration 775, loss = 0.10498500\n",
      "Iteration 776, loss = 0.10480265\n",
      "Iteration 777, loss = 0.10462171\n",
      "Iteration 778, loss = 0.10444184\n",
      "Iteration 779, loss = 0.10426248\n",
      "Iteration 780, loss = 0.10408389\n",
      "Iteration 781, loss = 0.10390603\n",
      "Iteration 782, loss = 0.10372860\n",
      "Iteration 783, loss = 0.10355199\n",
      "Iteration 784, loss = 0.10337805\n",
      "Iteration 785, loss = 0.10320365\n",
      "Iteration 786, loss = 0.10302847\n",
      "Iteration 787, loss = 0.10285647\n",
      "Iteration 788, loss = 0.10268542\n",
      "Iteration 789, loss = 0.10251463\n",
      "Iteration 790, loss = 0.10234412\n",
      "Iteration 791, loss = 0.10217424\n",
      "Iteration 792, loss = 0.10200526\n",
      "Iteration 793, loss = 0.10183655\n",
      "Iteration 794, loss = 0.10166817\n",
      "Iteration 795, loss = 0.10150299\n",
      "Iteration 796, loss = 0.10133829\n",
      "Iteration 797, loss = 0.10117298\n",
      "Iteration 798, loss = 0.10100714\n",
      "Iteration 799, loss = 0.10084379\n",
      "Iteration 800, loss = 0.10068213\n",
      "Iteration 801, loss = 0.10052070\n",
      "Iteration 802, loss = 0.10035944\n",
      "Iteration 803, loss = 0.10019869\n",
      "Iteration 804, loss = 0.10003863\n",
      "Iteration 805, loss = 0.09987893\n",
      "Iteration 806, loss = 0.09972133\n",
      "Iteration 807, loss = 0.09956480\n",
      "Iteration 808, loss = 0.09940834\n",
      "Iteration 809, loss = 0.09925152\n",
      "Iteration 810, loss = 0.09909623\n",
      "Iteration 811, loss = 0.09894242\n",
      "Iteration 812, loss = 0.09878936\n",
      "Iteration 813, loss = 0.09863648\n",
      "Iteration 814, loss = 0.09848370\n",
      "Iteration 815, loss = 0.09833124\n",
      "Iteration 816, loss = 0.09818115\n",
      "Iteration 817, loss = 0.09803124\n",
      "Iteration 818, loss = 0.09788156\n",
      "Iteration 819, loss = 0.09773249\n",
      "Iteration 820, loss = 0.09758452\n",
      "Iteration 821, loss = 0.09743696\n",
      "Iteration 822, loss = 0.09728997\n",
      "Iteration 823, loss = 0.09714448\n",
      "Iteration 824, loss = 0.09699925\n",
      "Iteration 825, loss = 0.09685385\n",
      "Iteration 826, loss = 0.09670926\n",
      "Iteration 827, loss = 0.09656606\n",
      "Iteration 828, loss = 0.09642301\n",
      "Iteration 829, loss = 0.09628114\n",
      "Iteration 830, loss = 0.09613967\n",
      "Iteration 831, loss = 0.09599829\n",
      "Iteration 832, loss = 0.09585836\n",
      "Iteration 833, loss = 0.09571902\n",
      "Iteration 834, loss = 0.09557974\n",
      "Iteration 835, loss = 0.09544065\n",
      "Iteration 836, loss = 0.09530332\n",
      "Iteration 837, loss = 0.09516612\n",
      "Iteration 838, loss = 0.09502904\n",
      "Iteration 839, loss = 0.09489306\n",
      "Iteration 840, loss = 0.09475791\n",
      "Iteration 841, loss = 0.09462268\n",
      "Iteration 842, loss = 0.09448822\n",
      "Iteration 843, loss = 0.09435478\n",
      "Iteration 844, loss = 0.09422146\n",
      "Iteration 845, loss = 0.09408885\n",
      "Iteration 846, loss = 0.09395690\n",
      "Iteration 847, loss = 0.09382550\n",
      "Iteration 848, loss = 0.09369504\n",
      "Iteration 849, loss = 0.09356494\n",
      "Iteration 850, loss = 0.09343502\n",
      "Iteration 851, loss = 0.09330622\n",
      "Iteration 852, loss = 0.09317776\n",
      "Iteration 853, loss = 0.09304990\n",
      "Iteration 854, loss = 0.09292254\n",
      "Iteration 855, loss = 0.09279573\n",
      "Iteration 856, loss = 0.09266956\n",
      "Iteration 857, loss = 0.09254394\n",
      "Iteration 858, loss = 0.09241871\n",
      "Iteration 859, loss = 0.09229425\n",
      "Iteration 860, loss = 0.09217048\n",
      "Iteration 861, loss = 0.09204691\n",
      "Iteration 862, loss = 0.09192426\n",
      "Iteration 863, loss = 0.09180175\n",
      "Iteration 864, loss = 0.09167959\n",
      "Iteration 865, loss = 0.09155861\n",
      "Iteration 866, loss = 0.09143776\n",
      "Iteration 867, loss = 0.09131746\n",
      "Iteration 868, loss = 0.09119754\n",
      "Iteration 869, loss = 0.09107831\n",
      "Iteration 870, loss = 0.09095969\n",
      "Iteration 871, loss = 0.09084163\n",
      "Iteration 872, loss = 0.09072383\n",
      "Iteration 873, loss = 0.09060658\n",
      "Iteration 874, loss = 0.09048965\n",
      "Iteration 875, loss = 0.09037350\n",
      "Iteration 876, loss = 0.09025750\n",
      "Iteration 877, loss = 0.09014185\n",
      "Iteration 878, loss = 0.09002651\n",
      "Iteration 879, loss = 0.08991182\n",
      "Iteration 880, loss = 0.08979760\n",
      "Iteration 881, loss = 0.08968391\n",
      "Iteration 882, loss = 0.08957041\n",
      "Iteration 883, loss = 0.08945733\n",
      "Iteration 884, loss = 0.08934468\n",
      "Iteration 885, loss = 0.08923290\n",
      "Iteration 886, loss = 0.08912134\n",
      "Iteration 887, loss = 0.08901061\n",
      "Iteration 888, loss = 0.08890001\n",
      "Iteration 889, loss = 0.08878991\n",
      "Iteration 890, loss = 0.08868021\n",
      "Iteration 891, loss = 0.08857093\n",
      "Iteration 892, loss = 0.08846178\n",
      "Iteration 893, loss = 0.08835280\n",
      "Iteration 894, loss = 0.08824445\n",
      "Iteration 895, loss = 0.08813657\n",
      "Iteration 896, loss = 0.08802902\n",
      "Iteration 897, loss = 0.08792192\n",
      "Iteration 898, loss = 0.08781527\n",
      "Iteration 899, loss = 0.08770920\n",
      "Iteration 900, loss = 0.08760314\n",
      "Iteration 901, loss = 0.08749762\n",
      "Iteration 902, loss = 0.08739267\n",
      "Iteration 903, loss = 0.08728805\n",
      "Iteration 904, loss = 0.08718381\n",
      "Iteration 905, loss = 0.08708017\n",
      "Iteration 906, loss = 0.08697695\n",
      "Iteration 907, loss = 0.08687404\n",
      "Iteration 908, loss = 0.08677155\n",
      "Iteration 909, loss = 0.08666966\n",
      "Iteration 910, loss = 0.08656800\n",
      "Iteration 911, loss = 0.08646692\n",
      "Iteration 912, loss = 0.08636628\n",
      "Iteration 913, loss = 0.08626654\n",
      "Iteration 914, loss = 0.08616703\n",
      "Iteration 915, loss = 0.08606787\n",
      "Iteration 916, loss = 0.08596904\n",
      "Iteration 917, loss = 0.08587067\n",
      "Iteration 918, loss = 0.08577273\n",
      "Iteration 919, loss = 0.08567515\n",
      "Iteration 920, loss = 0.08557816\n",
      "Iteration 921, loss = 0.08548130\n",
      "Iteration 922, loss = 0.08538512\n",
      "Iteration 923, loss = 0.08528934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.32460029\n",
      "Iteration 3, loss = 1.29803981\n",
      "Iteration 4, loss = 1.27197671\n",
      "Iteration 5, loss = 1.24647155\n",
      "Iteration 6, loss = 1.22154064\n",
      "Iteration 7, loss = 1.19721776\n",
      "Iteration 8, loss = 1.17350108\n",
      "Iteration 9, loss = 1.15031721\n",
      "Iteration 10, loss = 1.12771675\n",
      "Iteration 11, loss = 1.10577085\n",
      "Iteration 12, loss = 1.08449108\n",
      "Iteration 13, loss = 1.06392865\n",
      "Iteration 14, loss = 1.04405508\n",
      "Iteration 15, loss = 1.02489620\n",
      "Iteration 16, loss = 1.00644322\n",
      "Iteration 17, loss = 0.98871259\n",
      "Iteration 18, loss = 0.97169940\n",
      "Iteration 19, loss = 0.95540346\n",
      "Iteration 20, loss = 0.93980704\n",
      "Iteration 21, loss = 0.92490506\n",
      "Iteration 22, loss = 0.91068260\n",
      "Iteration 23, loss = 0.89713212\n",
      "Iteration 24, loss = 0.88428439\n",
      "Iteration 25, loss = 0.87211160\n",
      "Iteration 26, loss = 0.86061628\n",
      "Iteration 27, loss = 0.84973554\n",
      "Iteration 28, loss = 0.83948533\n",
      "Iteration 29, loss = 0.82984071\n",
      "Iteration 30, loss = 0.82076867\n",
      "Iteration 31, loss = 0.81225250\n",
      "Iteration 32, loss = 0.80423470\n",
      "Iteration 33, loss = 0.79671948\n",
      "Iteration 34, loss = 0.78971164\n",
      "Iteration 35, loss = 0.78319289\n",
      "Iteration 36, loss = 0.77714119\n",
      "Iteration 37, loss = 0.77149977\n",
      "Iteration 38, loss = 0.76623721\n",
      "Iteration 39, loss = 0.76133817\n",
      "Iteration 40, loss = 0.75679972\n",
      "Iteration 41, loss = 0.75263636\n",
      "Iteration 42, loss = 0.74875049\n",
      "Iteration 43, loss = 0.74511369\n",
      "Iteration 44, loss = 0.74170520\n",
      "Iteration 45, loss = 0.73864152\n",
      "Iteration 46, loss = 0.73570697\n",
      "Iteration 47, loss = 0.73290186\n",
      "Iteration 48, loss = 0.73019962\n",
      "Iteration 49, loss = 0.72756466\n",
      "Iteration 50, loss = 0.72499530\n",
      "Iteration 51, loss = 0.72249707\n",
      "Iteration 52, loss = 0.72005426\n",
      "Iteration 53, loss = 0.71763022\n",
      "Iteration 54, loss = 0.71523272\n",
      "Iteration 55, loss = 0.71284248\n",
      "Iteration 56, loss = 0.71045720\n",
      "Iteration 57, loss = 0.70806824\n",
      "Iteration 58, loss = 0.70567535\n",
      "Iteration 59, loss = 0.70328095\n",
      "Iteration 60, loss = 0.70084248\n",
      "Iteration 61, loss = 0.69837132\n",
      "Iteration 62, loss = 0.69589272\n",
      "Iteration 63, loss = 0.69340343\n",
      "Iteration 64, loss = 0.69088935\n",
      "Iteration 65, loss = 0.68835795\n",
      "Iteration 66, loss = 0.68581496\n",
      "Iteration 67, loss = 0.68324991\n",
      "Iteration 68, loss = 0.68066820\n",
      "Iteration 69, loss = 0.67805259\n",
      "Iteration 70, loss = 0.67538110\n",
      "Iteration 71, loss = 0.67260794\n",
      "Iteration 72, loss = 0.66974308\n",
      "Iteration 73, loss = 0.66683139\n",
      "Iteration 74, loss = 0.66393126\n",
      "Iteration 75, loss = 0.66101652\n",
      "Iteration 76, loss = 0.65809005\n",
      "Iteration 77, loss = 0.65518326\n",
      "Iteration 78, loss = 0.65226726\n",
      "Iteration 79, loss = 0.64942048\n",
      "Iteration 80, loss = 0.64660488\n",
      "Iteration 81, loss = 0.64382064\n",
      "Iteration 82, loss = 0.64106729\n",
      "Iteration 83, loss = 0.63833163\n",
      "Iteration 84, loss = 0.63564417\n",
      "Iteration 85, loss = 0.63298239\n",
      "Iteration 86, loss = 0.63037915\n",
      "Iteration 87, loss = 0.62778162\n",
      "Iteration 88, loss = 0.62519417\n",
      "Iteration 89, loss = 0.62261836\n",
      "Iteration 90, loss = 0.62005700\n",
      "Iteration 91, loss = 0.61751968\n",
      "Iteration 92, loss = 0.61499715\n",
      "Iteration 93, loss = 0.61247882\n",
      "Iteration 94, loss = 0.60997362\n",
      "Iteration 95, loss = 0.60748215\n",
      "Iteration 96, loss = 0.60499891\n",
      "Iteration 97, loss = 0.60250909\n",
      "Iteration 98, loss = 0.60002927\n",
      "Iteration 99, loss = 0.59756214\n",
      "Iteration 100, loss = 0.59510922\n",
      "Iteration 101, loss = 0.59267101\n",
      "Iteration 102, loss = 0.59024847\n",
      "Iteration 103, loss = 0.58784242\n",
      "Iteration 104, loss = 0.58545325\n",
      "Iteration 105, loss = 0.58308134\n",
      "Iteration 106, loss = 0.58072738\n",
      "Iteration 107, loss = 0.57839130\n",
      "Iteration 108, loss = 0.57607308\n",
      "Iteration 109, loss = 0.57377291\n",
      "Iteration 110, loss = 0.57149084\n",
      "Iteration 111, loss = 0.56922734\n",
      "Iteration 112, loss = 0.56698205\n",
      "Iteration 113, loss = 0.56475456\n",
      "Iteration 114, loss = 0.56254488\n",
      "Iteration 115, loss = 0.56035309\n",
      "Iteration 116, loss = 0.55819426\n",
      "Iteration 117, loss = 0.55615218\n",
      "Iteration 118, loss = 0.55416920\n",
      "Iteration 119, loss = 0.55224398\n",
      "Iteration 120, loss = 0.55044332\n",
      "Iteration 121, loss = 0.54884130\n",
      "Iteration 122, loss = 0.54730800\n",
      "Iteration 123, loss = 0.54578404\n",
      "Iteration 124, loss = 0.54425997\n",
      "Iteration 125, loss = 0.54271089\n",
      "Iteration 126, loss = 0.54111565\n",
      "Iteration 127, loss = 0.53948206\n",
      "Iteration 128, loss = 0.53782591\n",
      "Iteration 129, loss = 0.53617313\n",
      "Iteration 130, loss = 0.53453844\n",
      "Iteration 131, loss = 0.53292537\n",
      "Iteration 132, loss = 0.53134225\n",
      "Iteration 133, loss = 0.52977936\n",
      "Iteration 134, loss = 0.52826559\n",
      "Iteration 135, loss = 0.52677757\n",
      "Iteration 136, loss = 0.52533779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 137, loss = 0.52392579\n",
      "Iteration 138, loss = 0.52252162\n",
      "Iteration 139, loss = 0.52113328\n",
      "Iteration 140, loss = 0.51975230\n",
      "Iteration 141, loss = 0.51837123\n",
      "Iteration 142, loss = 0.51699074\n",
      "Iteration 143, loss = 0.51561179\n",
      "Iteration 144, loss = 0.51423914\n",
      "Iteration 145, loss = 0.51287557\n",
      "Iteration 146, loss = 0.51152512\n",
      "Iteration 147, loss = 0.51018511\n",
      "Iteration 148, loss = 0.50885718\n",
      "Iteration 149, loss = 0.50754964\n",
      "Iteration 150, loss = 0.50625749\n",
      "Iteration 151, loss = 0.50497536\n",
      "Iteration 152, loss = 0.50371265\n",
      "Iteration 153, loss = 0.50246426\n",
      "Iteration 154, loss = 0.50121816\n",
      "Iteration 155, loss = 0.49997440\n",
      "Iteration 156, loss = 0.49873352\n",
      "Iteration 157, loss = 0.49749757\n",
      "Iteration 158, loss = 0.49627453\n",
      "Iteration 159, loss = 0.49506245\n",
      "Iteration 160, loss = 0.49386249\n",
      "Iteration 161, loss = 0.49267240\n",
      "Iteration 162, loss = 0.49149010\n",
      "Iteration 163, loss = 0.49031860\n",
      "Iteration 164, loss = 0.48915386\n",
      "Iteration 165, loss = 0.48799572\n",
      "Iteration 166, loss = 0.48684418\n",
      "Iteration 167, loss = 0.48569923\n",
      "Iteration 168, loss = 0.48456086\n",
      "Iteration 169, loss = 0.48342905\n",
      "Iteration 170, loss = 0.48230486\n",
      "Iteration 171, loss = 0.48118763\n",
      "Iteration 172, loss = 0.48007696\n",
      "Iteration 173, loss = 0.47897280\n",
      "Iteration 174, loss = 0.47787680\n",
      "Iteration 175, loss = 0.47679157\n",
      "Iteration 176, loss = 0.47571149\n",
      "Iteration 177, loss = 0.47463605\n",
      "Iteration 178, loss = 0.47356740\n",
      "Iteration 179, loss = 0.47250510\n",
      "Iteration 180, loss = 0.47144901\n",
      "Iteration 181, loss = 0.47039966\n",
      "Iteration 182, loss = 0.46935696\n",
      "Iteration 183, loss = 0.46831940\n",
      "Iteration 184, loss = 0.46728696\n",
      "Iteration 185, loss = 0.46625962\n",
      "Iteration 186, loss = 0.46523790\n",
      "Iteration 187, loss = 0.46422275\n",
      "Iteration 188, loss = 0.46321265\n",
      "Iteration 189, loss = 0.46220916\n",
      "Iteration 190, loss = 0.46121440\n",
      "Iteration 191, loss = 0.46022635\n",
      "Iteration 192, loss = 0.45924410\n",
      "Iteration 193, loss = 0.45826700\n",
      "Iteration 194, loss = 0.45729522\n",
      "Iteration 195, loss = 0.45633134\n",
      "Iteration 196, loss = 0.45537472\n",
      "Iteration 197, loss = 0.45442368\n",
      "Iteration 198, loss = 0.45347830\n",
      "Iteration 199, loss = 0.45253773\n",
      "Iteration 200, loss = 0.45160113\n",
      "Iteration 201, loss = 0.45066855\n",
      "Iteration 202, loss = 0.44974008\n",
      "Iteration 203, loss = 0.44881620\n",
      "Iteration 204, loss = 0.44789686\n",
      "Iteration 205, loss = 0.44698177\n",
      "Iteration 206, loss = 0.44607090\n",
      "Iteration 207, loss = 0.44516459\n",
      "Iteration 208, loss = 0.44426360\n",
      "Iteration 209, loss = 0.44336690\n",
      "Iteration 210, loss = 0.44247426\n",
      "Iteration 211, loss = 0.44158559\n",
      "Iteration 212, loss = 0.44070085\n",
      "Iteration 213, loss = 0.43981998\n",
      "Iteration 214, loss = 0.43894318\n",
      "Iteration 215, loss = 0.43807018\n",
      "Iteration 216, loss = 0.43720060\n",
      "Iteration 217, loss = 0.43633482\n",
      "Iteration 218, loss = 0.43547271\n",
      "Iteration 219, loss = 0.43461406\n",
      "Iteration 220, loss = 0.43375885\n",
      "Iteration 221, loss = 0.43290708\n",
      "Iteration 222, loss = 0.43205856\n",
      "Iteration 223, loss = 0.43121341\n",
      "Iteration 224, loss = 0.43037153\n",
      "Iteration 225, loss = 0.42953286\n",
      "Iteration 226, loss = 0.42869736\n",
      "Iteration 227, loss = 0.42786498\n",
      "Iteration 228, loss = 0.42703570\n",
      "Iteration 229, loss = 0.42620937\n",
      "Iteration 230, loss = 0.42538601\n",
      "Iteration 231, loss = 0.42456585\n",
      "Iteration 232, loss = 0.42374932\n",
      "Iteration 233, loss = 0.42293566\n",
      "Iteration 234, loss = 0.42212472\n",
      "Iteration 235, loss = 0.42131624\n",
      "Iteration 236, loss = 0.42051055\n",
      "Iteration 237, loss = 0.41970759\n",
      "Iteration 238, loss = 0.41890729\n",
      "Iteration 239, loss = 0.41810961\n",
      "Iteration 240, loss = 0.41731452\n",
      "Iteration 241, loss = 0.41652197\n",
      "Iteration 242, loss = 0.41573190\n",
      "Iteration 243, loss = 0.41494428\n",
      "Iteration 244, loss = 0.41415905\n",
      "Iteration 245, loss = 0.41337617\n",
      "Iteration 246, loss = 0.41259559\n",
      "Iteration 247, loss = 0.41181726\n",
      "Iteration 248, loss = 0.41104113\n",
      "Iteration 249, loss = 0.41026717\n",
      "Iteration 250, loss = 0.40949530\n",
      "Iteration 251, loss = 0.40872552\n",
      "Iteration 252, loss = 0.40795776\n",
      "Iteration 253, loss = 0.40719198\n",
      "Iteration 254, loss = 0.40642816\n",
      "Iteration 255, loss = 0.40566624\n",
      "Iteration 256, loss = 0.40490682\n",
      "Iteration 257, loss = 0.40415014\n",
      "Iteration 258, loss = 0.40339514\n",
      "Iteration 259, loss = 0.40264182\n",
      "Iteration 260, loss = 0.40189019\n",
      "Iteration 261, loss = 0.40114025\n",
      "Iteration 262, loss = 0.40039200\n",
      "Iteration 263, loss = 0.39964546\n",
      "Iteration 264, loss = 0.39890101\n",
      "Iteration 265, loss = 0.39815834\n",
      "Iteration 266, loss = 0.39741731\n",
      "Iteration 267, loss = 0.39667791\n",
      "Iteration 268, loss = 0.39594015\n",
      "Iteration 269, loss = 0.39520470\n",
      "Iteration 270, loss = 0.39447070\n",
      "Iteration 271, loss = 0.39373798\n",
      "Iteration 272, loss = 0.39300691\n",
      "Iteration 273, loss = 0.39227774\n",
      "Iteration 274, loss = 0.39155075\n",
      "Iteration 275, loss = 0.39082530\n",
      "Iteration 276, loss = 0.39010137\n",
      "Iteration 277, loss = 0.38937895\n",
      "Iteration 278, loss = 0.38865802\n",
      "Iteration 279, loss = 0.38793875\n",
      "Iteration 280, loss = 0.38722076\n",
      "Iteration 281, loss = 0.38650403\n",
      "Iteration 282, loss = 0.38578872\n",
      "Iteration 283, loss = 0.38507540\n",
      "Iteration 284, loss = 0.38436375\n",
      "Iteration 285, loss = 0.38365348\n",
      "Iteration 286, loss = 0.38294455\n",
      "Iteration 287, loss = 0.38223698\n",
      "Iteration 288, loss = 0.38153071\n",
      "Iteration 289, loss = 0.38082574\n",
      "Iteration 290, loss = 0.38012203\n",
      "Iteration 291, loss = 0.37941956\n",
      "Iteration 292, loss = 0.37871858\n",
      "Iteration 293, loss = 0.37801894\n",
      "Iteration 294, loss = 0.37732039\n",
      "Iteration 295, loss = 0.37662353\n",
      "Iteration 296, loss = 0.37592793\n",
      "Iteration 297, loss = 0.37523346\n",
      "Iteration 298, loss = 0.37454069\n",
      "Iteration 299, loss = 0.37384964\n",
      "Iteration 300, loss = 0.37316005\n",
      "Iteration 301, loss = 0.37247120\n",
      "Iteration 302, loss = 0.37178328\n",
      "Iteration 303, loss = 0.37109665\n",
      "Iteration 304, loss = 0.37041160\n",
      "Iteration 305, loss = 0.36972759\n",
      "Iteration 306, loss = 0.36904461\n",
      "Iteration 307, loss = 0.36836265\n",
      "Iteration 308, loss = 0.36768168\n",
      "Iteration 309, loss = 0.36700169\n",
      "Iteration 310, loss = 0.36632273\n",
      "Iteration 311, loss = 0.36564501\n",
      "Iteration 312, loss = 0.36496840\n",
      "Iteration 313, loss = 0.36429266\n",
      "Iteration 314, loss = 0.36361793\n",
      "Iteration 315, loss = 0.36294456\n",
      "Iteration 316, loss = 0.36227247\n",
      "Iteration 317, loss = 0.36160148\n",
      "Iteration 318, loss = 0.36093274\n",
      "Iteration 319, loss = 0.36026497\n",
      "Iteration 320, loss = 0.35959792\n",
      "Iteration 321, loss = 0.35893237\n",
      "Iteration 322, loss = 0.35826641\n",
      "Iteration 323, loss = 0.35760196\n",
      "Iteration 324, loss = 0.35693833\n",
      "Iteration 325, loss = 0.35627557\n",
      "Iteration 326, loss = 0.35561385\n",
      "Iteration 327, loss = 0.35495301\n",
      "Iteration 328, loss = 0.35429323\n",
      "Iteration 329, loss = 0.35363440\n",
      "Iteration 330, loss = 0.35297659\n",
      "Iteration 331, loss = 0.35231965\n",
      "Iteration 332, loss = 0.35166396\n",
      "Iteration 333, loss = 0.35100899\n",
      "Iteration 334, loss = 0.35035464\n",
      "Iteration 335, loss = 0.34970135\n",
      "Iteration 336, loss = 0.34904898\n",
      "Iteration 337, loss = 0.34839744\n",
      "Iteration 338, loss = 0.34774672\n",
      "Iteration 339, loss = 0.34709679\n",
      "Iteration 340, loss = 0.34644767\n",
      "Iteration 341, loss = 0.34579936\n",
      "Iteration 342, loss = 0.34515199\n",
      "Iteration 343, loss = 0.34450561\n",
      "Iteration 344, loss = 0.34386003\n",
      "Iteration 345, loss = 0.34321526\n",
      "Iteration 346, loss = 0.34257129\n",
      "Iteration 347, loss = 0.34192817\n",
      "Iteration 348, loss = 0.34128593\n",
      "Iteration 349, loss = 0.34064466\n",
      "Iteration 350, loss = 0.34000418\n",
      "Iteration 351, loss = 0.33936448\n",
      "Iteration 352, loss = 0.33872556\n",
      "Iteration 353, loss = 0.33808741\n",
      "Iteration 354, loss = 0.33745003\n",
      "Iteration 355, loss = 0.33681353\n",
      "Iteration 356, loss = 0.33617806\n",
      "Iteration 357, loss = 0.33554343\n",
      "Iteration 358, loss = 0.33490959\n",
      "Iteration 359, loss = 0.33427668\n",
      "Iteration 360, loss = 0.33364446\n",
      "Iteration 361, loss = 0.33301291\n",
      "Iteration 362, loss = 0.33238196\n",
      "Iteration 363, loss = 0.33175107\n",
      "Iteration 364, loss = 0.33111178\n",
      "Iteration 365, loss = 0.33043829\n",
      "Iteration 366, loss = 0.32971091\n",
      "Iteration 367, loss = 0.32901673\n",
      "Iteration 368, loss = 0.32838884\n",
      "Iteration 369, loss = 0.32773152\n",
      "Iteration 370, loss = 0.32704646\n",
      "Iteration 371, loss = 0.32633998\n",
      "Iteration 372, loss = 0.32562533\n",
      "Iteration 373, loss = 0.32493477\n",
      "Iteration 374, loss = 0.32425074\n",
      "Iteration 375, loss = 0.32352917\n",
      "Iteration 376, loss = 0.32282409\n",
      "Iteration 377, loss = 0.32211952\n",
      "Iteration 378, loss = 0.32140555\n",
      "Iteration 379, loss = 0.32067804\n",
      "Iteration 380, loss = 0.31994006\n",
      "Iteration 381, loss = 0.31919708\n",
      "Iteration 382, loss = 0.31845626\n",
      "Iteration 383, loss = 0.31771126\n",
      "Iteration 384, loss = 0.31694974\n",
      "Iteration 385, loss = 0.31618736\n",
      "Iteration 386, loss = 0.31542338\n",
      "Iteration 387, loss = 0.31465286\n",
      "Iteration 388, loss = 0.31387246\n",
      "Iteration 389, loss = 0.31308268\n",
      "Iteration 390, loss = 0.31228615\n",
      "Iteration 391, loss = 0.31148860\n",
      "Iteration 392, loss = 0.31068910\n",
      "Iteration 393, loss = 0.30987893\n",
      "Iteration 394, loss = 0.30906373\n",
      "Iteration 395, loss = 0.30824464\n",
      "Iteration 396, loss = 0.30742376\n",
      "Iteration 397, loss = 0.30659681\n",
      "Iteration 398, loss = 0.30576186\n",
      "Iteration 399, loss = 0.30492294\n",
      "Iteration 400, loss = 0.30407949\n",
      "Iteration 401, loss = 0.30323201\n",
      "Iteration 402, loss = 0.30238275\n",
      "Iteration 403, loss = 0.30152692\n",
      "Iteration 404, loss = 0.30066875\n",
      "Iteration 405, loss = 0.29980745\n",
      "Iteration 406, loss = 0.29894330\n",
      "Iteration 407, loss = 0.29807368\n",
      "Iteration 408, loss = 0.29720337\n",
      "Iteration 409, loss = 0.29632922\n",
      "Iteration 410, loss = 0.29545249\n",
      "Iteration 411, loss = 0.29457399\n",
      "Iteration 412, loss = 0.29369259\n",
      "Iteration 413, loss = 0.29280841\n",
      "Iteration 414, loss = 0.29191991\n",
      "Iteration 415, loss = 0.29102617\n",
      "Iteration 416, loss = 0.29014406\n",
      "Iteration 417, loss = 0.28924292\n",
      "Iteration 418, loss = 0.28835420\n",
      "Iteration 419, loss = 0.28746112\n",
      "Iteration 420, loss = 0.28656150\n",
      "Iteration 421, loss = 0.28565664\n",
      "Iteration 422, loss = 0.28475955\n",
      "Iteration 423, loss = 0.28385881\n",
      "Iteration 424, loss = 0.28295237\n",
      "Iteration 425, loss = 0.28204921\n",
      "Iteration 426, loss = 0.28114617\n",
      "Iteration 427, loss = 0.28023780\n",
      "Iteration 428, loss = 0.27933025\n",
      "Iteration 429, loss = 0.27842194\n",
      "Iteration 430, loss = 0.27751299\n",
      "Iteration 431, loss = 0.27660358\n",
      "Iteration 432, loss = 0.27569177\n",
      "Iteration 433, loss = 0.27477963\n",
      "Iteration 434, loss = 0.27386641\n",
      "Iteration 435, loss = 0.27295248\n",
      "Iteration 436, loss = 0.27203856\n",
      "Iteration 437, loss = 0.27112310\n",
      "Iteration 438, loss = 0.27020677\n",
      "Iteration 439, loss = 0.26928895\n",
      "Iteration 440, loss = 0.26837205\n",
      "Iteration 441, loss = 0.26745241\n",
      "Iteration 442, loss = 0.26653482\n",
      "Iteration 443, loss = 0.26561486\n",
      "Iteration 444, loss = 0.26469658\n",
      "Iteration 445, loss = 0.26377840\n",
      "Iteration 446, loss = 0.26286033\n",
      "Iteration 447, loss = 0.26194233\n",
      "Iteration 448, loss = 0.26102461\n",
      "Iteration 449, loss = 0.26010713\n",
      "Iteration 450, loss = 0.25918996\n",
      "Iteration 451, loss = 0.25827350\n",
      "Iteration 452, loss = 0.25735761\n",
      "Iteration 453, loss = 0.25644238\n",
      "Iteration 454, loss = 0.25552792\n",
      "Iteration 455, loss = 0.25461433\n",
      "Iteration 456, loss = 0.25370194\n",
      "Iteration 457, loss = 0.25279082\n",
      "Iteration 458, loss = 0.25188097\n",
      "Iteration 459, loss = 0.25097332\n",
      "Iteration 460, loss = 0.25006697\n",
      "Iteration 461, loss = 0.24916299\n",
      "Iteration 462, loss = 0.24826077\n",
      "Iteration 463, loss = 0.24736015\n",
      "Iteration 464, loss = 0.24646121\n",
      "Iteration 465, loss = 0.24556411\n",
      "Iteration 466, loss = 0.24467033\n",
      "Iteration 467, loss = 0.24378148\n",
      "Iteration 468, loss = 0.24289126\n",
      "Iteration 469, loss = 0.24200707\n",
      "Iteration 470, loss = 0.24112594\n",
      "Iteration 471, loss = 0.24024612\n",
      "Iteration 472, loss = 0.23936694\n",
      "Iteration 473, loss = 0.23848834\n",
      "Iteration 474, loss = 0.23761530\n",
      "Iteration 475, loss = 0.23675176\n",
      "Iteration 476, loss = 0.23588049\n",
      "Iteration 477, loss = 0.23501818\n",
      "Iteration 478, loss = 0.23415767\n",
      "Iteration 479, loss = 0.23329947\n",
      "Iteration 480, loss = 0.23244546\n",
      "Iteration 481, loss = 0.23159558\n",
      "Iteration 482, loss = 0.23074664\n",
      "Iteration 483, loss = 0.22990371\n",
      "Iteration 484, loss = 0.22906406\n",
      "Iteration 485, loss = 0.22822649\n",
      "Iteration 486, loss = 0.22739115\n",
      "Iteration 487, loss = 0.22656175\n",
      "Iteration 488, loss = 0.22573417\n",
      "Iteration 489, loss = 0.22491072\n",
      "Iteration 490, loss = 0.22409109\n",
      "Iteration 491, loss = 0.22327388\n",
      "Iteration 492, loss = 0.22246190\n",
      "Iteration 493, loss = 0.22165344\n",
      "Iteration 494, loss = 0.22084751\n",
      "Iteration 495, loss = 0.22004535\n",
      "Iteration 496, loss = 0.21924730\n",
      "Iteration 497, loss = 0.21845340\n",
      "Iteration 498, loss = 0.21766185\n",
      "Iteration 499, loss = 0.21687389\n",
      "Iteration 500, loss = 0.21609070\n",
      "Iteration 501, loss = 0.21531129\n",
      "Iteration 502, loss = 0.21453344\n",
      "Iteration 503, loss = 0.21376133\n",
      "Iteration 504, loss = 0.21299348\n",
      "Iteration 505, loss = 0.21222783\n",
      "Iteration 506, loss = 0.21146577\n",
      "Iteration 507, loss = 0.21070764\n",
      "Iteration 508, loss = 0.20995350\n",
      "Iteration 509, loss = 0.20920158\n",
      "Iteration 510, loss = 0.20845545\n",
      "Iteration 511, loss = 0.20771250\n",
      "Iteration 512, loss = 0.20697249\n",
      "Iteration 513, loss = 0.20623660\n",
      "Iteration 514, loss = 0.20550659\n",
      "Iteration 515, loss = 0.20478017\n",
      "Iteration 516, loss = 0.20405749\n",
      "Iteration 517, loss = 0.20333844\n",
      "Iteration 518, loss = 0.20262293\n",
      "Iteration 519, loss = 0.20191100\n",
      "Iteration 520, loss = 0.20120335\n",
      "Iteration 521, loss = 0.20049906\n",
      "Iteration 522, loss = 0.19979834\n",
      "Iteration 523, loss = 0.19910173\n",
      "Iteration 524, loss = 0.19840869\n",
      "Iteration 525, loss = 0.19771923\n",
      "Iteration 526, loss = 0.19703337\n",
      "Iteration 527, loss = 0.19635111\n",
      "Iteration 528, loss = 0.19567247\n",
      "Iteration 529, loss = 0.19499748\n",
      "Iteration 530, loss = 0.19432657\n",
      "Iteration 531, loss = 0.19365878\n",
      "Iteration 532, loss = 0.19299507\n",
      "Iteration 533, loss = 0.19233502\n",
      "Iteration 534, loss = 0.19167853\n",
      "Iteration 535, loss = 0.19102554\n",
      "Iteration 536, loss = 0.19037605\n",
      "Iteration 537, loss = 0.18973015\n",
      "Iteration 538, loss = 0.18908779\n",
      "Iteration 539, loss = 0.18844893\n",
      "Iteration 540, loss = 0.18781362\n",
      "Iteration 541, loss = 0.18718177\n",
      "Iteration 542, loss = 0.18655360\n",
      "Iteration 543, loss = 0.18592848\n",
      "Iteration 544, loss = 0.18530700\n",
      "Iteration 545, loss = 0.18468892\n",
      "Iteration 546, loss = 0.18407421\n",
      "Iteration 547, loss = 0.18346286\n",
      "Iteration 548, loss = 0.18285485\n",
      "Iteration 549, loss = 0.18225017\n",
      "Iteration 550, loss = 0.18164880\n",
      "Iteration 551, loss = 0.18105072\n",
      "Iteration 552, loss = 0.18045610\n",
      "Iteration 553, loss = 0.17986460\n",
      "Iteration 554, loss = 0.17927657\n",
      "Iteration 555, loss = 0.17869166\n",
      "Iteration 556, loss = 0.17810995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 557, loss = 0.17753146\n",
      "Iteration 558, loss = 0.17695660\n",
      "Iteration 559, loss = 0.17638460\n",
      "Iteration 560, loss = 0.17581534\n",
      "Iteration 561, loss = 0.17525039\n",
      "Iteration 562, loss = 0.17468780\n",
      "Iteration 563, loss = 0.17412814\n",
      "Iteration 564, loss = 0.17357136\n",
      "Iteration 565, loss = 0.17301884\n",
      "Iteration 566, loss = 0.17246897\n",
      "Iteration 567, loss = 0.17192146\n",
      "Iteration 568, loss = 0.17137677\n",
      "Iteration 569, loss = 0.17083604\n",
      "Iteration 570, loss = 0.17029799\n",
      "Iteration 571, loss = 0.16976273\n",
      "Iteration 572, loss = 0.16923075\n",
      "Iteration 573, loss = 0.16870172\n",
      "Iteration 574, loss = 0.16817552\n",
      "Iteration 575, loss = 0.16765264\n",
      "Iteration 576, loss = 0.16713238\n",
      "Iteration 577, loss = 0.16661475\n",
      "Iteration 578, loss = 0.16610067\n",
      "Iteration 579, loss = 0.16558886\n",
      "Iteration 580, loss = 0.16508011\n",
      "Iteration 581, loss = 0.16457431\n",
      "Iteration 582, loss = 0.16407166\n",
      "Iteration 583, loss = 0.16357200\n",
      "Iteration 584, loss = 0.16307495\n",
      "Iteration 585, loss = 0.16258088\n",
      "Iteration 586, loss = 0.16208912\n",
      "Iteration 587, loss = 0.16160026\n",
      "Iteration 588, loss = 0.16111405\n",
      "Iteration 589, loss = 0.16063079\n",
      "Iteration 590, loss = 0.16014970\n",
      "Iteration 591, loss = 0.15967157\n",
      "Iteration 592, loss = 0.15919600\n",
      "Iteration 593, loss = 0.15872384\n",
      "Iteration 594, loss = 0.15825329\n",
      "Iteration 595, loss = 0.15778570\n",
      "Iteration 596, loss = 0.15732115\n",
      "Iteration 597, loss = 0.15685919\n",
      "Iteration 598, loss = 0.15639981\n",
      "Iteration 599, loss = 0.15594301\n",
      "Iteration 600, loss = 0.15548863\n",
      "Iteration 601, loss = 0.15503659\n",
      "Iteration 602, loss = 0.15458686\n",
      "Iteration 603, loss = 0.15413949\n",
      "Iteration 604, loss = 0.15369495\n",
      "Iteration 605, loss = 0.15325322\n",
      "Iteration 606, loss = 0.15281407\n",
      "Iteration 607, loss = 0.15237722\n",
      "Iteration 608, loss = 0.15194260\n",
      "Iteration 609, loss = 0.15151021\n",
      "Iteration 610, loss = 0.15108023\n",
      "Iteration 611, loss = 0.15065266\n",
      "Iteration 612, loss = 0.15022781\n",
      "Iteration 613, loss = 0.14980503\n",
      "Iteration 614, loss = 0.14938472\n",
      "Iteration 615, loss = 0.14896649\n",
      "Iteration 616, loss = 0.14855054\n",
      "Iteration 617, loss = 0.14813696\n",
      "Iteration 618, loss = 0.14772569\n",
      "Iteration 619, loss = 0.14731695\n",
      "Iteration 620, loss = 0.14690977\n",
      "Iteration 621, loss = 0.14650532\n",
      "Iteration 622, loss = 0.14610290\n",
      "Iteration 623, loss = 0.14570257\n",
      "Iteration 624, loss = 0.14530411\n",
      "Iteration 625, loss = 0.14490791\n",
      "Iteration 626, loss = 0.14451457\n",
      "Iteration 627, loss = 0.14412318\n",
      "Iteration 628, loss = 0.14373232\n",
      "Iteration 629, loss = 0.14334536\n",
      "Iteration 630, loss = 0.14296015\n",
      "Iteration 631, loss = 0.14257658\n",
      "Iteration 632, loss = 0.14219527\n",
      "Iteration 633, loss = 0.14181616\n",
      "Iteration 634, loss = 0.14143879\n",
      "Iteration 635, loss = 0.14106317\n",
      "Iteration 636, loss = 0.14069071\n",
      "Iteration 637, loss = 0.14031963\n",
      "Iteration 638, loss = 0.13995032\n",
      "Iteration 639, loss = 0.13958269\n",
      "Iteration 640, loss = 0.13921829\n",
      "Iteration 641, loss = 0.13885445\n",
      "Iteration 642, loss = 0.13849330\n",
      "Iteration 643, loss = 0.13813424\n",
      "Iteration 644, loss = 0.13777639\n",
      "Iteration 645, loss = 0.13742016\n",
      "Iteration 646, loss = 0.13706751\n",
      "Iteration 647, loss = 0.13671448\n",
      "Iteration 648, loss = 0.13636398\n",
      "Iteration 649, loss = 0.13601579\n",
      "Iteration 650, loss = 0.13566935\n",
      "Iteration 651, loss = 0.13532404\n",
      "Iteration 652, loss = 0.13498244\n",
      "Iteration 653, loss = 0.13464061\n",
      "Iteration 654, loss = 0.13430124\n",
      "Iteration 655, loss = 0.13396446\n",
      "Iteration 656, loss = 0.13362882\n",
      "Iteration 657, loss = 0.13329419\n",
      "Iteration 658, loss = 0.13296095\n",
      "Iteration 659, loss = 0.13262940\n",
      "Iteration 660, loss = 0.13230052\n",
      "Iteration 661, loss = 0.13197246\n",
      "Iteration 662, loss = 0.13164396\n",
      "Iteration 663, loss = 0.13131825\n",
      "Iteration 664, loss = 0.13099369\n",
      "Iteration 665, loss = 0.13067026\n",
      "Iteration 666, loss = 0.13034872\n",
      "Iteration 667, loss = 0.13002842\n",
      "Iteration 668, loss = 0.12970973\n",
      "Iteration 669, loss = 0.12939325\n",
      "Iteration 670, loss = 0.12907674\n",
      "Iteration 671, loss = 0.12876253\n",
      "Iteration 672, loss = 0.12844958\n",
      "Iteration 673, loss = 0.12813832\n",
      "Iteration 674, loss = 0.12782879\n",
      "Iteration 675, loss = 0.12752053\n",
      "Iteration 676, loss = 0.12721412\n",
      "Iteration 677, loss = 0.12690871\n",
      "Iteration 678, loss = 0.12660514\n",
      "Iteration 679, loss = 0.12630271\n",
      "Iteration 680, loss = 0.12600201\n",
      "Iteration 681, loss = 0.12570318\n",
      "Iteration 682, loss = 0.12540529\n",
      "Iteration 683, loss = 0.12511020\n",
      "Iteration 684, loss = 0.12481591\n",
      "Iteration 685, loss = 0.12452438\n",
      "Iteration 686, loss = 0.12423630\n",
      "Iteration 687, loss = 0.12395005\n",
      "Iteration 688, loss = 0.12366545\n",
      "Iteration 689, loss = 0.12338239\n",
      "Iteration 690, loss = 0.12310097\n",
      "Iteration 691, loss = 0.12282111\n",
      "Iteration 692, loss = 0.12254271\n",
      "Iteration 693, loss = 0.12226589\n",
      "Iteration 694, loss = 0.12199041\n",
      "Iteration 695, loss = 0.12171672\n",
      "Iteration 696, loss = 0.12144398\n",
      "Iteration 697, loss = 0.12117291\n",
      "Iteration 698, loss = 0.12090321\n",
      "Iteration 699, loss = 0.12063495\n",
      "Iteration 700, loss = 0.12036793\n",
      "Iteration 701, loss = 0.12010271\n",
      "Iteration 702, loss = 0.11983853\n",
      "Iteration 703, loss = 0.11957525\n",
      "Iteration 704, loss = 0.11931354\n",
      "Iteration 705, loss = 0.11905324\n",
      "Iteration 706, loss = 0.11879470\n",
      "Iteration 707, loss = 0.11853740\n",
      "Iteration 708, loss = 0.11828154\n",
      "Iteration 709, loss = 0.11802723\n",
      "Iteration 710, loss = 0.11777399\n",
      "Iteration 711, loss = 0.11752176\n",
      "Iteration 712, loss = 0.11727106\n",
      "Iteration 713, loss = 0.11702157\n",
      "Iteration 714, loss = 0.11677331\n",
      "Iteration 715, loss = 0.11652618\n",
      "Iteration 716, loss = 0.11628034\n",
      "Iteration 717, loss = 0.11603577\n",
      "Iteration 718, loss = 0.11579228\n",
      "Iteration 719, loss = 0.11554972\n",
      "Iteration 720, loss = 0.11530830\n",
      "Iteration 721, loss = 0.11506867\n",
      "Iteration 722, loss = 0.11482980\n",
      "Iteration 723, loss = 0.11459167\n",
      "Iteration 724, loss = 0.11435502\n",
      "Iteration 725, loss = 0.11411942\n",
      "Iteration 726, loss = 0.11388484\n",
      "Iteration 727, loss = 0.11365164\n",
      "Iteration 728, loss = 0.11341912\n",
      "Iteration 729, loss = 0.11318766\n",
      "Iteration 730, loss = 0.11295752\n",
      "Iteration 731, loss = 0.11272796\n",
      "Iteration 732, loss = 0.11249944\n",
      "Iteration 733, loss = 0.11227185\n",
      "Iteration 734, loss = 0.11204492\n",
      "Iteration 735, loss = 0.11181949\n",
      "Iteration 736, loss = 0.11159552\n",
      "Iteration 737, loss = 0.11137151\n",
      "Iteration 738, loss = 0.11114898\n",
      "Iteration 739, loss = 0.11092809\n",
      "Iteration 740, loss = 0.11070765\n",
      "Iteration 741, loss = 0.11048770\n",
      "Iteration 742, loss = 0.11026963\n",
      "Iteration 743, loss = 0.11005320\n",
      "Iteration 744, loss = 0.10983687\n",
      "Iteration 745, loss = 0.10962157\n",
      "Iteration 746, loss = 0.10940822\n",
      "Iteration 747, loss = 0.10919583\n",
      "Iteration 748, loss = 0.10898390\n",
      "Iteration 749, loss = 0.10877298\n",
      "Iteration 750, loss = 0.10856355\n",
      "Iteration 751, loss = 0.10835572\n",
      "Iteration 752, loss = 0.10814765\n",
      "Iteration 753, loss = 0.10794006\n",
      "Iteration 754, loss = 0.10773473\n",
      "Iteration 755, loss = 0.10753091\n",
      "Iteration 756, loss = 0.10732765\n",
      "Iteration 757, loss = 0.10712473\n",
      "Iteration 758, loss = 0.10692257\n",
      "Iteration 759, loss = 0.10672173\n",
      "Iteration 760, loss = 0.10652201\n",
      "Iteration 761, loss = 0.10632279\n",
      "Iteration 762, loss = 0.10612444\n",
      "Iteration 763, loss = 0.10592685\n",
      "Iteration 764, loss = 0.10573087\n",
      "Iteration 765, loss = 0.10553549\n",
      "Iteration 766, loss = 0.10534081\n",
      "Iteration 767, loss = 0.10514706\n",
      "Iteration 768, loss = 0.10495442\n",
      "Iteration 769, loss = 0.10476229\n",
      "Iteration 770, loss = 0.10457117\n",
      "Iteration 771, loss = 0.10438091\n",
      "Iteration 772, loss = 0.10419149\n",
      "Iteration 773, loss = 0.10400288\n",
      "Iteration 774, loss = 0.10381530\n",
      "Iteration 775, loss = 0.10362841\n",
      "Iteration 776, loss = 0.10344247\n",
      "Iteration 777, loss = 0.10325727\n",
      "Iteration 778, loss = 0.10307277\n",
      "Iteration 779, loss = 0.10288902\n",
      "Iteration 780, loss = 0.10270621\n",
      "Iteration 781, loss = 0.10252410\n",
      "Iteration 782, loss = 0.10234281\n",
      "Iteration 783, loss = 0.10216225\n",
      "Iteration 784, loss = 0.10198261\n",
      "Iteration 785, loss = 0.10180359\n",
      "Iteration 786, loss = 0.10162545\n",
      "Iteration 787, loss = 0.10144802\n",
      "Iteration 788, loss = 0.10127124\n",
      "Iteration 789, loss = 0.10109531\n",
      "Iteration 790, loss = 0.10092021\n",
      "Iteration 791, loss = 0.10074557\n",
      "Iteration 792, loss = 0.10057160\n",
      "Iteration 793, loss = 0.10039890\n",
      "Iteration 794, loss = 0.10022667\n",
      "Iteration 795, loss = 0.10005481\n",
      "Iteration 796, loss = 0.09988392\n",
      "Iteration 797, loss = 0.09971397\n",
      "Iteration 798, loss = 0.09954449\n",
      "Iteration 799, loss = 0.09937564\n",
      "Iteration 800, loss = 0.09920765\n",
      "Iteration 801, loss = 0.09904054\n",
      "Iteration 802, loss = 0.09887383\n",
      "Iteration 803, loss = 0.09870781\n",
      "Iteration 804, loss = 0.09854278\n",
      "Iteration 805, loss = 0.09837811\n",
      "Iteration 806, loss = 0.09821411\n",
      "Iteration 807, loss = 0.09805111\n",
      "Iteration 808, loss = 0.09788868\n",
      "Iteration 809, loss = 0.09772680\n",
      "Iteration 810, loss = 0.09756563\n",
      "Iteration 811, loss = 0.09740523\n",
      "Iteration 812, loss = 0.09724545\n",
      "Iteration 813, loss = 0.09708628\n",
      "Iteration 814, loss = 0.09692780\n",
      "Iteration 815, loss = 0.09677000\n",
      "Iteration 816, loss = 0.09661269\n",
      "Iteration 817, loss = 0.09645617\n",
      "Iteration 818, loss = 0.09630019\n",
      "Iteration 819, loss = 0.09614490\n",
      "Iteration 820, loss = 0.09599046\n",
      "Iteration 821, loss = 0.09583648\n",
      "Iteration 822, loss = 0.09568316\n",
      "Iteration 823, loss = 0.09553062\n",
      "Iteration 824, loss = 0.09537857\n",
      "Iteration 825, loss = 0.09522717\n",
      "Iteration 826, loss = 0.09507649\n",
      "Iteration 827, loss = 0.09492603\n",
      "Iteration 828, loss = 0.09477626\n",
      "Iteration 829, loss = 0.09462764\n",
      "Iteration 830, loss = 0.09447943\n",
      "Iteration 831, loss = 0.09433174\n",
      "Iteration 832, loss = 0.09418441\n",
      "Iteration 833, loss = 0.09403759\n",
      "Iteration 834, loss = 0.09389171\n",
      "Iteration 835, loss = 0.09374634\n",
      "Iteration 836, loss = 0.09360173\n",
      "Iteration 837, loss = 0.09345752\n",
      "Iteration 838, loss = 0.09331369\n",
      "Iteration 839, loss = 0.09317060\n",
      "Iteration 840, loss = 0.09302826\n",
      "Iteration 841, loss = 0.09288631\n",
      "Iteration 842, loss = 0.09274499\n",
      "Iteration 843, loss = 0.09260425\n",
      "Iteration 844, loss = 0.09246396\n",
      "Iteration 845, loss = 0.09232420\n",
      "Iteration 846, loss = 0.09218504\n",
      "Iteration 847, loss = 0.09204653\n",
      "Iteration 848, loss = 0.09190847\n",
      "Iteration 849, loss = 0.09177114\n",
      "Iteration 850, loss = 0.09163425\n",
      "Iteration 851, loss = 0.09149780\n",
      "Iteration 852, loss = 0.09136187\n",
      "Iteration 853, loss = 0.09122657\n",
      "Iteration 854, loss = 0.09109183\n",
      "Iteration 855, loss = 0.09095758\n",
      "Iteration 856, loss = 0.09082375\n",
      "Iteration 857, loss = 0.09069064\n",
      "Iteration 858, loss = 0.09055800\n",
      "Iteration 859, loss = 0.09042582\n",
      "Iteration 860, loss = 0.09029401\n",
      "Iteration 861, loss = 0.09016283\n",
      "Iteration 862, loss = 0.09003225\n",
      "Iteration 863, loss = 0.08990215\n",
      "Iteration 864, loss = 0.08977240\n",
      "Iteration 865, loss = 0.08964320\n",
      "Iteration 866, loss = 0.08951459\n",
      "Iteration 867, loss = 0.08938635\n",
      "Iteration 868, loss = 0.08925873\n",
      "Iteration 869, loss = 0.08913158\n",
      "Iteration 870, loss = 0.08900491\n",
      "Iteration 871, loss = 0.08887866\n",
      "Iteration 872, loss = 0.08875302\n",
      "Iteration 873, loss = 0.08862777\n",
      "Iteration 874, loss = 0.08850301\n",
      "Iteration 875, loss = 0.08837871\n",
      "Iteration 876, loss = 0.08825498\n",
      "Iteration 877, loss = 0.08813165\n",
      "Iteration 878, loss = 0.08800873\n",
      "Iteration 879, loss = 0.08788627\n",
      "Iteration 880, loss = 0.08776420\n",
      "Iteration 881, loss = 0.08764288\n",
      "Iteration 882, loss = 0.08752192\n",
      "Iteration 883, loss = 0.08740127\n",
      "Iteration 884, loss = 0.08728094\n",
      "Iteration 885, loss = 0.08716139\n",
      "Iteration 886, loss = 0.08704223\n",
      "Iteration 887, loss = 0.08692346\n",
      "Iteration 888, loss = 0.08680529\n",
      "Iteration 889, loss = 0.08668726\n",
      "Iteration 890, loss = 0.08656980\n",
      "Iteration 891, loss = 0.08645278\n",
      "Iteration 892, loss = 0.08633627\n",
      "Iteration 893, loss = 0.08622019\n",
      "Iteration 894, loss = 0.08610465\n",
      "Iteration 895, loss = 0.08598944\n",
      "Iteration 896, loss = 0.08587471\n",
      "Iteration 897, loss = 0.08576046\n",
      "Iteration 898, loss = 0.08564656\n",
      "Iteration 899, loss = 0.08553304\n",
      "Iteration 900, loss = 0.08541993\n",
      "Iteration 901, loss = 0.08530725\n",
      "Iteration 902, loss = 0.08519518\n",
      "Iteration 903, loss = 0.08508338\n",
      "Iteration 904, loss = 0.08497184\n",
      "Iteration 905, loss = 0.08486091\n",
      "Iteration 906, loss = 0.08475057\n",
      "Iteration 907, loss = 0.08464037\n",
      "Iteration 908, loss = 0.08453069\n",
      "Iteration 909, loss = 0.08442139\n",
      "Iteration 910, loss = 0.08431246\n",
      "Iteration 911, loss = 0.08420391\n",
      "Iteration 912, loss = 0.08409576\n",
      "Iteration 913, loss = 0.08398801\n",
      "Iteration 914, loss = 0.08388073\n",
      "Iteration 915, loss = 0.08377386\n",
      "Iteration 916, loss = 0.08366752\n",
      "Iteration 917, loss = 0.08356149\n",
      "Iteration 918, loss = 0.08345591\n",
      "Iteration 919, loss = 0.08335069\n",
      "Iteration 920, loss = 0.08324581\n",
      "Iteration 921, loss = 0.08314130\n",
      "Iteration 922, loss = 0.08303714\n",
      "Iteration 923, loss = 0.08293334\n",
      "Iteration 924, loss = 0.08283001\n",
      "Iteration 925, loss = 0.08272691\n",
      "Iteration 926, loss = 0.08262422\n",
      "Iteration 927, loss = 0.08252193\n",
      "Iteration 928, loss = 0.08241999\n",
      "Iteration 929, loss = 0.08231841\n",
      "Iteration 930, loss = 0.08221722\n",
      "Iteration 931, loss = 0.08211639\n",
      "Iteration 932, loss = 0.08201591\n",
      "Iteration 933, loss = 0.08191579\n",
      "Iteration 934, loss = 0.08181602\n",
      "Iteration 935, loss = 0.08171661\n",
      "Iteration 936, loss = 0.08161755\n",
      "Iteration 937, loss = 0.08151882\n",
      "Iteration 938, loss = 0.08142050\n",
      "Iteration 939, loss = 0.08132243\n",
      "Iteration 940, loss = 0.08122476\n",
      "Iteration 941, loss = 0.08112743\n",
      "Iteration 942, loss = 0.08103043\n",
      "Iteration 943, loss = 0.08093377\n",
      "Iteration 944, loss = 0.08083745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.33028224\n",
      "Iteration 3, loss = 1.30337479\n",
      "Iteration 4, loss = 1.27698898\n",
      "Iteration 5, loss = 1.25110636\n",
      "Iteration 6, loss = 1.22581117\n",
      "Iteration 7, loss = 1.20113194\n",
      "Iteration 8, loss = 1.17706852\n",
      "Iteration 9, loss = 1.15360906\n",
      "Iteration 10, loss = 1.13074113\n",
      "Iteration 11, loss = 1.10852997\n",
      "Iteration 12, loss = 1.08698852\n",
      "Iteration 13, loss = 1.06616725\n",
      "Iteration 14, loss = 1.04604292\n",
      "Iteration 15, loss = 1.02663495\n",
      "Iteration 16, loss = 1.00794286\n",
      "Iteration 17, loss = 0.98999628\n",
      "Iteration 18, loss = 0.97275542\n",
      "Iteration 19, loss = 0.95624511\n",
      "Iteration 20, loss = 0.94044768\n",
      "Iteration 21, loss = 0.92533910\n",
      "Iteration 22, loss = 0.91093493\n",
      "Iteration 23, loss = 0.89719725\n",
      "Iteration 24, loss = 0.88413857\n",
      "Iteration 25, loss = 0.87175274\n",
      "Iteration 26, loss = 0.86004347\n",
      "Iteration 27, loss = 0.84897663\n",
      "Iteration 28, loss = 0.83850959\n",
      "Iteration 29, loss = 0.82864611\n",
      "Iteration 30, loss = 0.81933114\n",
      "Iteration 31, loss = 0.81058818\n",
      "Iteration 32, loss = 0.80235101\n",
      "Iteration 33, loss = 0.79461758\n",
      "Iteration 34, loss = 0.78743027\n",
      "Iteration 35, loss = 0.78076523\n",
      "Iteration 36, loss = 0.77454702\n",
      "Iteration 37, loss = 0.76874542\n",
      "Iteration 38, loss = 0.76335694\n",
      "Iteration 39, loss = 0.75830375\n",
      "Iteration 40, loss = 0.75362497\n",
      "Iteration 41, loss = 0.74929341\n",
      "Iteration 42, loss = 0.74526189\n",
      "Iteration 43, loss = 0.74155073\n",
      "Iteration 44, loss = 0.73806066\n",
      "Iteration 45, loss = 0.73489587\n",
      "Iteration 46, loss = 0.73186256\n",
      "Iteration 47, loss = 0.72894940\n",
      "Iteration 48, loss = 0.72613795\n",
      "Iteration 49, loss = 0.72339763\n",
      "Iteration 50, loss = 0.72071402\n",
      "Iteration 51, loss = 0.71807477\n",
      "Iteration 52, loss = 0.71549009\n",
      "Iteration 53, loss = 0.71293489\n",
      "Iteration 54, loss = 0.71040321\n",
      "Iteration 55, loss = 0.70787802\n",
      "Iteration 56, loss = 0.70535458\n",
      "Iteration 57, loss = 0.70283787\n",
      "Iteration 58, loss = 0.70031197\n",
      "Iteration 59, loss = 0.69776891\n",
      "Iteration 60, loss = 0.69518902\n",
      "Iteration 61, loss = 0.69259964\n",
      "Iteration 62, loss = 0.68998139\n",
      "Iteration 63, loss = 0.68734105\n",
      "Iteration 64, loss = 0.68466150\n",
      "Iteration 65, loss = 0.68193829\n",
      "Iteration 66, loss = 0.67911636\n",
      "Iteration 67, loss = 0.67623728\n",
      "Iteration 68, loss = 0.67331647\n",
      "Iteration 69, loss = 0.67039899\n",
      "Iteration 70, loss = 0.66747044\n",
      "Iteration 71, loss = 0.66453946\n",
      "Iteration 72, loss = 0.66162562\n",
      "Iteration 73, loss = 0.65874041\n",
      "Iteration 74, loss = 0.65595070\n",
      "Iteration 75, loss = 0.65320184\n",
      "Iteration 76, loss = 0.65050474\n",
      "Iteration 77, loss = 0.64783743\n",
      "Iteration 78, loss = 0.64521785\n",
      "Iteration 79, loss = 0.64260691\n",
      "Iteration 80, loss = 0.64002817\n",
      "Iteration 81, loss = 0.63748053\n",
      "Iteration 82, loss = 0.63493915\n",
      "Iteration 83, loss = 0.63237701\n",
      "Iteration 84, loss = 0.62979845\n",
      "Iteration 85, loss = 0.62724421\n",
      "Iteration 86, loss = 0.62471739\n",
      "Iteration 87, loss = 0.62218179\n",
      "Iteration 88, loss = 0.61964843\n",
      "Iteration 89, loss = 0.61712904\n",
      "Iteration 90, loss = 0.61461193\n",
      "Iteration 91, loss = 0.61209971\n",
      "Iteration 92, loss = 0.60959660\n",
      "Iteration 93, loss = 0.60710410\n",
      "Iteration 94, loss = 0.60462157\n",
      "Iteration 95, loss = 0.60215114\n",
      "Iteration 96, loss = 0.59969513\n",
      "Iteration 97, loss = 0.59725413\n",
      "Iteration 98, loss = 0.59482935\n",
      "Iteration 99, loss = 0.59242196\n",
      "Iteration 100, loss = 0.59003205\n",
      "Iteration 101, loss = 0.58766025\n",
      "Iteration 102, loss = 0.58530820\n",
      "Iteration 103, loss = 0.58297676\n",
      "Iteration 104, loss = 0.58066396\n",
      "Iteration 105, loss = 0.57836951\n",
      "Iteration 106, loss = 0.57609345\n",
      "Iteration 107, loss = 0.57383646\n",
      "Iteration 108, loss = 0.57159824\n",
      "Iteration 109, loss = 0.56937818\n",
      "Iteration 110, loss = 0.56717618\n",
      "Iteration 111, loss = 0.56499229\n",
      "Iteration 112, loss = 0.56282645\n",
      "Iteration 113, loss = 0.56067838\n",
      "Iteration 114, loss = 0.55854800\n",
      "Iteration 115, loss = 0.55646269\n",
      "Iteration 116, loss = 0.55449193\n",
      "Iteration 117, loss = 0.55257894\n",
      "Iteration 118, loss = 0.55070517\n",
      "Iteration 119, loss = 0.54891757\n",
      "Iteration 120, loss = 0.54729320\n",
      "Iteration 121, loss = 0.54574691\n",
      "Iteration 122, loss = 0.54422999\n",
      "Iteration 123, loss = 0.54271404\n",
      "Iteration 124, loss = 0.54119064\n",
      "Iteration 125, loss = 0.53964715\n",
      "Iteration 126, loss = 0.53808703\n",
      "Iteration 127, loss = 0.53650791\n",
      "Iteration 128, loss = 0.53492054\n",
      "Iteration 129, loss = 0.53333860\n",
      "Iteration 130, loss = 0.53176759\n",
      "Iteration 131, loss = 0.53021363\n",
      "Iteration 132, loss = 0.52869051\n",
      "Iteration 133, loss = 0.52718971\n",
      "Iteration 134, loss = 0.52570396\n",
      "Iteration 135, loss = 0.52424314\n",
      "Iteration 136, loss = 0.52281415\n",
      "Iteration 137, loss = 0.52142704\n",
      "Iteration 138, loss = 0.52006283\n",
      "Iteration 139, loss = 0.51871013\n",
      "Iteration 140, loss = 0.51737431\n",
      "Iteration 141, loss = 0.51604632\n",
      "Iteration 142, loss = 0.51472140\n",
      "Iteration 143, loss = 0.51339978\n",
      "Iteration 144, loss = 0.51208424\n",
      "Iteration 145, loss = 0.51077411\n",
      "Iteration 146, loss = 0.50946863\n",
      "Iteration 147, loss = 0.50818213\n",
      "Iteration 148, loss = 0.50690731\n",
      "Iteration 149, loss = 0.50564853\n",
      "Iteration 150, loss = 0.50440715\n",
      "Iteration 151, loss = 0.50317867\n",
      "Iteration 152, loss = 0.50196157\n",
      "Iteration 153, loss = 0.50075601\n",
      "Iteration 154, loss = 0.49955898\n",
      "Iteration 155, loss = 0.49836992\n",
      "Iteration 156, loss = 0.49718705\n",
      "Iteration 157, loss = 0.49601056\n",
      "Iteration 158, loss = 0.49484295\n",
      "Iteration 159, loss = 0.49368356\n",
      "Iteration 160, loss = 0.49253229\n",
      "Iteration 161, loss = 0.49138904\n",
      "Iteration 162, loss = 0.49025432\n",
      "Iteration 163, loss = 0.48912785\n",
      "Iteration 164, loss = 0.48801221\n",
      "Iteration 165, loss = 0.48690831\n",
      "Iteration 166, loss = 0.48581382\n",
      "Iteration 167, loss = 0.48472784\n",
      "Iteration 168, loss = 0.48364870\n",
      "Iteration 169, loss = 0.48257566\n",
      "Iteration 170, loss = 0.48150878\n",
      "Iteration 171, loss = 0.48044855\n",
      "Iteration 172, loss = 0.47939545\n",
      "Iteration 173, loss = 0.47835006\n",
      "Iteration 174, loss = 0.47731370\n",
      "Iteration 175, loss = 0.47628489\n",
      "Iteration 176, loss = 0.47526398\n",
      "Iteration 177, loss = 0.47425155\n",
      "Iteration 178, loss = 0.47324601\n",
      "Iteration 179, loss = 0.47224712\n",
      "Iteration 180, loss = 0.47125526\n",
      "Iteration 181, loss = 0.47027054\n",
      "Iteration 182, loss = 0.46929160\n",
      "Iteration 183, loss = 0.46831839\n",
      "Iteration 184, loss = 0.46735085\n",
      "Iteration 185, loss = 0.46638900\n",
      "Iteration 186, loss = 0.46543446\n",
      "Iteration 187, loss = 0.46448571\n",
      "Iteration 188, loss = 0.46354290\n",
      "Iteration 189, loss = 0.46260645\n",
      "Iteration 190, loss = 0.46167595\n",
      "Iteration 191, loss = 0.46075090\n",
      "Iteration 192, loss = 0.45983122\n",
      "Iteration 193, loss = 0.45891679\n",
      "Iteration 194, loss = 0.45800754\n",
      "Iteration 195, loss = 0.45710338\n",
      "Iteration 196, loss = 0.45620421\n",
      "Iteration 197, loss = 0.45530993\n",
      "Iteration 198, loss = 0.45442047\n",
      "Iteration 199, loss = 0.45353574\n",
      "Iteration 200, loss = 0.45265639\n",
      "Iteration 201, loss = 0.45178131\n",
      "Iteration 202, loss = 0.45091040\n",
      "Iteration 203, loss = 0.45004365\n",
      "Iteration 204, loss = 0.44918164\n",
      "Iteration 205, loss = 0.44832446\n",
      "Iteration 206, loss = 0.44747196\n",
      "Iteration 207, loss = 0.44662359\n",
      "Iteration 208, loss = 0.44577928\n",
      "Iteration 209, loss = 0.44493895\n",
      "Iteration 210, loss = 0.44410273\n",
      "Iteration 211, loss = 0.44327111\n",
      "Iteration 212, loss = 0.44244367\n",
      "Iteration 213, loss = 0.44161994\n",
      "Iteration 214, loss = 0.44079993\n",
      "Iteration 215, loss = 0.43998359\n",
      "Iteration 216, loss = 0.43917104\n",
      "Iteration 217, loss = 0.43836283\n",
      "Iteration 218, loss = 0.43755821\n",
      "Iteration 219, loss = 0.43675732\n",
      "Iteration 220, loss = 0.43595991\n",
      "Iteration 221, loss = 0.43516591\n",
      "Iteration 222, loss = 0.43437522\n",
      "Iteration 223, loss = 0.43358790\n",
      "Iteration 224, loss = 0.43280391\n",
      "Iteration 225, loss = 0.43202344\n",
      "Iteration 226, loss = 0.43124606\n",
      "Iteration 227, loss = 0.43047190\n",
      "Iteration 228, loss = 0.42970102\n",
      "Iteration 229, loss = 0.42893321\n",
      "Iteration 230, loss = 0.42816880\n",
      "Iteration 231, loss = 0.42740742\n",
      "Iteration 232, loss = 0.42664887\n",
      "Iteration 233, loss = 0.42589342\n",
      "Iteration 234, loss = 0.42514036\n",
      "Iteration 235, loss = 0.42438980\n",
      "Iteration 236, loss = 0.42364208\n",
      "Iteration 237, loss = 0.42289738\n",
      "Iteration 238, loss = 0.42215527\n",
      "Iteration 239, loss = 0.42141572\n",
      "Iteration 240, loss = 0.42067869\n",
      "Iteration 241, loss = 0.41994415\n",
      "Iteration 242, loss = 0.41921208\n",
      "Iteration 243, loss = 0.41848238\n",
      "Iteration 244, loss = 0.41775508\n",
      "Iteration 245, loss = 0.41703012\n",
      "Iteration 246, loss = 0.41630745\n",
      "Iteration 247, loss = 0.41558704\n",
      "Iteration 248, loss = 0.41486885\n",
      "Iteration 249, loss = 0.41415285\n",
      "Iteration 250, loss = 0.41343905\n",
      "Iteration 251, loss = 0.41272734\n",
      "Iteration 252, loss = 0.41201772\n",
      "Iteration 253, loss = 0.41131014\n",
      "Iteration 254, loss = 0.41060455\n",
      "Iteration 255, loss = 0.40990093\n",
      "Iteration 256, loss = 0.40919922\n",
      "Iteration 257, loss = 0.40849942\n",
      "Iteration 258, loss = 0.40780145\n",
      "Iteration 259, loss = 0.40710530\n",
      "Iteration 260, loss = 0.40641094\n",
      "Iteration 261, loss = 0.40571832\n",
      "Iteration 262, loss = 0.40502741\n",
      "Iteration 263, loss = 0.40433914\n",
      "Iteration 264, loss = 0.40365347\n",
      "Iteration 265, loss = 0.40296923\n",
      "Iteration 266, loss = 0.40228728\n",
      "Iteration 267, loss = 0.40160721\n",
      "Iteration 268, loss = 0.40092869\n",
      "Iteration 269, loss = 0.40025232\n",
      "Iteration 270, loss = 0.39957826\n",
      "Iteration 271, loss = 0.39890613\n",
      "Iteration 272, loss = 0.39823549\n",
      "Iteration 273, loss = 0.39756633\n",
      "Iteration 274, loss = 0.39689863\n",
      "Iteration 275, loss = 0.39623234\n",
      "Iteration 276, loss = 0.39556794\n",
      "Iteration 277, loss = 0.39490535\n",
      "Iteration 278, loss = 0.39424418\n",
      "Iteration 279, loss = 0.39358440\n",
      "Iteration 280, loss = 0.39292610\n",
      "Iteration 281, loss = 0.39226973\n",
      "Iteration 282, loss = 0.39161478\n",
      "Iteration 283, loss = 0.39096145\n",
      "Iteration 284, loss = 0.39030996\n",
      "Iteration 285, loss = 0.38965924\n",
      "Iteration 286, loss = 0.38901008\n",
      "Iteration 287, loss = 0.38836244\n",
      "Iteration 288, loss = 0.38771605\n",
      "Iteration 289, loss = 0.38707090\n",
      "Iteration 290, loss = 0.38642699\n",
      "Iteration 291, loss = 0.38578430\n",
      "Iteration 292, loss = 0.38514283\n",
      "Iteration 293, loss = 0.38450259\n",
      "Iteration 294, loss = 0.38386354\n",
      "Iteration 295, loss = 0.38322594\n",
      "Iteration 296, loss = 0.38258926\n",
      "Iteration 297, loss = 0.38195354\n",
      "Iteration 298, loss = 0.38131918\n",
      "Iteration 299, loss = 0.38068591\n",
      "Iteration 300, loss = 0.38005372\n",
      "Iteration 301, loss = 0.37942276\n",
      "Iteration 302, loss = 0.37879286\n",
      "Iteration 303, loss = 0.37816442\n",
      "Iteration 304, loss = 0.37753727\n",
      "Iteration 305, loss = 0.37691105\n",
      "Iteration 306, loss = 0.37628579\n",
      "Iteration 307, loss = 0.37566149\n",
      "Iteration 308, loss = 0.37503814\n",
      "Iteration 309, loss = 0.37441577\n",
      "Iteration 310, loss = 0.37379480\n",
      "Iteration 311, loss = 0.37317499\n",
      "Iteration 312, loss = 0.37255618\n",
      "Iteration 313, loss = 0.37193835\n",
      "Iteration 314, loss = 0.37132150\n",
      "Iteration 315, loss = 0.37070578\n",
      "Iteration 316, loss = 0.37009101\n",
      "Iteration 317, loss = 0.36947697\n",
      "Iteration 318, loss = 0.36886362\n",
      "Iteration 319, loss = 0.36825131\n",
      "Iteration 320, loss = 0.36764070\n",
      "Iteration 321, loss = 0.36703150\n",
      "Iteration 322, loss = 0.36642110\n",
      "Iteration 323, loss = 0.36580982\n",
      "Iteration 324, loss = 0.36517948\n",
      "Iteration 325, loss = 0.36450625\n",
      "Iteration 326, loss = 0.36380082\n",
      "Iteration 327, loss = 0.36315580\n",
      "Iteration 328, loss = 0.36255454\n",
      "Iteration 329, loss = 0.36191635\n",
      "Iteration 330, loss = 0.36124553\n",
      "Iteration 331, loss = 0.36055527\n",
      "Iteration 332, loss = 0.35986622\n",
      "Iteration 333, loss = 0.35923251\n",
      "Iteration 334, loss = 0.35856047\n",
      "Iteration 335, loss = 0.35785644\n",
      "Iteration 336, loss = 0.35718762\n",
      "Iteration 337, loss = 0.35652394\n",
      "Iteration 338, loss = 0.35584509\n",
      "Iteration 339, loss = 0.35514984\n",
      "Iteration 340, loss = 0.35444182\n",
      "Iteration 341, loss = 0.35372680\n",
      "Iteration 342, loss = 0.35301536\n",
      "Iteration 343, loss = 0.35231629\n",
      "Iteration 344, loss = 0.35160432\n",
      "Iteration 345, loss = 0.35086944\n",
      "Iteration 346, loss = 0.35014116\n",
      "Iteration 347, loss = 0.34941771\n",
      "Iteration 348, loss = 0.34868567\n",
      "Iteration 349, loss = 0.34794234\n",
      "Iteration 350, loss = 0.34718959\n",
      "Iteration 351, loss = 0.34643209\n",
      "Iteration 352, loss = 0.34567379\n",
      "Iteration 353, loss = 0.34491530\n",
      "Iteration 354, loss = 0.34414947\n",
      "Iteration 355, loss = 0.34337613\n",
      "Iteration 356, loss = 0.34259879\n",
      "Iteration 357, loss = 0.34181714\n",
      "Iteration 358, loss = 0.34103111\n",
      "Iteration 359, loss = 0.34024122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 360, loss = 0.33944395\n",
      "Iteration 361, loss = 0.33864077\n",
      "Iteration 362, loss = 0.33783492\n",
      "Iteration 363, loss = 0.33702579\n",
      "Iteration 364, loss = 0.33621328\n",
      "Iteration 365, loss = 0.33539608\n",
      "Iteration 366, loss = 0.33457407\n",
      "Iteration 367, loss = 0.33375006\n",
      "Iteration 368, loss = 0.33292377\n",
      "Iteration 369, loss = 0.33209298\n",
      "Iteration 370, loss = 0.33125892\n",
      "Iteration 371, loss = 0.33042265\n",
      "Iteration 372, loss = 0.32958395\n",
      "Iteration 373, loss = 0.32874243\n",
      "Iteration 374, loss = 0.32789807\n",
      "Iteration 375, loss = 0.32705093\n",
      "Iteration 376, loss = 0.32620166\n",
      "Iteration 377, loss = 0.32535059\n",
      "Iteration 378, loss = 0.32449757\n",
      "Iteration 379, loss = 0.32364252\n",
      "Iteration 380, loss = 0.32278553\n",
      "Iteration 381, loss = 0.32192672\n",
      "Iteration 382, loss = 0.32106620\n",
      "Iteration 383, loss = 0.32020449\n",
      "Iteration 384, loss = 0.31934136\n",
      "Iteration 385, loss = 0.31847688\n",
      "Iteration 386, loss = 0.31761118\n",
      "Iteration 387, loss = 0.31674437\n",
      "Iteration 388, loss = 0.31587658\n",
      "Iteration 389, loss = 0.31500793\n",
      "Iteration 390, loss = 0.31413881\n",
      "Iteration 391, loss = 0.31326935\n",
      "Iteration 392, loss = 0.31239955\n",
      "Iteration 393, loss = 0.31152913\n",
      "Iteration 394, loss = 0.31065819\n",
      "Iteration 395, loss = 0.30978728\n",
      "Iteration 396, loss = 0.30891627\n",
      "Iteration 397, loss = 0.30804527\n",
      "Iteration 398, loss = 0.30717492\n",
      "Iteration 399, loss = 0.30630575\n",
      "Iteration 400, loss = 0.30543547\n",
      "Iteration 401, loss = 0.30456640\n",
      "Iteration 402, loss = 0.30369783\n",
      "Iteration 403, loss = 0.30282986\n",
      "Iteration 404, loss = 0.30196255\n",
      "Iteration 405, loss = 0.30109598\n",
      "Iteration 406, loss = 0.30023018\n",
      "Iteration 407, loss = 0.29936522\n",
      "Iteration 408, loss = 0.29850119\n",
      "Iteration 409, loss = 0.29763853\n",
      "Iteration 410, loss = 0.29677650\n",
      "Iteration 411, loss = 0.29591577\n",
      "Iteration 412, loss = 0.29505613\n",
      "Iteration 413, loss = 0.29419762\n",
      "Iteration 414, loss = 0.29333948\n",
      "Iteration 415, loss = 0.29248157\n",
      "Iteration 416, loss = 0.29162729\n",
      "Iteration 417, loss = 0.29077386\n",
      "Iteration 418, loss = 0.28992357\n",
      "Iteration 419, loss = 0.28907077\n",
      "Iteration 420, loss = 0.28822714\n",
      "Iteration 421, loss = 0.28738137\n",
      "Iteration 422, loss = 0.28653052\n",
      "Iteration 423, loss = 0.28568837\n",
      "Iteration 424, loss = 0.28484617\n",
      "Iteration 425, loss = 0.28400270\n",
      "Iteration 426, loss = 0.28316613\n",
      "Iteration 427, loss = 0.28232722\n",
      "Iteration 428, loss = 0.28148746\n",
      "Iteration 429, loss = 0.28065264\n",
      "Iteration 430, loss = 0.27981773\n",
      "Iteration 431, loss = 0.27898536\n",
      "Iteration 432, loss = 0.27815330\n",
      "Iteration 433, loss = 0.27732079\n",
      "Iteration 434, loss = 0.27649036\n",
      "Iteration 435, loss = 0.27566101\n",
      "Iteration 436, loss = 0.27483125\n",
      "Iteration 437, loss = 0.27400460\n",
      "Iteration 438, loss = 0.27317623\n",
      "Iteration 439, loss = 0.27235149\n",
      "Iteration 440, loss = 0.27152615\n",
      "Iteration 441, loss = 0.27070192\n",
      "Iteration 442, loss = 0.26987888\n",
      "Iteration 443, loss = 0.26906016\n",
      "Iteration 444, loss = 0.26824170\n",
      "Iteration 445, loss = 0.26742552\n",
      "Iteration 446, loss = 0.26661012\n",
      "Iteration 447, loss = 0.26579772\n",
      "Iteration 448, loss = 0.26498490\n",
      "Iteration 449, loss = 0.26417476\n",
      "Iteration 450, loss = 0.26336832\n",
      "Iteration 451, loss = 0.26255975\n",
      "Iteration 452, loss = 0.26175341\n",
      "Iteration 453, loss = 0.26094957\n",
      "Iteration 454, loss = 0.26014810\n",
      "Iteration 455, loss = 0.25934763\n",
      "Iteration 456, loss = 0.25855008\n",
      "Iteration 457, loss = 0.25775471\n",
      "Iteration 458, loss = 0.25696178\n",
      "Iteration 459, loss = 0.25617019\n",
      "Iteration 460, loss = 0.25538120\n",
      "Iteration 461, loss = 0.25459436\n",
      "Iteration 462, loss = 0.25381005\n",
      "Iteration 463, loss = 0.25302748\n",
      "Iteration 464, loss = 0.25224750\n",
      "Iteration 465, loss = 0.25147025\n",
      "Iteration 466, loss = 0.25069456\n",
      "Iteration 467, loss = 0.24992172\n",
      "Iteration 468, loss = 0.24915129\n",
      "Iteration 469, loss = 0.24838329\n",
      "Iteration 470, loss = 0.24761776\n",
      "Iteration 471, loss = 0.24685478\n",
      "Iteration 472, loss = 0.24609485\n",
      "Iteration 473, loss = 0.24533745\n",
      "Iteration 474, loss = 0.24458267\n",
      "Iteration 475, loss = 0.24383079\n",
      "Iteration 476, loss = 0.24308187\n",
      "Iteration 477, loss = 0.24233778\n",
      "Iteration 478, loss = 0.24159671\n",
      "Iteration 479, loss = 0.24085847\n",
      "Iteration 480, loss = 0.24012308\n",
      "Iteration 481, loss = 0.23939060\n",
      "Iteration 482, loss = 0.23866109\n",
      "Iteration 483, loss = 0.23793462\n",
      "Iteration 484, loss = 0.23721137\n",
      "Iteration 485, loss = 0.23649124\n",
      "Iteration 486, loss = 0.23577428\n",
      "Iteration 487, loss = 0.23506051\n",
      "Iteration 488, loss = 0.23434998\n",
      "Iteration 489, loss = 0.23364269\n",
      "Iteration 490, loss = 0.23293876\n",
      "Iteration 491, loss = 0.23223828\n",
      "Iteration 492, loss = 0.23154136\n",
      "Iteration 493, loss = 0.23084775\n",
      "Iteration 494, loss = 0.23015756\n",
      "Iteration 495, loss = 0.22947058\n",
      "Iteration 496, loss = 0.22878702\n",
      "Iteration 497, loss = 0.22810678\n",
      "Iteration 498, loss = 0.22742989\n",
      "Iteration 499, loss = 0.22675628\n",
      "Iteration 500, loss = 0.22608601\n",
      "Iteration 501, loss = 0.22541904\n",
      "Iteration 502, loss = 0.22475538\n",
      "Iteration 503, loss = 0.22409585\n",
      "Iteration 504, loss = 0.22343848\n",
      "Iteration 505, loss = 0.22278522\n",
      "Iteration 506, loss = 0.22213545\n",
      "Iteration 507, loss = 0.22148891\n",
      "Iteration 508, loss = 0.22084563\n",
      "Iteration 509, loss = 0.22020554\n",
      "Iteration 510, loss = 0.21956904\n",
      "Iteration 511, loss = 0.21893510\n",
      "Iteration 512, loss = 0.21830488\n",
      "Iteration 513, loss = 0.21767773\n",
      "Iteration 514, loss = 0.21705466\n",
      "Iteration 515, loss = 0.21643453\n",
      "Iteration 516, loss = 0.21581761\n",
      "Iteration 517, loss = 0.21520425\n",
      "Iteration 518, loss = 0.21459331\n",
      "Iteration 519, loss = 0.21398673\n",
      "Iteration 520, loss = 0.21338316\n",
      "Iteration 521, loss = 0.21278201\n",
      "Iteration 522, loss = 0.21218381\n",
      "Iteration 523, loss = 0.21158994\n",
      "Iteration 524, loss = 0.21099916\n",
      "Iteration 525, loss = 0.21041051\n",
      "Iteration 526, loss = 0.20982524\n",
      "Iteration 527, loss = 0.20924417\n",
      "Iteration 528, loss = 0.20866527\n",
      "Iteration 529, loss = 0.20808866\n",
      "Iteration 530, loss = 0.20751671\n",
      "Iteration 531, loss = 0.20694746\n",
      "Iteration 532, loss = 0.20638050\n",
      "Iteration 533, loss = 0.20581663\n",
      "Iteration 534, loss = 0.20525656\n",
      "Iteration 535, loss = 0.20469926\n",
      "Iteration 536, loss = 0.20414493\n",
      "Iteration 537, loss = 0.20359342\n",
      "Iteration 538, loss = 0.20304531\n",
      "Iteration 539, loss = 0.20250048\n",
      "Iteration 540, loss = 0.20195751\n",
      "Iteration 541, loss = 0.20141777\n",
      "Iteration 542, loss = 0.20088146\n",
      "Iteration 543, loss = 0.20034944\n",
      "Iteration 544, loss = 0.19981980\n",
      "Iteration 545, loss = 0.19929304\n",
      "Iteration 546, loss = 0.19876895\n",
      "Iteration 547, loss = 0.19824810\n",
      "Iteration 548, loss = 0.19772971\n",
      "Iteration 549, loss = 0.19721425\n",
      "Iteration 550, loss = 0.19670205\n",
      "Iteration 551, loss = 0.19619204\n",
      "Iteration 552, loss = 0.19568509\n",
      "Iteration 553, loss = 0.19518061\n",
      "Iteration 554, loss = 0.19467951\n",
      "Iteration 555, loss = 0.19418082\n",
      "Iteration 556, loss = 0.19368435\n",
      "Iteration 557, loss = 0.19319179\n",
      "Iteration 558, loss = 0.19270147\n",
      "Iteration 559, loss = 0.19221346\n",
      "Iteration 560, loss = 0.19172777\n",
      "Iteration 561, loss = 0.19124533\n",
      "Iteration 562, loss = 0.19076590\n",
      "Iteration 563, loss = 0.19028775\n",
      "Iteration 564, loss = 0.18981349\n",
      "Iteration 565, loss = 0.18934177\n",
      "Iteration 566, loss = 0.18887182\n",
      "Iteration 567, loss = 0.18840508\n",
      "Iteration 568, loss = 0.18794111\n",
      "Iteration 569, loss = 0.18747966\n",
      "Iteration 570, loss = 0.18702091\n",
      "Iteration 571, loss = 0.18656458\n",
      "Iteration 572, loss = 0.18611092\n",
      "Iteration 573, loss = 0.18565969\n",
      "Iteration 574, loss = 0.18521125\n",
      "Iteration 575, loss = 0.18476501\n",
      "Iteration 576, loss = 0.18432100\n",
      "Iteration 577, loss = 0.18387923\n",
      "Iteration 578, loss = 0.18344001\n",
      "Iteration 579, loss = 0.18300366\n",
      "Iteration 580, loss = 0.18256933\n",
      "Iteration 581, loss = 0.18213786\n",
      "Iteration 582, loss = 0.18170854\n",
      "Iteration 583, loss = 0.18128137\n",
      "Iteration 584, loss = 0.18085637\n",
      "Iteration 585, loss = 0.18043357\n",
      "Iteration 586, loss = 0.18001302\n",
      "Iteration 587, loss = 0.17959472\n",
      "Iteration 588, loss = 0.17917924\n",
      "Iteration 589, loss = 0.17876607\n",
      "Iteration 590, loss = 0.17835463\n",
      "Iteration 591, loss = 0.17794532\n",
      "Iteration 592, loss = 0.17753866\n",
      "Iteration 593, loss = 0.17713408\n",
      "Iteration 594, loss = 0.17673158\n",
      "Iteration 595, loss = 0.17633116\n",
      "Iteration 596, loss = 0.17593281\n",
      "Iteration 597, loss = 0.17553655\n",
      "Iteration 598, loss = 0.17514282\n",
      "Iteration 599, loss = 0.17475084\n",
      "Iteration 600, loss = 0.17436075\n",
      "Iteration 601, loss = 0.17397304\n",
      "Iteration 602, loss = 0.17358732\n",
      "Iteration 603, loss = 0.17320371\n",
      "Iteration 604, loss = 0.17282226\n",
      "Iteration 605, loss = 0.17244288\n",
      "Iteration 606, loss = 0.17206533\n",
      "Iteration 607, loss = 0.17168987\n",
      "Iteration 608, loss = 0.17131644\n",
      "Iteration 609, loss = 0.17094499\n",
      "Iteration 610, loss = 0.17057547\n",
      "Iteration 611, loss = 0.17020787\n",
      "Iteration 612, loss = 0.16984219\n",
      "Iteration 613, loss = 0.16947871\n",
      "Iteration 614, loss = 0.16911679\n",
      "Iteration 615, loss = 0.16875686\n",
      "Iteration 616, loss = 0.16839893\n",
      "Iteration 617, loss = 0.16804269\n",
      "Iteration 618, loss = 0.16768833\n",
      "Iteration 619, loss = 0.16733578\n",
      "Iteration 620, loss = 0.16698497\n",
      "Iteration 621, loss = 0.16663603\n",
      "Iteration 622, loss = 0.16628896\n",
      "Iteration 623, loss = 0.16594360\n",
      "Iteration 624, loss = 0.16560007\n",
      "Iteration 625, loss = 0.16525831\n",
      "Iteration 626, loss = 0.16491846\n",
      "Iteration 627, loss = 0.16458017\n",
      "Iteration 628, loss = 0.16424397\n",
      "Iteration 629, loss = 0.16390929\n",
      "Iteration 630, loss = 0.16357611\n",
      "Iteration 631, loss = 0.16324518\n",
      "Iteration 632, loss = 0.16291586\n",
      "Iteration 633, loss = 0.16258788\n",
      "Iteration 634, loss = 0.16226152\n",
      "Iteration 635, loss = 0.16193758\n",
      "Iteration 636, loss = 0.16161501\n",
      "Iteration 637, loss = 0.16129376\n",
      "Iteration 638, loss = 0.16097408\n",
      "Iteration 639, loss = 0.16065658\n",
      "Iteration 640, loss = 0.16034046\n",
      "Iteration 641, loss = 0.16002571\n",
      "Iteration 642, loss = 0.15971310\n",
      "Iteration 643, loss = 0.15940172\n",
      "Iteration 644, loss = 0.15909171\n",
      "Iteration 645, loss = 0.15878301\n",
      "Iteration 646, loss = 0.15847621\n",
      "Iteration 647, loss = 0.15817091\n",
      "Iteration 648, loss = 0.15786667\n",
      "Iteration 649, loss = 0.15756421\n",
      "Iteration 650, loss = 0.15726325\n",
      "Iteration 651, loss = 0.15696373\n",
      "Iteration 652, loss = 0.15666559\n",
      "Iteration 653, loss = 0.15636884\n",
      "Iteration 654, loss = 0.15607356\n",
      "Iteration 655, loss = 0.15577968\n",
      "Iteration 656, loss = 0.15548725\n",
      "Iteration 657, loss = 0.15519636\n",
      "Iteration 658, loss = 0.15490698\n",
      "Iteration 659, loss = 0.15461900\n",
      "Iteration 660, loss = 0.15433243\n",
      "Iteration 661, loss = 0.15404728\n",
      "Iteration 662, loss = 0.15376291\n",
      "Iteration 663, loss = 0.15347902\n",
      "Iteration 664, loss = 0.15319642\n",
      "Iteration 665, loss = 0.15291501\n",
      "Iteration 666, loss = 0.15263487\n",
      "Iteration 667, loss = 0.15235568\n",
      "Iteration 668, loss = 0.15207756\n",
      "Iteration 669, loss = 0.15180069\n",
      "Iteration 670, loss = 0.15152507\n",
      "Iteration 671, loss = 0.15125071\n",
      "Iteration 672, loss = 0.15097760\n",
      "Iteration 673, loss = 0.15070581\n",
      "Iteration 674, loss = 0.15043559\n",
      "Iteration 675, loss = 0.15016672\n",
      "Iteration 676, loss = 0.14989954\n",
      "Iteration 677, loss = 0.14963402\n",
      "Iteration 678, loss = 0.14936984\n",
      "Iteration 679, loss = 0.14910700\n",
      "Iteration 680, loss = 0.14884550\n",
      "Iteration 681, loss = 0.14858522\n",
      "Iteration 682, loss = 0.14832617\n",
      "Iteration 683, loss = 0.14806903\n",
      "Iteration 684, loss = 0.14781311\n",
      "Iteration 685, loss = 0.14755849\n",
      "Iteration 686, loss = 0.14730673\n",
      "Iteration 687, loss = 0.14705636\n",
      "Iteration 688, loss = 0.14680738\n",
      "Iteration 689, loss = 0.14656005\n",
      "Iteration 690, loss = 0.14631412\n",
      "Iteration 691, loss = 0.14606952\n",
      "Iteration 692, loss = 0.14582621\n",
      "Iteration 693, loss = 0.14558416\n",
      "Iteration 694, loss = 0.14534335\n",
      "Iteration 695, loss = 0.14510376\n",
      "Iteration 696, loss = 0.14486542\n",
      "Iteration 697, loss = 0.14462812\n",
      "Iteration 698, loss = 0.14439193\n",
      "Iteration 699, loss = 0.14415692\n",
      "Iteration 700, loss = 0.14392305\n",
      "Iteration 701, loss = 0.14369034\n",
      "Iteration 702, loss = 0.14345863\n",
      "Iteration 703, loss = 0.14322720\n",
      "Iteration 704, loss = 0.14299680\n",
      "Iteration 705, loss = 0.14276745\n",
      "Iteration 706, loss = 0.14253824\n",
      "Iteration 707, loss = 0.14230982\n",
      "Iteration 708, loss = 0.14208230\n",
      "Iteration 709, loss = 0.14185573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 710, loss = 0.14163005\n",
      "Iteration 711, loss = 0.14140544\n",
      "Iteration 712, loss = 0.14118189\n",
      "Iteration 713, loss = 0.14095956\n",
      "Iteration 714, loss = 0.14073812\n",
      "Iteration 715, loss = 0.14051752\n",
      "Iteration 716, loss = 0.14029761\n",
      "Iteration 717, loss = 0.14007859\n",
      "Iteration 718, loss = 0.13986053\n",
      "Iteration 719, loss = 0.13964535\n",
      "Iteration 720, loss = 0.13943148\n",
      "Iteration 721, loss = 0.13921859\n",
      "Iteration 722, loss = 0.13900674\n",
      "Iteration 723, loss = 0.13879592\n",
      "Iteration 724, loss = 0.13858611\n",
      "Iteration 725, loss = 0.13837739\n",
      "Iteration 726, loss = 0.13816997\n",
      "Iteration 727, loss = 0.13796366\n",
      "Iteration 728, loss = 0.13775835\n",
      "Iteration 729, loss = 0.13755403\n",
      "Iteration 730, loss = 0.13735100\n",
      "Iteration 731, loss = 0.13714960\n",
      "Iteration 732, loss = 0.13694901\n",
      "Iteration 733, loss = 0.13674930\n",
      "Iteration 734, loss = 0.13655054\n",
      "Iteration 735, loss = 0.13635246\n",
      "Iteration 736, loss = 0.13615485\n",
      "Iteration 737, loss = 0.13595792\n",
      "Iteration 738, loss = 0.13576191\n",
      "Iteration 739, loss = 0.13556675\n",
      "Iteration 740, loss = 0.13537257\n",
      "Iteration 741, loss = 0.13517929\n",
      "Iteration 742, loss = 0.13498682\n",
      "Iteration 743, loss = 0.13479517\n",
      "Iteration 744, loss = 0.13460469\n",
      "Iteration 745, loss = 0.13441538\n",
      "Iteration 746, loss = 0.13422677\n",
      "Iteration 747, loss = 0.13403908\n",
      "Iteration 748, loss = 0.13385230\n",
      "Iteration 749, loss = 0.13366661\n",
      "Iteration 750, loss = 0.13348217\n",
      "Iteration 751, loss = 0.13329862\n",
      "Iteration 752, loss = 0.13311613\n",
      "Iteration 753, loss = 0.13293458\n",
      "Iteration 754, loss = 0.13275385\n",
      "Iteration 755, loss = 0.13257396\n",
      "Iteration 756, loss = 0.13239495\n",
      "Iteration 757, loss = 0.13221683\n",
      "Iteration 758, loss = 0.13203964\n",
      "Iteration 759, loss = 0.13186323\n",
      "Iteration 760, loss = 0.13168760\n",
      "Iteration 761, loss = 0.13151273\n",
      "Iteration 762, loss = 0.13133857\n",
      "Iteration 763, loss = 0.13116514\n",
      "Iteration 764, loss = 0.13099246\n",
      "Iteration 765, loss = 0.13082059\n",
      "Iteration 766, loss = 0.13064946\n",
      "Iteration 767, loss = 0.13047905\n",
      "Iteration 768, loss = 0.13030939\n",
      "Iteration 769, loss = 0.13014063\n",
      "Iteration 770, loss = 0.12997270\n",
      "Iteration 771, loss = 0.12980536\n",
      "Iteration 772, loss = 0.12963835\n",
      "Iteration 773, loss = 0.12947217\n",
      "Iteration 774, loss = 0.12930658\n",
      "Iteration 775, loss = 0.12914176\n",
      "Iteration 776, loss = 0.12897750\n",
      "Iteration 777, loss = 0.12881406\n",
      "Iteration 778, loss = 0.12865130\n",
      "Iteration 779, loss = 0.12848908\n",
      "Iteration 780, loss = 0.12832767\n",
      "Iteration 781, loss = 0.12816687\n",
      "Iteration 782, loss = 0.12800669\n",
      "Iteration 783, loss = 0.12784736\n",
      "Iteration 784, loss = 0.12768860\n",
      "Iteration 785, loss = 0.12753045\n",
      "Iteration 786, loss = 0.12737321\n",
      "Iteration 787, loss = 0.12721635\n",
      "Iteration 788, loss = 0.12706037\n",
      "Iteration 789, loss = 0.12690509\n",
      "Iteration 790, loss = 0.12675028\n",
      "Iteration 791, loss = 0.12659598\n",
      "Iteration 792, loss = 0.12644234\n",
      "Iteration 793, loss = 0.12628987\n",
      "Iteration 794, loss = 0.12613756\n",
      "Iteration 795, loss = 0.12598564\n",
      "Iteration 796, loss = 0.12583492\n",
      "Iteration 797, loss = 0.12568498\n",
      "Iteration 798, loss = 0.12553571\n",
      "Iteration 799, loss = 0.12538722\n",
      "Iteration 800, loss = 0.12523936\n",
      "Iteration 801, loss = 0.12509198\n",
      "Iteration 802, loss = 0.12494550\n",
      "Iteration 803, loss = 0.12479960\n",
      "Iteration 804, loss = 0.12465408\n",
      "Iteration 805, loss = 0.12450960\n",
      "Iteration 806, loss = 0.12436561\n",
      "Iteration 807, loss = 0.12422202\n",
      "Iteration 808, loss = 0.12407893\n",
      "Iteration 809, loss = 0.12393676\n",
      "Iteration 810, loss = 0.12379509\n",
      "Iteration 811, loss = 0.12365378\n",
      "Iteration 812, loss = 0.12351311\n",
      "Iteration 813, loss = 0.12337293\n",
      "Iteration 814, loss = 0.12323326\n",
      "Iteration 815, loss = 0.12309413\n",
      "Iteration 816, loss = 0.12295564\n",
      "Iteration 817, loss = 0.12281764\n",
      "Iteration 818, loss = 0.12268025\n",
      "Iteration 819, loss = 0.12254343\n",
      "Iteration 820, loss = 0.12240715\n",
      "Iteration 821, loss = 0.12227141\n",
      "Iteration 822, loss = 0.12213631\n",
      "Iteration 823, loss = 0.12200133\n",
      "Iteration 824, loss = 0.12186695\n",
      "Iteration 825, loss = 0.12173303\n",
      "Iteration 826, loss = 0.12159959\n",
      "Iteration 827, loss = 0.12146664\n",
      "Iteration 828, loss = 0.12133438\n",
      "Iteration 829, loss = 0.12120235\n",
      "Iteration 830, loss = 0.12107097\n",
      "Iteration 831, loss = 0.12094008\n",
      "Iteration 832, loss = 0.12080968\n",
      "Iteration 833, loss = 0.12067975\n",
      "Iteration 834, loss = 0.12055033\n",
      "Iteration 835, loss = 0.12042144\n",
      "Iteration 836, loss = 0.12029302\n",
      "Iteration 837, loss = 0.12016510\n",
      "Iteration 838, loss = 0.12003771\n",
      "Iteration 839, loss = 0.11991038\n",
      "Iteration 840, loss = 0.11978332\n",
      "Iteration 841, loss = 0.11965667\n",
      "Iteration 842, loss = 0.11953047\n",
      "Iteration 843, loss = 0.11940472\n",
      "Iteration 844, loss = 0.11927941\n",
      "Iteration 845, loss = 0.11915466\n",
      "Iteration 846, loss = 0.11903038\n",
      "Iteration 847, loss = 0.11890659\n",
      "Iteration 848, loss = 0.11878329\n",
      "Iteration 849, loss = 0.11866045\n",
      "Iteration 850, loss = 0.11853806\n",
      "Iteration 851, loss = 0.11841612\n",
      "Iteration 852, loss = 0.11829461\n",
      "Iteration 853, loss = 0.11817345\n",
      "Iteration 854, loss = 0.11805276\n",
      "Iteration 855, loss = 0.11793251\n",
      "Iteration 856, loss = 0.11781262\n",
      "Iteration 857, loss = 0.11769314\n",
      "Iteration 858, loss = 0.11757410\n",
      "Iteration 859, loss = 0.11745547\n",
      "Iteration 860, loss = 0.11733728\n",
      "Iteration 861, loss = 0.11721963\n",
      "Iteration 862, loss = 0.11710278\n",
      "Iteration 863, loss = 0.11698640\n",
      "Iteration 864, loss = 0.11687048\n",
      "Iteration 865, loss = 0.11675502\n",
      "Iteration 866, loss = 0.11663999\n",
      "Iteration 867, loss = 0.11652542\n",
      "Iteration 868, loss = 0.11641128\n",
      "Iteration 869, loss = 0.11629757\n",
      "Iteration 870, loss = 0.11618429\n",
      "Iteration 871, loss = 0.11607143\n",
      "Iteration 872, loss = 0.11595899\n",
      "Iteration 873, loss = 0.11584699\n",
      "Iteration 874, loss = 0.11573540\n",
      "Iteration 875, loss = 0.11562473\n",
      "Iteration 876, loss = 0.11551484\n",
      "Iteration 877, loss = 0.11540541\n",
      "Iteration 878, loss = 0.11529682\n",
      "Iteration 879, loss = 0.11518876\n",
      "Iteration 880, loss = 0.11508111\n",
      "Iteration 881, loss = 0.11497387\n",
      "Iteration 882, loss = 0.11486703\n",
      "Iteration 883, loss = 0.11476060\n",
      "Iteration 884, loss = 0.11465456\n",
      "Iteration 885, loss = 0.11454892\n",
      "Iteration 886, loss = 0.11444369\n",
      "Iteration 887, loss = 0.11433884\n",
      "Iteration 888, loss = 0.11423438\n",
      "Iteration 889, loss = 0.11413031\n",
      "Iteration 890, loss = 0.11402662\n",
      "Iteration 891, loss = 0.11392332\n",
      "Iteration 892, loss = 0.11382039\n",
      "Iteration 893, loss = 0.11371782\n",
      "Iteration 894, loss = 0.11361564\n",
      "Iteration 895, loss = 0.11351382\n",
      "Iteration 896, loss = 0.11341238\n",
      "Iteration 897, loss = 0.11331130\n",
      "Iteration 898, loss = 0.11321059\n",
      "Iteration 899, loss = 0.11311023\n",
      "Iteration 900, loss = 0.11301025\n",
      "Iteration 901, loss = 0.11291061\n",
      "Iteration 902, loss = 0.11281134\n",
      "Iteration 903, loss = 0.11271241\n",
      "Iteration 904, loss = 0.11261385\n",
      "Iteration 905, loss = 0.11251562\n",
      "Iteration 906, loss = 0.11241775\n",
      "Iteration 907, loss = 0.11232023\n",
      "Iteration 908, loss = 0.11222305\n",
      "Iteration 909, loss = 0.11212622\n",
      "Iteration 910, loss = 0.11202972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35721204\n",
      "Iteration 2, loss = 1.31986774\n",
      "Iteration 3, loss = 1.26960379\n",
      "Iteration 4, loss = 1.21066236\n",
      "Iteration 5, loss = 1.14741642\n",
      "Iteration 6, loss = 1.08366221\n",
      "Iteration 7, loss = 1.02259707\n",
      "Iteration 8, loss = 0.96694630\n",
      "Iteration 9, loss = 0.91839658\n",
      "Iteration 10, loss = 0.87769878\n",
      "Iteration 11, loss = 0.84479822\n",
      "Iteration 12, loss = 0.81886173\n",
      "Iteration 13, loss = 0.79877292\n",
      "Iteration 14, loss = 0.78331000\n",
      "Iteration 15, loss = 0.77135527\n",
      "Iteration 16, loss = 0.76197620\n",
      "Iteration 17, loss = 0.75416249\n",
      "Iteration 18, loss = 0.74766735\n",
      "Iteration 19, loss = 0.74236604\n",
      "Iteration 20, loss = 0.73795198\n",
      "Iteration 21, loss = 0.73444844\n",
      "Iteration 22, loss = 0.73124983\n",
      "Iteration 23, loss = 0.72775944\n",
      "Iteration 24, loss = 0.72374460\n",
      "Iteration 25, loss = 0.71924251\n",
      "Iteration 26, loss = 0.71419911\n",
      "Iteration 27, loss = 0.70879021\n",
      "Iteration 28, loss = 0.70320371\n",
      "Iteration 29, loss = 0.69756896\n",
      "Iteration 30, loss = 0.69196993\n",
      "Iteration 31, loss = 0.68655128\n",
      "Iteration 32, loss = 0.68141736\n",
      "Iteration 33, loss = 0.67664773\n",
      "Iteration 34, loss = 0.67226124\n",
      "Iteration 35, loss = 0.66822175\n",
      "Iteration 36, loss = 0.66453417\n",
      "Iteration 37, loss = 0.66114076\n",
      "Iteration 38, loss = 0.65801124\n",
      "Iteration 39, loss = 0.65510711\n",
      "Iteration 40, loss = 0.65237702\n",
      "Iteration 41, loss = 0.64977236\n",
      "Iteration 42, loss = 0.64726074\n",
      "Iteration 43, loss = 0.64481444\n",
      "Iteration 44, loss = 0.64241761\n",
      "Iteration 45, loss = 0.64004022\n",
      "Iteration 46, loss = 0.63767659\n",
      "Iteration 47, loss = 0.63531160\n",
      "Iteration 48, loss = 0.63294611\n",
      "Iteration 49, loss = 0.63057566\n",
      "Iteration 50, loss = 0.62820425\n",
      "Iteration 51, loss = 0.62583726\n",
      "Iteration 52, loss = 0.62348095\n",
      "Iteration 53, loss = 0.62114176\n",
      "Iteration 54, loss = 0.61882579\n",
      "Iteration 55, loss = 0.61653838\n",
      "Iteration 56, loss = 0.61428394\n",
      "Iteration 57, loss = 0.61206572\n",
      "Iteration 58, loss = 0.60988589\n",
      "Iteration 59, loss = 0.60774555\n",
      "Iteration 60, loss = 0.60564485\n",
      "Iteration 61, loss = 0.60358315\n",
      "Iteration 62, loss = 0.60155920\n",
      "Iteration 63, loss = 0.59957132\n",
      "Iteration 64, loss = 0.59761753\n",
      "Iteration 65, loss = 0.59569573\n",
      "Iteration 66, loss = 0.59380383\n",
      "Iteration 67, loss = 0.59193981\n",
      "Iteration 68, loss = 0.59010183\n",
      "Iteration 69, loss = 0.58828827\n",
      "Iteration 70, loss = 0.58649773\n",
      "Iteration 71, loss = 0.58472904\n",
      "Iteration 72, loss = 0.58298128\n",
      "Iteration 73, loss = 0.58125368\n",
      "Iteration 74, loss = 0.57954573\n",
      "Iteration 75, loss = 0.57785703\n",
      "Iteration 76, loss = 0.57618728\n",
      "Iteration 77, loss = 0.57453778\n",
      "Iteration 78, loss = 0.57290892\n",
      "Iteration 79, loss = 0.57129991\n",
      "Iteration 80, loss = 0.56971218\n",
      "Iteration 81, loss = 0.56814319\n",
      "Iteration 82, loss = 0.56659276\n",
      "Iteration 83, loss = 0.56506069\n",
      "Iteration 84, loss = 0.56355408\n",
      "Iteration 85, loss = 0.56207132\n",
      "Iteration 86, loss = 0.56060565\n",
      "Iteration 87, loss = 0.55915682\n",
      "Iteration 88, loss = 0.55772528\n",
      "Iteration 89, loss = 0.55631187\n",
      "Iteration 90, loss = 0.55491460\n",
      "Iteration 91, loss = 0.55353313\n",
      "Iteration 92, loss = 0.55216709\n",
      "Iteration 93, loss = 0.55081763\n",
      "Iteration 94, loss = 0.54948375\n",
      "Iteration 95, loss = 0.54816448\n",
      "Iteration 96, loss = 0.54685944\n",
      "Iteration 97, loss = 0.54556833\n",
      "Iteration 98, loss = 0.54429079\n",
      "Iteration 99, loss = 0.54302650\n",
      "Iteration 100, loss = 0.54177525\n",
      "Iteration 101, loss = 0.54053675\n",
      "Iteration 102, loss = 0.53931073\n",
      "Iteration 103, loss = 0.53809693\n",
      "Iteration 104, loss = 0.53689509\n",
      "Iteration 105, loss = 0.53570498\n",
      "Iteration 106, loss = 0.53452636\n",
      "Iteration 107, loss = 0.53335902\n",
      "Iteration 108, loss = 0.53220272\n",
      "Iteration 109, loss = 0.53105726\n",
      "Iteration 110, loss = 0.52992241\n",
      "Iteration 111, loss = 0.52879799\n",
      "Iteration 112, loss = 0.52768376\n",
      "Iteration 113, loss = 0.52657955\n",
      "Iteration 114, loss = 0.52548514\n",
      "Iteration 115, loss = 0.52440035\n",
      "Iteration 116, loss = 0.52332499\n",
      "Iteration 117, loss = 0.52225887\n",
      "Iteration 118, loss = 0.52120181\n",
      "Iteration 119, loss = 0.52015363\n",
      "Iteration 120, loss = 0.51911416\n",
      "Iteration 121, loss = 0.51808323\n",
      "Iteration 122, loss = 0.51706069\n",
      "Iteration 123, loss = 0.51604636\n",
      "Iteration 124, loss = 0.51504010\n",
      "Iteration 125, loss = 0.51404175\n",
      "Iteration 126, loss = 0.51305117\n",
      "Iteration 127, loss = 0.51206821\n",
      "Iteration 128, loss = 0.51109405\n",
      "Iteration 129, loss = 0.51012945\n",
      "Iteration 130, loss = 0.50917232\n",
      "Iteration 131, loss = 0.50822254\n",
      "Iteration 132, loss = 0.50728427\n",
      "Iteration 133, loss = 0.50635521\n",
      "Iteration 134, loss = 0.50543332\n",
      "Iteration 135, loss = 0.50452128\n",
      "Iteration 136, loss = 0.50361710\n",
      "Iteration 137, loss = 0.50271980\n",
      "Iteration 138, loss = 0.50183137\n",
      "Iteration 139, loss = 0.50095049\n",
      "Iteration 140, loss = 0.50007604\n",
      "Iteration 141, loss = 0.49920858\n",
      "Iteration 142, loss = 0.49834729\n",
      "Iteration 143, loss = 0.49749220\n",
      "Iteration 144, loss = 0.49664331\n",
      "Iteration 145, loss = 0.49580063\n",
      "Iteration 146, loss = 0.49496414\n",
      "Iteration 147, loss = 0.49413380\n",
      "Iteration 148, loss = 0.49330957\n",
      "Iteration 149, loss = 0.49249139\n",
      "Iteration 150, loss = 0.49167921\n",
      "Iteration 151, loss = 0.49087295\n",
      "Iteration 152, loss = 0.49007254\n",
      "Iteration 153, loss = 0.48927807\n",
      "Iteration 154, loss = 0.48848936\n",
      "Iteration 155, loss = 0.48770616\n",
      "Iteration 156, loss = 0.48692837\n",
      "Iteration 157, loss = 0.48615589\n",
      "Iteration 158, loss = 0.48538861\n",
      "Iteration 159, loss = 0.48462645\n",
      "Iteration 160, loss = 0.48386931\n",
      "Iteration 161, loss = 0.48311740\n",
      "Iteration 162, loss = 0.48237044\n",
      "Iteration 163, loss = 0.48162834\n",
      "Iteration 164, loss = 0.48089104\n",
      "Iteration 165, loss = 0.48015845\n",
      "Iteration 166, loss = 0.47943050\n",
      "Iteration 167, loss = 0.47870712\n",
      "Iteration 168, loss = 0.47798824\n",
      "Iteration 169, loss = 0.47727377\n",
      "Iteration 170, loss = 0.47656365\n",
      "Iteration 171, loss = 0.47585781\n",
      "Iteration 172, loss = 0.47515618\n",
      "Iteration 173, loss = 0.47445868\n",
      "Iteration 174, loss = 0.47376530\n",
      "Iteration 175, loss = 0.47307604\n",
      "Iteration 176, loss = 0.47239066\n",
      "Iteration 177, loss = 0.47170928\n",
      "Iteration 178, loss = 0.47103166\n",
      "Iteration 179, loss = 0.47035789\n",
      "Iteration 180, loss = 0.46968774\n",
      "Iteration 181, loss = 0.46902139\n",
      "Iteration 182, loss = 0.46835852\n",
      "Iteration 183, loss = 0.46769928\n",
      "Iteration 184, loss = 0.46704344\n",
      "Iteration 185, loss = 0.46639116\n",
      "Iteration 186, loss = 0.46574215\n",
      "Iteration 187, loss = 0.46509650\n",
      "Iteration 188, loss = 0.46445421\n",
      "Iteration 189, loss = 0.46381506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190, loss = 0.46317911\n",
      "Iteration 191, loss = 0.46254628\n",
      "Iteration 192, loss = 0.46191657\n",
      "Iteration 193, loss = 0.46128989\n",
      "Iteration 194, loss = 0.46066618\n",
      "Iteration 195, loss = 0.46004542\n",
      "Iteration 196, loss = 0.45942755\n",
      "Iteration 197, loss = 0.45881254\n",
      "Iteration 198, loss = 0.45820035\n",
      "Iteration 199, loss = 0.45759093\n",
      "Iteration 200, loss = 0.45698424\n",
      "Iteration 201, loss = 0.45638024\n",
      "Iteration 202, loss = 0.45577890\n",
      "Iteration 203, loss = 0.45518016\n",
      "Iteration 204, loss = 0.45458400\n",
      "Iteration 205, loss = 0.45399037\n",
      "Iteration 206, loss = 0.45339924\n",
      "Iteration 207, loss = 0.45281057\n",
      "Iteration 208, loss = 0.45222432\n",
      "Iteration 209, loss = 0.45164046\n",
      "Iteration 210, loss = 0.45105895\n",
      "Iteration 211, loss = 0.45047975\n",
      "Iteration 212, loss = 0.44990284\n",
      "Iteration 213, loss = 0.44932817\n",
      "Iteration 214, loss = 0.44875572\n",
      "Iteration 215, loss = 0.44818545\n",
      "Iteration 216, loss = 0.44761733\n",
      "Iteration 217, loss = 0.44705165\n",
      "Iteration 218, loss = 0.44648954\n",
      "Iteration 219, loss = 0.44592959\n",
      "Iteration 220, loss = 0.44537188\n",
      "Iteration 221, loss = 0.44481628\n",
      "Iteration 222, loss = 0.44426275\n",
      "Iteration 223, loss = 0.44371134\n",
      "Iteration 224, loss = 0.44316194\n",
      "Iteration 225, loss = 0.44261452\n",
      "Iteration 226, loss = 0.44206963\n",
      "Iteration 227, loss = 0.44152750\n",
      "Iteration 228, loss = 0.44098742\n",
      "Iteration 229, loss = 0.44044924\n",
      "Iteration 230, loss = 0.43991313\n",
      "Iteration 231, loss = 0.43937879\n",
      "Iteration 232, loss = 0.43884655\n",
      "Iteration 233, loss = 0.43831653\n",
      "Iteration 234, loss = 0.43778890\n",
      "Iteration 235, loss = 0.43726317\n",
      "Iteration 236, loss = 0.43673931\n",
      "Iteration 237, loss = 0.43621718\n",
      "Iteration 238, loss = 0.43569700\n",
      "Iteration 239, loss = 0.43517845\n",
      "Iteration 240, loss = 0.43466182\n",
      "Iteration 241, loss = 0.43414684\n",
      "Iteration 242, loss = 0.43363362\n",
      "Iteration 243, loss = 0.43312215\n",
      "Iteration 244, loss = 0.43261233\n",
      "Iteration 245, loss = 0.43210428\n",
      "Iteration 246, loss = 0.43159777\n",
      "Iteration 247, loss = 0.43109303\n",
      "Iteration 248, loss = 0.43058974\n",
      "Iteration 249, loss = 0.43008822\n",
      "Iteration 250, loss = 0.42958815\n",
      "Iteration 251, loss = 0.42908959\n",
      "Iteration 252, loss = 0.42859275\n",
      "Iteration 253, loss = 0.42809722\n",
      "Iteration 254, loss = 0.42760330\n",
      "Iteration 255, loss = 0.42711079\n",
      "Iteration 256, loss = 0.42661971\n",
      "Iteration 257, loss = 0.42613018\n",
      "Iteration 258, loss = 0.42564195\n",
      "Iteration 259, loss = 0.42515515\n",
      "Iteration 260, loss = 0.42467051\n",
      "Iteration 261, loss = 0.42418771\n",
      "Iteration 262, loss = 0.42370637\n",
      "Iteration 263, loss = 0.42322627\n",
      "Iteration 264, loss = 0.42274755\n",
      "Iteration 265, loss = 0.42227029\n",
      "Iteration 266, loss = 0.42179429\n",
      "Iteration 267, loss = 0.42131960\n",
      "Iteration 268, loss = 0.42084625\n",
      "Iteration 269, loss = 0.42037430\n",
      "Iteration 270, loss = 0.41990360\n",
      "Iteration 271, loss = 0.41943420\n",
      "Iteration 272, loss = 0.41896618\n",
      "Iteration 273, loss = 0.41849937\n",
      "Iteration 274, loss = 0.41803382\n",
      "Iteration 275, loss = 0.41756960\n",
      "Iteration 276, loss = 0.41710655\n",
      "Iteration 277, loss = 0.41664475\n",
      "Iteration 278, loss = 0.41618419\n",
      "Iteration 279, loss = 0.41572485\n",
      "Iteration 280, loss = 0.41526667\n",
      "Iteration 281, loss = 0.41480966\n",
      "Iteration 282, loss = 0.41435379\n",
      "Iteration 283, loss = 0.41389921\n",
      "Iteration 284, loss = 0.41344558\n",
      "Iteration 285, loss = 0.41299315\n",
      "Iteration 286, loss = 0.41254184\n",
      "Iteration 287, loss = 0.41209167\n",
      "Iteration 288, loss = 0.41164254\n",
      "Iteration 289, loss = 0.41119448\n",
      "Iteration 290, loss = 0.41074747\n",
      "Iteration 291, loss = 0.41030150\n",
      "Iteration 292, loss = 0.40985656\n",
      "Iteration 293, loss = 0.40941280\n",
      "Iteration 294, loss = 0.40896981\n",
      "Iteration 295, loss = 0.40852793\n",
      "Iteration 296, loss = 0.40808705\n",
      "Iteration 297, loss = 0.40764713\n",
      "Iteration 298, loss = 0.40720825\n",
      "Iteration 299, loss = 0.40677025\n",
      "Iteration 300, loss = 0.40633323\n",
      "Iteration 301, loss = 0.40589714\n",
      "Iteration 302, loss = 0.40546198\n",
      "Iteration 303, loss = 0.40502773\n",
      "Iteration 304, loss = 0.40459439\n",
      "Iteration 305, loss = 0.40416195\n",
      "Iteration 306, loss = 0.40373041\n",
      "Iteration 307, loss = 0.40329974\n",
      "Iteration 308, loss = 0.40286995\n",
      "Iteration 309, loss = 0.40244102\n",
      "Iteration 310, loss = 0.40201295\n",
      "Iteration 311, loss = 0.40158573\n",
      "Iteration 312, loss = 0.40115935\n",
      "Iteration 313, loss = 0.40073380\n",
      "Iteration 314, loss = 0.40030907\n",
      "Iteration 315, loss = 0.39988515\n",
      "Iteration 316, loss = 0.39946204\n",
      "Iteration 317, loss = 0.39903973\n",
      "Iteration 318, loss = 0.39861821\n",
      "Iteration 319, loss = 0.39819748\n",
      "Iteration 320, loss = 0.39777751\n",
      "Iteration 321, loss = 0.39735832\n",
      "Iteration 322, loss = 0.39693988\n",
      "Iteration 323, loss = 0.39652220\n",
      "Iteration 324, loss = 0.39610526\n",
      "Iteration 325, loss = 0.39568906\n",
      "Iteration 326, loss = 0.39527359\n",
      "Iteration 327, loss = 0.39485885\n",
      "Iteration 328, loss = 0.39444483\n",
      "Iteration 329, loss = 0.39403151\n",
      "Iteration 330, loss = 0.39361890\n",
      "Iteration 331, loss = 0.39320699\n",
      "Iteration 332, loss = 0.39279577\n",
      "Iteration 333, loss = 0.39238524\n",
      "Iteration 334, loss = 0.39197557\n",
      "Iteration 335, loss = 0.39156797\n",
      "Iteration 336, loss = 0.39116111\n",
      "Iteration 337, loss = 0.39075493\n",
      "Iteration 338, loss = 0.39034942\n",
      "Iteration 339, loss = 0.38994459\n",
      "Iteration 340, loss = 0.38954045\n",
      "Iteration 341, loss = 0.38913716\n",
      "Iteration 342, loss = 0.38873458\n",
      "Iteration 343, loss = 0.38833262\n",
      "Iteration 344, loss = 0.38793129\n",
      "Iteration 345, loss = 0.38753061\n",
      "Iteration 346, loss = 0.38713059\n",
      "Iteration 347, loss = 0.38673123\n",
      "Iteration 348, loss = 0.38633254\n",
      "Iteration 349, loss = 0.38593453\n",
      "Iteration 350, loss = 0.38553729\n",
      "Iteration 351, loss = 0.38514081\n",
      "Iteration 352, loss = 0.38474502\n",
      "Iteration 353, loss = 0.38434988\n",
      "Iteration 354, loss = 0.38395541\n",
      "Iteration 355, loss = 0.38356159\n",
      "Iteration 356, loss = 0.38316841\n",
      "Iteration 357, loss = 0.38277587\n",
      "Iteration 358, loss = 0.38238397\n",
      "Iteration 359, loss = 0.38199271\n",
      "Iteration 360, loss = 0.38160214\n",
      "Iteration 361, loss = 0.38121220\n",
      "Iteration 362, loss = 0.38082289\n",
      "Iteration 363, loss = 0.38043419\n",
      "Iteration 364, loss = 0.38004614\n",
      "Iteration 365, loss = 0.37965870\n",
      "Iteration 366, loss = 0.37927275\n",
      "Iteration 367, loss = 0.37888885\n",
      "Iteration 368, loss = 0.37850565\n",
      "Iteration 369, loss = 0.37812311\n",
      "Iteration 370, loss = 0.37774124\n",
      "Iteration 371, loss = 0.37736002\n",
      "Iteration 372, loss = 0.37697945\n",
      "Iteration 373, loss = 0.37659953\n",
      "Iteration 374, loss = 0.37622024\n",
      "Iteration 375, loss = 0.37584157\n",
      "Iteration 376, loss = 0.37546352\n",
      "Iteration 377, loss = 0.37508608\n",
      "Iteration 378, loss = 0.37470924\n",
      "Iteration 379, loss = 0.37433299\n",
      "Iteration 380, loss = 0.37395732\n",
      "Iteration 381, loss = 0.37358223\n",
      "Iteration 382, loss = 0.37320771\n",
      "Iteration 383, loss = 0.37283439\n",
      "Iteration 384, loss = 0.37246254\n",
      "Iteration 385, loss = 0.37209122\n",
      "Iteration 386, loss = 0.37172045\n",
      "Iteration 387, loss = 0.37135020\n",
      "Iteration 388, loss = 0.37098049\n",
      "Iteration 389, loss = 0.37061131\n",
      "Iteration 390, loss = 0.37024265\n",
      "Iteration 391, loss = 0.36987450\n",
      "Iteration 392, loss = 0.36950687\n",
      "Iteration 393, loss = 0.36913975\n",
      "Iteration 394, loss = 0.36877313\n",
      "Iteration 395, loss = 0.36840701\n",
      "Iteration 396, loss = 0.36804138\n",
      "Iteration 397, loss = 0.36767624\n",
      "Iteration 398, loss = 0.36731159\n",
      "Iteration 399, loss = 0.36694742\n",
      "Iteration 400, loss = 0.36658373\n",
      "Iteration 401, loss = 0.36622051\n",
      "Iteration 402, loss = 0.36585776\n",
      "Iteration 403, loss = 0.36549548\n",
      "Iteration 404, loss = 0.36513367\n",
      "Iteration 405, loss = 0.36477232\n",
      "Iteration 406, loss = 0.36441144\n",
      "Iteration 407, loss = 0.36405110\n",
      "Iteration 408, loss = 0.36369141\n",
      "Iteration 409, loss = 0.36333217\n",
      "Iteration 410, loss = 0.36297340\n",
      "Iteration 411, loss = 0.36261509\n",
      "Iteration 412, loss = 0.36225723\n",
      "Iteration 413, loss = 0.36189984\n",
      "Iteration 414, loss = 0.36154290\n",
      "Iteration 415, loss = 0.36118643\n",
      "Iteration 416, loss = 0.36083041\n",
      "Iteration 417, loss = 0.36047485\n",
      "Iteration 418, loss = 0.36011975\n",
      "Iteration 419, loss = 0.35976510\n",
      "Iteration 420, loss = 0.35941090\n",
      "Iteration 421, loss = 0.35905715\n",
      "Iteration 422, loss = 0.35870386\n",
      "Iteration 423, loss = 0.35835102\n",
      "Iteration 424, loss = 0.35799862\n",
      "Iteration 425, loss = 0.35764666\n",
      "Iteration 426, loss = 0.35729515\n",
      "Iteration 427, loss = 0.35694409\n",
      "Iteration 428, loss = 0.35659346\n",
      "Iteration 429, loss = 0.35624327\n",
      "Iteration 430, loss = 0.35589352\n",
      "Iteration 431, loss = 0.35554421\n",
      "Iteration 432, loss = 0.35519533\n",
      "Iteration 433, loss = 0.35484688\n",
      "Iteration 434, loss = 0.35449886\n",
      "Iteration 435, loss = 0.35415126\n",
      "Iteration 436, loss = 0.35380410\n",
      "Iteration 437, loss = 0.35345736\n",
      "Iteration 438, loss = 0.35311105\n",
      "Iteration 439, loss = 0.35276516\n",
      "Iteration 440, loss = 0.35241969\n",
      "Iteration 441, loss = 0.35207464\n",
      "Iteration 442, loss = 0.35173000\n",
      "Iteration 443, loss = 0.35138579\n",
      "Iteration 444, loss = 0.35104199\n",
      "Iteration 445, loss = 0.35069861\n",
      "Iteration 446, loss = 0.35035564\n",
      "Iteration 447, loss = 0.35001308\n",
      "Iteration 448, loss = 0.34967093\n",
      "Iteration 449, loss = 0.34932920\n",
      "Iteration 450, loss = 0.34898787\n",
      "Iteration 451, loss = 0.34864695\n",
      "Iteration 452, loss = 0.34830644\n",
      "Iteration 453, loss = 0.34796633\n",
      "Iteration 454, loss = 0.34762663\n",
      "Iteration 455, loss = 0.34728733\n",
      "Iteration 456, loss = 0.34694843\n",
      "Iteration 457, loss = 0.34660994\n",
      "Iteration 458, loss = 0.34627185\n",
      "Iteration 459, loss = 0.34593415\n",
      "Iteration 460, loss = 0.34559686\n",
      "Iteration 461, loss = 0.34525996\n",
      "Iteration 462, loss = 0.34492347\n",
      "Iteration 463, loss = 0.34458736\n",
      "Iteration 464, loss = 0.34425166\n",
      "Iteration 465, loss = 0.34391635\n",
      "Iteration 466, loss = 0.34358143\n",
      "Iteration 467, loss = 0.34324691\n",
      "Iteration 468, loss = 0.34291278\n",
      "Iteration 469, loss = 0.34257905\n",
      "Iteration 470, loss = 0.34224570\n",
      "Iteration 471, loss = 0.34191275\n",
      "Iteration 472, loss = 0.34158019\n",
      "Iteration 473, loss = 0.34124802\n",
      "Iteration 474, loss = 0.34091623\n",
      "Iteration 475, loss = 0.34058484\n",
      "Iteration 476, loss = 0.34025383\n",
      "Iteration 477, loss = 0.33992321\n",
      "Iteration 478, loss = 0.33959298\n",
      "Iteration 479, loss = 0.33926313\n",
      "Iteration 480, loss = 0.33893367\n",
      "Iteration 481, loss = 0.33860459\n",
      "Iteration 482, loss = 0.33827590\n",
      "Iteration 483, loss = 0.33794760\n",
      "Iteration 484, loss = 0.33761968\n",
      "Iteration 485, loss = 0.33729214\n",
      "Iteration 486, loss = 0.33696499\n",
      "Iteration 487, loss = 0.33663821\n",
      "Iteration 488, loss = 0.33631182\n",
      "Iteration 489, loss = 0.33598582\n",
      "Iteration 490, loss = 0.33566019\n",
      "Iteration 491, loss = 0.33533494\n",
      "Iteration 492, loss = 0.33501008\n",
      "Iteration 493, loss = 0.33468560\n",
      "Iteration 494, loss = 0.33436149\n",
      "Iteration 495, loss = 0.33403777\n",
      "Iteration 496, loss = 0.33371443\n",
      "Iteration 497, loss = 0.33339146\n",
      "Iteration 498, loss = 0.33306888\n",
      "Iteration 499, loss = 0.33274667\n",
      "Iteration 500, loss = 0.33242484\n",
      "Iteration 501, loss = 0.33210339\n",
      "Iteration 502, loss = 0.33178232\n",
      "Iteration 503, loss = 0.33146162\n",
      "Iteration 504, loss = 0.33114131\n",
      "Iteration 505, loss = 0.33082137\n",
      "Iteration 506, loss = 0.33050180\n",
      "Iteration 507, loss = 0.33018262\n",
      "Iteration 508, loss = 0.32986381\n",
      "Iteration 509, loss = 0.32954538\n",
      "Iteration 510, loss = 0.32922732\n",
      "Iteration 511, loss = 0.32890964\n",
      "Iteration 512, loss = 0.32859233\n",
      "Iteration 513, loss = 0.32827541\n",
      "Iteration 514, loss = 0.32795885\n",
      "Iteration 515, loss = 0.32764267\n",
      "Iteration 516, loss = 0.32732687\n",
      "Iteration 517, loss = 0.32701144\n",
      "Iteration 518, loss = 0.32669639\n",
      "Iteration 519, loss = 0.32638204\n",
      "Iteration 520, loss = 0.32606812\n",
      "Iteration 521, loss = 0.32575458\n",
      "Iteration 522, loss = 0.32544141\n",
      "Iteration 523, loss = 0.32512862\n",
      "Iteration 524, loss = 0.32481621\n",
      "Iteration 525, loss = 0.32450419\n",
      "Iteration 526, loss = 0.32419254\n",
      "Iteration 527, loss = 0.32388127\n",
      "Iteration 528, loss = 0.32357038\n",
      "Iteration 529, loss = 0.32325987\n",
      "Iteration 530, loss = 0.32294975\n",
      "Iteration 531, loss = 0.32264002\n",
      "Iteration 532, loss = 0.32233070\n",
      "Iteration 533, loss = 0.32202184\n",
      "Iteration 534, loss = 0.32171329\n",
      "Iteration 535, loss = 0.32140513\n",
      "Iteration 536, loss = 0.32109738\n",
      "Iteration 537, loss = 0.32079000\n",
      "Iteration 538, loss = 0.32048305\n",
      "Iteration 539, loss = 0.32017644\n",
      "Iteration 540, loss = 0.31987021\n",
      "Iteration 541, loss = 0.31956435\n",
      "Iteration 542, loss = 0.31925892\n",
      "Iteration 543, loss = 0.31895383\n",
      "Iteration 544, loss = 0.31864911\n",
      "Iteration 545, loss = 0.31834483\n",
      "Iteration 546, loss = 0.31804086\n",
      "Iteration 547, loss = 0.31773729\n",
      "Iteration 548, loss = 0.31743414\n",
      "Iteration 549, loss = 0.31713132\n",
      "Iteration 550, loss = 0.31682890\n",
      "Iteration 551, loss = 0.31652684\n",
      "Iteration 552, loss = 0.31622524\n",
      "Iteration 553, loss = 0.31592391\n",
      "Iteration 554, loss = 0.31562300\n",
      "Iteration 555, loss = 0.31532249\n",
      "Iteration 556, loss = 0.31502235\n",
      "Iteration 557, loss = 0.31472258\n",
      "Iteration 558, loss = 0.31442318\n",
      "Iteration 559, loss = 0.31412415\n",
      "Iteration 560, loss = 0.31382558\n",
      "Iteration 561, loss = 0.31352727\n",
      "Iteration 562, loss = 0.31322938\n",
      "Iteration 563, loss = 0.31293192\n",
      "Iteration 564, loss = 0.31263478\n",
      "Iteration 565, loss = 0.31233803\n",
      "Iteration 566, loss = 0.31204165\n",
      "Iteration 567, loss = 0.31174567\n",
      "Iteration 568, loss = 0.31145006\n",
      "Iteration 569, loss = 0.31115481\n",
      "Iteration 570, loss = 0.31085994\n",
      "Iteration 571, loss = 0.31056547\n",
      "Iteration 572, loss = 0.31027136\n",
      "Iteration 573, loss = 0.30997762\n",
      "Iteration 574, loss = 0.30968425\n",
      "Iteration 575, loss = 0.30939132\n",
      "Iteration 576, loss = 0.30909868\n",
      "Iteration 577, loss = 0.30880644\n",
      "Iteration 578, loss = 0.30851461\n",
      "Iteration 579, loss = 0.30822314\n",
      "Iteration 580, loss = 0.30793203\n",
      "Iteration 581, loss = 0.30764130\n",
      "Iteration 582, loss = 0.30735096\n",
      "Iteration 583, loss = 0.30706099\n",
      "Iteration 584, loss = 0.30677139\n",
      "Iteration 585, loss = 0.30648215\n",
      "Iteration 586, loss = 0.30619335\n",
      "Iteration 587, loss = 0.30590485\n",
      "Iteration 588, loss = 0.30561674\n",
      "Iteration 589, loss = 0.30532905\n",
      "Iteration 590, loss = 0.30504170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 591, loss = 0.30475473\n",
      "Iteration 592, loss = 0.30446812\n",
      "Iteration 593, loss = 0.30418192\n",
      "Iteration 594, loss = 0.30389607\n",
      "Iteration 595, loss = 0.30361060\n",
      "Iteration 596, loss = 0.30332549\n",
      "Iteration 597, loss = 0.30304081\n",
      "Iteration 598, loss = 0.30275646\n",
      "Iteration 599, loss = 0.30247248\n",
      "Iteration 600, loss = 0.30218887\n",
      "Iteration 601, loss = 0.30190568\n",
      "Iteration 602, loss = 0.30162283\n",
      "Iteration 603, loss = 0.30134035\n",
      "Iteration 604, loss = 0.30105828\n",
      "Iteration 605, loss = 0.30077654\n",
      "Iteration 606, loss = 0.30049519\n",
      "Iteration 607, loss = 0.30021424\n",
      "Iteration 608, loss = 0.29993364\n",
      "Iteration 609, loss = 0.29965341\n",
      "Iteration 610, loss = 0.29937355\n",
      "Iteration 611, loss = 0.29909412\n",
      "Iteration 612, loss = 0.29881499\n",
      "Iteration 613, loss = 0.29853625\n",
      "Iteration 614, loss = 0.29825793\n",
      "Iteration 615, loss = 0.29797995\n",
      "Iteration 616, loss = 0.29770234\n",
      "Iteration 617, loss = 0.29742510\n",
      "Iteration 618, loss = 0.29714829\n",
      "Iteration 619, loss = 0.29687190\n",
      "Iteration 620, loss = 0.29659595\n",
      "Iteration 621, loss = 0.29632041\n",
      "Iteration 622, loss = 0.29604519\n",
      "Iteration 623, loss = 0.29577035\n",
      "Iteration 624, loss = 0.29549586\n",
      "Iteration 625, loss = 0.29522181\n",
      "Iteration 626, loss = 0.29494803\n",
      "Iteration 627, loss = 0.29467466\n",
      "Iteration 628, loss = 0.29440171\n",
      "Iteration 629, loss = 0.29412908\n",
      "Iteration 630, loss = 0.29385683\n",
      "Iteration 631, loss = 0.29358496\n",
      "Iteration 632, loss = 0.29331350\n",
      "Iteration 633, loss = 0.29304238\n",
      "Iteration 634, loss = 0.29277163\n",
      "Iteration 635, loss = 0.29250125\n",
      "Iteration 636, loss = 0.29223133\n",
      "Iteration 637, loss = 0.29196169\n",
      "Iteration 638, loss = 0.29169251\n",
      "Iteration 639, loss = 0.29142369\n",
      "Iteration 640, loss = 0.29115524\n",
      "Iteration 641, loss = 0.29088716\n",
      "Iteration 642, loss = 0.29061944\n",
      "Iteration 643, loss = 0.29035216\n",
      "Iteration 644, loss = 0.29008519\n",
      "Iteration 645, loss = 0.28981860\n",
      "Iteration 646, loss = 0.28955245\n",
      "Iteration 647, loss = 0.28928663\n",
      "Iteration 648, loss = 0.28902119\n",
      "Iteration 649, loss = 0.28875610\n",
      "Iteration 650, loss = 0.28849140\n",
      "Iteration 651, loss = 0.28822709\n",
      "Iteration 652, loss = 0.28796313\n",
      "Iteration 653, loss = 0.28769954\n",
      "Iteration 654, loss = 0.28743639\n",
      "Iteration 655, loss = 0.28717356\n",
      "Iteration 656, loss = 0.28691110\n",
      "Iteration 657, loss = 0.28664901\n",
      "Iteration 658, loss = 0.28638733\n",
      "Iteration 659, loss = 0.28612599\n",
      "Iteration 660, loss = 0.28586502\n",
      "Iteration 661, loss = 0.28560447\n",
      "Iteration 662, loss = 0.28534424\n",
      "Iteration 663, loss = 0.28508439\n",
      "Iteration 664, loss = 0.28482493\n",
      "Iteration 665, loss = 0.28456586\n",
      "Iteration 666, loss = 0.28430713\n",
      "Iteration 667, loss = 0.28404877\n",
      "Iteration 668, loss = 0.28379079\n",
      "Iteration 669, loss = 0.28353321\n",
      "Iteration 670, loss = 0.28327598\n",
      "Iteration 671, loss = 0.28301910\n",
      "Iteration 672, loss = 0.28276263\n",
      "Iteration 673, loss = 0.28250651\n",
      "Iteration 674, loss = 0.28225076\n",
      "Iteration 675, loss = 0.28199537\n",
      "Iteration 676, loss = 0.28174040\n",
      "Iteration 677, loss = 0.28148576\n",
      "Iteration 678, loss = 0.28123149\n",
      "Iteration 679, loss = 0.28097758\n",
      "Iteration 680, loss = 0.28072411\n",
      "Iteration 681, loss = 0.28047090\n",
      "Iteration 682, loss = 0.28021814\n",
      "Iteration 683, loss = 0.27996574\n",
      "Iteration 684, loss = 0.27971369\n",
      "Iteration 685, loss = 0.27946202\n",
      "Iteration 686, loss = 0.27921070\n",
      "Iteration 687, loss = 0.27895979\n",
      "Iteration 688, loss = 0.27870920\n",
      "Iteration 689, loss = 0.27845900\n",
      "Iteration 690, loss = 0.27820919\n",
      "Iteration 691, loss = 0.27795972\n",
      "Iteration 692, loss = 0.27771063\n",
      "Iteration 693, loss = 0.27746198\n",
      "Iteration 694, loss = 0.27721360\n",
      "Iteration 695, loss = 0.27696565\n",
      "Iteration 696, loss = 0.27671806\n",
      "Iteration 697, loss = 0.27647083\n",
      "Iteration 698, loss = 0.27622399\n",
      "Iteration 699, loss = 0.27597749\n",
      "Iteration 700, loss = 0.27573135\n",
      "Iteration 701, loss = 0.27548568\n",
      "Iteration 702, loss = 0.27524021\n",
      "Iteration 703, loss = 0.27499523\n",
      "Iteration 704, loss = 0.27475060\n",
      "Iteration 705, loss = 0.27450632\n",
      "Iteration 706, loss = 0.27426239\n",
      "Iteration 707, loss = 0.27401882\n",
      "Iteration 708, loss = 0.27377562\n",
      "Iteration 709, loss = 0.27353286\n",
      "Iteration 710, loss = 0.27329036\n",
      "Iteration 711, loss = 0.27304826\n",
      "Iteration 712, loss = 0.27280659\n",
      "Iteration 713, loss = 0.27256517\n",
      "Iteration 714, loss = 0.27232420\n",
      "Iteration 715, loss = 0.27208361\n",
      "Iteration 716, loss = 0.27184335\n",
      "Iteration 717, loss = 0.27160343\n",
      "Iteration 718, loss = 0.27136387\n",
      "Iteration 719, loss = 0.27112467\n",
      "Iteration 720, loss = 0.27088591\n",
      "Iteration 721, loss = 0.27064743\n",
      "Iteration 722, loss = 0.27040933\n",
      "Iteration 723, loss = 0.27017166\n",
      "Iteration 724, loss = 0.26993425\n",
      "Iteration 725, loss = 0.26969726\n",
      "Iteration 726, loss = 0.26946066\n",
      "Iteration 727, loss = 0.26922438\n",
      "Iteration 728, loss = 0.26898846\n",
      "Iteration 729, loss = 0.26875288\n",
      "Iteration 730, loss = 0.26851770\n",
      "Iteration 731, loss = 0.26828287\n",
      "Iteration 732, loss = 0.26804838\n",
      "Iteration 733, loss = 0.26781429\n",
      "Iteration 734, loss = 0.26758052\n",
      "Iteration 735, loss = 0.26734712\n",
      "Iteration 736, loss = 0.26711410\n",
      "Iteration 737, loss = 0.26688144\n",
      "Iteration 738, loss = 0.26664912\n",
      "Iteration 739, loss = 0.26641714\n",
      "Iteration 740, loss = 0.26618557\n",
      "Iteration 741, loss = 0.26595431\n",
      "Iteration 742, loss = 0.26572342\n",
      "Iteration 743, loss = 0.26549292\n",
      "Iteration 744, loss = 0.26526274\n",
      "Iteration 745, loss = 0.26503291\n",
      "Iteration 746, loss = 0.26480346\n",
      "Iteration 747, loss = 0.26457437\n",
      "Iteration 748, loss = 0.26434561\n",
      "Iteration 749, loss = 0.26411727\n",
      "Iteration 750, loss = 0.26388922\n",
      "Iteration 751, loss = 0.26366152\n",
      "Iteration 752, loss = 0.26343421\n",
      "Iteration 753, loss = 0.26320724\n",
      "Iteration 754, loss = 0.26298063\n",
      "Iteration 755, loss = 0.26275438\n",
      "Iteration 756, loss = 0.26252847\n",
      "Iteration 757, loss = 0.26230290\n",
      "Iteration 758, loss = 0.26207772\n",
      "Iteration 759, loss = 0.26185290\n",
      "Iteration 760, loss = 0.26162839\n",
      "Iteration 761, loss = 0.26140425\n",
      "Iteration 762, loss = 0.26118044\n",
      "Iteration 763, loss = 0.26095705\n",
      "Iteration 764, loss = 0.26073391\n",
      "Iteration 765, loss = 0.26051125\n",
      "Iteration 766, loss = 0.26028883\n",
      "Iteration 767, loss = 0.26006679\n",
      "Iteration 768, loss = 0.25984514\n",
      "Iteration 769, loss = 0.25962378\n",
      "Iteration 770, loss = 0.25940280\n",
      "Iteration 771, loss = 0.25918220\n",
      "Iteration 772, loss = 0.25896191\n",
      "Iteration 773, loss = 0.25874197\n",
      "Iteration 774, loss = 0.25852240\n",
      "Iteration 775, loss = 0.25830314\n",
      "Iteration 776, loss = 0.25808429\n",
      "Iteration 777, loss = 0.25786570\n",
      "Iteration 778, loss = 0.25764755\n",
      "Iteration 779, loss = 0.25742970\n",
      "Iteration 780, loss = 0.25721224\n",
      "Iteration 781, loss = 0.25699511\n",
      "Iteration 782, loss = 0.25677829\n",
      "Iteration 783, loss = 0.25656185\n",
      "Iteration 784, loss = 0.25634575\n",
      "Iteration 785, loss = 0.25613003\n",
      "Iteration 786, loss = 0.25591463\n",
      "Iteration 787, loss = 0.25569954\n",
      "Iteration 788, loss = 0.25548487\n",
      "Iteration 789, loss = 0.25527048\n",
      "Iteration 790, loss = 0.25505647\n",
      "Iteration 791, loss = 0.25484277\n",
      "Iteration 792, loss = 0.25462944\n",
      "Iteration 793, loss = 0.25441645\n",
      "Iteration 794, loss = 0.25420378\n",
      "Iteration 795, loss = 0.25399149\n",
      "Iteration 796, loss = 0.25377950\n",
      "Iteration 797, loss = 0.25356791\n",
      "Iteration 798, loss = 0.25335659\n",
      "Iteration 799, loss = 0.25314565\n",
      "Iteration 800, loss = 0.25293505\n",
      "Iteration 801, loss = 0.25272481\n",
      "Iteration 802, loss = 0.25251486\n",
      "Iteration 803, loss = 0.25230529\n",
      "Iteration 804, loss = 0.25209602\n",
      "Iteration 805, loss = 0.25188714\n",
      "Iteration 806, loss = 0.25167854\n",
      "Iteration 807, loss = 0.25147034\n",
      "Iteration 808, loss = 0.25126244\n",
      "Iteration 809, loss = 0.25105490\n",
      "Iteration 810, loss = 0.25084765\n",
      "Iteration 811, loss = 0.25064080\n",
      "Iteration 812, loss = 0.25043424\n",
      "Iteration 813, loss = 0.25022805\n",
      "Iteration 814, loss = 0.25002215\n",
      "Iteration 815, loss = 0.24981662\n",
      "Iteration 816, loss = 0.24961140\n",
      "Iteration 817, loss = 0.24940654\n",
      "Iteration 818, loss = 0.24920201\n",
      "Iteration 819, loss = 0.24899781\n",
      "Iteration 820, loss = 0.24879395\n",
      "Iteration 821, loss = 0.24859040\n",
      "Iteration 822, loss = 0.24838722\n",
      "Iteration 823, loss = 0.24818433\n",
      "Iteration 824, loss = 0.24798181\n",
      "Iteration 825, loss = 0.24777956\n",
      "Iteration 826, loss = 0.24757773\n",
      "Iteration 827, loss = 0.24737614\n",
      "Iteration 828, loss = 0.24717496\n",
      "Iteration 829, loss = 0.24697407\n",
      "Iteration 830, loss = 0.24677349\n",
      "Iteration 831, loss = 0.24657330\n",
      "Iteration 832, loss = 0.24637337\n",
      "Iteration 833, loss = 0.24617383\n",
      "Iteration 834, loss = 0.24597459\n",
      "Iteration 835, loss = 0.24577566\n",
      "Iteration 836, loss = 0.24557712\n",
      "Iteration 837, loss = 0.24537885\n",
      "Iteration 838, loss = 0.24518090\n",
      "Iteration 839, loss = 0.24498336\n",
      "Iteration 840, loss = 0.24478607\n",
      "Iteration 841, loss = 0.24458912\n",
      "Iteration 842, loss = 0.24439249\n",
      "Iteration 843, loss = 0.24419617\n",
      "Iteration 844, loss = 0.24400024\n",
      "Iteration 845, loss = 0.24380455\n",
      "Iteration 846, loss = 0.24360926\n",
      "Iteration 847, loss = 0.24341425\n",
      "Iteration 848, loss = 0.24321955\n",
      "Iteration 849, loss = 0.24302519\n",
      "Iteration 850, loss = 0.24283115\n",
      "Iteration 851, loss = 0.24263742\n",
      "Iteration 852, loss = 0.24244405\n",
      "Iteration 853, loss = 0.24225097\n",
      "Iteration 854, loss = 0.24205821\n",
      "Iteration 855, loss = 0.24186578\n",
      "Iteration 856, loss = 0.24167367\n",
      "Iteration 857, loss = 0.24148185\n",
      "Iteration 858, loss = 0.24129041\n",
      "Iteration 859, loss = 0.24109921\n",
      "Iteration 860, loss = 0.24090837\n",
      "Iteration 861, loss = 0.24071786\n",
      "Iteration 862, loss = 0.24052763\n",
      "Iteration 863, loss = 0.24033774\n",
      "Iteration 864, loss = 0.24014817\n",
      "Iteration 865, loss = 0.23995890\n",
      "Iteration 866, loss = 0.23976994\n",
      "Iteration 867, loss = 0.23958133\n",
      "Iteration 868, loss = 0.23939300\n",
      "Iteration 869, loss = 0.23920500\n",
      "Iteration 870, loss = 0.23901730\n",
      "Iteration 871, loss = 0.23882993\n",
      "Iteration 872, loss = 0.23864286\n",
      "Iteration 873, loss = 0.23845610\n",
      "Iteration 874, loss = 0.23826965\n",
      "Iteration 875, loss = 0.23808354\n",
      "Iteration 876, loss = 0.23789770\n",
      "Iteration 877, loss = 0.23771219\n",
      "Iteration 878, loss = 0.23752701\n",
      "Iteration 879, loss = 0.23734210\n",
      "Iteration 880, loss = 0.23715751\n",
      "Iteration 881, loss = 0.23697325\n",
      "Iteration 882, loss = 0.23678928\n",
      "Iteration 883, loss = 0.23660562\n",
      "Iteration 884, loss = 0.23642227\n",
      "Iteration 885, loss = 0.23623921\n",
      "Iteration 886, loss = 0.23605652\n",
      "Iteration 887, loss = 0.23587405\n",
      "Iteration 888, loss = 0.23569193\n",
      "Iteration 889, loss = 0.23551010\n",
      "Iteration 890, loss = 0.23532863\n",
      "Iteration 891, loss = 0.23514739\n",
      "Iteration 892, loss = 0.23496648\n",
      "Iteration 893, loss = 0.23478588\n",
      "Iteration 894, loss = 0.23460557\n",
      "Iteration 895, loss = 0.23442560\n",
      "Iteration 896, loss = 0.23424589\n",
      "Iteration 897, loss = 0.23406650\n",
      "Iteration 898, loss = 0.23388740\n",
      "Iteration 899, loss = 0.23370862\n",
      "Iteration 900, loss = 0.23353013\n",
      "Iteration 901, loss = 0.23335195\n",
      "Iteration 902, loss = 0.23317404\n",
      "Iteration 903, loss = 0.23299645\n",
      "Iteration 904, loss = 0.23281915\n",
      "Iteration 905, loss = 0.23264218\n",
      "Iteration 906, loss = 0.23246546\n",
      "Iteration 907, loss = 0.23228907\n",
      "Iteration 908, loss = 0.23211296\n",
      "Iteration 909, loss = 0.23193716\n",
      "Iteration 910, loss = 0.23176164\n",
      "Iteration 911, loss = 0.23158643\n",
      "Iteration 912, loss = 0.23141152\n",
      "Iteration 913, loss = 0.23123689\n",
      "Iteration 914, loss = 0.23106257\n",
      "Iteration 915, loss = 0.23088852\n",
      "Iteration 916, loss = 0.23071479\n",
      "Iteration 917, loss = 0.23054133\n",
      "Iteration 918, loss = 0.23036818\n",
      "Iteration 919, loss = 0.23019532\n",
      "Iteration 920, loss = 0.23002274\n",
      "Iteration 921, loss = 0.22985046\n",
      "Iteration 922, loss = 0.22967847\n",
      "Iteration 923, loss = 0.22950677\n",
      "Iteration 924, loss = 0.22933537\n",
      "Iteration 925, loss = 0.22916424\n",
      "Iteration 926, loss = 0.22899341\n",
      "Iteration 927, loss = 0.22882286\n",
      "Iteration 928, loss = 0.22865261\n",
      "Iteration 929, loss = 0.22848264\n",
      "Iteration 930, loss = 0.22831297\n",
      "Iteration 931, loss = 0.22814358\n",
      "Iteration 932, loss = 0.22797447\n",
      "Iteration 933, loss = 0.22780565\n",
      "Iteration 934, loss = 0.22763712\n",
      "Iteration 935, loss = 0.22746886\n",
      "Iteration 936, loss = 0.22730090\n",
      "Iteration 937, loss = 0.22713322\n",
      "Iteration 938, loss = 0.22696582\n",
      "Iteration 939, loss = 0.22679870\n",
      "Iteration 940, loss = 0.22663188\n",
      "Iteration 941, loss = 0.22646533\n",
      "Iteration 942, loss = 0.22629906\n",
      "Iteration 943, loss = 0.22613307\n",
      "Iteration 944, loss = 0.22596738\n",
      "Iteration 945, loss = 0.22580194\n",
      "Iteration 946, loss = 0.22563682\n",
      "Iteration 947, loss = 0.22547194\n",
      "Iteration 948, loss = 0.22530736\n",
      "Iteration 949, loss = 0.22514305\n",
      "Iteration 950, loss = 0.22497903\n",
      "Iteration 951, loss = 0.22481528\n",
      "Iteration 952, loss = 0.22465181\n",
      "Iteration 953, loss = 0.22448861\n",
      "Iteration 954, loss = 0.22432571\n",
      "Iteration 955, loss = 0.22416306\n",
      "Iteration 956, loss = 0.22400068\n",
      "Iteration 957, loss = 0.22383860\n",
      "Iteration 958, loss = 0.22367678\n",
      "Iteration 959, loss = 0.22351524\n",
      "Iteration 960, loss = 0.22335397\n",
      "Iteration 961, loss = 0.22319298\n",
      "Iteration 962, loss = 0.22303225\n",
      "Iteration 963, loss = 0.22287181\n",
      "Iteration 964, loss = 0.22271162\n",
      "Iteration 965, loss = 0.22255173\n",
      "Iteration 966, loss = 0.22239209\n",
      "Iteration 967, loss = 0.22223272\n",
      "Iteration 968, loss = 0.22207362\n",
      "Iteration 969, loss = 0.22191480\n",
      "Iteration 970, loss = 0.22175624\n",
      "Iteration 971, loss = 0.22159797\n",
      "Iteration 972, loss = 0.22143994\n",
      "Iteration 973, loss = 0.22128220\n",
      "Iteration 974, loss = 0.22112471\n",
      "Iteration 975, loss = 0.22096750\n",
      "Iteration 976, loss = 0.22081054\n",
      "Iteration 977, loss = 0.22065387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 978, loss = 0.22049745\n",
      "Iteration 979, loss = 0.22034129\n",
      "Iteration 980, loss = 0.22018541\n",
      "Iteration 981, loss = 0.22002979\n",
      "Iteration 982, loss = 0.21987442\n",
      "Iteration 983, loss = 0.21971933\n",
      "Iteration 984, loss = 0.21956450\n",
      "Iteration 985, loss = 0.21940993\n",
      "Iteration 986, loss = 0.21925562\n",
      "Iteration 987, loss = 0.21910157\n",
      "Iteration 988, loss = 0.21894779\n",
      "Iteration 989, loss = 0.21879426\n",
      "Iteration 990, loss = 0.21864100\n",
      "Iteration 991, loss = 0.21848799\n",
      "Iteration 992, loss = 0.21833525\n",
      "Iteration 993, loss = 0.21818276\n",
      "Iteration 994, loss = 0.21803053\n",
      "Iteration 995, loss = 0.21787855\n",
      "Iteration 996, loss = 0.21772685\n",
      "Iteration 997, loss = 0.21757538\n",
      "Iteration 998, loss = 0.21742419\n",
      "Iteration 999, loss = 0.21727324\n",
      "Iteration 1000, loss = 0.21712255\n",
      "Iteration 1, loss = 1.36148097\n",
      "Iteration 2, loss = 1.32450113\n",
      "Iteration 3, loss = 1.27472347\n",
      "Iteration 4, loss = 1.21631650\n",
      "Iteration 5, loss = 1.15351700\n",
      "Iteration 6, loss = 1.09020961\n",
      "Iteration 7, loss = 1.02958053\n",
      "Iteration 8, loss = 0.97428959\n",
      "Iteration 9, loss = 0.92596325\n",
      "Iteration 10, loss = 0.88538384\n",
      "Iteration 11, loss = 0.85255106\n",
      "Iteration 12, loss = 0.82656239\n",
      "Iteration 13, loss = 0.80638828\n",
      "Iteration 14, loss = 0.79075824\n",
      "Iteration 15, loss = 0.77849601\n",
      "Iteration 16, loss = 0.76869616\n",
      "Iteration 17, loss = 0.76063668\n",
      "Iteration 18, loss = 0.75384468\n",
      "Iteration 19, loss = 0.74822788\n",
      "Iteration 20, loss = 0.74346250\n",
      "Iteration 21, loss = 0.73952440\n",
      "Iteration 22, loss = 0.73595039\n",
      "Iteration 23, loss = 0.73212255\n",
      "Iteration 24, loss = 0.72798934\n",
      "Iteration 25, loss = 0.72340162\n",
      "Iteration 26, loss = 0.71826116\n",
      "Iteration 27, loss = 0.71284127\n",
      "Iteration 28, loss = 0.70723956\n",
      "Iteration 29, loss = 0.70152970\n",
      "Iteration 30, loss = 0.69589369\n",
      "Iteration 31, loss = 0.69043535\n",
      "Iteration 32, loss = 0.68525653\n",
      "Iteration 33, loss = 0.68044306\n",
      "Iteration 34, loss = 0.67598206\n",
      "Iteration 35, loss = 0.67189548\n",
      "Iteration 36, loss = 0.66815188\n",
      "Iteration 37, loss = 0.66470241\n",
      "Iteration 38, loss = 0.66151842\n",
      "Iteration 39, loss = 0.65856630\n",
      "Iteration 40, loss = 0.65578194\n",
      "Iteration 41, loss = 0.65312172\n",
      "Iteration 42, loss = 0.65055373\n",
      "Iteration 43, loss = 0.64805211\n",
      "Iteration 44, loss = 0.64558872\n",
      "Iteration 45, loss = 0.64314527\n",
      "Iteration 46, loss = 0.64070663\n",
      "Iteration 47, loss = 0.63826215\n",
      "Iteration 48, loss = 0.63581004\n",
      "Iteration 49, loss = 0.63335199\n",
      "Iteration 50, loss = 0.63089220\n",
      "Iteration 51, loss = 0.62843649\n",
      "Iteration 52, loss = 0.62599146\n",
      "Iteration 53, loss = 0.62356382\n",
      "Iteration 54, loss = 0.62115982\n",
      "Iteration 55, loss = 0.61878519\n",
      "Iteration 56, loss = 0.61644431\n",
      "Iteration 57, loss = 0.61413978\n",
      "Iteration 58, loss = 0.61187428\n",
      "Iteration 59, loss = 0.60964937\n",
      "Iteration 60, loss = 0.60746552\n",
      "Iteration 61, loss = 0.60532773\n",
      "Iteration 62, loss = 0.60322990\n",
      "Iteration 63, loss = 0.60116931\n",
      "Iteration 64, loss = 0.59914376\n",
      "Iteration 65, loss = 0.59715147\n",
      "Iteration 66, loss = 0.59519033\n",
      "Iteration 67, loss = 0.59325806\n",
      "Iteration 68, loss = 0.59135273\n",
      "Iteration 69, loss = 0.58947265\n",
      "Iteration 70, loss = 0.58761996\n",
      "Iteration 71, loss = 0.58579297\n",
      "Iteration 72, loss = 0.58398890\n",
      "Iteration 73, loss = 0.58220753\n",
      "Iteration 74, loss = 0.58044706\n",
      "Iteration 75, loss = 0.57870703\n",
      "Iteration 76, loss = 0.57698709\n",
      "Iteration 77, loss = 0.57528695\n",
      "Iteration 78, loss = 0.57360639\n",
      "Iteration 79, loss = 0.57194520\n",
      "Iteration 80, loss = 0.57030319\n",
      "Iteration 81, loss = 0.56868016\n",
      "Iteration 82, loss = 0.56707588\n",
      "Iteration 83, loss = 0.56549110\n",
      "Iteration 84, loss = 0.56392971\n",
      "Iteration 85, loss = 0.56238741\n",
      "Iteration 86, loss = 0.56086309\n",
      "Iteration 87, loss = 0.55935915\n",
      "Iteration 88, loss = 0.55787241\n",
      "Iteration 89, loss = 0.55640297\n",
      "Iteration 90, loss = 0.55494931\n",
      "Iteration 91, loss = 0.55351158\n",
      "Iteration 92, loss = 0.55208926\n",
      "Iteration 93, loss = 0.55068187\n",
      "Iteration 94, loss = 0.54928894\n",
      "Iteration 95, loss = 0.54791016\n",
      "Iteration 96, loss = 0.54654519\n",
      "Iteration 97, loss = 0.54519372\n",
      "Iteration 98, loss = 0.54385578\n",
      "Iteration 99, loss = 0.54253154\n",
      "Iteration 100, loss = 0.54122005\n",
      "Iteration 101, loss = 0.53992105\n",
      "Iteration 102, loss = 0.53863427\n",
      "Iteration 103, loss = 0.53736079\n",
      "Iteration 104, loss = 0.53610146\n",
      "Iteration 105, loss = 0.53485403\n",
      "Iteration 106, loss = 0.53361826\n",
      "Iteration 107, loss = 0.53239395\n",
      "Iteration 108, loss = 0.53118091\n",
      "Iteration 109, loss = 0.52997892\n",
      "Iteration 110, loss = 0.52878777\n",
      "Iteration 111, loss = 0.52760738\n",
      "Iteration 112, loss = 0.52643766\n",
      "Iteration 113, loss = 0.52527817\n",
      "Iteration 114, loss = 0.52412874\n",
      "Iteration 115, loss = 0.52298920\n",
      "Iteration 116, loss = 0.52186256\n",
      "Iteration 117, loss = 0.52074581\n",
      "Iteration 118, loss = 0.51963885\n",
      "Iteration 119, loss = 0.51854310\n",
      "Iteration 120, loss = 0.51746315\n",
      "Iteration 121, loss = 0.51639265\n",
      "Iteration 122, loss = 0.51533186\n",
      "Iteration 123, loss = 0.51428099\n",
      "Iteration 124, loss = 0.51323882\n",
      "Iteration 125, loss = 0.51220527\n",
      "Iteration 126, loss = 0.51118029\n",
      "Iteration 127, loss = 0.51016455\n",
      "Iteration 128, loss = 0.50915701\n",
      "Iteration 129, loss = 0.50815770\n",
      "Iteration 130, loss = 0.50716657\n",
      "Iteration 131, loss = 0.50618357\n",
      "Iteration 132, loss = 0.50520863\n",
      "Iteration 133, loss = 0.50424168\n",
      "Iteration 134, loss = 0.50328262\n",
      "Iteration 135, loss = 0.50233136\n",
      "Iteration 136, loss = 0.50138778\n",
      "Iteration 137, loss = 0.50045176\n",
      "Iteration 138, loss = 0.49952319\n",
      "Iteration 139, loss = 0.49860195\n",
      "Iteration 140, loss = 0.49768792\n",
      "Iteration 141, loss = 0.49678097\n",
      "Iteration 142, loss = 0.49588098\n",
      "Iteration 143, loss = 0.49498784\n",
      "Iteration 144, loss = 0.49410143\n",
      "Iteration 145, loss = 0.49322163\n",
      "Iteration 146, loss = 0.49234831\n",
      "Iteration 147, loss = 0.49148135\n",
      "Iteration 148, loss = 0.49062063\n",
      "Iteration 149, loss = 0.48976606\n",
      "Iteration 150, loss = 0.48891772\n",
      "Iteration 151, loss = 0.48807561\n",
      "Iteration 152, loss = 0.48723942\n",
      "Iteration 153, loss = 0.48640906\n",
      "Iteration 154, loss = 0.48558443\n",
      "Iteration 155, loss = 0.48476545\n",
      "Iteration 156, loss = 0.48395203\n",
      "Iteration 157, loss = 0.48314407\n",
      "Iteration 158, loss = 0.48234148\n",
      "Iteration 159, loss = 0.48154417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 160, loss = 0.48075206\n",
      "Iteration 161, loss = 0.47996505\n",
      "Iteration 162, loss = 0.47918307\n",
      "Iteration 163, loss = 0.47840601\n",
      "Iteration 164, loss = 0.47763380\n",
      "Iteration 165, loss = 0.47686635\n",
      "Iteration 166, loss = 0.47610358\n",
      "Iteration 167, loss = 0.47534540\n",
      "Iteration 168, loss = 0.47459237\n",
      "Iteration 169, loss = 0.47384388\n",
      "Iteration 170, loss = 0.47309983\n",
      "Iteration 171, loss = 0.47236016\n",
      "Iteration 172, loss = 0.47162481\n",
      "Iteration 173, loss = 0.47089370\n",
      "Iteration 174, loss = 0.47016679\n",
      "Iteration 175, loss = 0.46944399\n",
      "Iteration 176, loss = 0.46872525\n",
      "Iteration 177, loss = 0.46801050\n",
      "Iteration 178, loss = 0.46729968\n",
      "Iteration 179, loss = 0.46659272\n",
      "Iteration 180, loss = 0.46588956\n",
      "Iteration 181, loss = 0.46519014\n",
      "Iteration 182, loss = 0.46449439\n",
      "Iteration 183, loss = 0.46380226\n",
      "Iteration 184, loss = 0.46311369\n",
      "Iteration 185, loss = 0.46242861\n",
      "Iteration 186, loss = 0.46174698\n",
      "Iteration 187, loss = 0.46106872\n",
      "Iteration 188, loss = 0.46039380\n",
      "Iteration 189, loss = 0.45972214\n",
      "Iteration 190, loss = 0.45905370\n",
      "Iteration 191, loss = 0.45838843\n",
      "Iteration 192, loss = 0.45772627\n",
      "Iteration 193, loss = 0.45706717\n",
      "Iteration 194, loss = 0.45641109\n",
      "Iteration 195, loss = 0.45575797\n",
      "Iteration 196, loss = 0.45510777\n",
      "Iteration 197, loss = 0.45446044\n",
      "Iteration 198, loss = 0.45381593\n",
      "Iteration 199, loss = 0.45317420\n",
      "Iteration 200, loss = 0.45253521\n",
      "Iteration 201, loss = 0.45189891\n",
      "Iteration 202, loss = 0.45126526\n",
      "Iteration 203, loss = 0.45063422\n",
      "Iteration 204, loss = 0.45000575\n",
      "Iteration 205, loss = 0.44937981\n",
      "Iteration 206, loss = 0.44875635\n",
      "Iteration 207, loss = 0.44813535\n",
      "Iteration 208, loss = 0.44751677\n",
      "Iteration 209, loss = 0.44690093\n",
      "Iteration 210, loss = 0.44628823\n",
      "Iteration 211, loss = 0.44567787\n",
      "Iteration 212, loss = 0.44506982\n",
      "Iteration 213, loss = 0.44446437\n",
      "Iteration 214, loss = 0.44386266\n",
      "Iteration 215, loss = 0.44326331\n",
      "Iteration 216, loss = 0.44266629\n",
      "Iteration 217, loss = 0.44207158\n",
      "Iteration 218, loss = 0.44147913\n",
      "Iteration 219, loss = 0.44088893\n",
      "Iteration 220, loss = 0.44030093\n",
      "Iteration 221, loss = 0.43971510\n",
      "Iteration 222, loss = 0.43913141\n",
      "Iteration 223, loss = 0.43855072\n",
      "Iteration 224, loss = 0.43797182\n",
      "Iteration 225, loss = 0.43739487\n",
      "Iteration 226, loss = 0.43681988\n",
      "Iteration 227, loss = 0.43624784\n",
      "Iteration 228, loss = 0.43567835\n",
      "Iteration 229, loss = 0.43511087\n",
      "Iteration 230, loss = 0.43454541\n",
      "Iteration 231, loss = 0.43398217\n",
      "Iteration 232, loss = 0.43342090\n",
      "Iteration 233, loss = 0.43286145\n",
      "Iteration 234, loss = 0.43230403\n",
      "Iteration 235, loss = 0.43174847\n",
      "Iteration 236, loss = 0.43119476\n",
      "Iteration 237, loss = 0.43064307\n",
      "Iteration 238, loss = 0.43009297\n",
      "Iteration 239, loss = 0.42954481\n",
      "Iteration 240, loss = 0.42899848\n",
      "Iteration 241, loss = 0.42845384\n",
      "Iteration 242, loss = 0.42791093\n",
      "Iteration 243, loss = 0.42736983\n",
      "Iteration 244, loss = 0.42683105\n",
      "Iteration 245, loss = 0.42629460\n",
      "Iteration 246, loss = 0.42575983\n",
      "Iteration 247, loss = 0.42522673\n",
      "Iteration 248, loss = 0.42469529\n",
      "Iteration 249, loss = 0.42416550\n",
      "Iteration 250, loss = 0.42363747\n",
      "Iteration 251, loss = 0.42311099\n",
      "Iteration 252, loss = 0.42258615\n",
      "Iteration 253, loss = 0.42206287\n",
      "Iteration 254, loss = 0.42154126\n",
      "Iteration 255, loss = 0.42102105\n",
      "Iteration 256, loss = 0.42050250\n",
      "Iteration 257, loss = 0.41998538\n",
      "Iteration 258, loss = 0.41946975\n",
      "Iteration 259, loss = 0.41895569\n",
      "Iteration 260, loss = 0.41844298\n",
      "Iteration 261, loss = 0.41793167\n",
      "Iteration 262, loss = 0.41742189\n",
      "Iteration 263, loss = 0.41691340\n",
      "Iteration 264, loss = 0.41640640\n",
      "Iteration 265, loss = 0.41590065\n",
      "Iteration 266, loss = 0.41539630\n",
      "Iteration 267, loss = 0.41489322\n",
      "Iteration 268, loss = 0.41439155\n",
      "Iteration 269, loss = 0.41389106\n",
      "Iteration 270, loss = 0.41339193\n",
      "Iteration 271, loss = 0.41289397\n",
      "Iteration 272, loss = 0.41239738\n",
      "Iteration 273, loss = 0.41190191\n",
      "Iteration 274, loss = 0.41140765\n",
      "Iteration 275, loss = 0.41091467\n",
      "Iteration 276, loss = 0.41042279\n",
      "Iteration 277, loss = 0.40993219\n",
      "Iteration 278, loss = 0.40944261\n",
      "Iteration 279, loss = 0.40895432\n",
      "Iteration 280, loss = 0.40846708\n",
      "Iteration 281, loss = 0.40798094\n",
      "Iteration 282, loss = 0.40749589\n",
      "Iteration 283, loss = 0.40701208\n",
      "Iteration 284, loss = 0.40653082\n",
      "Iteration 285, loss = 0.40605232\n",
      "Iteration 286, loss = 0.40557542\n",
      "Iteration 287, loss = 0.40509936\n",
      "Iteration 288, loss = 0.40462411\n",
      "Iteration 289, loss = 0.40414972\n",
      "Iteration 290, loss = 0.40367632\n",
      "Iteration 291, loss = 0.40320382\n",
      "Iteration 292, loss = 0.40273230\n",
      "Iteration 293, loss = 0.40226187\n",
      "Iteration 294, loss = 0.40179241\n",
      "Iteration 295, loss = 0.40132404\n",
      "Iteration 296, loss = 0.40085714\n",
      "Iteration 297, loss = 0.40039185\n",
      "Iteration 298, loss = 0.39992764\n",
      "Iteration 299, loss = 0.39946447\n",
      "Iteration 300, loss = 0.39900241\n",
      "Iteration 301, loss = 0.39854129\n",
      "Iteration 302, loss = 0.39808120\n",
      "Iteration 303, loss = 0.39762214\n",
      "Iteration 304, loss = 0.39716404\n",
      "Iteration 305, loss = 0.39670690\n",
      "Iteration 306, loss = 0.39625074\n",
      "Iteration 307, loss = 0.39579585\n",
      "Iteration 308, loss = 0.39534189\n",
      "Iteration 309, loss = 0.39488891\n",
      "Iteration 310, loss = 0.39443691\n",
      "Iteration 311, loss = 0.39398592\n",
      "Iteration 312, loss = 0.39353584\n",
      "Iteration 313, loss = 0.39308675\n",
      "Iteration 314, loss = 0.39263853\n",
      "Iteration 315, loss = 0.39219131\n",
      "Iteration 316, loss = 0.39174497\n",
      "Iteration 317, loss = 0.39129956\n",
      "Iteration 318, loss = 0.39085507\n",
      "Iteration 319, loss = 0.39041156\n",
      "Iteration 320, loss = 0.38996883\n",
      "Iteration 321, loss = 0.38952705\n",
      "Iteration 322, loss = 0.38908613\n",
      "Iteration 323, loss = 0.38864610\n",
      "Iteration 324, loss = 0.38820699\n",
      "Iteration 325, loss = 0.38776867\n",
      "Iteration 326, loss = 0.38733121\n",
      "Iteration 327, loss = 0.38689465\n",
      "Iteration 328, loss = 0.38645888\n",
      "Iteration 329, loss = 0.38602398\n",
      "Iteration 330, loss = 0.38558997\n",
      "Iteration 331, loss = 0.38515668\n",
      "Iteration 332, loss = 0.38472424\n",
      "Iteration 333, loss = 0.38429263\n",
      "Iteration 334, loss = 0.38386183\n",
      "Iteration 335, loss = 0.38343182\n",
      "Iteration 336, loss = 0.38300261\n",
      "Iteration 337, loss = 0.38257418\n",
      "Iteration 338, loss = 0.38214657\n",
      "Iteration 339, loss = 0.38171972\n",
      "Iteration 340, loss = 0.38129366\n",
      "Iteration 341, loss = 0.38086835\n",
      "Iteration 342, loss = 0.38044380\n",
      "Iteration 343, loss = 0.38002002\n",
      "Iteration 344, loss = 0.37959698\n",
      "Iteration 345, loss = 0.37917470\n",
      "Iteration 346, loss = 0.37875315\n",
      "Iteration 347, loss = 0.37833234\n",
      "Iteration 348, loss = 0.37791226\n",
      "Iteration 349, loss = 0.37749291\n",
      "Iteration 350, loss = 0.37707428\n",
      "Iteration 351, loss = 0.37665637\n",
      "Iteration 352, loss = 0.37623916\n",
      "Iteration 353, loss = 0.37582266\n",
      "Iteration 354, loss = 0.37540702\n",
      "Iteration 355, loss = 0.37499201\n",
      "Iteration 356, loss = 0.37457769\n",
      "Iteration 357, loss = 0.37416404\n",
      "Iteration 358, loss = 0.37375107\n",
      "Iteration 359, loss = 0.37333878\n",
      "Iteration 360, loss = 0.37292720\n",
      "Iteration 361, loss = 0.37251632\n",
      "Iteration 362, loss = 0.37210612\n",
      "Iteration 363, loss = 0.37169660\n",
      "Iteration 364, loss = 0.37128775\n",
      "Iteration 365, loss = 0.37087956\n",
      "Iteration 366, loss = 0.37047205\n",
      "Iteration 367, loss = 0.37006518\n",
      "Iteration 368, loss = 0.36965897\n",
      "Iteration 369, loss = 0.36925343\n",
      "Iteration 370, loss = 0.36885061\n",
      "Iteration 371, loss = 0.36844873\n",
      "Iteration 372, loss = 0.36804756\n",
      "Iteration 373, loss = 0.36764708\n",
      "Iteration 374, loss = 0.36724730\n",
      "Iteration 375, loss = 0.36684821\n",
      "Iteration 376, loss = 0.36644985\n",
      "Iteration 377, loss = 0.36605209\n",
      "Iteration 378, loss = 0.36565503\n",
      "Iteration 379, loss = 0.36525863\n",
      "Iteration 380, loss = 0.36486288\n",
      "Iteration 381, loss = 0.36446776\n",
      "Iteration 382, loss = 0.36407328\n",
      "Iteration 383, loss = 0.36367942\n",
      "Iteration 384, loss = 0.36328617\n",
      "Iteration 385, loss = 0.36289354\n",
      "Iteration 386, loss = 0.36250151\n",
      "Iteration 387, loss = 0.36211008\n",
      "Iteration 388, loss = 0.36171925\n",
      "Iteration 389, loss = 0.36132900\n",
      "Iteration 390, loss = 0.36093934\n",
      "Iteration 391, loss = 0.36055026\n",
      "Iteration 392, loss = 0.36016176\n",
      "Iteration 393, loss = 0.35977384\n",
      "Iteration 394, loss = 0.35938648\n",
      "Iteration 395, loss = 0.35899969\n",
      "Iteration 396, loss = 0.35861346\n",
      "Iteration 397, loss = 0.35822779\n",
      "Iteration 398, loss = 0.35784268\n",
      "Iteration 399, loss = 0.35745812\n",
      "Iteration 400, loss = 0.35707411\n",
      "Iteration 401, loss = 0.35669064\n",
      "Iteration 402, loss = 0.35630771\n",
      "Iteration 403, loss = 0.35592532\n",
      "Iteration 404, loss = 0.35554348\n",
      "Iteration 405, loss = 0.35516230\n",
      "Iteration 406, loss = 0.35478172\n",
      "Iteration 407, loss = 0.35440194\n",
      "Iteration 408, loss = 0.35402270\n",
      "Iteration 409, loss = 0.35364400\n",
      "Iteration 410, loss = 0.35326584\n",
      "Iteration 411, loss = 0.35288821\n",
      "Iteration 412, loss = 0.35251116\n",
      "Iteration 413, loss = 0.35213488\n",
      "Iteration 414, loss = 0.35175908\n",
      "Iteration 415, loss = 0.35138380\n",
      "Iteration 416, loss = 0.35100904\n",
      "Iteration 417, loss = 0.35063479\n",
      "Iteration 418, loss = 0.35026106\n",
      "Iteration 419, loss = 0.34988785\n",
      "Iteration 420, loss = 0.34951516\n",
      "Iteration 421, loss = 0.34914298\n",
      "Iteration 422, loss = 0.34877132\n",
      "Iteration 423, loss = 0.34840031\n",
      "Iteration 424, loss = 0.34802961\n",
      "Iteration 425, loss = 0.34765952\n",
      "Iteration 426, loss = 0.34728995\n",
      "Iteration 427, loss = 0.34692088\n",
      "Iteration 428, loss = 0.34655232\n",
      "Iteration 429, loss = 0.34618428\n",
      "Iteration 430, loss = 0.34581675\n",
      "Iteration 431, loss = 0.34544971\n",
      "Iteration 432, loss = 0.34508318\n",
      "Iteration 433, loss = 0.34471727\n",
      "Iteration 434, loss = 0.34435168\n",
      "Iteration 435, loss = 0.34398668\n",
      "Iteration 436, loss = 0.34362217\n",
      "Iteration 437, loss = 0.34325815\n",
      "Iteration 438, loss = 0.34289462\n",
      "Iteration 439, loss = 0.34253158\n",
      "Iteration 440, loss = 0.34216903\n",
      "Iteration 441, loss = 0.34180697\n",
      "Iteration 442, loss = 0.34144540\n",
      "Iteration 443, loss = 0.34108442\n",
      "Iteration 444, loss = 0.34072377\n",
      "Iteration 445, loss = 0.34036368\n",
      "Iteration 446, loss = 0.34000407\n",
      "Iteration 447, loss = 0.33964495\n",
      "Iteration 448, loss = 0.33928630\n",
      "Iteration 449, loss = 0.33892812\n",
      "Iteration 450, loss = 0.33857043\n",
      "Iteration 451, loss = 0.33821321\n",
      "Iteration 452, loss = 0.33785647\n",
      "Iteration 453, loss = 0.33750021\n",
      "Iteration 454, loss = 0.33714446\n",
      "Iteration 455, loss = 0.33678914\n",
      "Iteration 456, loss = 0.33643432\n",
      "Iteration 457, loss = 0.33607996\n",
      "Iteration 458, loss = 0.33572606\n",
      "Iteration 459, loss = 0.33537264\n",
      "Iteration 460, loss = 0.33502007\n",
      "Iteration 461, loss = 0.33466827\n",
      "Iteration 462, loss = 0.33431692\n",
      "Iteration 463, loss = 0.33396605\n",
      "Iteration 464, loss = 0.33361564\n",
      "Iteration 465, loss = 0.33326569\n",
      "Iteration 466, loss = 0.33291620\n",
      "Iteration 467, loss = 0.33256718\n",
      "Iteration 468, loss = 0.33221868\n",
      "Iteration 469, loss = 0.33187058\n",
      "Iteration 470, loss = 0.33152297\n",
      "Iteration 471, loss = 0.33117581\n",
      "Iteration 472, loss = 0.33082910\n",
      "Iteration 473, loss = 0.33048284\n",
      "Iteration 474, loss = 0.33013703\n",
      "Iteration 475, loss = 0.32979178\n",
      "Iteration 476, loss = 0.32944684\n",
      "Iteration 477, loss = 0.32910240\n",
      "Iteration 478, loss = 0.32875840\n",
      "Iteration 479, loss = 0.32841484\n",
      "Iteration 480, loss = 0.32807181\n",
      "Iteration 481, loss = 0.32772914\n",
      "Iteration 482, loss = 0.32738693\n",
      "Iteration 483, loss = 0.32704516\n",
      "Iteration 484, loss = 0.32670382\n",
      "Iteration 485, loss = 0.32636294\n",
      "Iteration 486, loss = 0.32602255\n",
      "Iteration 487, loss = 0.32568255\n",
      "Iteration 488, loss = 0.32534299\n",
      "Iteration 489, loss = 0.32500386\n",
      "Iteration 490, loss = 0.32466516\n",
      "Iteration 491, loss = 0.32432699\n",
      "Iteration 492, loss = 0.32398916\n",
      "Iteration 493, loss = 0.32365180\n",
      "Iteration 494, loss = 0.32331487\n",
      "Iteration 495, loss = 0.32297837\n",
      "Iteration 496, loss = 0.32264231\n",
      "Iteration 497, loss = 0.32230675\n",
      "Iteration 498, loss = 0.32197158\n",
      "Iteration 499, loss = 0.32163683\n",
      "Iteration 500, loss = 0.32130251\n",
      "Iteration 501, loss = 0.32096862\n",
      "Iteration 502, loss = 0.32063516\n",
      "Iteration 503, loss = 0.32030218\n",
      "Iteration 504, loss = 0.31996961\n",
      "Iteration 505, loss = 0.31963747\n",
      "Iteration 506, loss = 0.31930576\n",
      "Iteration 507, loss = 0.31897447\n",
      "Iteration 508, loss = 0.31864361\n",
      "Iteration 509, loss = 0.31831321\n",
      "Iteration 510, loss = 0.31798326\n",
      "Iteration 511, loss = 0.31765372\n",
      "Iteration 512, loss = 0.31732460\n",
      "Iteration 513, loss = 0.31699590\n",
      "Iteration 514, loss = 0.31666763\n",
      "Iteration 515, loss = 0.31633978\n",
      "Iteration 516, loss = 0.31601245\n",
      "Iteration 517, loss = 0.31568546\n",
      "Iteration 518, loss = 0.31535892\n",
      "Iteration 519, loss = 0.31503282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 520, loss = 0.31470713\n",
      "Iteration 521, loss = 0.31438187\n",
      "Iteration 522, loss = 0.31405708\n",
      "Iteration 523, loss = 0.31373270\n",
      "Iteration 524, loss = 0.31340875\n",
      "Iteration 525, loss = 0.31308522\n",
      "Iteration 526, loss = 0.31276211\n",
      "Iteration 527, loss = 0.31243942\n",
      "Iteration 528, loss = 0.31211716\n",
      "Iteration 529, loss = 0.31179538\n",
      "Iteration 530, loss = 0.31147399\n",
      "Iteration 531, loss = 0.31115304\n",
      "Iteration 532, loss = 0.31083250\n",
      "Iteration 533, loss = 0.31051239\n",
      "Iteration 534, loss = 0.31019270\n",
      "Iteration 535, loss = 0.30987344\n",
      "Iteration 536, loss = 0.30955470\n",
      "Iteration 537, loss = 0.30923625\n",
      "Iteration 538, loss = 0.30891829\n",
      "Iteration 539, loss = 0.30860075\n",
      "Iteration 540, loss = 0.30828364\n",
      "Iteration 541, loss = 0.30796694\n",
      "Iteration 542, loss = 0.30765067\n",
      "Iteration 543, loss = 0.30733491\n",
      "Iteration 544, loss = 0.30701947\n",
      "Iteration 545, loss = 0.30670450\n",
      "Iteration 546, loss = 0.30638994\n",
      "Iteration 547, loss = 0.30607582\n",
      "Iteration 548, loss = 0.30576211\n",
      "Iteration 549, loss = 0.30544882\n",
      "Iteration 550, loss = 0.30513624\n",
      "Iteration 551, loss = 0.30482421\n",
      "Iteration 552, loss = 0.30451258\n",
      "Iteration 553, loss = 0.30420136\n",
      "Iteration 554, loss = 0.30389057\n",
      "Iteration 555, loss = 0.30358020\n",
      "Iteration 556, loss = 0.30327025\n",
      "Iteration 557, loss = 0.30296072\n",
      "Iteration 558, loss = 0.30265171\n",
      "Iteration 559, loss = 0.30234302\n",
      "Iteration 560, loss = 0.30203481\n",
      "Iteration 561, loss = 0.30172702\n",
      "Iteration 562, loss = 0.30141965\n",
      "Iteration 563, loss = 0.30111271\n",
      "Iteration 564, loss = 0.30080620\n",
      "Iteration 565, loss = 0.30050022\n",
      "Iteration 566, loss = 0.30019454\n",
      "Iteration 567, loss = 0.29988934\n",
      "Iteration 568, loss = 0.29958457\n",
      "Iteration 569, loss = 0.29928022\n",
      "Iteration 570, loss = 0.29897629\n",
      "Iteration 571, loss = 0.29867281\n",
      "Iteration 572, loss = 0.29836981\n",
      "Iteration 573, loss = 0.29806720\n",
      "Iteration 574, loss = 0.29776501\n",
      "Iteration 575, loss = 0.29746324\n",
      "Iteration 576, loss = 0.29716190\n",
      "Iteration 577, loss = 0.29686098\n",
      "Iteration 578, loss = 0.29656060\n",
      "Iteration 579, loss = 0.29626050\n",
      "Iteration 580, loss = 0.29596089\n",
      "Iteration 581, loss = 0.29566170\n",
      "Iteration 582, loss = 0.29536293\n",
      "Iteration 583, loss = 0.29506467\n",
      "Iteration 584, loss = 0.29476676\n",
      "Iteration 585, loss = 0.29446929\n",
      "Iteration 586, loss = 0.29417224\n",
      "Iteration 587, loss = 0.29387562\n",
      "Iteration 588, loss = 0.29357941\n",
      "Iteration 589, loss = 0.29328377\n",
      "Iteration 590, loss = 0.29298839\n",
      "Iteration 591, loss = 0.29269348\n",
      "Iteration 592, loss = 0.29239901\n",
      "Iteration 593, loss = 0.29210496\n",
      "Iteration 594, loss = 0.29181144\n",
      "Iteration 595, loss = 0.29151821\n",
      "Iteration 596, loss = 0.29122545\n",
      "Iteration 597, loss = 0.29093314\n",
      "Iteration 598, loss = 0.29064120\n",
      "Iteration 599, loss = 0.29034978\n",
      "Iteration 600, loss = 0.29005874\n",
      "Iteration 601, loss = 0.28976813\n",
      "Iteration 602, loss = 0.28947793\n",
      "Iteration 603, loss = 0.28918816\n",
      "Iteration 604, loss = 0.28889885\n",
      "Iteration 605, loss = 0.28860996\n",
      "Iteration 606, loss = 0.28832149\n",
      "Iteration 607, loss = 0.28803342\n",
      "Iteration 608, loss = 0.28774578\n",
      "Iteration 609, loss = 0.28745860\n",
      "Iteration 610, loss = 0.28717182\n",
      "Iteration 611, loss = 0.28688548\n",
      "Iteration 612, loss = 0.28659952\n",
      "Iteration 613, loss = 0.28631401\n",
      "Iteration 614, loss = 0.28602900\n",
      "Iteration 615, loss = 0.28574430\n",
      "Iteration 616, loss = 0.28546008\n",
      "Iteration 617, loss = 0.28517625\n",
      "Iteration 618, loss = 0.28489285\n",
      "Iteration 619, loss = 0.28460994\n",
      "Iteration 620, loss = 0.28432742\n",
      "Iteration 621, loss = 0.28404528\n",
      "Iteration 622, loss = 0.28376358\n",
      "Iteration 623, loss = 0.28348227\n",
      "Iteration 624, loss = 0.28320155\n",
      "Iteration 625, loss = 0.28292101\n",
      "Iteration 626, loss = 0.28264100\n",
      "Iteration 627, loss = 0.28236143\n",
      "Iteration 628, loss = 0.28208232\n",
      "Iteration 629, loss = 0.28180358\n",
      "Iteration 630, loss = 0.28152526\n",
      "Iteration 631, loss = 0.28124734\n",
      "Iteration 632, loss = 0.28096983\n",
      "Iteration 633, loss = 0.28069287\n",
      "Iteration 634, loss = 0.28041617\n",
      "Iteration 635, loss = 0.28013993\n",
      "Iteration 636, loss = 0.27986415\n",
      "Iteration 637, loss = 0.27958882\n",
      "Iteration 638, loss = 0.27931386\n",
      "Iteration 639, loss = 0.27903932\n",
      "Iteration 640, loss = 0.27876517\n",
      "Iteration 641, loss = 0.27849146\n",
      "Iteration 642, loss = 0.27821829\n",
      "Iteration 643, loss = 0.27794533\n",
      "Iteration 644, loss = 0.27767287\n",
      "Iteration 645, loss = 0.27740089\n",
      "Iteration 646, loss = 0.27712930\n",
      "Iteration 647, loss = 0.27685810\n",
      "Iteration 648, loss = 0.27658732\n",
      "Iteration 649, loss = 0.27631694\n",
      "Iteration 650, loss = 0.27604703\n",
      "Iteration 651, loss = 0.27577752\n",
      "Iteration 652, loss = 0.27550839\n",
      "Iteration 653, loss = 0.27523970\n",
      "Iteration 654, loss = 0.27497145\n",
      "Iteration 655, loss = 0.27470360\n",
      "Iteration 656, loss = 0.27443615\n",
      "Iteration 657, loss = 0.27416911\n",
      "Iteration 658, loss = 0.27390248\n",
      "Iteration 659, loss = 0.27363635\n",
      "Iteration 660, loss = 0.27337056\n",
      "Iteration 661, loss = 0.27310517\n",
      "Iteration 662, loss = 0.27284020\n",
      "Iteration 663, loss = 0.27257575\n",
      "Iteration 664, loss = 0.27231155\n",
      "Iteration 665, loss = 0.27204783\n",
      "Iteration 666, loss = 0.27178455\n",
      "Iteration 667, loss = 0.27152170\n",
      "Iteration 668, loss = 0.27125923\n",
      "Iteration 669, loss = 0.27099716\n",
      "Iteration 670, loss = 0.27073548\n",
      "Iteration 671, loss = 0.27047430\n",
      "Iteration 672, loss = 0.27021345\n",
      "Iteration 673, loss = 0.26995302\n",
      "Iteration 674, loss = 0.26969300\n",
      "Iteration 675, loss = 0.26943348\n",
      "Iteration 676, loss = 0.26917431\n",
      "Iteration 677, loss = 0.26891552\n",
      "Iteration 678, loss = 0.26865713\n",
      "Iteration 679, loss = 0.26839923\n",
      "Iteration 680, loss = 0.26814166\n",
      "Iteration 681, loss = 0.26788452\n",
      "Iteration 682, loss = 0.26762779\n",
      "Iteration 683, loss = 0.26737154\n",
      "Iteration 684, loss = 0.26711563\n",
      "Iteration 685, loss = 0.26686012\n",
      "Iteration 686, loss = 0.26660499\n",
      "Iteration 687, loss = 0.26635041\n",
      "Iteration 688, loss = 0.26609606\n",
      "Iteration 689, loss = 0.26584218\n",
      "Iteration 690, loss = 0.26558879\n",
      "Iteration 691, loss = 0.26533571\n",
      "Iteration 692, loss = 0.26508306\n",
      "Iteration 693, loss = 0.26483080\n",
      "Iteration 694, loss = 0.26457900\n",
      "Iteration 695, loss = 0.26432757\n",
      "Iteration 696, loss = 0.26407654\n",
      "Iteration 697, loss = 0.26382590\n",
      "Iteration 698, loss = 0.26357575\n",
      "Iteration 699, loss = 0.26332590\n",
      "Iteration 700, loss = 0.26307647\n",
      "Iteration 701, loss = 0.26282747\n",
      "Iteration 702, loss = 0.26257892\n",
      "Iteration 703, loss = 0.26233072\n",
      "Iteration 704, loss = 0.26208290\n",
      "Iteration 705, loss = 0.26183548\n",
      "Iteration 706, loss = 0.26158854\n",
      "Iteration 707, loss = 0.26134195\n",
      "Iteration 708, loss = 0.26109573\n",
      "Iteration 709, loss = 0.26084999\n",
      "Iteration 710, loss = 0.26060457\n",
      "Iteration 711, loss = 0.26035957\n",
      "Iteration 712, loss = 0.26011500\n",
      "Iteration 713, loss = 0.25987084\n",
      "Iteration 714, loss = 0.25962704\n",
      "Iteration 715, loss = 0.25938362\n",
      "Iteration 716, loss = 0.25914067\n",
      "Iteration 717, loss = 0.25889807\n",
      "Iteration 718, loss = 0.25865585\n",
      "Iteration 719, loss = 0.25841406\n",
      "Iteration 720, loss = 0.25817270\n",
      "Iteration 721, loss = 0.25793169\n",
      "Iteration 722, loss = 0.25769105\n",
      "Iteration 723, loss = 0.25745087\n",
      "Iteration 724, loss = 0.25721105\n",
      "Iteration 725, loss = 0.25697161\n",
      "Iteration 726, loss = 0.25673260\n",
      "Iteration 727, loss = 0.25649399\n",
      "Iteration 728, loss = 0.25625575\n",
      "Iteration 729, loss = 0.25601788\n",
      "Iteration 730, loss = 0.25578050\n",
      "Iteration 731, loss = 0.25554339\n",
      "Iteration 732, loss = 0.25530671\n",
      "Iteration 733, loss = 0.25507052\n",
      "Iteration 734, loss = 0.25483460\n",
      "Iteration 735, loss = 0.25459910\n",
      "Iteration 736, loss = 0.25436402\n",
      "Iteration 737, loss = 0.25412934\n",
      "Iteration 738, loss = 0.25389502\n",
      "Iteration 739, loss = 0.25366107\n",
      "Iteration 740, loss = 0.25342761\n",
      "Iteration 741, loss = 0.25319441\n",
      "Iteration 742, loss = 0.25296164\n",
      "Iteration 743, loss = 0.25272935\n",
      "Iteration 744, loss = 0.25249737\n",
      "Iteration 745, loss = 0.25226576\n",
      "Iteration 746, loss = 0.25203451\n",
      "Iteration 747, loss = 0.25180383\n",
      "Iteration 748, loss = 0.25157330\n",
      "Iteration 749, loss = 0.25134332\n",
      "Iteration 750, loss = 0.25111367\n",
      "Iteration 751, loss = 0.25088438\n",
      "Iteration 752, loss = 0.25065546\n",
      "Iteration 753, loss = 0.25042702\n",
      "Iteration 754, loss = 0.25019885\n",
      "Iteration 755, loss = 0.24997113\n",
      "Iteration 756, loss = 0.24974382\n",
      "Iteration 757, loss = 0.24951684\n",
      "Iteration 758, loss = 0.24929023\n",
      "Iteration 759, loss = 0.24906410\n",
      "Iteration 760, loss = 0.24883823\n",
      "Iteration 761, loss = 0.24861279\n",
      "Iteration 762, loss = 0.24838780\n",
      "Iteration 763, loss = 0.24816311\n",
      "Iteration 764, loss = 0.24793879\n",
      "Iteration 765, loss = 0.24771491\n",
      "Iteration 766, loss = 0.24749136\n",
      "Iteration 767, loss = 0.24726817\n",
      "Iteration 768, loss = 0.24704549\n",
      "Iteration 769, loss = 0.24682302\n",
      "Iteration 770, loss = 0.24660101\n",
      "Iteration 771, loss = 0.24637942\n",
      "Iteration 772, loss = 0.24615814\n",
      "Iteration 773, loss = 0.24593722\n",
      "Iteration 774, loss = 0.24571676\n",
      "Iteration 775, loss = 0.24549660\n",
      "Iteration 776, loss = 0.24527684\n",
      "Iteration 777, loss = 0.24505753\n",
      "Iteration 778, loss = 0.24483851\n",
      "Iteration 779, loss = 0.24461985\n",
      "Iteration 780, loss = 0.24440167\n",
      "Iteration 781, loss = 0.24418372\n",
      "Iteration 782, loss = 0.24396625\n",
      "Iteration 783, loss = 0.24374912\n",
      "Iteration 784, loss = 0.24353234\n",
      "Iteration 785, loss = 0.24331595\n",
      "Iteration 786, loss = 0.24309997\n",
      "Iteration 787, loss = 0.24288430\n",
      "Iteration 788, loss = 0.24266902\n",
      "Iteration 789, loss = 0.24245413\n",
      "Iteration 790, loss = 0.24223957\n",
      "Iteration 791, loss = 0.24202543\n",
      "Iteration 792, loss = 0.24181161\n",
      "Iteration 793, loss = 0.24159815\n",
      "Iteration 794, loss = 0.24138517\n",
      "Iteration 795, loss = 0.24117240\n",
      "Iteration 796, loss = 0.24096009\n",
      "Iteration 797, loss = 0.24074814\n",
      "Iteration 798, loss = 0.24053652\n",
      "Iteration 799, loss = 0.24032526\n",
      "Iteration 800, loss = 0.24011444\n",
      "Iteration 801, loss = 0.23990391\n",
      "Iteration 802, loss = 0.23969376\n",
      "Iteration 803, loss = 0.23948400\n",
      "Iteration 804, loss = 0.23927455\n",
      "Iteration 805, loss = 0.23906555\n",
      "Iteration 806, loss = 0.23885680\n",
      "Iteration 807, loss = 0.23864849\n",
      "Iteration 808, loss = 0.23844053\n",
      "Iteration 809, loss = 0.23823290\n",
      "Iteration 810, loss = 0.23802565\n",
      "Iteration 811, loss = 0.23781877\n",
      "Iteration 812, loss = 0.23761220\n",
      "Iteration 813, loss = 0.23740609\n",
      "Iteration 814, loss = 0.23720021\n",
      "Iteration 815, loss = 0.23699478\n",
      "Iteration 816, loss = 0.23678968\n",
      "Iteration 817, loss = 0.23658491\n",
      "Iteration 818, loss = 0.23638055\n",
      "Iteration 819, loss = 0.23617650\n",
      "Iteration 820, loss = 0.23597279\n",
      "Iteration 821, loss = 0.23576953\n",
      "Iteration 822, loss = 0.23556654\n",
      "Iteration 823, loss = 0.23536388\n",
      "Iteration 824, loss = 0.23516167\n",
      "Iteration 825, loss = 0.23495972\n",
      "Iteration 826, loss = 0.23475820\n",
      "Iteration 827, loss = 0.23455695\n",
      "Iteration 828, loss = 0.23435612\n",
      "Iteration 829, loss = 0.23415561\n",
      "Iteration 830, loss = 0.23395542\n",
      "Iteration 831, loss = 0.23375568\n",
      "Iteration 832, loss = 0.23355616\n",
      "Iteration 833, loss = 0.23335710\n",
      "Iteration 834, loss = 0.23315832\n",
      "Iteration 835, loss = 0.23295987\n",
      "Iteration 836, loss = 0.23276188\n",
      "Iteration 837, loss = 0.23256409\n",
      "Iteration 838, loss = 0.23236677\n",
      "Iteration 839, loss = 0.23216972\n",
      "Iteration 840, loss = 0.23197301\n",
      "Iteration 841, loss = 0.23177675\n",
      "Iteration 842, loss = 0.23158073\n",
      "Iteration 843, loss = 0.23138504\n",
      "Iteration 844, loss = 0.23118979\n",
      "Iteration 845, loss = 0.23099478\n",
      "Iteration 846, loss = 0.23080022\n",
      "Iteration 847, loss = 0.23060588\n",
      "Iteration 848, loss = 0.23041201\n",
      "Iteration 849, loss = 0.23021839\n",
      "Iteration 850, loss = 0.23002509\n",
      "Iteration 851, loss = 0.22983226\n",
      "Iteration 852, loss = 0.22963963\n",
      "Iteration 853, loss = 0.22944741\n",
      "Iteration 854, loss = 0.22925548\n",
      "Iteration 855, loss = 0.22906393\n",
      "Iteration 856, loss = 0.22887270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 857, loss = 0.22868180\n",
      "Iteration 858, loss = 0.22849129\n",
      "Iteration 859, loss = 0.22830104\n",
      "Iteration 860, loss = 0.22811120\n",
      "Iteration 861, loss = 0.22792160\n",
      "Iteration 862, loss = 0.22773244\n",
      "Iteration 863, loss = 0.22754352\n",
      "Iteration 864, loss = 0.22735500\n",
      "Iteration 865, loss = 0.22716679\n",
      "Iteration 866, loss = 0.22697888\n",
      "Iteration 867, loss = 0.22679141\n",
      "Iteration 868, loss = 0.22660413\n",
      "Iteration 869, loss = 0.22641730\n",
      "Iteration 870, loss = 0.22623072\n",
      "Iteration 871, loss = 0.22604446\n",
      "Iteration 872, loss = 0.22585861\n",
      "Iteration 873, loss = 0.22567299\n",
      "Iteration 874, loss = 0.22548784\n",
      "Iteration 875, loss = 0.22530288\n",
      "Iteration 876, loss = 0.22511830\n",
      "Iteration 877, loss = 0.22493401\n",
      "Iteration 878, loss = 0.22475013\n",
      "Iteration 879, loss = 0.22456646\n",
      "Iteration 880, loss = 0.22438324\n",
      "Iteration 881, loss = 0.22420023\n",
      "Iteration 882, loss = 0.22401764\n",
      "Iteration 883, loss = 0.22383531\n",
      "Iteration 884, loss = 0.22365332\n",
      "Iteration 885, loss = 0.22347168\n",
      "Iteration 886, loss = 0.22329030\n",
      "Iteration 887, loss = 0.22310935\n",
      "Iteration 888, loss = 0.22292861\n",
      "Iteration 889, loss = 0.22274826\n",
      "Iteration 890, loss = 0.22256817\n",
      "Iteration 891, loss = 0.22238846\n",
      "Iteration 892, loss = 0.22220900\n",
      "Iteration 893, loss = 0.22202995\n",
      "Iteration 894, loss = 0.22185111\n",
      "Iteration 895, loss = 0.22167270\n",
      "Iteration 896, loss = 0.22149450\n",
      "Iteration 897, loss = 0.22131671\n",
      "Iteration 898, loss = 0.22113916\n",
      "Iteration 899, loss = 0.22096198\n",
      "Iteration 900, loss = 0.22078509\n",
      "Iteration 901, loss = 0.22060850\n",
      "Iteration 902, loss = 0.22043228\n",
      "Iteration 903, loss = 0.22025629\n",
      "Iteration 904, loss = 0.22008071\n",
      "Iteration 905, loss = 0.21990535\n",
      "Iteration 906, loss = 0.21973039\n",
      "Iteration 907, loss = 0.21955566\n",
      "Iteration 908, loss = 0.21938128\n",
      "Iteration 909, loss = 0.21920717\n",
      "Iteration 910, loss = 0.21903346\n",
      "Iteration 911, loss = 0.21885996\n",
      "Iteration 912, loss = 0.21868686\n",
      "Iteration 913, loss = 0.21851398\n",
      "Iteration 914, loss = 0.21834147\n",
      "Iteration 915, loss = 0.21816921\n",
      "Iteration 916, loss = 0.21799735\n",
      "Iteration 917, loss = 0.21782570\n",
      "Iteration 918, loss = 0.21765440\n",
      "Iteration 919, loss = 0.21748336\n",
      "Iteration 920, loss = 0.21731273\n",
      "Iteration 921, loss = 0.21714229\n",
      "Iteration 922, loss = 0.21697224\n",
      "Iteration 923, loss = 0.21680242\n",
      "Iteration 924, loss = 0.21663294\n",
      "Iteration 925, loss = 0.21646373\n",
      "Iteration 926, loss = 0.21629490\n",
      "Iteration 927, loss = 0.21612627\n",
      "Iteration 928, loss = 0.21595806\n",
      "Iteration 929, loss = 0.21579005\n",
      "Iteration 930, loss = 0.21562233\n",
      "Iteration 931, loss = 0.21545497\n",
      "Iteration 932, loss = 0.21528787\n",
      "Iteration 933, loss = 0.21512110\n",
      "Iteration 934, loss = 0.21495460\n",
      "Iteration 935, loss = 0.21478842\n",
      "Iteration 936, loss = 0.21462250\n",
      "Iteration 937, loss = 0.21445694\n",
      "Iteration 938, loss = 0.21429159\n",
      "Iteration 939, loss = 0.21412665\n",
      "Iteration 940, loss = 0.21396189\n",
      "Iteration 941, loss = 0.21379748\n",
      "Iteration 942, loss = 0.21363332\n",
      "Iteration 943, loss = 0.21346952\n",
      "Iteration 944, loss = 0.21330593\n",
      "Iteration 945, loss = 0.21314274\n",
      "Iteration 946, loss = 0.21297976\n",
      "Iteration 947, loss = 0.21281705\n",
      "Iteration 948, loss = 0.21265470\n",
      "Iteration 949, loss = 0.21249257\n",
      "Iteration 950, loss = 0.21233082\n",
      "Iteration 951, loss = 0.21216927\n",
      "Iteration 952, loss = 0.21200809\n",
      "Iteration 953, loss = 0.21184711\n",
      "Iteration 954, loss = 0.21168649\n",
      "Iteration 955, loss = 0.21152610\n",
      "Iteration 956, loss = 0.21136602\n",
      "Iteration 957, loss = 0.21120622\n",
      "Iteration 958, loss = 0.21104671\n",
      "Iteration 959, loss = 0.21088750\n",
      "Iteration 960, loss = 0.21072853\n",
      "Iteration 961, loss = 0.21056992\n",
      "Iteration 962, loss = 0.21041152\n",
      "Iteration 963, loss = 0.21025344\n",
      "Iteration 964, loss = 0.21009561\n",
      "Iteration 965, loss = 0.20993810\n",
      "Iteration 966, loss = 0.20978084\n",
      "Iteration 967, loss = 0.20962387\n",
      "Iteration 968, loss = 0.20946721\n",
      "Iteration 969, loss = 0.20931077\n",
      "Iteration 970, loss = 0.20915468\n",
      "Iteration 971, loss = 0.20899879\n",
      "Iteration 972, loss = 0.20884325\n",
      "Iteration 973, loss = 0.20868800\n",
      "Iteration 974, loss = 0.20853301\n",
      "Iteration 975, loss = 0.20837829\n",
      "Iteration 976, loss = 0.20822380\n",
      "Iteration 977, loss = 0.20806962\n",
      "Iteration 978, loss = 0.20791573\n",
      "Iteration 979, loss = 0.20776210\n",
      "Iteration 980, loss = 0.20760878\n",
      "Iteration 981, loss = 0.20745568\n",
      "Iteration 982, loss = 0.20730288\n",
      "Iteration 983, loss = 0.20715038\n",
      "Iteration 984, loss = 0.20699811\n",
      "Iteration 985, loss = 0.20684611\n",
      "Iteration 986, loss = 0.20669443\n",
      "Iteration 987, loss = 0.20654296\n",
      "Iteration 988, loss = 0.20639179\n",
      "Iteration 989, loss = 0.20624089\n",
      "Iteration 990, loss = 0.20609028\n",
      "Iteration 991, loss = 0.20593990\n",
      "Iteration 992, loss = 0.20578978\n",
      "Iteration 993, loss = 0.20564000\n",
      "Iteration 994, loss = 0.20549040\n",
      "Iteration 995, loss = 0.20534109\n",
      "Iteration 996, loss = 0.20519212\n",
      "Iteration 997, loss = 0.20504333\n",
      "Iteration 998, loss = 0.20489483\n",
      "Iteration 999, loss = 0.20474657\n",
      "Iteration 1000, loss = 0.20459866\n",
      "Iteration 1, loss = 1.35303276\n",
      "Iteration 2, loss = 1.31599158\n",
      "Iteration 3, loss = 1.26612351\n",
      "Iteration 4, loss = 1.20761350\n",
      "Iteration 5, loss = 1.14474263\n",
      "Iteration 6, loss = 1.08131907\n",
      "Iteration 7, loss = 1.02058590\n",
      "Iteration 8, loss = 0.96516032\n",
      "Iteration 9, loss = 0.91667007\n",
      "Iteration 10, loss = 0.87589729\n",
      "Iteration 11, loss = 0.84277230\n",
      "Iteration 12, loss = 0.81652024\n",
      "Iteration 13, loss = 0.79613708\n",
      "Iteration 14, loss = 0.78035390\n",
      "Iteration 15, loss = 0.76810262\n",
      "Iteration 16, loss = 0.75848384\n",
      "Iteration 17, loss = 0.75070554\n",
      "Iteration 18, loss = 0.74431720\n",
      "Iteration 19, loss = 0.73898611\n",
      "Iteration 20, loss = 0.73448886\n",
      "Iteration 21, loss = 0.73072165\n",
      "Iteration 22, loss = 0.72714395\n",
      "Iteration 23, loss = 0.72322114\n",
      "Iteration 24, loss = 0.71900412\n",
      "Iteration 25, loss = 0.71442113\n",
      "Iteration 26, loss = 0.70934791\n",
      "Iteration 27, loss = 0.70393448\n",
      "Iteration 28, loss = 0.69835204\n",
      "Iteration 29, loss = 0.69264527\n",
      "Iteration 30, loss = 0.68700039\n",
      "Iteration 31, loss = 0.68153306\n",
      "Iteration 32, loss = 0.67635869\n",
      "Iteration 33, loss = 0.67151066\n",
      "Iteration 34, loss = 0.66702492\n",
      "Iteration 35, loss = 0.66287852\n",
      "Iteration 36, loss = 0.65907178\n",
      "Iteration 37, loss = 0.65556385\n",
      "Iteration 38, loss = 0.65233588\n",
      "Iteration 39, loss = 0.64934261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 40, loss = 0.64653006\n",
      "Iteration 41, loss = 0.64385280\n",
      "Iteration 42, loss = 0.64127591\n",
      "Iteration 43, loss = 0.63878289\n",
      "Iteration 44, loss = 0.63633267\n",
      "Iteration 45, loss = 0.63390494\n",
      "Iteration 46, loss = 0.63149261\n",
      "Iteration 47, loss = 0.62907871\n",
      "Iteration 48, loss = 0.62665940\n",
      "Iteration 49, loss = 0.62423486\n",
      "Iteration 50, loss = 0.62180906\n",
      "Iteration 51, loss = 0.61938721\n",
      "Iteration 52, loss = 0.61697315\n",
      "Iteration 53, loss = 0.61457351\n",
      "Iteration 54, loss = 0.61219467\n",
      "Iteration 55, loss = 0.60984237\n",
      "Iteration 56, loss = 0.60752143\n",
      "Iteration 57, loss = 0.60523560\n",
      "Iteration 58, loss = 0.60298746\n",
      "Iteration 59, loss = 0.60077848\n",
      "Iteration 60, loss = 0.59860915\n",
      "Iteration 61, loss = 0.59647999\n",
      "Iteration 62, loss = 0.59439563\n",
      "Iteration 63, loss = 0.59234853\n",
      "Iteration 64, loss = 0.59033670\n",
      "Iteration 65, loss = 0.58835799\n",
      "Iteration 66, loss = 0.58641021\n",
      "Iteration 67, loss = 0.58449124\n",
      "Iteration 68, loss = 0.58259910\n",
      "Iteration 69, loss = 0.58073202\n",
      "Iteration 70, loss = 0.57888849\n",
      "Iteration 71, loss = 0.57707084\n",
      "Iteration 72, loss = 0.57527733\n",
      "Iteration 73, loss = 0.57350441\n",
      "Iteration 74, loss = 0.57175499\n",
      "Iteration 75, loss = 0.57002883\n",
      "Iteration 76, loss = 0.56832404\n",
      "Iteration 77, loss = 0.56664029\n",
      "Iteration 78, loss = 0.56497615\n",
      "Iteration 79, loss = 0.56333132\n",
      "Iteration 80, loss = 0.56170557\n",
      "Iteration 81, loss = 0.56009866\n",
      "Iteration 82, loss = 0.55851033\n",
      "Iteration 83, loss = 0.55694030\n",
      "Iteration 84, loss = 0.55538848\n",
      "Iteration 85, loss = 0.55385701\n",
      "Iteration 86, loss = 0.55234427\n",
      "Iteration 87, loss = 0.55084862\n",
      "Iteration 88, loss = 0.54937128\n",
      "Iteration 89, loss = 0.54791146\n",
      "Iteration 90, loss = 0.54646893\n",
      "Iteration 91, loss = 0.54504114\n",
      "Iteration 92, loss = 0.54362750\n",
      "Iteration 93, loss = 0.54222820\n",
      "Iteration 94, loss = 0.54084606\n",
      "Iteration 95, loss = 0.53947868\n",
      "Iteration 96, loss = 0.53812509\n",
      "Iteration 97, loss = 0.53678523\n",
      "Iteration 98, loss = 0.53545927\n",
      "Iteration 99, loss = 0.53414624\n",
      "Iteration 100, loss = 0.53284582\n",
      "Iteration 101, loss = 0.53155773\n",
      "Iteration 102, loss = 0.53028172\n",
      "Iteration 103, loss = 0.52901753\n",
      "Iteration 104, loss = 0.52776496\n",
      "Iteration 105, loss = 0.52652376\n",
      "Iteration 106, loss = 0.52529386\n",
      "Iteration 107, loss = 0.52407481\n",
      "Iteration 108, loss = 0.52286850\n",
      "Iteration 109, loss = 0.52167435\n",
      "Iteration 110, loss = 0.52049090\n",
      "Iteration 111, loss = 0.51931952\n",
      "Iteration 112, loss = 0.51816259\n",
      "Iteration 113, loss = 0.51701597\n",
      "Iteration 114, loss = 0.51587959\n",
      "Iteration 115, loss = 0.51475309\n",
      "Iteration 116, loss = 0.51363658\n",
      "Iteration 117, loss = 0.51253073\n",
      "Iteration 118, loss = 0.51143500\n",
      "Iteration 119, loss = 0.51035078\n",
      "Iteration 120, loss = 0.50927611\n",
      "Iteration 121, loss = 0.50821087\n",
      "Iteration 122, loss = 0.50715401\n",
      "Iteration 123, loss = 0.50610569\n",
      "Iteration 124, loss = 0.50506596\n",
      "Iteration 125, loss = 0.50403466\n",
      "Iteration 126, loss = 0.50301179\n",
      "Iteration 127, loss = 0.50199775\n",
      "Iteration 128, loss = 0.50099233\n",
      "Iteration 129, loss = 0.49999518\n",
      "Iteration 130, loss = 0.49900623\n",
      "Iteration 131, loss = 0.49802527\n",
      "Iteration 132, loss = 0.49705216\n",
      "Iteration 133, loss = 0.49608680\n",
      "Iteration 134, loss = 0.49512907\n",
      "Iteration 135, loss = 0.49417874\n",
      "Iteration 136, loss = 0.49323570\n",
      "Iteration 137, loss = 0.49229990\n",
      "Iteration 138, loss = 0.49137111\n",
      "Iteration 139, loss = 0.49044923\n",
      "Iteration 140, loss = 0.48953413\n",
      "Iteration 141, loss = 0.48862577\n",
      "Iteration 142, loss = 0.48772386\n",
      "Iteration 143, loss = 0.48682840\n",
      "Iteration 144, loss = 0.48593924\n",
      "Iteration 145, loss = 0.48505626\n",
      "Iteration 146, loss = 0.48417937\n",
      "Iteration 147, loss = 0.48330903\n",
      "Iteration 148, loss = 0.48244463\n",
      "Iteration 149, loss = 0.48158617\n",
      "Iteration 150, loss = 0.48073347\n",
      "Iteration 151, loss = 0.47988645\n",
      "Iteration 152, loss = 0.47904501\n",
      "Iteration 153, loss = 0.47820907\n",
      "Iteration 154, loss = 0.47737853\n",
      "Iteration 155, loss = 0.47655330\n",
      "Iteration 156, loss = 0.47573329\n",
      "Iteration 157, loss = 0.47491842\n",
      "Iteration 158, loss = 0.47410859\n",
      "Iteration 159, loss = 0.47330372\n",
      "Iteration 160, loss = 0.47250372\n",
      "Iteration 161, loss = 0.47170850\n",
      "Iteration 162, loss = 0.47091799\n",
      "Iteration 163, loss = 0.47013209\n",
      "Iteration 164, loss = 0.46935073\n",
      "Iteration 165, loss = 0.46857383\n",
      "Iteration 166, loss = 0.46780131\n",
      "Iteration 167, loss = 0.46703309\n",
      "Iteration 168, loss = 0.46626910\n",
      "Iteration 169, loss = 0.46550926\n",
      "Iteration 170, loss = 0.46475350\n",
      "Iteration 171, loss = 0.46400175\n",
      "Iteration 172, loss = 0.46325394\n",
      "Iteration 173, loss = 0.46251001\n",
      "Iteration 174, loss = 0.46176988\n",
      "Iteration 175, loss = 0.46103348\n",
      "Iteration 176, loss = 0.46030077\n",
      "Iteration 177, loss = 0.45957166\n",
      "Iteration 178, loss = 0.45884611\n",
      "Iteration 179, loss = 0.45812405\n",
      "Iteration 180, loss = 0.45740541\n",
      "Iteration 181, loss = 0.45669016\n",
      "Iteration 182, loss = 0.45597822\n",
      "Iteration 183, loss = 0.45526954\n",
      "Iteration 184, loss = 0.45456408\n",
      "Iteration 185, loss = 0.45386176\n",
      "Iteration 186, loss = 0.45316255\n",
      "Iteration 187, loss = 0.45246640\n",
      "Iteration 188, loss = 0.45177324\n",
      "Iteration 189, loss = 0.45108304\n",
      "Iteration 190, loss = 0.45039689\n",
      "Iteration 191, loss = 0.44971439\n",
      "Iteration 192, loss = 0.44903485\n",
      "Iteration 193, loss = 0.44835822\n",
      "Iteration 194, loss = 0.44768446\n",
      "Iteration 195, loss = 0.44701351\n",
      "Iteration 196, loss = 0.44634534\n",
      "Iteration 197, loss = 0.44568007\n",
      "Iteration 198, loss = 0.44501893\n",
      "Iteration 199, loss = 0.44436052\n",
      "Iteration 200, loss = 0.44370479\n",
      "Iteration 201, loss = 0.44305172\n",
      "Iteration 202, loss = 0.44240127\n",
      "Iteration 203, loss = 0.44175340\n",
      "Iteration 204, loss = 0.44110914\n",
      "Iteration 205, loss = 0.44046766\n",
      "Iteration 206, loss = 0.43982869\n",
      "Iteration 207, loss = 0.43919222\n",
      "Iteration 208, loss = 0.43855897\n",
      "Iteration 209, loss = 0.43792851\n",
      "Iteration 210, loss = 0.43730044\n",
      "Iteration 211, loss = 0.43667474\n",
      "Iteration 212, loss = 0.43605141\n",
      "Iteration 213, loss = 0.43543040\n",
      "Iteration 214, loss = 0.43481171\n",
      "Iteration 215, loss = 0.43419530\n",
      "Iteration 216, loss = 0.43358114\n",
      "Iteration 217, loss = 0.43296921\n",
      "Iteration 218, loss = 0.43235996\n",
      "Iteration 219, loss = 0.43175271\n",
      "Iteration 220, loss = 0.43114744\n",
      "Iteration 221, loss = 0.43054416\n",
      "Iteration 222, loss = 0.42994293\n",
      "Iteration 223, loss = 0.42934483\n",
      "Iteration 224, loss = 0.42874900\n",
      "Iteration 225, loss = 0.42815571\n",
      "Iteration 226, loss = 0.42756433\n",
      "Iteration 227, loss = 0.42697489\n",
      "Iteration 228, loss = 0.42638750\n",
      "Iteration 229, loss = 0.42580207\n",
      "Iteration 230, loss = 0.42521867\n",
      "Iteration 231, loss = 0.42463717\n",
      "Iteration 232, loss = 0.42405764\n",
      "Iteration 233, loss = 0.42348055\n",
      "Iteration 234, loss = 0.42290503\n",
      "Iteration 235, loss = 0.42233164\n",
      "Iteration 236, loss = 0.42176015\n",
      "Iteration 237, loss = 0.42119055\n",
      "Iteration 238, loss = 0.42062280\n",
      "Iteration 239, loss = 0.42005689\n",
      "Iteration 240, loss = 0.41949280\n",
      "Iteration 241, loss = 0.41893050\n",
      "Iteration 242, loss = 0.41836996\n",
      "Iteration 243, loss = 0.41781117\n",
      "Iteration 244, loss = 0.41725410\n",
      "Iteration 245, loss = 0.41669872\n",
      "Iteration 246, loss = 0.41614501\n",
      "Iteration 247, loss = 0.41559294\n",
      "Iteration 248, loss = 0.41504250\n",
      "Iteration 249, loss = 0.41449366\n",
      "Iteration 250, loss = 0.41394640\n",
      "Iteration 251, loss = 0.41340069\n",
      "Iteration 252, loss = 0.41285652\n",
      "Iteration 253, loss = 0.41231386\n",
      "Iteration 254, loss = 0.41177270\n",
      "Iteration 255, loss = 0.41123300\n",
      "Iteration 256, loss = 0.41069476\n",
      "Iteration 257, loss = 0.41015795\n",
      "Iteration 258, loss = 0.40962256\n",
      "Iteration 259, loss = 0.40908864\n",
      "Iteration 260, loss = 0.40855598\n",
      "Iteration 261, loss = 0.40802474\n",
      "Iteration 262, loss = 0.40749486\n",
      "Iteration 263, loss = 0.40696634\n",
      "Iteration 264, loss = 0.40643910\n",
      "Iteration 265, loss = 0.40591316\n",
      "Iteration 266, loss = 0.40538848\n",
      "Iteration 267, loss = 0.40486507\n",
      "Iteration 268, loss = 0.40434296\n",
      "Iteration 269, loss = 0.40382204\n",
      "Iteration 270, loss = 0.40330236\n",
      "Iteration 271, loss = 0.40278391\n",
      "Iteration 272, loss = 0.40226660\n",
      "Iteration 273, loss = 0.40175054\n",
      "Iteration 274, loss = 0.40123557\n",
      "Iteration 275, loss = 0.40072181\n",
      "Iteration 276, loss = 0.40020918\n",
      "Iteration 277, loss = 0.39969763\n",
      "Iteration 278, loss = 0.39918731\n",
      "Iteration 279, loss = 0.39867796\n",
      "Iteration 280, loss = 0.39816979\n",
      "Iteration 281, loss = 0.39766263\n",
      "Iteration 282, loss = 0.39715660\n",
      "Iteration 283, loss = 0.39665156\n",
      "Iteration 284, loss = 0.39614766\n",
      "Iteration 285, loss = 0.39564469\n",
      "Iteration 286, loss = 0.39514282\n",
      "Iteration 287, loss = 0.39464191\n",
      "Iteration 288, loss = 0.39414200\n",
      "Iteration 289, loss = 0.39364313\n",
      "Iteration 290, loss = 0.39314538\n",
      "Iteration 291, loss = 0.39265071\n",
      "Iteration 292, loss = 0.39215698\n",
      "Iteration 293, loss = 0.39166511\n",
      "Iteration 294, loss = 0.39117405\n",
      "Iteration 295, loss = 0.39068365\n",
      "Iteration 296, loss = 0.39019399\n",
      "Iteration 297, loss = 0.38970511\n",
      "Iteration 298, loss = 0.38921701\n",
      "Iteration 299, loss = 0.38872977\n",
      "Iteration 300, loss = 0.38824385\n",
      "Iteration 301, loss = 0.38775906\n",
      "Iteration 302, loss = 0.38727554\n",
      "Iteration 303, loss = 0.38679304\n",
      "Iteration 304, loss = 0.38631149\n",
      "Iteration 305, loss = 0.38583096\n",
      "Iteration 306, loss = 0.38535136\n",
      "Iteration 307, loss = 0.38487269\n",
      "Iteration 308, loss = 0.38439503\n",
      "Iteration 309, loss = 0.38391821\n",
      "Iteration 310, loss = 0.38344232\n",
      "Iteration 311, loss = 0.38296741\n",
      "Iteration 312, loss = 0.38249333\n",
      "Iteration 313, loss = 0.38202012\n",
      "Iteration 314, loss = 0.38154788\n",
      "Iteration 315, loss = 0.38107664\n",
      "Iteration 316, loss = 0.38060625\n",
      "Iteration 317, loss = 0.38013681\n",
      "Iteration 318, loss = 0.37966815\n",
      "Iteration 319, loss = 0.37920038\n",
      "Iteration 320, loss = 0.37873355\n",
      "Iteration 321, loss = 0.37826751\n",
      "Iteration 322, loss = 0.37780231\n",
      "Iteration 323, loss = 0.37733794\n",
      "Iteration 324, loss = 0.37687452\n",
      "Iteration 325, loss = 0.37641177\n",
      "Iteration 326, loss = 0.37594998\n",
      "Iteration 327, loss = 0.37548896\n",
      "Iteration 328, loss = 0.37502874\n",
      "Iteration 329, loss = 0.37456932\n",
      "Iteration 330, loss = 0.37411069\n",
      "Iteration 331, loss = 0.37365296\n",
      "Iteration 332, loss = 0.37319586\n",
      "Iteration 333, loss = 0.37273965\n",
      "Iteration 334, loss = 0.37228418\n",
      "Iteration 335, loss = 0.37182949\n",
      "Iteration 336, loss = 0.37137554\n",
      "Iteration 337, loss = 0.37092235\n",
      "Iteration 338, loss = 0.37046994\n",
      "Iteration 339, loss = 0.37001826\n",
      "Iteration 340, loss = 0.36956731\n",
      "Iteration 341, loss = 0.36911710\n",
      "Iteration 342, loss = 0.36866760\n",
      "Iteration 343, loss = 0.36821891\n",
      "Iteration 344, loss = 0.36777082\n",
      "Iteration 345, loss = 0.36732349\n",
      "Iteration 346, loss = 0.36687687\n",
      "Iteration 347, loss = 0.36643094\n",
      "Iteration 348, loss = 0.36598576\n",
      "Iteration 349, loss = 0.36554139\n",
      "Iteration 350, loss = 0.36509772\n",
      "Iteration 351, loss = 0.36465473\n",
      "Iteration 352, loss = 0.36421242\n",
      "Iteration 353, loss = 0.36377079\n",
      "Iteration 354, loss = 0.36332985\n",
      "Iteration 355, loss = 0.36289136\n",
      "Iteration 356, loss = 0.36245427\n",
      "Iteration 357, loss = 0.36201793\n",
      "Iteration 358, loss = 0.36158233\n",
      "Iteration 359, loss = 0.36114748\n",
      "Iteration 360, loss = 0.36071341\n",
      "Iteration 361, loss = 0.36028003\n",
      "Iteration 362, loss = 0.35984738\n",
      "Iteration 363, loss = 0.35941546\n",
      "Iteration 364, loss = 0.35898426\n",
      "Iteration 365, loss = 0.35855373\n",
      "Iteration 366, loss = 0.35812389\n",
      "Iteration 367, loss = 0.35769476\n",
      "Iteration 368, loss = 0.35726628\n",
      "Iteration 369, loss = 0.35683845\n",
      "Iteration 370, loss = 0.35641239\n",
      "Iteration 371, loss = 0.35598754\n",
      "Iteration 372, loss = 0.35556330\n",
      "Iteration 373, loss = 0.35513970\n",
      "Iteration 374, loss = 0.35471677\n",
      "Iteration 375, loss = 0.35429442\n",
      "Iteration 376, loss = 0.35387280\n",
      "Iteration 377, loss = 0.35345170\n",
      "Iteration 378, loss = 0.35303129\n",
      "Iteration 379, loss = 0.35261142\n",
      "Iteration 380, loss = 0.35219225\n",
      "Iteration 381, loss = 0.35177378\n",
      "Iteration 382, loss = 0.35135613\n",
      "Iteration 383, loss = 0.35093897\n",
      "Iteration 384, loss = 0.35052241\n",
      "Iteration 385, loss = 0.35010656\n",
      "Iteration 386, loss = 0.34969119\n",
      "Iteration 387, loss = 0.34927650\n",
      "Iteration 388, loss = 0.34886237\n",
      "Iteration 389, loss = 0.34844881\n",
      "Iteration 390, loss = 0.34803586\n",
      "Iteration 391, loss = 0.34762344\n",
      "Iteration 392, loss = 0.34721169\n",
      "Iteration 393, loss = 0.34680034\n",
      "Iteration 394, loss = 0.34638983\n",
      "Iteration 395, loss = 0.34597964\n",
      "Iteration 396, loss = 0.34557000\n",
      "Iteration 397, loss = 0.34516106\n",
      "Iteration 398, loss = 0.34475260\n",
      "Iteration 399, loss = 0.34434468\n",
      "Iteration 400, loss = 0.34393740\n",
      "Iteration 401, loss = 0.34353055\n",
      "Iteration 402, loss = 0.34312438\n",
      "Iteration 403, loss = 0.34271859\n",
      "Iteration 404, loss = 0.34231354\n",
      "Iteration 405, loss = 0.34190881\n",
      "Iteration 406, loss = 0.34150484\n",
      "Iteration 407, loss = 0.34110119\n",
      "Iteration 408, loss = 0.34069829\n",
      "Iteration 409, loss = 0.34029571\n",
      "Iteration 410, loss = 0.33989388\n",
      "Iteration 411, loss = 0.33949236\n",
      "Iteration 412, loss = 0.33909159\n",
      "Iteration 413, loss = 0.33869113\n",
      "Iteration 414, loss = 0.33829141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 415, loss = 0.33789200\n",
      "Iteration 416, loss = 0.33749339\n",
      "Iteration 417, loss = 0.33709506\n",
      "Iteration 418, loss = 0.33669724\n",
      "Iteration 419, loss = 0.33630006\n",
      "Iteration 420, loss = 0.33590334\n",
      "Iteration 421, loss = 0.33550712\n",
      "Iteration 422, loss = 0.33511151\n",
      "Iteration 423, loss = 0.33471624\n",
      "Iteration 424, loss = 0.33432171\n",
      "Iteration 425, loss = 0.33392746\n",
      "Iteration 426, loss = 0.33353395\n",
      "Iteration 427, loss = 0.33314076\n",
      "Iteration 428, loss = 0.33274810\n",
      "Iteration 429, loss = 0.33235597\n",
      "Iteration 430, loss = 0.33196440\n",
      "Iteration 431, loss = 0.33157322\n",
      "Iteration 432, loss = 0.33118270\n",
      "Iteration 433, loss = 0.33079249\n",
      "Iteration 434, loss = 0.33040299\n",
      "Iteration 435, loss = 0.33001377\n",
      "Iteration 436, loss = 0.32962526\n",
      "Iteration 437, loss = 0.32923705\n",
      "Iteration 438, loss = 0.32884950\n",
      "Iteration 439, loss = 0.32846233\n",
      "Iteration 440, loss = 0.32807582\n",
      "Iteration 441, loss = 0.32768985\n",
      "Iteration 442, loss = 0.32730428\n",
      "Iteration 443, loss = 0.32691936\n",
      "Iteration 444, loss = 0.32653473\n",
      "Iteration 445, loss = 0.32615077\n",
      "Iteration 446, loss = 0.32576711\n",
      "Iteration 447, loss = 0.32538412\n",
      "Iteration 448, loss = 0.32500147\n",
      "Iteration 449, loss = 0.32461941\n",
      "Iteration 450, loss = 0.32423785\n",
      "Iteration 451, loss = 0.32385688\n",
      "Iteration 452, loss = 0.32347623\n",
      "Iteration 453, loss = 0.32309620\n",
      "Iteration 454, loss = 0.32271658\n",
      "Iteration 455, loss = 0.32233753\n",
      "Iteration 456, loss = 0.32195895\n",
      "Iteration 457, loss = 0.32158080\n",
      "Iteration 458, loss = 0.32120330\n",
      "Iteration 459, loss = 0.32082613\n",
      "Iteration 460, loss = 0.32044959\n",
      "Iteration 461, loss = 0.32007337\n",
      "Iteration 462, loss = 0.31969788\n",
      "Iteration 463, loss = 0.31932266\n",
      "Iteration 464, loss = 0.31894799\n",
      "Iteration 465, loss = 0.31857385\n",
      "Iteration 466, loss = 0.31820019\n",
      "Iteration 467, loss = 0.31782701\n",
      "Iteration 468, loss = 0.31745431\n",
      "Iteration 469, loss = 0.31708214\n",
      "Iteration 470, loss = 0.31671036\n",
      "Iteration 471, loss = 0.31633925\n",
      "Iteration 472, loss = 0.31596844\n",
      "Iteration 473, loss = 0.31559827\n",
      "Iteration 474, loss = 0.31522841\n",
      "Iteration 475, loss = 0.31485926\n",
      "Iteration 476, loss = 0.31449041\n",
      "Iteration 477, loss = 0.31412209\n",
      "Iteration 478, loss = 0.31375428\n",
      "Iteration 479, loss = 0.31338695\n",
      "Iteration 480, loss = 0.31302013\n",
      "Iteration 481, loss = 0.31265373\n",
      "Iteration 482, loss = 0.31228793\n",
      "Iteration 483, loss = 0.31192247\n",
      "Iteration 484, loss = 0.31155770\n",
      "Iteration 485, loss = 0.31119318\n",
      "Iteration 486, loss = 0.31082931\n",
      "Iteration 487, loss = 0.31046581\n",
      "Iteration 488, loss = 0.31010287\n",
      "Iteration 489, loss = 0.30974035\n",
      "Iteration 490, loss = 0.30937839\n",
      "Iteration 491, loss = 0.30901686\n",
      "Iteration 492, loss = 0.30865581\n",
      "Iteration 493, loss = 0.30829532\n",
      "Iteration 494, loss = 0.30793517\n",
      "Iteration 495, loss = 0.30757572\n",
      "Iteration 496, loss = 0.30721652\n",
      "Iteration 497, loss = 0.30685797\n",
      "Iteration 498, loss = 0.30649978\n",
      "Iteration 499, loss = 0.30614212\n",
      "Iteration 500, loss = 0.30578495\n",
      "Iteration 501, loss = 0.30542825\n",
      "Iteration 502, loss = 0.30507206\n",
      "Iteration 503, loss = 0.30471626\n",
      "Iteration 504, loss = 0.30436112\n",
      "Iteration 505, loss = 0.30400628\n",
      "Iteration 506, loss = 0.30365205\n",
      "Iteration 507, loss = 0.30329813\n",
      "Iteration 508, loss = 0.30294493\n",
      "Iteration 509, loss = 0.30259195\n",
      "Iteration 510, loss = 0.30223974\n",
      "Iteration 511, loss = 0.30188781\n",
      "Iteration 512, loss = 0.30153631\n",
      "Iteration 513, loss = 0.30118547\n",
      "Iteration 514, loss = 0.30083493\n",
      "Iteration 515, loss = 0.30048501\n",
      "Iteration 516, loss = 0.30013545\n",
      "Iteration 517, loss = 0.29978648\n",
      "Iteration 518, loss = 0.29943788\n",
      "Iteration 519, loss = 0.29908987\n",
      "Iteration 520, loss = 0.29874225\n",
      "Iteration 521, loss = 0.29839515\n",
      "Iteration 522, loss = 0.29804856\n",
      "Iteration 523, loss = 0.29770234\n",
      "Iteration 524, loss = 0.29735678\n",
      "Iteration 525, loss = 0.29701148\n",
      "Iteration 526, loss = 0.29666688\n",
      "Iteration 527, loss = 0.29632260\n",
      "Iteration 528, loss = 0.29597875\n",
      "Iteration 529, loss = 0.29563556\n",
      "Iteration 530, loss = 0.29529268\n",
      "Iteration 531, loss = 0.29495047\n",
      "Iteration 532, loss = 0.29460852\n",
      "Iteration 533, loss = 0.29426721\n",
      "Iteration 534, loss = 0.29392626\n",
      "Iteration 535, loss = 0.29358585\n",
      "Iteration 536, loss = 0.29324591\n",
      "Iteration 537, loss = 0.29290644\n",
      "Iteration 538, loss = 0.29256750\n",
      "Iteration 539, loss = 0.29222893\n",
      "Iteration 540, loss = 0.29189102\n",
      "Iteration 541, loss = 0.29155337\n",
      "Iteration 542, loss = 0.29121639\n",
      "Iteration 543, loss = 0.29087975\n",
      "Iteration 544, loss = 0.29054357\n",
      "Iteration 545, loss = 0.29020800\n",
      "Iteration 546, loss = 0.28987276\n",
      "Iteration 547, loss = 0.28953820\n",
      "Iteration 548, loss = 0.28920389\n",
      "Iteration 549, loss = 0.28887022\n",
      "Iteration 550, loss = 0.28853692\n",
      "Iteration 551, loss = 0.28820412\n",
      "Iteration 552, loss = 0.28787184\n",
      "Iteration 553, loss = 0.28753996\n",
      "Iteration 554, loss = 0.28720870\n",
      "Iteration 555, loss = 0.28687776\n",
      "Iteration 556, loss = 0.28654743\n",
      "Iteration 557, loss = 0.28621742\n",
      "Iteration 558, loss = 0.28588807\n",
      "Iteration 559, loss = 0.28555902\n",
      "Iteration 560, loss = 0.28523058\n",
      "Iteration 561, loss = 0.28490255\n",
      "Iteration 562, loss = 0.28457496\n",
      "Iteration 563, loss = 0.28424802\n",
      "Iteration 564, loss = 0.28392138\n",
      "Iteration 565, loss = 0.28359524\n",
      "Iteration 566, loss = 0.28326962\n",
      "Iteration 567, loss = 0.28294445\n",
      "Iteration 568, loss = 0.28261981\n",
      "Iteration 569, loss = 0.28229554\n",
      "Iteration 570, loss = 0.28197194\n",
      "Iteration 571, loss = 0.28164859\n",
      "Iteration 572, loss = 0.28132589\n",
      "Iteration 573, loss = 0.28100355\n",
      "Iteration 574, loss = 0.28068184\n",
      "Iteration 575, loss = 0.28036060\n",
      "Iteration 576, loss = 0.28003965\n",
      "Iteration 577, loss = 0.27971940\n",
      "Iteration 578, loss = 0.27939943\n",
      "Iteration 579, loss = 0.27908012\n",
      "Iteration 580, loss = 0.27876118\n",
      "Iteration 581, loss = 0.27844273\n",
      "Iteration 582, loss = 0.27812482\n",
      "Iteration 583, loss = 0.27780726\n",
      "Iteration 584, loss = 0.27749030\n",
      "Iteration 585, loss = 0.27717380\n",
      "Iteration 586, loss = 0.27685771\n",
      "Iteration 587, loss = 0.27654215\n",
      "Iteration 588, loss = 0.27622705\n",
      "Iteration 589, loss = 0.27591241\n",
      "Iteration 590, loss = 0.27559832\n",
      "Iteration 591, loss = 0.27528458\n",
      "Iteration 592, loss = 0.27497140\n",
      "Iteration 593, loss = 0.27465873\n",
      "Iteration 594, loss = 0.27434645\n",
      "Iteration 595, loss = 0.27403466\n",
      "Iteration 596, loss = 0.27372346\n",
      "Iteration 597, loss = 0.27341252\n",
      "Iteration 598, loss = 0.27310223\n",
      "Iteration 599, loss = 0.27279234\n",
      "Iteration 600, loss = 0.27248289\n",
      "Iteration 601, loss = 0.27217405\n",
      "Iteration 602, loss = 0.27186556\n",
      "Iteration 603, loss = 0.27155757\n",
      "Iteration 604, loss = 0.27125013\n",
      "Iteration 605, loss = 0.27094306\n",
      "Iteration 606, loss = 0.27063650\n",
      "Iteration 607, loss = 0.27033047\n",
      "Iteration 608, loss = 0.27002481\n",
      "Iteration 609, loss = 0.26971971\n",
      "Iteration 610, loss = 0.26941505\n",
      "Iteration 611, loss = 0.26911081\n",
      "Iteration 612, loss = 0.26880719\n",
      "Iteration 613, loss = 0.26850394\n",
      "Iteration 614, loss = 0.26820110\n",
      "Iteration 615, loss = 0.26789889\n",
      "Iteration 616, loss = 0.26759701\n",
      "Iteration 617, loss = 0.26729574\n",
      "Iteration 618, loss = 0.26699480\n",
      "Iteration 619, loss = 0.26669437\n",
      "Iteration 620, loss = 0.26639455\n",
      "Iteration 621, loss = 0.26609505\n",
      "Iteration 622, loss = 0.26579603\n",
      "Iteration 623, loss = 0.26549758\n",
      "Iteration 624, loss = 0.26519951\n",
      "Iteration 625, loss = 0.26490199\n",
      "Iteration 626, loss = 0.26460486\n",
      "Iteration 627, loss = 0.26430825\n",
      "Iteration 628, loss = 0.26401217\n",
      "Iteration 629, loss = 0.26371646\n",
      "Iteration 630, loss = 0.26342121\n",
      "Iteration 631, loss = 0.26312658\n",
      "Iteration 632, loss = 0.26283227\n",
      "Iteration 633, loss = 0.26253849\n",
      "Iteration 634, loss = 0.26224516\n",
      "Iteration 635, loss = 0.26195229\n",
      "Iteration 636, loss = 0.26165998\n",
      "Iteration 637, loss = 0.26136804\n",
      "Iteration 638, loss = 0.26107657\n",
      "Iteration 639, loss = 0.26078566\n",
      "Iteration 640, loss = 0.26049510\n",
      "Iteration 641, loss = 0.26020512\n",
      "Iteration 642, loss = 0.25991552\n",
      "Iteration 643, loss = 0.25962639\n",
      "Iteration 644, loss = 0.25933786\n",
      "Iteration 645, loss = 0.25904965\n",
      "Iteration 646, loss = 0.25876193\n",
      "Iteration 647, loss = 0.25847474\n",
      "Iteration 648, loss = 0.25818796\n",
      "Iteration 649, loss = 0.25790173\n",
      "Iteration 650, loss = 0.25761582\n",
      "Iteration 651, loss = 0.25733051\n",
      "Iteration 652, loss = 0.25704562\n",
      "Iteration 653, loss = 0.25676115\n",
      "Iteration 654, loss = 0.25647724\n",
      "Iteration 655, loss = 0.25619372\n",
      "Iteration 656, loss = 0.25591067\n",
      "Iteration 657, loss = 0.25562816\n",
      "Iteration 658, loss = 0.25534606\n",
      "Iteration 659, loss = 0.25506437\n",
      "Iteration 660, loss = 0.25478323\n",
      "Iteration 661, loss = 0.25450250\n",
      "Iteration 662, loss = 0.25422233\n",
      "Iteration 663, loss = 0.25394244\n",
      "Iteration 664, loss = 0.25366321\n",
      "Iteration 665, loss = 0.25338432\n",
      "Iteration 666, loss = 0.25310590\n",
      "Iteration 667, loss = 0.25282805\n",
      "Iteration 668, loss = 0.25255056\n",
      "Iteration 669, loss = 0.25227349\n",
      "Iteration 670, loss = 0.25199708\n",
      "Iteration 671, loss = 0.25172089\n",
      "Iteration 672, loss = 0.25144532\n",
      "Iteration 673, loss = 0.25117013\n",
      "Iteration 674, loss = 0.25089538\n",
      "Iteration 675, loss = 0.25062121\n",
      "Iteration 676, loss = 0.25034737\n",
      "Iteration 677, loss = 0.25007410\n",
      "Iteration 678, loss = 0.24980118\n",
      "Iteration 679, loss = 0.24952884\n",
      "Iteration 680, loss = 0.24925686\n",
      "Iteration 681, loss = 0.24898532\n",
      "Iteration 682, loss = 0.24871442\n",
      "Iteration 683, loss = 0.24844378\n",
      "Iteration 684, loss = 0.24817362\n",
      "Iteration 685, loss = 0.24790403\n",
      "Iteration 686, loss = 0.24763477\n",
      "Iteration 687, loss = 0.24736613\n",
      "Iteration 688, loss = 0.24709774\n",
      "Iteration 689, loss = 0.24683001\n",
      "Iteration 690, loss = 0.24656264\n",
      "Iteration 691, loss = 0.24629577\n",
      "Iteration 692, loss = 0.24602952\n",
      "Iteration 693, loss = 0.24576355\n",
      "Iteration 694, loss = 0.24549814\n",
      "Iteration 695, loss = 0.24523304\n",
      "Iteration 696, loss = 0.24496860\n",
      "Iteration 697, loss = 0.24470439\n",
      "Iteration 698, loss = 0.24444075\n",
      "Iteration 699, loss = 0.24417757\n",
      "Iteration 700, loss = 0.24391475\n",
      "Iteration 701, loss = 0.24365254\n",
      "Iteration 702, loss = 0.24339057\n",
      "Iteration 703, loss = 0.24312929\n",
      "Iteration 704, loss = 0.24286828\n",
      "Iteration 705, loss = 0.24260770\n",
      "Iteration 706, loss = 0.24234775\n",
      "Iteration 707, loss = 0.24208811\n",
      "Iteration 708, loss = 0.24182893\n",
      "Iteration 709, loss = 0.24157017\n",
      "Iteration 710, loss = 0.24131206\n",
      "Iteration 711, loss = 0.24105436\n",
      "Iteration 712, loss = 0.24079723\n",
      "Iteration 713, loss = 0.24054042\n",
      "Iteration 714, loss = 0.24028410\n",
      "Iteration 715, loss = 0.24002823\n",
      "Iteration 716, loss = 0.23977286\n",
      "Iteration 717, loss = 0.23951789\n",
      "Iteration 718, loss = 0.23926341\n",
      "Iteration 719, loss = 0.23900937\n",
      "Iteration 720, loss = 0.23875577\n",
      "Iteration 721, loss = 0.23850261\n",
      "Iteration 722, loss = 0.23824991\n",
      "Iteration 723, loss = 0.23799765\n",
      "Iteration 724, loss = 0.23774580\n",
      "Iteration 725, loss = 0.23749451\n",
      "Iteration 726, loss = 0.23724349\n",
      "Iteration 727, loss = 0.23699306\n",
      "Iteration 728, loss = 0.23674298\n",
      "Iteration 729, loss = 0.23649343\n",
      "Iteration 730, loss = 0.23624418\n",
      "Iteration 731, loss = 0.23599553\n",
      "Iteration 732, loss = 0.23574718\n",
      "Iteration 733, loss = 0.23549937\n",
      "Iteration 734, loss = 0.23525195\n",
      "Iteration 735, loss = 0.23500497\n",
      "Iteration 736, loss = 0.23475842\n",
      "Iteration 737, loss = 0.23451235\n",
      "Iteration 738, loss = 0.23426668\n",
      "Iteration 739, loss = 0.23402141\n",
      "Iteration 740, loss = 0.23377669\n",
      "Iteration 741, loss = 0.23353223\n",
      "Iteration 742, loss = 0.23328843\n",
      "Iteration 743, loss = 0.23304488\n",
      "Iteration 744, loss = 0.23280179\n",
      "Iteration 745, loss = 0.23255912\n",
      "Iteration 746, loss = 0.23231702\n",
      "Iteration 747, loss = 0.23207520\n",
      "Iteration 748, loss = 0.23183386\n",
      "Iteration 749, loss = 0.23159293\n",
      "Iteration 750, loss = 0.23135248\n",
      "Iteration 751, loss = 0.23111242\n",
      "Iteration 752, loss = 0.23087280\n",
      "Iteration 753, loss = 0.23063359\n",
      "Iteration 754, loss = 0.23039484\n",
      "Iteration 755, loss = 0.23015650\n",
      "Iteration 756, loss = 0.22991855\n",
      "Iteration 757, loss = 0.22968119\n",
      "Iteration 758, loss = 0.22944405\n",
      "Iteration 759, loss = 0.22920750\n",
      "Iteration 760, loss = 0.22897124\n",
      "Iteration 761, loss = 0.22873548\n",
      "Iteration 762, loss = 0.22850013\n",
      "Iteration 763, loss = 0.22826518\n",
      "Iteration 764, loss = 0.22803072\n",
      "Iteration 765, loss = 0.22779658\n",
      "Iteration 766, loss = 0.22756299\n",
      "Iteration 767, loss = 0.22732972\n",
      "Iteration 768, loss = 0.22709694\n",
      "Iteration 769, loss = 0.22686449\n",
      "Iteration 770, loss = 0.22663252\n",
      "Iteration 771, loss = 0.22640098\n",
      "Iteration 772, loss = 0.22616979\n",
      "Iteration 773, loss = 0.22593915\n",
      "Iteration 774, loss = 0.22570876\n",
      "Iteration 775, loss = 0.22547892\n",
      "Iteration 776, loss = 0.22524940\n",
      "Iteration 777, loss = 0.22502033\n",
      "Iteration 778, loss = 0.22479170\n",
      "Iteration 779, loss = 0.22456342\n",
      "Iteration 780, loss = 0.22433572\n",
      "Iteration 781, loss = 0.22410822\n",
      "Iteration 782, loss = 0.22388129\n",
      "Iteration 783, loss = 0.22365466\n",
      "Iteration 784, loss = 0.22342847\n",
      "Iteration 785, loss = 0.22320276\n",
      "Iteration 786, loss = 0.22297736\n",
      "Iteration 787, loss = 0.22275250\n",
      "Iteration 788, loss = 0.22252787\n",
      "Iteration 789, loss = 0.22230384\n",
      "Iteration 790, loss = 0.22208006\n",
      "Iteration 791, loss = 0.22185676\n",
      "Iteration 792, loss = 0.22163392\n",
      "Iteration 793, loss = 0.22141138\n",
      "Iteration 794, loss = 0.22118930\n",
      "Iteration 795, loss = 0.22096763\n",
      "Iteration 796, loss = 0.22074630\n",
      "Iteration 797, loss = 0.22052553\n",
      "Iteration 798, loss = 0.22030501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 799, loss = 0.22008493\n",
      "Iteration 800, loss = 0.21986529\n",
      "Iteration 801, loss = 0.21964600\n",
      "Iteration 802, loss = 0.21942721\n",
      "Iteration 803, loss = 0.21920867\n",
      "Iteration 804, loss = 0.21899071\n",
      "Iteration 805, loss = 0.21877302\n",
      "Iteration 806, loss = 0.21855575\n",
      "Iteration 807, loss = 0.21833886\n",
      "Iteration 808, loss = 0.21812242\n",
      "Iteration 809, loss = 0.21790633\n",
      "Iteration 810, loss = 0.21769070\n",
      "Iteration 811, loss = 0.21747540\n",
      "Iteration 812, loss = 0.21726052\n",
      "Iteration 813, loss = 0.21704607\n",
      "Iteration 814, loss = 0.21683193\n",
      "Iteration 815, loss = 0.21661832\n",
      "Iteration 816, loss = 0.21640499\n",
      "Iteration 817, loss = 0.21619203\n",
      "Iteration 818, loss = 0.21597962\n",
      "Iteration 819, loss = 0.21576739\n",
      "Iteration 820, loss = 0.21555574\n",
      "Iteration 821, loss = 0.21534437\n",
      "Iteration 822, loss = 0.21513341\n",
      "Iteration 823, loss = 0.21492283\n",
      "Iteration 824, loss = 0.21471268\n",
      "Iteration 825, loss = 0.21450285\n",
      "Iteration 826, loss = 0.21429351\n",
      "Iteration 827, loss = 0.21408448\n",
      "Iteration 828, loss = 0.21387582\n",
      "Iteration 829, loss = 0.21366766\n",
      "Iteration 830, loss = 0.21345971\n",
      "Iteration 831, loss = 0.21325233\n",
      "Iteration 832, loss = 0.21304522\n",
      "Iteration 833, loss = 0.21283853\n",
      "Iteration 834, loss = 0.21263214\n",
      "Iteration 835, loss = 0.21242632\n",
      "Iteration 836, loss = 0.21222067\n",
      "Iteration 837, loss = 0.21201554\n",
      "Iteration 838, loss = 0.21181071\n",
      "Iteration 839, loss = 0.21160630\n",
      "Iteration 840, loss = 0.21140227\n",
      "Iteration 841, loss = 0.21119859\n",
      "Iteration 842, loss = 0.21099529\n",
      "Iteration 843, loss = 0.21079244\n",
      "Iteration 844, loss = 0.21058987\n",
      "Iteration 845, loss = 0.21038771\n",
      "Iteration 846, loss = 0.21018599\n",
      "Iteration 847, loss = 0.20998458\n",
      "Iteration 848, loss = 0.20978354\n",
      "Iteration 849, loss = 0.20958286\n",
      "Iteration 850, loss = 0.20938262\n",
      "Iteration 851, loss = 0.20918267\n",
      "Iteration 852, loss = 0.20898320\n",
      "Iteration 853, loss = 0.20878401\n",
      "Iteration 854, loss = 0.20858518\n",
      "Iteration 855, loss = 0.20838680\n",
      "Iteration 856, loss = 0.20818874\n",
      "Iteration 857, loss = 0.20799102\n",
      "Iteration 858, loss = 0.20779376\n",
      "Iteration 859, loss = 0.20759680\n",
      "Iteration 860, loss = 0.20740020\n",
      "Iteration 861, loss = 0.20720396\n",
      "Iteration 862, loss = 0.20700818\n",
      "Iteration 863, loss = 0.20681266\n",
      "Iteration 864, loss = 0.20661754\n",
      "Iteration 865, loss = 0.20642280\n",
      "Iteration 866, loss = 0.20622839\n",
      "Iteration 867, loss = 0.20603432\n",
      "Iteration 868, loss = 0.20584075\n",
      "Iteration 869, loss = 0.20564739\n",
      "Iteration 870, loss = 0.20545443\n",
      "Iteration 871, loss = 0.20526186\n",
      "Iteration 872, loss = 0.20506966\n",
      "Iteration 873, loss = 0.20487776\n",
      "Iteration 874, loss = 0.20468626\n",
      "Iteration 875, loss = 0.20449519\n",
      "Iteration 876, loss = 0.20430463\n",
      "Iteration 877, loss = 0.20411439\n",
      "Iteration 878, loss = 0.20392451\n",
      "Iteration 879, loss = 0.20373503\n",
      "Iteration 880, loss = 0.20354587\n",
      "Iteration 881, loss = 0.20335712\n",
      "Iteration 882, loss = 0.20316869\n",
      "Iteration 883, loss = 0.20298066\n",
      "Iteration 884, loss = 0.20279294\n",
      "Iteration 885, loss = 0.20260561\n",
      "Iteration 886, loss = 0.20241860\n",
      "Iteration 887, loss = 0.20223199\n",
      "Iteration 888, loss = 0.20204569\n",
      "Iteration 889, loss = 0.20185980\n",
      "Iteration 890, loss = 0.20167421\n",
      "Iteration 891, loss = 0.20148898\n",
      "Iteration 892, loss = 0.20130411\n",
      "Iteration 893, loss = 0.20111958\n",
      "Iteration 894, loss = 0.20093540\n",
      "Iteration 895, loss = 0.20075155\n",
      "Iteration 896, loss = 0.20056809\n",
      "Iteration 897, loss = 0.20038492\n",
      "Iteration 898, loss = 0.20020212\n",
      "Iteration 899, loss = 0.20001965\n",
      "Iteration 900, loss = 0.19983753\n",
      "Iteration 901, loss = 0.19965575\n",
      "Iteration 902, loss = 0.19947431\n",
      "Iteration 903, loss = 0.19929321\n",
      "Iteration 904, loss = 0.19911246\n",
      "Iteration 905, loss = 0.19893209\n",
      "Iteration 906, loss = 0.19875197\n",
      "Iteration 907, loss = 0.19857224\n",
      "Iteration 908, loss = 0.19839283\n",
      "Iteration 909, loss = 0.19821379\n",
      "Iteration 910, loss = 0.19803503\n",
      "Iteration 911, loss = 0.19785666\n",
      "Iteration 912, loss = 0.19767858\n",
      "Iteration 913, loss = 0.19750084\n",
      "Iteration 914, loss = 0.19732345\n",
      "Iteration 915, loss = 0.19714637\n",
      "Iteration 916, loss = 0.19696965\n",
      "Iteration 917, loss = 0.19679327\n",
      "Iteration 918, loss = 0.19661720\n",
      "Iteration 919, loss = 0.19644144\n",
      "Iteration 920, loss = 0.19626606\n",
      "Iteration 921, loss = 0.19609097\n",
      "Iteration 922, loss = 0.19591619\n",
      "Iteration 923, loss = 0.19574180\n",
      "Iteration 924, loss = 0.19556767\n",
      "Iteration 925, loss = 0.19539389\n",
      "Iteration 926, loss = 0.19522045\n",
      "Iteration 927, loss = 0.19504734\n",
      "Iteration 928, loss = 0.19487453\n",
      "Iteration 929, loss = 0.19470206\n",
      "Iteration 930, loss = 0.19452991\n",
      "Iteration 931, loss = 0.19435806\n",
      "Iteration 932, loss = 0.19418657\n",
      "Iteration 933, loss = 0.19401535\n",
      "Iteration 934, loss = 0.19384453\n",
      "Iteration 935, loss = 0.19367396\n",
      "Iteration 936, loss = 0.19350373\n",
      "Iteration 937, loss = 0.19333384\n",
      "Iteration 938, loss = 0.19316424\n",
      "Iteration 939, loss = 0.19299496\n",
      "Iteration 940, loss = 0.19282601\n",
      "Iteration 941, loss = 0.19265736\n",
      "Iteration 942, loss = 0.19248908\n",
      "Iteration 943, loss = 0.19232105\n",
      "Iteration 944, loss = 0.19215337\n",
      "Iteration 945, loss = 0.19198598\n",
      "Iteration 946, loss = 0.19181890\n",
      "Iteration 947, loss = 0.19165216\n",
      "Iteration 948, loss = 0.19148571\n",
      "Iteration 949, loss = 0.19131960\n",
      "Iteration 950, loss = 0.19115378\n",
      "Iteration 951, loss = 0.19098826\n",
      "Iteration 952, loss = 0.19082307\n",
      "Iteration 953, loss = 0.19065815\n",
      "Iteration 954, loss = 0.19049360\n",
      "Iteration 955, loss = 0.19032932\n",
      "Iteration 956, loss = 0.19016537\n",
      "Iteration 957, loss = 0.19000171\n",
      "Iteration 958, loss = 0.18983835\n",
      "Iteration 959, loss = 0.18967529\n",
      "Iteration 960, loss = 0.18951257\n",
      "Iteration 961, loss = 0.18935013\n",
      "Iteration 962, loss = 0.18918801\n",
      "Iteration 963, loss = 0.18902616\n",
      "Iteration 964, loss = 0.18886464\n",
      "Iteration 965, loss = 0.18870341\n",
      "Iteration 966, loss = 0.18854252\n",
      "Iteration 967, loss = 0.18838188\n",
      "Iteration 968, loss = 0.18822155\n",
      "Iteration 969, loss = 0.18806151\n",
      "Iteration 970, loss = 0.18790180\n",
      "Iteration 971, loss = 0.18774242\n",
      "Iteration 972, loss = 0.18758325\n",
      "Iteration 973, loss = 0.18742445\n",
      "Iteration 974, loss = 0.18726589\n",
      "Iteration 975, loss = 0.18710766\n",
      "Iteration 976, loss = 0.18694976\n",
      "Iteration 977, loss = 0.18679207\n",
      "Iteration 978, loss = 0.18663474\n",
      "Iteration 979, loss = 0.18647766\n",
      "Iteration 980, loss = 0.18632089\n",
      "Iteration 981, loss = 0.18616448\n",
      "Iteration 982, loss = 0.18600826\n",
      "Iteration 983, loss = 0.18585236\n",
      "Iteration 984, loss = 0.18569677\n",
      "Iteration 985, loss = 0.18554148\n",
      "Iteration 986, loss = 0.18538647\n",
      "Iteration 987, loss = 0.18523174\n",
      "Iteration 988, loss = 0.18507729\n",
      "Iteration 989, loss = 0.18492314\n",
      "Iteration 990, loss = 0.18476930\n",
      "Iteration 991, loss = 0.18461570\n",
      "Iteration 992, loss = 0.18446242\n",
      "Iteration 993, loss = 0.18430941\n",
      "Iteration 994, loss = 0.18415672\n",
      "Iteration 995, loss = 0.18400428\n",
      "Iteration 996, loss = 0.18385211\n",
      "Iteration 997, loss = 0.18370024\n",
      "Iteration 998, loss = 0.18354869\n",
      "Iteration 999, loss = 0.18339738\n",
      "Iteration 1000, loss = 0.18324634\n",
      "Iteration 1, loss = 1.35165820\n",
      "Iteration 2, loss = 1.31479672\n",
      "Iteration 3, loss = 1.26518330\n",
      "Iteration 4, loss = 1.20695536\n",
      "Iteration 5, loss = 1.14435940\n",
      "Iteration 6, loss = 1.08116064\n",
      "Iteration 7, loss = 1.02062790\n",
      "Iteration 8, loss = 0.96544874\n",
      "Iteration 9, loss = 0.91730971\n",
      "Iteration 10, loss = 0.87688140\n",
      "Iteration 11, loss = 0.84409054\n",
      "Iteration 12, loss = 0.81809185\n",
      "Iteration 13, loss = 0.79793326\n",
      "Iteration 14, loss = 0.78237749\n",
      "Iteration 15, loss = 0.77031758\n",
      "Iteration 16, loss = 0.76090277\n",
      "Iteration 17, loss = 0.75313911\n",
      "Iteration 18, loss = 0.74672448\n",
      "Iteration 19, loss = 0.74136719\n",
      "Iteration 20, loss = 0.73677897\n",
      "Iteration 21, loss = 0.73289239\n",
      "Iteration 22, loss = 0.72940609\n",
      "Iteration 23, loss = 0.72562339\n",
      "Iteration 24, loss = 0.72150246\n",
      "Iteration 25, loss = 0.71689034\n",
      "Iteration 26, loss = 0.71177166\n",
      "Iteration 27, loss = 0.70637501\n",
      "Iteration 28, loss = 0.70077283\n",
      "Iteration 29, loss = 0.69506261\n",
      "Iteration 30, loss = 0.68942855\n",
      "Iteration 31, loss = 0.68397271\n",
      "Iteration 32, loss = 0.67881663\n",
      "Iteration 33, loss = 0.67401590\n",
      "Iteration 34, loss = 0.66957716\n",
      "Iteration 35, loss = 0.66547880\n",
      "Iteration 36, loss = 0.66172206\n",
      "Iteration 37, loss = 0.65826401\n",
      "Iteration 38, loss = 0.65507485\n",
      "Iteration 39, loss = 0.65210329\n",
      "Iteration 40, loss = 0.64930088\n",
      "Iteration 41, loss = 0.64662609\n",
      "Iteration 42, loss = 0.64404742\n",
      "Iteration 43, loss = 0.64153827\n",
      "Iteration 44, loss = 0.63907490\n",
      "Iteration 45, loss = 0.63663705\n",
      "Iteration 46, loss = 0.63420178\n",
      "Iteration 47, loss = 0.63176098\n",
      "Iteration 48, loss = 0.62931335\n",
      "Iteration 49, loss = 0.62686443\n",
      "Iteration 50, loss = 0.62441103\n",
      "Iteration 51, loss = 0.62195874\n",
      "Iteration 52, loss = 0.61951604\n",
      "Iteration 53, loss = 0.61709314\n",
      "Iteration 54, loss = 0.61469369\n",
      "Iteration 55, loss = 0.61232366\n",
      "Iteration 56, loss = 0.60998785\n",
      "Iteration 57, loss = 0.60768983\n",
      "Iteration 58, loss = 0.60543189\n",
      "Iteration 59, loss = 0.60321513\n",
      "Iteration 60, loss = 0.60103962\n",
      "Iteration 61, loss = 0.59890456\n",
      "Iteration 62, loss = 0.59680850\n",
      "Iteration 63, loss = 0.59474953\n",
      "Iteration 64, loss = 0.59272545\n",
      "Iteration 65, loss = 0.59073396\n",
      "Iteration 66, loss = 0.58877278\n",
      "Iteration 67, loss = 0.58684498\n",
      "Iteration 68, loss = 0.58494449\n",
      "Iteration 69, loss = 0.58306880\n",
      "Iteration 70, loss = 0.58121681\n",
      "Iteration 71, loss = 0.57938875\n",
      "Iteration 72, loss = 0.57758231\n",
      "Iteration 73, loss = 0.57579671\n",
      "Iteration 74, loss = 0.57403135\n",
      "Iteration 75, loss = 0.57228580\n",
      "Iteration 76, loss = 0.57055970\n",
      "Iteration 77, loss = 0.56885283\n",
      "Iteration 78, loss = 0.56716498\n",
      "Iteration 79, loss = 0.56550060\n",
      "Iteration 80, loss = 0.56385495\n",
      "Iteration 81, loss = 0.56222784\n",
      "Iteration 82, loss = 0.56062208\n",
      "Iteration 83, loss = 0.55903723\n",
      "Iteration 84, loss = 0.55747066\n",
      "Iteration 85, loss = 0.55592394\n",
      "Iteration 86, loss = 0.55439485\n",
      "Iteration 87, loss = 0.55288306\n",
      "Iteration 88, loss = 0.55138822\n",
      "Iteration 89, loss = 0.54990998\n",
      "Iteration 90, loss = 0.54844798\n",
      "Iteration 91, loss = 0.54700185\n",
      "Iteration 92, loss = 0.54557125\n",
      "Iteration 93, loss = 0.54415577\n",
      "Iteration 94, loss = 0.54275514\n",
      "Iteration 95, loss = 0.54136912\n",
      "Iteration 96, loss = 0.53999731\n",
      "Iteration 97, loss = 0.53863996\n",
      "Iteration 98, loss = 0.53729636\n",
      "Iteration 99, loss = 0.53596615\n",
      "Iteration 100, loss = 0.53464904\n",
      "Iteration 101, loss = 0.53334481\n",
      "Iteration 102, loss = 0.53205319\n",
      "Iteration 103, loss = 0.53077452\n",
      "Iteration 104, loss = 0.52950885\n",
      "Iteration 105, loss = 0.52825496\n",
      "Iteration 106, loss = 0.52701261\n",
      "Iteration 107, loss = 0.52578156\n",
      "Iteration 108, loss = 0.52456160\n",
      "Iteration 109, loss = 0.52335251\n",
      "Iteration 110, loss = 0.52215405\n",
      "Iteration 111, loss = 0.52096602\n",
      "Iteration 112, loss = 0.51978972\n",
      "Iteration 113, loss = 0.51862458\n",
      "Iteration 114, loss = 0.51746952\n",
      "Iteration 115, loss = 0.51632435\n",
      "Iteration 116, loss = 0.51518887\n",
      "Iteration 117, loss = 0.51406290\n",
      "Iteration 118, loss = 0.51294625\n",
      "Iteration 119, loss = 0.51183874\n",
      "Iteration 120, loss = 0.51074018\n",
      "Iteration 121, loss = 0.50965041\n",
      "Iteration 122, loss = 0.50856924\n",
      "Iteration 123, loss = 0.50749650\n",
      "Iteration 124, loss = 0.50643204\n",
      "Iteration 125, loss = 0.50537602\n",
      "Iteration 126, loss = 0.50432831\n",
      "Iteration 127, loss = 0.50328840\n",
      "Iteration 128, loss = 0.50225618\n",
      "Iteration 129, loss = 0.50123149\n",
      "Iteration 130, loss = 0.50021423\n",
      "Iteration 131, loss = 0.49920426\n",
      "Iteration 132, loss = 0.49820345\n",
      "Iteration 133, loss = 0.49721312\n",
      "Iteration 134, loss = 0.49623080\n",
      "Iteration 135, loss = 0.49525725\n",
      "Iteration 136, loss = 0.49429147\n",
      "Iteration 137, loss = 0.49333262\n",
      "Iteration 138, loss = 0.49238190\n",
      "Iteration 139, loss = 0.49143941\n",
      "Iteration 140, loss = 0.49050369\n",
      "Iteration 141, loss = 0.48957501\n",
      "Iteration 142, loss = 0.48865250\n",
      "Iteration 143, loss = 0.48773618\n",
      "Iteration 144, loss = 0.48682606\n",
      "Iteration 145, loss = 0.48592215\n",
      "Iteration 146, loss = 0.48502444\n",
      "Iteration 147, loss = 0.48413288\n",
      "Iteration 148, loss = 0.48324745\n",
      "Iteration 149, loss = 0.48236809\n",
      "Iteration 150, loss = 0.48149473\n",
      "Iteration 151, loss = 0.48062731\n",
      "Iteration 152, loss = 0.47976576\n",
      "Iteration 153, loss = 0.47890999\n",
      "Iteration 154, loss = 0.47805991\n",
      "Iteration 155, loss = 0.47721557\n",
      "Iteration 156, loss = 0.47637676\n",
      "Iteration 157, loss = 0.47554327\n",
      "Iteration 158, loss = 0.47471497\n",
      "Iteration 159, loss = 0.47389179\n",
      "Iteration 160, loss = 0.47307373\n",
      "Iteration 161, loss = 0.47226080\n",
      "Iteration 162, loss = 0.47145282\n",
      "Iteration 163, loss = 0.47064970\n",
      "Iteration 164, loss = 0.46985137\n",
      "Iteration 165, loss = 0.46905774\n",
      "Iteration 166, loss = 0.46826874\n",
      "Iteration 167, loss = 0.46748429\n",
      "Iteration 168, loss = 0.46670432\n",
      "Iteration 169, loss = 0.46592874\n",
      "Iteration 170, loss = 0.46515749\n",
      "Iteration 171, loss = 0.46439049\n",
      "Iteration 172, loss = 0.46362767\n",
      "Iteration 173, loss = 0.46286896\n",
      "Iteration 174, loss = 0.46211429\n",
      "Iteration 175, loss = 0.46136358\n",
      "Iteration 176, loss = 0.46061678\n",
      "Iteration 177, loss = 0.45987381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 178, loss = 0.45913491\n",
      "Iteration 179, loss = 0.45840011\n",
      "Iteration 180, loss = 0.45766902\n",
      "Iteration 181, loss = 0.45694161\n",
      "Iteration 182, loss = 0.45621780\n",
      "Iteration 183, loss = 0.45549887\n",
      "Iteration 184, loss = 0.45478344\n",
      "Iteration 185, loss = 0.45407146\n",
      "Iteration 186, loss = 0.45336290\n",
      "Iteration 187, loss = 0.45265773\n",
      "Iteration 188, loss = 0.45195591\n",
      "Iteration 189, loss = 0.45125742\n",
      "Iteration 190, loss = 0.45056222\n",
      "Iteration 191, loss = 0.44987026\n",
      "Iteration 192, loss = 0.44918151\n",
      "Iteration 193, loss = 0.44849592\n",
      "Iteration 194, loss = 0.44781345\n",
      "Iteration 195, loss = 0.44713405\n",
      "Iteration 196, loss = 0.44645766\n",
      "Iteration 197, loss = 0.44578426\n",
      "Iteration 198, loss = 0.44511378\n",
      "Iteration 199, loss = 0.44444619\n",
      "Iteration 200, loss = 0.44378143\n",
      "Iteration 201, loss = 0.44311947\n",
      "Iteration 202, loss = 0.44246026\n",
      "Iteration 203, loss = 0.44180376\n",
      "Iteration 204, loss = 0.44114992\n",
      "Iteration 205, loss = 0.44049871\n",
      "Iteration 206, loss = 0.43985009\n",
      "Iteration 207, loss = 0.43920402\n",
      "Iteration 208, loss = 0.43856047\n",
      "Iteration 209, loss = 0.43791938\n",
      "Iteration 210, loss = 0.43728074\n",
      "Iteration 211, loss = 0.43664449\n",
      "Iteration 212, loss = 0.43601061\n",
      "Iteration 213, loss = 0.43537907\n",
      "Iteration 214, loss = 0.43474982\n",
      "Iteration 215, loss = 0.43412283\n",
      "Iteration 216, loss = 0.43349807\n",
      "Iteration 217, loss = 0.43287551\n",
      "Iteration 218, loss = 0.43225512\n",
      "Iteration 219, loss = 0.43163685\n",
      "Iteration 220, loss = 0.43102070\n",
      "Iteration 221, loss = 0.43040661\n",
      "Iteration 222, loss = 0.42979456\n",
      "Iteration 223, loss = 0.42918453\n",
      "Iteration 224, loss = 0.42857649\n",
      "Iteration 225, loss = 0.42797040\n",
      "Iteration 226, loss = 0.42736624\n",
      "Iteration 227, loss = 0.42676399\n",
      "Iteration 228, loss = 0.42616361\n",
      "Iteration 229, loss = 0.42556508\n",
      "Iteration 230, loss = 0.42496838\n",
      "Iteration 231, loss = 0.42437348\n",
      "Iteration 232, loss = 0.42378036\n",
      "Iteration 233, loss = 0.42318899\n",
      "Iteration 234, loss = 0.42259934\n",
      "Iteration 235, loss = 0.42201141\n",
      "Iteration 236, loss = 0.42142516\n",
      "Iteration 237, loss = 0.42084061\n",
      "Iteration 238, loss = 0.42025931\n",
      "Iteration 239, loss = 0.41967975\n",
      "Iteration 240, loss = 0.41910189\n",
      "Iteration 241, loss = 0.41852571\n",
      "Iteration 242, loss = 0.41795118\n",
      "Iteration 243, loss = 0.41737829\n",
      "Iteration 244, loss = 0.41680701\n",
      "Iteration 245, loss = 0.41623732\n",
      "Iteration 246, loss = 0.41566920\n",
      "Iteration 247, loss = 0.41510262\n",
      "Iteration 248, loss = 0.41453758\n",
      "Iteration 249, loss = 0.41397404\n",
      "Iteration 250, loss = 0.41341198\n",
      "Iteration 251, loss = 0.41285140\n",
      "Iteration 252, loss = 0.41229226\n",
      "Iteration 253, loss = 0.41173455\n",
      "Iteration 254, loss = 0.41117825\n",
      "Iteration 255, loss = 0.41062335\n",
      "Iteration 256, loss = 0.41007119\n",
      "Iteration 257, loss = 0.40952166\n",
      "Iteration 258, loss = 0.40897354\n",
      "Iteration 259, loss = 0.40842683\n",
      "Iteration 260, loss = 0.40788153\n",
      "Iteration 261, loss = 0.40733763\n",
      "Iteration 262, loss = 0.40679511\n",
      "Iteration 263, loss = 0.40625532\n",
      "Iteration 264, loss = 0.40571702\n",
      "Iteration 265, loss = 0.40518001\n",
      "Iteration 266, loss = 0.40464430\n",
      "Iteration 267, loss = 0.40410991\n",
      "Iteration 268, loss = 0.40357682\n",
      "Iteration 269, loss = 0.40304505\n",
      "Iteration 270, loss = 0.40251458\n",
      "Iteration 271, loss = 0.40198542\n",
      "Iteration 272, loss = 0.40145756\n",
      "Iteration 273, loss = 0.40093098\n",
      "Iteration 274, loss = 0.40040567\n",
      "Iteration 275, loss = 0.39988163\n",
      "Iteration 276, loss = 0.39935882\n",
      "Iteration 277, loss = 0.39883725\n",
      "Iteration 278, loss = 0.39831690\n",
      "Iteration 279, loss = 0.39779775\n",
      "Iteration 280, loss = 0.39727979\n",
      "Iteration 281, loss = 0.39676302\n",
      "Iteration 282, loss = 0.39624741\n",
      "Iteration 283, loss = 0.39573295\n",
      "Iteration 284, loss = 0.39521965\n",
      "Iteration 285, loss = 0.39470747\n",
      "Iteration 286, loss = 0.39419643\n",
      "Iteration 287, loss = 0.39368650\n",
      "Iteration 288, loss = 0.39317767\n",
      "Iteration 289, loss = 0.39266994\n",
      "Iteration 290, loss = 0.39216329\n",
      "Iteration 291, loss = 0.39165772\n",
      "Iteration 292, loss = 0.39115321\n",
      "Iteration 293, loss = 0.39064975\n",
      "Iteration 294, loss = 0.39014734\n",
      "Iteration 295, loss = 0.38964595\n",
      "Iteration 296, loss = 0.38914559\n",
      "Iteration 297, loss = 0.38864623\n",
      "Iteration 298, loss = 0.38814787\n",
      "Iteration 299, loss = 0.38765051\n",
      "Iteration 300, loss = 0.38715412\n",
      "Iteration 301, loss = 0.38665870\n",
      "Iteration 302, loss = 0.38616457\n",
      "Iteration 303, loss = 0.38567297\n",
      "Iteration 304, loss = 0.38518231\n",
      "Iteration 305, loss = 0.38469280\n",
      "Iteration 306, loss = 0.38420434\n",
      "Iteration 307, loss = 0.38371673\n",
      "Iteration 308, loss = 0.38322999\n",
      "Iteration 309, loss = 0.38274414\n",
      "Iteration 310, loss = 0.38225920\n",
      "Iteration 311, loss = 0.38177520\n",
      "Iteration 312, loss = 0.38129212\n",
      "Iteration 313, loss = 0.38081000\n",
      "Iteration 314, loss = 0.38032884\n",
      "Iteration 315, loss = 0.37984877\n",
      "Iteration 316, loss = 0.37936980\n",
      "Iteration 317, loss = 0.37889180\n",
      "Iteration 318, loss = 0.37841474\n",
      "Iteration 319, loss = 0.37793862\n",
      "Iteration 320, loss = 0.37746343\n",
      "Iteration 321, loss = 0.37698915\n",
      "Iteration 322, loss = 0.37651578\n",
      "Iteration 323, loss = 0.37604331\n",
      "Iteration 324, loss = 0.37557173\n",
      "Iteration 325, loss = 0.37510102\n",
      "Iteration 326, loss = 0.37463119\n",
      "Iteration 327, loss = 0.37416221\n",
      "Iteration 328, loss = 0.37369409\n",
      "Iteration 329, loss = 0.37322681\n",
      "Iteration 330, loss = 0.37276048\n",
      "Iteration 331, loss = 0.37229504\n",
      "Iteration 332, loss = 0.37183044\n",
      "Iteration 333, loss = 0.37136667\n",
      "Iteration 334, loss = 0.37090373\n",
      "Iteration 335, loss = 0.37044162\n",
      "Iteration 336, loss = 0.36998033\n",
      "Iteration 337, loss = 0.36951986\n",
      "Iteration 338, loss = 0.36906021\n",
      "Iteration 339, loss = 0.36860137\n",
      "Iteration 340, loss = 0.36814334\n",
      "Iteration 341, loss = 0.36768611\n",
      "Iteration 342, loss = 0.36722967\n",
      "Iteration 343, loss = 0.36677403\n",
      "Iteration 344, loss = 0.36631918\n",
      "Iteration 345, loss = 0.36586510\n",
      "Iteration 346, loss = 0.36541181\n",
      "Iteration 347, loss = 0.36495929\n",
      "Iteration 348, loss = 0.36450753\n",
      "Iteration 349, loss = 0.36405653\n",
      "Iteration 350, loss = 0.36360629\n",
      "Iteration 351, loss = 0.36315680\n",
      "Iteration 352, loss = 0.36270806\n",
      "Iteration 353, loss = 0.36226005\n",
      "Iteration 354, loss = 0.36181278\n",
      "Iteration 355, loss = 0.36136625\n",
      "Iteration 356, loss = 0.36092044\n",
      "Iteration 357, loss = 0.36047535\n",
      "Iteration 358, loss = 0.36003097\n",
      "Iteration 359, loss = 0.35958731\n",
      "Iteration 360, loss = 0.35914436\n",
      "Iteration 361, loss = 0.35870211\n",
      "Iteration 362, loss = 0.35826055\n",
      "Iteration 363, loss = 0.35781970\n",
      "Iteration 364, loss = 0.35737953\n",
      "Iteration 365, loss = 0.35694005\n",
      "Iteration 366, loss = 0.35650125\n",
      "Iteration 367, loss = 0.35606313\n",
      "Iteration 368, loss = 0.35562568\n",
      "Iteration 369, loss = 0.35518891\n",
      "Iteration 370, loss = 0.35475280\n",
      "Iteration 371, loss = 0.35431735\n",
      "Iteration 372, loss = 0.35388276\n",
      "Iteration 373, loss = 0.35344886\n",
      "Iteration 374, loss = 0.35301561\n",
      "Iteration 375, loss = 0.35258302\n",
      "Iteration 376, loss = 0.35215107\n",
      "Iteration 377, loss = 0.35171978\n",
      "Iteration 378, loss = 0.35129045\n",
      "Iteration 379, loss = 0.35086315\n",
      "Iteration 380, loss = 0.35043657\n",
      "Iteration 381, loss = 0.35001072\n",
      "Iteration 382, loss = 0.34958560\n",
      "Iteration 383, loss = 0.34916119\n",
      "Iteration 384, loss = 0.34873748\n",
      "Iteration 385, loss = 0.34831448\n",
      "Iteration 386, loss = 0.34789217\n",
      "Iteration 387, loss = 0.34747055\n",
      "Iteration 388, loss = 0.34704960\n",
      "Iteration 389, loss = 0.34662933\n",
      "Iteration 390, loss = 0.34620971\n",
      "Iteration 391, loss = 0.34579074\n",
      "Iteration 392, loss = 0.34537242\n",
      "Iteration 393, loss = 0.34495474\n",
      "Iteration 394, loss = 0.34453769\n",
      "Iteration 395, loss = 0.34412127\n",
      "Iteration 396, loss = 0.34370546\n",
      "Iteration 397, loss = 0.34329028\n",
      "Iteration 398, loss = 0.34287571\n",
      "Iteration 399, loss = 0.34246174\n",
      "Iteration 400, loss = 0.34204838\n",
      "Iteration 401, loss = 0.34163563\n",
      "Iteration 402, loss = 0.34122346\n",
      "Iteration 403, loss = 0.34081190\n",
      "Iteration 404, loss = 0.34040092\n",
      "Iteration 405, loss = 0.33999053\n",
      "Iteration 406, loss = 0.33958073\n",
      "Iteration 407, loss = 0.33917151\n",
      "Iteration 408, loss = 0.33876286\n",
      "Iteration 409, loss = 0.33835479\n",
      "Iteration 410, loss = 0.33794730\n",
      "Iteration 411, loss = 0.33754037\n",
      "Iteration 412, loss = 0.33713401\n",
      "Iteration 413, loss = 0.33672822\n",
      "Iteration 414, loss = 0.33632299\n",
      "Iteration 415, loss = 0.33591832\n",
      "Iteration 416, loss = 0.33551421\n",
      "Iteration 417, loss = 0.33511066\n",
      "Iteration 418, loss = 0.33470766\n",
      "Iteration 419, loss = 0.33430522\n",
      "Iteration 420, loss = 0.33390333\n",
      "Iteration 421, loss = 0.33350199\n",
      "Iteration 422, loss = 0.33310120\n",
      "Iteration 423, loss = 0.33270095\n",
      "Iteration 424, loss = 0.33230126\n",
      "Iteration 425, loss = 0.33190213\n",
      "Iteration 426, loss = 0.33150372\n",
      "Iteration 427, loss = 0.33110585\n",
      "Iteration 428, loss = 0.33070851\n",
      "Iteration 429, loss = 0.33031171\n",
      "Iteration 430, loss = 0.32991545\n",
      "Iteration 431, loss = 0.32951973\n",
      "Iteration 432, loss = 0.32912455\n",
      "Iteration 433, loss = 0.32872992\n",
      "Iteration 434, loss = 0.32833582\n",
      "Iteration 435, loss = 0.32794227\n",
      "Iteration 436, loss = 0.32754927\n",
      "Iteration 437, loss = 0.32715739\n",
      "Iteration 438, loss = 0.32676628\n",
      "Iteration 439, loss = 0.32637573\n",
      "Iteration 440, loss = 0.32598573\n",
      "Iteration 441, loss = 0.32559631\n",
      "Iteration 442, loss = 0.32520744\n",
      "Iteration 443, loss = 0.32481911\n",
      "Iteration 444, loss = 0.32443134\n",
      "Iteration 445, loss = 0.32404412\n",
      "Iteration 446, loss = 0.32365744\n",
      "Iteration 447, loss = 0.32327130\n",
      "Iteration 448, loss = 0.32288571\n",
      "Iteration 449, loss = 0.32250065\n",
      "Iteration 450, loss = 0.32211613\n",
      "Iteration 451, loss = 0.32173215\n",
      "Iteration 452, loss = 0.32134869\n",
      "Iteration 453, loss = 0.32096577\n",
      "Iteration 454, loss = 0.32058337\n",
      "Iteration 455, loss = 0.32020150\n",
      "Iteration 456, loss = 0.31982016\n",
      "Iteration 457, loss = 0.31943935\n",
      "Iteration 458, loss = 0.31905906\n",
      "Iteration 459, loss = 0.31867930\n",
      "Iteration 460, loss = 0.31830005\n",
      "Iteration 461, loss = 0.31792133\n",
      "Iteration 462, loss = 0.31754314\n",
      "Iteration 463, loss = 0.31716546\n",
      "Iteration 464, loss = 0.31678830\n",
      "Iteration 465, loss = 0.31641166\n",
      "Iteration 466, loss = 0.31603554\n",
      "Iteration 467, loss = 0.31565993\n",
      "Iteration 468, loss = 0.31528485\n",
      "Iteration 469, loss = 0.31491027\n",
      "Iteration 470, loss = 0.31453621\n",
      "Iteration 471, loss = 0.31416267\n",
      "Iteration 472, loss = 0.31378963\n",
      "Iteration 473, loss = 0.31341711\n",
      "Iteration 474, loss = 0.31304510\n",
      "Iteration 475, loss = 0.31267360\n",
      "Iteration 476, loss = 0.31230261\n",
      "Iteration 477, loss = 0.31193214\n",
      "Iteration 478, loss = 0.31156217\n",
      "Iteration 479, loss = 0.31119271\n",
      "Iteration 480, loss = 0.31082376\n",
      "Iteration 481, loss = 0.31045531\n",
      "Iteration 482, loss = 0.31008738\n",
      "Iteration 483, loss = 0.30971995\n",
      "Iteration 484, loss = 0.30935312\n",
      "Iteration 485, loss = 0.30898677\n",
      "Iteration 486, loss = 0.30862088\n",
      "Iteration 487, loss = 0.30825549\n",
      "Iteration 488, loss = 0.30789065\n",
      "Iteration 489, loss = 0.30752633\n",
      "Iteration 490, loss = 0.30716252\n",
      "Iteration 491, loss = 0.30679919\n",
      "Iteration 492, loss = 0.30643635\n",
      "Iteration 493, loss = 0.30607408\n",
      "Iteration 494, loss = 0.30571222\n",
      "Iteration 495, loss = 0.30535095\n",
      "Iteration 496, loss = 0.30499014\n",
      "Iteration 497, loss = 0.30462981\n",
      "Iteration 498, loss = 0.30426997\n",
      "Iteration 499, loss = 0.30391072\n",
      "Iteration 500, loss = 0.30355187\n",
      "Iteration 501, loss = 0.30319356\n",
      "Iteration 502, loss = 0.30283575\n",
      "Iteration 503, loss = 0.30247842\n",
      "Iteration 504, loss = 0.30212165\n",
      "Iteration 505, loss = 0.30176530\n",
      "Iteration 506, loss = 0.30140951\n",
      "Iteration 507, loss = 0.30105420\n",
      "Iteration 508, loss = 0.30069938\n",
      "Iteration 509, loss = 0.30034505\n",
      "Iteration 510, loss = 0.29999127\n",
      "Iteration 511, loss = 0.29963792\n",
      "Iteration 512, loss = 0.29928510\n",
      "Iteration 513, loss = 0.29893281\n",
      "Iteration 514, loss = 0.29858098\n",
      "Iteration 515, loss = 0.29822965\n",
      "Iteration 516, loss = 0.29787881\n",
      "Iteration 517, loss = 0.29752850\n",
      "Iteration 518, loss = 0.29717865\n",
      "Iteration 519, loss = 0.29682931\n",
      "Iteration 520, loss = 0.29648049\n",
      "Iteration 521, loss = 0.29613214\n",
      "Iteration 522, loss = 0.29578429\n",
      "Iteration 523, loss = 0.29543697\n",
      "Iteration 524, loss = 0.29509009\n",
      "Iteration 525, loss = 0.29474374\n",
      "Iteration 526, loss = 0.29439790\n",
      "Iteration 527, loss = 0.29405253\n",
      "Iteration 528, loss = 0.29370766\n",
      "Iteration 529, loss = 0.29336330\n",
      "Iteration 530, loss = 0.29301942\n",
      "Iteration 531, loss = 0.29267603\n",
      "Iteration 532, loss = 0.29233317\n",
      "Iteration 533, loss = 0.29199077\n",
      "Iteration 534, loss = 0.29164888\n",
      "Iteration 535, loss = 0.29130750\n",
      "Iteration 536, loss = 0.29096659\n",
      "Iteration 537, loss = 0.29062618\n",
      "Iteration 538, loss = 0.29028629\n",
      "Iteration 539, loss = 0.28994687\n",
      "Iteration 540, loss = 0.28960795\n",
      "Iteration 541, loss = 0.28926953\n",
      "Iteration 542, loss = 0.28893161\n",
      "Iteration 543, loss = 0.28859417\n",
      "Iteration 544, loss = 0.28825722\n",
      "Iteration 545, loss = 0.28792080\n",
      "Iteration 546, loss = 0.28758484\n",
      "Iteration 547, loss = 0.28724937\n",
      "Iteration 548, loss = 0.28691445\n",
      "Iteration 549, loss = 0.28657994\n",
      "Iteration 550, loss = 0.28624601\n",
      "Iteration 551, loss = 0.28591253\n",
      "Iteration 552, loss = 0.28557954\n",
      "Iteration 553, loss = 0.28524703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 554, loss = 0.28491505\n",
      "Iteration 555, loss = 0.28458353\n",
      "Iteration 556, loss = 0.28425252\n",
      "Iteration 557, loss = 0.28392202\n",
      "Iteration 558, loss = 0.28359199\n",
      "Iteration 559, loss = 0.28326245\n",
      "Iteration 560, loss = 0.28293344\n",
      "Iteration 561, loss = 0.28260487\n",
      "Iteration 562, loss = 0.28227683\n",
      "Iteration 563, loss = 0.28194928\n",
      "Iteration 564, loss = 0.28162221\n",
      "Iteration 565, loss = 0.28129562\n",
      "Iteration 566, loss = 0.28096958\n",
      "Iteration 567, loss = 0.28064396\n",
      "Iteration 568, loss = 0.28031888\n",
      "Iteration 569, loss = 0.27999428\n",
      "Iteration 570, loss = 0.27967016\n",
      "Iteration 571, loss = 0.27934653\n",
      "Iteration 572, loss = 0.27902375\n",
      "Iteration 573, loss = 0.27870143\n",
      "Iteration 574, loss = 0.27837958\n",
      "Iteration 575, loss = 0.27805821\n",
      "Iteration 576, loss = 0.27773732\n",
      "Iteration 577, loss = 0.27741697\n",
      "Iteration 578, loss = 0.27709716\n",
      "Iteration 579, loss = 0.27677781\n",
      "Iteration 580, loss = 0.27645905\n",
      "Iteration 581, loss = 0.27614070\n",
      "Iteration 582, loss = 0.27582283\n",
      "Iteration 583, loss = 0.27550555\n",
      "Iteration 584, loss = 0.27518865\n",
      "Iteration 585, loss = 0.27487226\n",
      "Iteration 586, loss = 0.27455649\n",
      "Iteration 587, loss = 0.27424112\n",
      "Iteration 588, loss = 0.27392641\n",
      "Iteration 589, loss = 0.27361224\n",
      "Iteration 590, loss = 0.27329851\n",
      "Iteration 591, loss = 0.27298525\n",
      "Iteration 592, loss = 0.27267245\n",
      "Iteration 593, loss = 0.27236024\n",
      "Iteration 594, loss = 0.27204842\n",
      "Iteration 595, loss = 0.27173706\n",
      "Iteration 596, loss = 0.27142630\n",
      "Iteration 597, loss = 0.27111587\n",
      "Iteration 598, loss = 0.27080604\n",
      "Iteration 599, loss = 0.27049666\n",
      "Iteration 600, loss = 0.27018775\n",
      "Iteration 601, loss = 0.26987934\n",
      "Iteration 602, loss = 0.26957144\n",
      "Iteration 603, loss = 0.26926396\n",
      "Iteration 604, loss = 0.26895705\n",
      "Iteration 605, loss = 0.26865055\n",
      "Iteration 606, loss = 0.26834471\n",
      "Iteration 607, loss = 0.26803932\n",
      "Iteration 608, loss = 0.26773427\n",
      "Iteration 609, loss = 0.26742971\n",
      "Iteration 610, loss = 0.26712581\n",
      "Iteration 611, loss = 0.26682225\n",
      "Iteration 612, loss = 0.26651931\n",
      "Iteration 613, loss = 0.26621674\n",
      "Iteration 614, loss = 0.26591475\n",
      "Iteration 615, loss = 0.26561315\n",
      "Iteration 616, loss = 0.26531208\n",
      "Iteration 617, loss = 0.26501151\n",
      "Iteration 618, loss = 0.26471140\n",
      "Iteration 619, loss = 0.26441177\n",
      "Iteration 620, loss = 0.26411262\n",
      "Iteration 621, loss = 0.26381400\n",
      "Iteration 622, loss = 0.26351575\n",
      "Iteration 623, loss = 0.26321814\n",
      "Iteration 624, loss = 0.26292087\n",
      "Iteration 625, loss = 0.26262418\n",
      "Iteration 626, loss = 0.26232787\n",
      "Iteration 627, loss = 0.26203215\n",
      "Iteration 628, loss = 0.26173681\n",
      "Iteration 629, loss = 0.26144202\n",
      "Iteration 630, loss = 0.26114764\n",
      "Iteration 631, loss = 0.26085385\n",
      "Iteration 632, loss = 0.26056042\n",
      "Iteration 633, loss = 0.26026751\n",
      "Iteration 634, loss = 0.25997509\n",
      "Iteration 635, loss = 0.25968313\n",
      "Iteration 636, loss = 0.25939163\n",
      "Iteration 637, loss = 0.25910067\n",
      "Iteration 638, loss = 0.25881012\n",
      "Iteration 639, loss = 0.25852008\n",
      "Iteration 640, loss = 0.25823053\n",
      "Iteration 641, loss = 0.25794142\n",
      "Iteration 642, loss = 0.25765279\n",
      "Iteration 643, loss = 0.25736467\n",
      "Iteration 644, loss = 0.25707699\n",
      "Iteration 645, loss = 0.25678977\n",
      "Iteration 646, loss = 0.25650311\n",
      "Iteration 647, loss = 0.25621677\n",
      "Iteration 648, loss = 0.25593105\n",
      "Iteration 649, loss = 0.25564569\n",
      "Iteration 650, loss = 0.25536093\n",
      "Iteration 651, loss = 0.25507655\n",
      "Iteration 652, loss = 0.25479259\n",
      "Iteration 653, loss = 0.25450919\n",
      "Iteration 654, loss = 0.25422622\n",
      "Iteration 655, loss = 0.25394376\n",
      "Iteration 656, loss = 0.25366172\n",
      "Iteration 657, loss = 0.25338021\n",
      "Iteration 658, loss = 0.25309910\n",
      "Iteration 659, loss = 0.25281850\n",
      "Iteration 660, loss = 0.25253836\n",
      "Iteration 661, loss = 0.25225871\n",
      "Iteration 662, loss = 0.25197947\n",
      "Iteration 663, loss = 0.25170081\n",
      "Iteration 664, loss = 0.25142248\n",
      "Iteration 665, loss = 0.25114469\n",
      "Iteration 666, loss = 0.25086732\n",
      "Iteration 667, loss = 0.25059048\n",
      "Iteration 668, loss = 0.25031404\n",
      "Iteration 669, loss = 0.25003816\n",
      "Iteration 670, loss = 0.24976266\n",
      "Iteration 671, loss = 0.24948758\n",
      "Iteration 672, loss = 0.24921313\n",
      "Iteration 673, loss = 0.24893901\n",
      "Iteration 674, loss = 0.24866532\n",
      "Iteration 675, loss = 0.24839215\n",
      "Iteration 676, loss = 0.24811951\n",
      "Iteration 677, loss = 0.24784722\n",
      "Iteration 678, loss = 0.24757545\n",
      "Iteration 679, loss = 0.24730411\n",
      "Iteration 680, loss = 0.24703326\n",
      "Iteration 681, loss = 0.24676288\n",
      "Iteration 682, loss = 0.24649290\n",
      "Iteration 683, loss = 0.24622346\n",
      "Iteration 684, loss = 0.24595440\n",
      "Iteration 685, loss = 0.24568585\n",
      "Iteration 686, loss = 0.24541772\n",
      "Iteration 687, loss = 0.24515010\n",
      "Iteration 688, loss = 0.24488287\n",
      "Iteration 689, loss = 0.24461614\n",
      "Iteration 690, loss = 0.24434987\n",
      "Iteration 691, loss = 0.24408401\n",
      "Iteration 692, loss = 0.24381867\n",
      "Iteration 693, loss = 0.24355373\n",
      "Iteration 694, loss = 0.24328928\n",
      "Iteration 695, loss = 0.24302526\n",
      "Iteration 696, loss = 0.24276168\n",
      "Iteration 697, loss = 0.24249860\n",
      "Iteration 698, loss = 0.24223596\n",
      "Iteration 699, loss = 0.24197373\n",
      "Iteration 700, loss = 0.24171198\n",
      "Iteration 701, loss = 0.24145064\n",
      "Iteration 702, loss = 0.24118986\n",
      "Iteration 703, loss = 0.24092940\n",
      "Iteration 704, loss = 0.24066947\n",
      "Iteration 705, loss = 0.24040995\n",
      "Iteration 706, loss = 0.24015088\n",
      "Iteration 707, loss = 0.23989232\n",
      "Iteration 708, loss = 0.23963412\n",
      "Iteration 709, loss = 0.23937644\n",
      "Iteration 710, loss = 0.23911914\n",
      "Iteration 711, loss = 0.23886232\n",
      "Iteration 712, loss = 0.23860598\n",
      "Iteration 713, loss = 0.23835002\n",
      "Iteration 714, loss = 0.23809452\n",
      "Iteration 715, loss = 0.23783950\n",
      "Iteration 716, loss = 0.23758488\n",
      "Iteration 717, loss = 0.23733076\n",
      "Iteration 718, loss = 0.23707701\n",
      "Iteration 719, loss = 0.23682374\n",
      "Iteration 720, loss = 0.23657094\n",
      "Iteration 721, loss = 0.23631851\n",
      "Iteration 722, loss = 0.23606659\n",
      "Iteration 723, loss = 0.23581507\n",
      "Iteration 724, loss = 0.23556399\n",
      "Iteration 725, loss = 0.23531339\n",
      "Iteration 726, loss = 0.23506318\n",
      "Iteration 727, loss = 0.23481341\n",
      "Iteration 728, loss = 0.23456413\n",
      "Iteration 729, loss = 0.23431522\n",
      "Iteration 730, loss = 0.23406676\n",
      "Iteration 731, loss = 0.23381882\n",
      "Iteration 732, loss = 0.23357120\n",
      "Iteration 733, loss = 0.23332409\n",
      "Iteration 734, loss = 0.23307739\n",
      "Iteration 735, loss = 0.23283108\n",
      "Iteration 736, loss = 0.23258523\n",
      "Iteration 737, loss = 0.23233985\n",
      "Iteration 738, loss = 0.23209488\n",
      "Iteration 739, loss = 0.23185033\n",
      "Iteration 740, loss = 0.23160623\n",
      "Iteration 741, loss = 0.23136251\n",
      "Iteration 742, loss = 0.23111933\n",
      "Iteration 743, loss = 0.23087646\n",
      "Iteration 744, loss = 0.23063406\n",
      "Iteration 745, loss = 0.23039215\n",
      "Iteration 746, loss = 0.23015058\n",
      "Iteration 747, loss = 0.22990944\n",
      "Iteration 748, loss = 0.22966884\n",
      "Iteration 749, loss = 0.22942856\n",
      "Iteration 750, loss = 0.22918874\n",
      "Iteration 751, loss = 0.22894929\n",
      "Iteration 752, loss = 0.22871038\n",
      "Iteration 753, loss = 0.22847179\n",
      "Iteration 754, loss = 0.22823368\n",
      "Iteration 755, loss = 0.22799596\n",
      "Iteration 756, loss = 0.22775866\n",
      "Iteration 757, loss = 0.22752178\n",
      "Iteration 758, loss = 0.22728535\n",
      "Iteration 759, loss = 0.22704932\n",
      "Iteration 760, loss = 0.22681368\n",
      "Iteration 761, loss = 0.22657857\n",
      "Iteration 762, loss = 0.22634379\n",
      "Iteration 763, loss = 0.22610941\n",
      "Iteration 764, loss = 0.22587549\n",
      "Iteration 765, loss = 0.22564205\n",
      "Iteration 766, loss = 0.22540886\n",
      "Iteration 767, loss = 0.22517621\n",
      "Iteration 768, loss = 0.22494397\n",
      "Iteration 769, loss = 0.22471210\n",
      "Iteration 770, loss = 0.22448068\n",
      "Iteration 771, loss = 0.22424965\n",
      "Iteration 772, loss = 0.22401905\n",
      "Iteration 773, loss = 0.22378886\n",
      "Iteration 774, loss = 0.22355904\n",
      "Iteration 775, loss = 0.22332973\n",
      "Iteration 776, loss = 0.22310073\n",
      "Iteration 777, loss = 0.22287219\n",
      "Iteration 778, loss = 0.22264403\n",
      "Iteration 779, loss = 0.22241633\n",
      "Iteration 780, loss = 0.22218903\n",
      "Iteration 781, loss = 0.22196210\n",
      "Iteration 782, loss = 0.22173556\n",
      "Iteration 783, loss = 0.22150955\n",
      "Iteration 784, loss = 0.22128379\n",
      "Iteration 785, loss = 0.22105849\n",
      "Iteration 786, loss = 0.22083364\n",
      "Iteration 787, loss = 0.22060915\n",
      "Iteration 788, loss = 0.22038508\n",
      "Iteration 789, loss = 0.22016140\n",
      "Iteration 790, loss = 0.21993812\n",
      "Iteration 791, loss = 0.21971531\n",
      "Iteration 792, loss = 0.21949279\n",
      "Iteration 793, loss = 0.21927074\n",
      "Iteration 794, loss = 0.21904907\n",
      "Iteration 795, loss = 0.21882782\n",
      "Iteration 796, loss = 0.21860698\n",
      "Iteration 797, loss = 0.21838650\n",
      "Iteration 798, loss = 0.21816642\n",
      "Iteration 799, loss = 0.21794673\n",
      "Iteration 800, loss = 0.21772751\n",
      "Iteration 801, loss = 0.21750860\n",
      "Iteration 802, loss = 0.21729011\n",
      "Iteration 803, loss = 0.21707202\n",
      "Iteration 804, loss = 0.21685436\n",
      "Iteration 805, loss = 0.21663705\n",
      "Iteration 806, loss = 0.21642015\n",
      "Iteration 807, loss = 0.21620362\n",
      "Iteration 808, loss = 0.21598748\n",
      "Iteration 809, loss = 0.21577177\n",
      "Iteration 810, loss = 0.21555643\n",
      "Iteration 811, loss = 0.21534148\n",
      "Iteration 812, loss = 0.21512691\n",
      "Iteration 813, loss = 0.21491274\n",
      "Iteration 814, loss = 0.21469896\n",
      "Iteration 815, loss = 0.21448556\n",
      "Iteration 816, loss = 0.21427256\n",
      "Iteration 817, loss = 0.21405993\n",
      "Iteration 818, loss = 0.21384768\n",
      "Iteration 819, loss = 0.21363583\n",
      "Iteration 820, loss = 0.21342441\n",
      "Iteration 821, loss = 0.21321328\n",
      "Iteration 822, loss = 0.21300260\n",
      "Iteration 823, loss = 0.21279227\n",
      "Iteration 824, loss = 0.21258232\n",
      "Iteration 825, loss = 0.21237278\n",
      "Iteration 826, loss = 0.21216363\n",
      "Iteration 827, loss = 0.21195483\n",
      "Iteration 828, loss = 0.21174641\n",
      "Iteration 829, loss = 0.21153839\n",
      "Iteration 830, loss = 0.21133070\n",
      "Iteration 831, loss = 0.21112343\n",
      "Iteration 832, loss = 0.21091652\n",
      "Iteration 833, loss = 0.21071004\n",
      "Iteration 834, loss = 0.21050385\n",
      "Iteration 835, loss = 0.21029808\n",
      "Iteration 836, loss = 0.21009268\n",
      "Iteration 837, loss = 0.20988765\n",
      "Iteration 838, loss = 0.20968299\n",
      "Iteration 839, loss = 0.20947872\n",
      "Iteration 840, loss = 0.20927482\n",
      "Iteration 841, loss = 0.20907129\n",
      "Iteration 842, loss = 0.20886812\n",
      "Iteration 843, loss = 0.20866534\n",
      "Iteration 844, loss = 0.20846290\n",
      "Iteration 845, loss = 0.20826083\n",
      "Iteration 846, loss = 0.20805915\n",
      "Iteration 847, loss = 0.20785782\n",
      "Iteration 848, loss = 0.20765686\n",
      "Iteration 849, loss = 0.20745631\n",
      "Iteration 850, loss = 0.20725606\n",
      "Iteration 851, loss = 0.20705622\n",
      "Iteration 852, loss = 0.20685672\n",
      "Iteration 853, loss = 0.20665759\n",
      "Iteration 854, loss = 0.20645882\n",
      "Iteration 855, loss = 0.20626043\n",
      "Iteration 856, loss = 0.20606238\n",
      "Iteration 857, loss = 0.20586470\n",
      "Iteration 858, loss = 0.20566738\n",
      "Iteration 859, loss = 0.20547043\n",
      "Iteration 860, loss = 0.20527384\n",
      "Iteration 861, loss = 0.20507761\n",
      "Iteration 862, loss = 0.20488173\n",
      "Iteration 863, loss = 0.20468620\n",
      "Iteration 864, loss = 0.20449103\n",
      "Iteration 865, loss = 0.20429624\n",
      "Iteration 866, loss = 0.20410177\n",
      "Iteration 867, loss = 0.20390767\n",
      "Iteration 868, loss = 0.20371392\n",
      "Iteration 869, loss = 0.20352054\n",
      "Iteration 870, loss = 0.20332751\n",
      "Iteration 871, loss = 0.20313482\n",
      "Iteration 872, loss = 0.20294249\n",
      "Iteration 873, loss = 0.20275051\n",
      "Iteration 874, loss = 0.20255888\n",
      "Iteration 875, loss = 0.20236760\n",
      "Iteration 876, loss = 0.20217668\n",
      "Iteration 877, loss = 0.20198610\n",
      "Iteration 878, loss = 0.20179587\n",
      "Iteration 879, loss = 0.20160598\n",
      "Iteration 880, loss = 0.20141644\n",
      "Iteration 881, loss = 0.20122726\n",
      "Iteration 882, loss = 0.20103841\n",
      "Iteration 883, loss = 0.20084991\n",
      "Iteration 884, loss = 0.20066175\n",
      "Iteration 885, loss = 0.20047394\n",
      "Iteration 886, loss = 0.20028647\n",
      "Iteration 887, loss = 0.20009935\n",
      "Iteration 888, loss = 0.19991256\n",
      "Iteration 889, loss = 0.19972612\n",
      "Iteration 890, loss = 0.19954002\n",
      "Iteration 891, loss = 0.19935426\n",
      "Iteration 892, loss = 0.19916883\n",
      "Iteration 893, loss = 0.19898375\n",
      "Iteration 894, loss = 0.19879902\n",
      "Iteration 895, loss = 0.19861460\n",
      "Iteration 896, loss = 0.19843053\n",
      "Iteration 897, loss = 0.19824679\n",
      "Iteration 898, loss = 0.19806340\n",
      "Iteration 899, loss = 0.19788033\n",
      "Iteration 900, loss = 0.19769760\n",
      "Iteration 901, loss = 0.19751521\n",
      "Iteration 902, loss = 0.19733315\n",
      "Iteration 903, loss = 0.19715142\n",
      "Iteration 904, loss = 0.19697002\n",
      "Iteration 905, loss = 0.19678896\n",
      "Iteration 906, loss = 0.19660822\n",
      "Iteration 907, loss = 0.19642782\n",
      "Iteration 908, loss = 0.19624774\n",
      "Iteration 909, loss = 0.19606799\n",
      "Iteration 910, loss = 0.19588857\n",
      "Iteration 911, loss = 0.19570948\n",
      "Iteration 912, loss = 0.19553072\n",
      "Iteration 913, loss = 0.19535229\n",
      "Iteration 914, loss = 0.19517417\n",
      "Iteration 915, loss = 0.19499638\n",
      "Iteration 916, loss = 0.19481892\n",
      "Iteration 917, loss = 0.19464178\n",
      "Iteration 918, loss = 0.19446497\n",
      "Iteration 919, loss = 0.19428848\n",
      "Iteration 920, loss = 0.19411230\n",
      "Iteration 921, loss = 0.19393645\n",
      "Iteration 922, loss = 0.19376092\n",
      "Iteration 923, loss = 0.19358572\n",
      "Iteration 924, loss = 0.19341083\n",
      "Iteration 925, loss = 0.19323626\n",
      "Iteration 926, loss = 0.19306201\n",
      "Iteration 927, loss = 0.19288807\n",
      "Iteration 928, loss = 0.19271445\n",
      "Iteration 929, loss = 0.19254115\n",
      "Iteration 930, loss = 0.19236817\n",
      "Iteration 931, loss = 0.19219550\n",
      "Iteration 932, loss = 0.19202314\n",
      "Iteration 933, loss = 0.19185110\n",
      "Iteration 934, loss = 0.19167937\n",
      "Iteration 935, loss = 0.19150795\n",
      "Iteration 936, loss = 0.19133685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 937, loss = 0.19116605\n",
      "Iteration 938, loss = 0.19099557\n",
      "Iteration 939, loss = 0.19082540\n",
      "Iteration 940, loss = 0.19065554\n",
      "Iteration 941, loss = 0.19048599\n",
      "Iteration 942, loss = 0.19031675\n",
      "Iteration 943, loss = 0.19014781\n",
      "Iteration 944, loss = 0.18997918\n",
      "Iteration 945, loss = 0.18981086\n",
      "Iteration 946, loss = 0.18964284\n",
      "Iteration 947, loss = 0.18947513\n",
      "Iteration 948, loss = 0.18930772\n",
      "Iteration 949, loss = 0.18914062\n",
      "Iteration 950, loss = 0.18897382\n",
      "Iteration 951, loss = 0.18880732\n",
      "Iteration 952, loss = 0.18864113\n",
      "Iteration 953, loss = 0.18847524\n",
      "Iteration 954, loss = 0.18830964\n",
      "Iteration 955, loss = 0.18814437\n",
      "Iteration 956, loss = 0.18797939\n",
      "Iteration 957, loss = 0.18781471\n",
      "Iteration 958, loss = 0.18765034\n",
      "Iteration 959, loss = 0.18748626\n",
      "Iteration 960, loss = 0.18732249\n",
      "Iteration 961, loss = 0.18715900\n",
      "Iteration 962, loss = 0.18699582\n",
      "Iteration 963, loss = 0.18683293\n",
      "Iteration 964, loss = 0.18667034\n",
      "Iteration 965, loss = 0.18650804\n",
      "Iteration 966, loss = 0.18634604\n",
      "Iteration 967, loss = 0.18618433\n",
      "Iteration 968, loss = 0.18602291\n",
      "Iteration 969, loss = 0.18586179\n",
      "Iteration 970, loss = 0.18570096\n",
      "Iteration 971, loss = 0.18554042\n",
      "Iteration 972, loss = 0.18538017\n",
      "Iteration 973, loss = 0.18522021\n",
      "Iteration 974, loss = 0.18506054\n",
      "Iteration 975, loss = 0.18490115\n",
      "Iteration 976, loss = 0.18474206\n",
      "Iteration 977, loss = 0.18458325\n",
      "Iteration 978, loss = 0.18442474\n",
      "Iteration 979, loss = 0.18426651\n",
      "Iteration 980, loss = 0.18410856\n",
      "Iteration 981, loss = 0.18395091\n",
      "Iteration 982, loss = 0.18379353\n",
      "Iteration 983, loss = 0.18363644\n",
      "Iteration 984, loss = 0.18347964\n",
      "Iteration 985, loss = 0.18332312\n",
      "Iteration 986, loss = 0.18316688\n",
      "Iteration 987, loss = 0.18301092\n",
      "Iteration 988, loss = 0.18285525\n",
      "Iteration 989, loss = 0.18269985\n",
      "Iteration 990, loss = 0.18254473\n",
      "Iteration 991, loss = 0.18238990\n",
      "Iteration 992, loss = 0.18223534\n",
      "Iteration 993, loss = 0.18208106\n",
      "Iteration 994, loss = 0.18192706\n",
      "Iteration 995, loss = 0.18177334\n",
      "Iteration 996, loss = 0.18161989\n",
      "Iteration 997, loss = 0.18146673\n",
      "Iteration 998, loss = 0.18131384\n",
      "Iteration 999, loss = 0.18116123\n",
      "Iteration 1000, loss = 0.18100890\n",
      "Iteration 1, loss = 1.35771515\n",
      "Iteration 2, loss = 1.31975062\n",
      "Iteration 3, loss = 1.26863347\n",
      "Iteration 4, loss = 1.20866780\n",
      "Iteration 5, loss = 1.14427403\n",
      "Iteration 6, loss = 1.07938645\n",
      "Iteration 7, loss = 1.01738115\n",
      "Iteration 8, loss = 0.96093608\n",
      "Iteration 9, loss = 0.91178455\n",
      "Iteration 10, loss = 0.87063331\n",
      "Iteration 11, loss = 0.83738992\n",
      "Iteration 12, loss = 0.81125295\n",
      "Iteration 13, loss = 0.79125128\n",
      "Iteration 14, loss = 0.77600337\n",
      "Iteration 15, loss = 0.76451479\n",
      "Iteration 16, loss = 0.75573784\n",
      "Iteration 17, loss = 0.74854392\n",
      "Iteration 18, loss = 0.74269769\n",
      "Iteration 19, loss = 0.73779653\n",
      "Iteration 20, loss = 0.73361557\n",
      "Iteration 21, loss = 0.72992070\n",
      "Iteration 22, loss = 0.72649481\n",
      "Iteration 23, loss = 0.72260664\n",
      "Iteration 24, loss = 0.71815908\n",
      "Iteration 25, loss = 0.71321542\n",
      "Iteration 26, loss = 0.70777862\n",
      "Iteration 27, loss = 0.70208874\n",
      "Iteration 28, loss = 0.69628204\n",
      "Iteration 29, loss = 0.69037184\n",
      "Iteration 30, loss = 0.68455621\n",
      "Iteration 31, loss = 0.67894645\n",
      "Iteration 32, loss = 0.67366091\n",
      "Iteration 33, loss = 0.66877354\n",
      "Iteration 34, loss = 0.66426474\n",
      "Iteration 35, loss = 0.66013927\n",
      "Iteration 36, loss = 0.65640425\n",
      "Iteration 37, loss = 0.65298539\n",
      "Iteration 38, loss = 0.64986078\n",
      "Iteration 39, loss = 0.64697358\n",
      "Iteration 40, loss = 0.64426236\n",
      "Iteration 41, loss = 0.64167719\n",
      "Iteration 42, loss = 0.63918213\n",
      "Iteration 43, loss = 0.63674773\n",
      "Iteration 44, loss = 0.63434665\n",
      "Iteration 45, loss = 0.63196471\n",
      "Iteration 46, loss = 0.62958669\n",
      "Iteration 47, loss = 0.62720331\n",
      "Iteration 48, loss = 0.62480863\n",
      "Iteration 49, loss = 0.62240445\n",
      "Iteration 50, loss = 0.61999554\n",
      "Iteration 51, loss = 0.61758839\n",
      "Iteration 52, loss = 0.61519099\n",
      "Iteration 53, loss = 0.61281442\n",
      "Iteration 54, loss = 0.61046211\n",
      "Iteration 55, loss = 0.60813910\n",
      "Iteration 56, loss = 0.60585064\n",
      "Iteration 57, loss = 0.60360063\n",
      "Iteration 58, loss = 0.60139160\n",
      "Iteration 59, loss = 0.59922488\n",
      "Iteration 60, loss = 0.59710132\n",
      "Iteration 61, loss = 0.59501880\n",
      "Iteration 62, loss = 0.59297523\n",
      "Iteration 63, loss = 0.59096902\n",
      "Iteration 64, loss = 0.58899763\n",
      "Iteration 65, loss = 0.58705863\n",
      "Iteration 66, loss = 0.58515001\n",
      "Iteration 67, loss = 0.58327289\n",
      "Iteration 68, loss = 0.58142543\n",
      "Iteration 69, loss = 0.57960294\n",
      "Iteration 70, loss = 0.57780391\n",
      "Iteration 71, loss = 0.57602709\n",
      "Iteration 72, loss = 0.57427163\n",
      "Iteration 73, loss = 0.57253932\n",
      "Iteration 74, loss = 0.57082733\n",
      "Iteration 75, loss = 0.56913522\n",
      "Iteration 76, loss = 0.56746264\n",
      "Iteration 77, loss = 0.56580933\n",
      "Iteration 78, loss = 0.56417506\n",
      "Iteration 79, loss = 0.56255965\n",
      "Iteration 80, loss = 0.56096374\n",
      "Iteration 81, loss = 0.55938967\n",
      "Iteration 82, loss = 0.55783392\n",
      "Iteration 83, loss = 0.55629937\n",
      "Iteration 84, loss = 0.55478520\n",
      "Iteration 85, loss = 0.55328885\n",
      "Iteration 86, loss = 0.55181067\n",
      "Iteration 87, loss = 0.55035072\n",
      "Iteration 88, loss = 0.54890753\n",
      "Iteration 89, loss = 0.54748076\n",
      "Iteration 90, loss = 0.54607003\n",
      "Iteration 91, loss = 0.54467502\n",
      "Iteration 92, loss = 0.54329536\n",
      "Iteration 93, loss = 0.54193068\n",
      "Iteration 94, loss = 0.54058068\n",
      "Iteration 95, loss = 0.53924503\n",
      "Iteration 96, loss = 0.53792346\n",
      "Iteration 97, loss = 0.53661567\n",
      "Iteration 98, loss = 0.53532138\n",
      "Iteration 99, loss = 0.53404033\n",
      "Iteration 100, loss = 0.53277226\n",
      "Iteration 101, loss = 0.53151692\n",
      "Iteration 102, loss = 0.53027407\n",
      "Iteration 103, loss = 0.52904346\n",
      "Iteration 104, loss = 0.52782486\n",
      "Iteration 105, loss = 0.52661804\n",
      "Iteration 106, loss = 0.52542278\n",
      "Iteration 107, loss = 0.52423884\n",
      "Iteration 108, loss = 0.52306600\n",
      "Iteration 109, loss = 0.52190407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110, loss = 0.52075366\n",
      "Iteration 111, loss = 0.51961351\n",
      "Iteration 112, loss = 0.51848341\n",
      "Iteration 113, loss = 0.51736316\n",
      "Iteration 114, loss = 0.51625411\n",
      "Iteration 115, loss = 0.51515562\n",
      "Iteration 116, loss = 0.51406668\n",
      "Iteration 117, loss = 0.51298710\n",
      "Iteration 118, loss = 0.51191670\n",
      "Iteration 119, loss = 0.51085531\n",
      "Iteration 120, loss = 0.50980321\n",
      "Iteration 121, loss = 0.50876000\n",
      "Iteration 122, loss = 0.50772534\n",
      "Iteration 123, loss = 0.50669908\n",
      "Iteration 124, loss = 0.50568105\n",
      "Iteration 125, loss = 0.50467110\n",
      "Iteration 126, loss = 0.50366909\n",
      "Iteration 127, loss = 0.50267486\n",
      "Iteration 128, loss = 0.50168828\n",
      "Iteration 129, loss = 0.50070943\n",
      "Iteration 130, loss = 0.49974052\n",
      "Iteration 131, loss = 0.49877906\n",
      "Iteration 132, loss = 0.49782491\n",
      "Iteration 133, loss = 0.49687795\n",
      "Iteration 134, loss = 0.49594001\n",
      "Iteration 135, loss = 0.49501047\n",
      "Iteration 136, loss = 0.49408943\n",
      "Iteration 137, loss = 0.49317659\n",
      "Iteration 138, loss = 0.49227040\n",
      "Iteration 139, loss = 0.49137144\n",
      "Iteration 140, loss = 0.49047932\n",
      "Iteration 141, loss = 0.48959355\n",
      "Iteration 142, loss = 0.48871411\n",
      "Iteration 143, loss = 0.48784096\n",
      "Iteration 144, loss = 0.48697408\n",
      "Iteration 145, loss = 0.48611339\n",
      "Iteration 146, loss = 0.48525886\n",
      "Iteration 147, loss = 0.48441039\n",
      "Iteration 148, loss = 0.48356793\n",
      "Iteration 149, loss = 0.48273139\n",
      "Iteration 150, loss = 0.48190068\n",
      "Iteration 151, loss = 0.48107573\n",
      "Iteration 152, loss = 0.48025642\n",
      "Iteration 153, loss = 0.47944269\n",
      "Iteration 154, loss = 0.47863442\n",
      "Iteration 155, loss = 0.47783154\n",
      "Iteration 156, loss = 0.47703406\n",
      "Iteration 157, loss = 0.47624192\n",
      "Iteration 158, loss = 0.47545493\n",
      "Iteration 159, loss = 0.47467303\n",
      "Iteration 160, loss = 0.47389615\n",
      "Iteration 161, loss = 0.47312420\n",
      "Iteration 162, loss = 0.47235712\n",
      "Iteration 163, loss = 0.47159482\n",
      "Iteration 164, loss = 0.47083723\n",
      "Iteration 165, loss = 0.47008428\n",
      "Iteration 166, loss = 0.46933589\n",
      "Iteration 167, loss = 0.46859198\n",
      "Iteration 168, loss = 0.46785247\n",
      "Iteration 169, loss = 0.46711730\n",
      "Iteration 170, loss = 0.46638639\n",
      "Iteration 171, loss = 0.46565966\n",
      "Iteration 172, loss = 0.46493706\n",
      "Iteration 173, loss = 0.46421850\n",
      "Iteration 174, loss = 0.46350392\n",
      "Iteration 175, loss = 0.46279324\n",
      "Iteration 176, loss = 0.46208642\n",
      "Iteration 177, loss = 0.46138337\n",
      "Iteration 178, loss = 0.46068403\n",
      "Iteration 179, loss = 0.45998835\n",
      "Iteration 180, loss = 0.45929626\n",
      "Iteration 181, loss = 0.45860874\n",
      "Iteration 182, loss = 0.45792501\n",
      "Iteration 183, loss = 0.45724470\n",
      "Iteration 184, loss = 0.45656777\n",
      "Iteration 185, loss = 0.45589420\n",
      "Iteration 186, loss = 0.45522395\n",
      "Iteration 187, loss = 0.45455698\n",
      "Iteration 188, loss = 0.45389327\n",
      "Iteration 189, loss = 0.45323277\n",
      "Iteration 190, loss = 0.45257543\n",
      "Iteration 191, loss = 0.45192121\n",
      "Iteration 192, loss = 0.45127065\n",
      "Iteration 193, loss = 0.45062357\n",
      "Iteration 194, loss = 0.44997954\n",
      "Iteration 195, loss = 0.44933852\n",
      "Iteration 196, loss = 0.44870046\n",
      "Iteration 197, loss = 0.44806532\n",
      "Iteration 198, loss = 0.44743305\n",
      "Iteration 199, loss = 0.44680361\n",
      "Iteration 200, loss = 0.44617696\n",
      "Iteration 201, loss = 0.44555306\n",
      "Iteration 202, loss = 0.44493186\n",
      "Iteration 203, loss = 0.44431334\n",
      "Iteration 204, loss = 0.44369744\n",
      "Iteration 205, loss = 0.44308414\n",
      "Iteration 206, loss = 0.44247339\n",
      "Iteration 207, loss = 0.44186517\n",
      "Iteration 208, loss = 0.44125942\n",
      "Iteration 209, loss = 0.44065612\n",
      "Iteration 210, loss = 0.44005523\n",
      "Iteration 211, loss = 0.43945672\n",
      "Iteration 212, loss = 0.43886055\n",
      "Iteration 213, loss = 0.43826669\n",
      "Iteration 214, loss = 0.43767510\n",
      "Iteration 215, loss = 0.43708575\n",
      "Iteration 216, loss = 0.43649861\n",
      "Iteration 217, loss = 0.43591365\n",
      "Iteration 218, loss = 0.43533082\n",
      "Iteration 219, loss = 0.43475011\n",
      "Iteration 220, loss = 0.43417149\n",
      "Iteration 221, loss = 0.43359491\n",
      "Iteration 222, loss = 0.43302036\n",
      "Iteration 223, loss = 0.43244780\n",
      "Iteration 224, loss = 0.43187721\n",
      "Iteration 225, loss = 0.43130856\n",
      "Iteration 226, loss = 0.43074182\n",
      "Iteration 227, loss = 0.43017696\n",
      "Iteration 228, loss = 0.42961396\n",
      "Iteration 229, loss = 0.42905279\n",
      "Iteration 230, loss = 0.42849343\n",
      "Iteration 231, loss = 0.42793585\n",
      "Iteration 232, loss = 0.42738003\n",
      "Iteration 233, loss = 0.42682594\n",
      "Iteration 234, loss = 0.42627357\n",
      "Iteration 235, loss = 0.42572288\n",
      "Iteration 236, loss = 0.42517386\n",
      "Iteration 237, loss = 0.42462659\n",
      "Iteration 238, loss = 0.42408234\n",
      "Iteration 239, loss = 0.42353978\n",
      "Iteration 240, loss = 0.42299890\n",
      "Iteration 241, loss = 0.42245968\n",
      "Iteration 242, loss = 0.42192209\n",
      "Iteration 243, loss = 0.42138611\n",
      "Iteration 244, loss = 0.42085172\n",
      "Iteration 245, loss = 0.42031890\n",
      "Iteration 246, loss = 0.41978762\n",
      "Iteration 247, loss = 0.41925788\n",
      "Iteration 248, loss = 0.41873024\n",
      "Iteration 249, loss = 0.41820465\n",
      "Iteration 250, loss = 0.41768059\n",
      "Iteration 251, loss = 0.41715803\n",
      "Iteration 252, loss = 0.41663697\n",
      "Iteration 253, loss = 0.41611739\n",
      "Iteration 254, loss = 0.41559927\n",
      "Iteration 255, loss = 0.41508345\n",
      "Iteration 256, loss = 0.41456905\n",
      "Iteration 257, loss = 0.41405606\n",
      "Iteration 258, loss = 0.41354460\n",
      "Iteration 259, loss = 0.41303587\n",
      "Iteration 260, loss = 0.41252847\n",
      "Iteration 261, loss = 0.41202243\n",
      "Iteration 262, loss = 0.41151775\n",
      "Iteration 263, loss = 0.41101443\n",
      "Iteration 264, loss = 0.41051248\n",
      "Iteration 265, loss = 0.41001189\n",
      "Iteration 266, loss = 0.40951266\n",
      "Iteration 267, loss = 0.40901478\n",
      "Iteration 268, loss = 0.40851824\n",
      "Iteration 269, loss = 0.40802302\n",
      "Iteration 270, loss = 0.40752912\n",
      "Iteration 271, loss = 0.40703652\n",
      "Iteration 272, loss = 0.40654520\n",
      "Iteration 273, loss = 0.40605516\n",
      "Iteration 274, loss = 0.40556637\n",
      "Iteration 275, loss = 0.40507882\n",
      "Iteration 276, loss = 0.40459251\n",
      "Iteration 277, loss = 0.40410741\n",
      "Iteration 278, loss = 0.40362352\n",
      "Iteration 279, loss = 0.40314083\n",
      "Iteration 280, loss = 0.40265932\n",
      "Iteration 281, loss = 0.40217898\n",
      "Iteration 282, loss = 0.40169981\n",
      "Iteration 283, loss = 0.40122179\n",
      "Iteration 284, loss = 0.40074491\n",
      "Iteration 285, loss = 0.40026915\n",
      "Iteration 286, loss = 0.39979452\n",
      "Iteration 287, loss = 0.39932099\n",
      "Iteration 288, loss = 0.39884855\n",
      "Iteration 289, loss = 0.39837720\n",
      "Iteration 290, loss = 0.39790691\n",
      "Iteration 291, loss = 0.39743769\n",
      "Iteration 292, loss = 0.39696952\n",
      "Iteration 293, loss = 0.39650238\n",
      "Iteration 294, loss = 0.39603627\n",
      "Iteration 295, loss = 0.39557118\n",
      "Iteration 296, loss = 0.39510709\n",
      "Iteration 297, loss = 0.39464399\n",
      "Iteration 298, loss = 0.39418192\n",
      "Iteration 299, loss = 0.39372140\n",
      "Iteration 300, loss = 0.39326187\n",
      "Iteration 301, loss = 0.39280334\n",
      "Iteration 302, loss = 0.39234578\n",
      "Iteration 303, loss = 0.39188920\n",
      "Iteration 304, loss = 0.39143359\n",
      "Iteration 305, loss = 0.39097893\n",
      "Iteration 306, loss = 0.39052524\n",
      "Iteration 307, loss = 0.39007248\n",
      "Iteration 308, loss = 0.38962067\n",
      "Iteration 309, loss = 0.38916978\n",
      "Iteration 310, loss = 0.38871982\n",
      "Iteration 311, loss = 0.38827078\n",
      "Iteration 312, loss = 0.38782264\n",
      "Iteration 313, loss = 0.38737540\n",
      "Iteration 314, loss = 0.38692905\n",
      "Iteration 315, loss = 0.38648358\n",
      "Iteration 316, loss = 0.38603899\n",
      "Iteration 317, loss = 0.38559527\n",
      "Iteration 318, loss = 0.38515241\n",
      "Iteration 319, loss = 0.38471040\n",
      "Iteration 320, loss = 0.38426923\n",
      "Iteration 321, loss = 0.38382891\n",
      "Iteration 322, loss = 0.38338941\n",
      "Iteration 323, loss = 0.38295074\n",
      "Iteration 324, loss = 0.38251288\n",
      "Iteration 325, loss = 0.38207583\n",
      "Iteration 326, loss = 0.38163958\n",
      "Iteration 327, loss = 0.38120413\n",
      "Iteration 328, loss = 0.38076947\n",
      "Iteration 329, loss = 0.38033559\n",
      "Iteration 330, loss = 0.37990249\n",
      "Iteration 331, loss = 0.37947015\n",
      "Iteration 332, loss = 0.37903858\n",
      "Iteration 333, loss = 0.37860777\n",
      "Iteration 334, loss = 0.37817771\n",
      "Iteration 335, loss = 0.37774839\n",
      "Iteration 336, loss = 0.37731982\n",
      "Iteration 337, loss = 0.37689198\n",
      "Iteration 338, loss = 0.37646487\n",
      "Iteration 339, loss = 0.37603848\n",
      "Iteration 340, loss = 0.37561281\n",
      "Iteration 341, loss = 0.37518786\n",
      "Iteration 342, loss = 0.37476361\n",
      "Iteration 343, loss = 0.37434007\n",
      "Iteration 344, loss = 0.37391723\n",
      "Iteration 345, loss = 0.37349508\n",
      "Iteration 346, loss = 0.37307362\n",
      "Iteration 347, loss = 0.37265285\n",
      "Iteration 348, loss = 0.37223276\n",
      "Iteration 349, loss = 0.37181334\n",
      "Iteration 350, loss = 0.37139525\n",
      "Iteration 351, loss = 0.37097877\n",
      "Iteration 352, loss = 0.37056298\n",
      "Iteration 353, loss = 0.37014784\n",
      "Iteration 354, loss = 0.36973334\n",
      "Iteration 355, loss = 0.36931951\n",
      "Iteration 356, loss = 0.36890633\n",
      "Iteration 357, loss = 0.36849383\n",
      "Iteration 358, loss = 0.36808200\n",
      "Iteration 359, loss = 0.36767085\n",
      "Iteration 360, loss = 0.36726038\n",
      "Iteration 361, loss = 0.36685060\n",
      "Iteration 362, loss = 0.36644149\n",
      "Iteration 363, loss = 0.36603307\n",
      "Iteration 364, loss = 0.36562532\n",
      "Iteration 365, loss = 0.36521826\n",
      "Iteration 366, loss = 0.36481188\n",
      "Iteration 367, loss = 0.36440618\n",
      "Iteration 368, loss = 0.36400115\n",
      "Iteration 369, loss = 0.36359679\n",
      "Iteration 370, loss = 0.36319310\n",
      "Iteration 371, loss = 0.36279007\n",
      "Iteration 372, loss = 0.36238771\n",
      "Iteration 373, loss = 0.36198601\n",
      "Iteration 374, loss = 0.36158496\n",
      "Iteration 375, loss = 0.36118456\n",
      "Iteration 376, loss = 0.36078480\n",
      "Iteration 377, loss = 0.36038569\n",
      "Iteration 378, loss = 0.35998722\n",
      "Iteration 379, loss = 0.35958939\n",
      "Iteration 380, loss = 0.35919218\n",
      "Iteration 381, loss = 0.35879561\n",
      "Iteration 382, loss = 0.35839966\n",
      "Iteration 383, loss = 0.35800433\n",
      "Iteration 384, loss = 0.35760962\n",
      "Iteration 385, loss = 0.35721553\n",
      "Iteration 386, loss = 0.35682205\n",
      "Iteration 387, loss = 0.35642917\n",
      "Iteration 388, loss = 0.35603690\n",
      "Iteration 389, loss = 0.35564540\n",
      "Iteration 390, loss = 0.35525646\n",
      "Iteration 391, loss = 0.35486817\n",
      "Iteration 392, loss = 0.35448052\n",
      "Iteration 393, loss = 0.35409351\n",
      "Iteration 394, loss = 0.35370714\n",
      "Iteration 395, loss = 0.35332139\n",
      "Iteration 396, loss = 0.35293627\n",
      "Iteration 397, loss = 0.35255176\n",
      "Iteration 398, loss = 0.35216786\n",
      "Iteration 399, loss = 0.35178456\n",
      "Iteration 400, loss = 0.35140186\n",
      "Iteration 401, loss = 0.35101974\n",
      "Iteration 402, loss = 0.35063820\n",
      "Iteration 403, loss = 0.35025724\n",
      "Iteration 404, loss = 0.34987684\n",
      "Iteration 405, loss = 0.34949701\n",
      "Iteration 406, loss = 0.34911774\n",
      "Iteration 407, loss = 0.34873903\n",
      "Iteration 408, loss = 0.34836169\n",
      "Iteration 409, loss = 0.34798524\n",
      "Iteration 410, loss = 0.34760930\n",
      "Iteration 411, loss = 0.34723388\n",
      "Iteration 412, loss = 0.34685898\n",
      "Iteration 413, loss = 0.34648485\n",
      "Iteration 414, loss = 0.34611125\n",
      "Iteration 415, loss = 0.34573816\n",
      "Iteration 416, loss = 0.34536561\n",
      "Iteration 417, loss = 0.34499358\n",
      "Iteration 418, loss = 0.34462209\n",
      "Iteration 419, loss = 0.34425112\n",
      "Iteration 420, loss = 0.34388068\n",
      "Iteration 421, loss = 0.34351078\n",
      "Iteration 422, loss = 0.34314140\n",
      "Iteration 423, loss = 0.34277255\n",
      "Iteration 424, loss = 0.34240422\n",
      "Iteration 425, loss = 0.34203643\n",
      "Iteration 426, loss = 0.34166915\n",
      "Iteration 427, loss = 0.34130241\n",
      "Iteration 428, loss = 0.34093618\n",
      "Iteration 429, loss = 0.34057048\n",
      "Iteration 430, loss = 0.34020530\n",
      "Iteration 431, loss = 0.33984064\n",
      "Iteration 432, loss = 0.33947649\n",
      "Iteration 433, loss = 0.33911287\n",
      "Iteration 434, loss = 0.33874976\n",
      "Iteration 435, loss = 0.33838716\n",
      "Iteration 436, loss = 0.33802508\n",
      "Iteration 437, loss = 0.33766350\n",
      "Iteration 438, loss = 0.33730244\n",
      "Iteration 439, loss = 0.33694188\n",
      "Iteration 440, loss = 0.33658183\n",
      "Iteration 441, loss = 0.33622228\n",
      "Iteration 442, loss = 0.33586324\n",
      "Iteration 443, loss = 0.33550470\n",
      "Iteration 444, loss = 0.33514665\n",
      "Iteration 445, loss = 0.33478911\n",
      "Iteration 446, loss = 0.33443207\n",
      "Iteration 447, loss = 0.33407552\n",
      "Iteration 448, loss = 0.33371947\n",
      "Iteration 449, loss = 0.33336392\n",
      "Iteration 450, loss = 0.33300886\n",
      "Iteration 451, loss = 0.33265429\n",
      "Iteration 452, loss = 0.33230022\n",
      "Iteration 453, loss = 0.33194663\n",
      "Iteration 454, loss = 0.33159354\n",
      "Iteration 455, loss = 0.33124094\n",
      "Iteration 456, loss = 0.33088882\n",
      "Iteration 457, loss = 0.33053720\n",
      "Iteration 458, loss = 0.33018606\n",
      "Iteration 459, loss = 0.32983540\n",
      "Iteration 460, loss = 0.32948524\n",
      "Iteration 461, loss = 0.32913555\n",
      "Iteration 462, loss = 0.32878636\n",
      "Iteration 463, loss = 0.32843764\n",
      "Iteration 464, loss = 0.32808941\n",
      "Iteration 465, loss = 0.32774166\n",
      "Iteration 466, loss = 0.32739440\n",
      "Iteration 467, loss = 0.32704761\n",
      "Iteration 468, loss = 0.32670131\n",
      "Iteration 469, loss = 0.32635548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 470, loss = 0.32601014\n",
      "Iteration 471, loss = 0.32566527\n",
      "Iteration 472, loss = 0.32532088\n",
      "Iteration 473, loss = 0.32497697\n",
      "Iteration 474, loss = 0.32463354\n",
      "Iteration 475, loss = 0.32429059\n",
      "Iteration 476, loss = 0.32394811\n",
      "Iteration 477, loss = 0.32360611\n",
      "Iteration 478, loss = 0.32326458\n",
      "Iteration 479, loss = 0.32292353\n",
      "Iteration 480, loss = 0.32258295\n",
      "Iteration 481, loss = 0.32224285\n",
      "Iteration 482, loss = 0.32190322\n",
      "Iteration 483, loss = 0.32156406\n",
      "Iteration 484, loss = 0.32122538\n",
      "Iteration 485, loss = 0.32088717\n",
      "Iteration 486, loss = 0.32054943\n",
      "Iteration 487, loss = 0.32021217\n",
      "Iteration 488, loss = 0.31987554\n",
      "Iteration 489, loss = 0.31953945\n",
      "Iteration 490, loss = 0.31920376\n",
      "Iteration 491, loss = 0.31886848\n",
      "Iteration 492, loss = 0.31853364\n",
      "Iteration 493, loss = 0.31819923\n",
      "Iteration 494, loss = 0.31786551\n",
      "Iteration 495, loss = 0.31753219\n",
      "Iteration 496, loss = 0.31719931\n",
      "Iteration 497, loss = 0.31686687\n",
      "Iteration 498, loss = 0.31653488\n",
      "Iteration 499, loss = 0.31620337\n",
      "Iteration 500, loss = 0.31587236\n",
      "Iteration 501, loss = 0.31554186\n",
      "Iteration 502, loss = 0.31521174\n",
      "Iteration 503, loss = 0.31488218\n",
      "Iteration 504, loss = 0.31455301\n",
      "Iteration 505, loss = 0.31422435\n",
      "Iteration 506, loss = 0.31389616\n",
      "Iteration 507, loss = 0.31356842\n",
      "Iteration 508, loss = 0.31324116\n",
      "Iteration 509, loss = 0.31291438\n",
      "Iteration 510, loss = 0.31258803\n",
      "Iteration 511, loss = 0.31226223\n",
      "Iteration 512, loss = 0.31193677\n",
      "Iteration 513, loss = 0.31161191\n",
      "Iteration 514, loss = 0.31128744\n",
      "Iteration 515, loss = 0.31096342\n",
      "Iteration 516, loss = 0.31063986\n",
      "Iteration 517, loss = 0.31031682\n",
      "Iteration 518, loss = 0.30999419\n",
      "Iteration 519, loss = 0.30967211\n",
      "Iteration 520, loss = 0.30935037\n",
      "Iteration 521, loss = 0.30902921\n",
      "Iteration 522, loss = 0.30870846\n",
      "Iteration 523, loss = 0.30838814\n",
      "Iteration 524, loss = 0.30806832\n",
      "Iteration 525, loss = 0.30774895\n",
      "Iteration 526, loss = 0.30743006\n",
      "Iteration 527, loss = 0.30711163\n",
      "Iteration 528, loss = 0.30679364\n",
      "Iteration 529, loss = 0.30647616\n",
      "Iteration 530, loss = 0.30615908\n",
      "Iteration 531, loss = 0.30584251\n",
      "Iteration 532, loss = 0.30552638\n",
      "Iteration 533, loss = 0.30521069\n",
      "Iteration 534, loss = 0.30489553\n",
      "Iteration 535, loss = 0.30458075\n",
      "Iteration 536, loss = 0.30426649\n",
      "Iteration 537, loss = 0.30395266\n",
      "Iteration 538, loss = 0.30363928\n",
      "Iteration 539, loss = 0.30332642\n",
      "Iteration 540, loss = 0.30301394\n",
      "Iteration 541, loss = 0.30270199\n",
      "Iteration 542, loss = 0.30239045\n",
      "Iteration 543, loss = 0.30207970\n",
      "Iteration 544, loss = 0.30176947\n",
      "Iteration 545, loss = 0.30145958\n",
      "Iteration 546, loss = 0.30115027\n",
      "Iteration 547, loss = 0.30084134\n",
      "Iteration 548, loss = 0.30053285\n",
      "Iteration 549, loss = 0.30022484\n",
      "Iteration 550, loss = 0.29991729\n",
      "Iteration 551, loss = 0.29961021\n",
      "Iteration 552, loss = 0.29930359\n",
      "Iteration 553, loss = 0.29899740\n",
      "Iteration 554, loss = 0.29869175\n",
      "Iteration 555, loss = 0.29838645\n",
      "Iteration 556, loss = 0.29808172\n",
      "Iteration 557, loss = 0.29777737\n",
      "Iteration 558, loss = 0.29747350\n",
      "Iteration 559, loss = 0.29717016\n",
      "Iteration 560, loss = 0.29686721\n",
      "Iteration 561, loss = 0.29656471\n",
      "Iteration 562, loss = 0.29626277\n",
      "Iteration 563, loss = 0.29596117\n",
      "Iteration 564, loss = 0.29566012\n",
      "Iteration 565, loss = 0.29535948\n",
      "Iteration 566, loss = 0.29505930\n",
      "Iteration 567, loss = 0.29475965\n",
      "Iteration 568, loss = 0.29446036\n",
      "Iteration 569, loss = 0.29416163\n",
      "Iteration 570, loss = 0.29386330\n",
      "Iteration 571, loss = 0.29356542\n",
      "Iteration 572, loss = 0.29326802\n",
      "Iteration 573, loss = 0.29297107\n",
      "Iteration 574, loss = 0.29267459\n",
      "Iteration 575, loss = 0.29237858\n",
      "Iteration 576, loss = 0.29208299\n",
      "Iteration 577, loss = 0.29178790\n",
      "Iteration 578, loss = 0.29149324\n",
      "Iteration 579, loss = 0.29119903\n",
      "Iteration 580, loss = 0.29090533\n",
      "Iteration 581, loss = 0.29061204\n",
      "Iteration 582, loss = 0.29031918\n",
      "Iteration 583, loss = 0.29002688\n",
      "Iteration 584, loss = 0.28973491\n",
      "Iteration 585, loss = 0.28944348\n",
      "Iteration 586, loss = 0.28915247\n",
      "Iteration 587, loss = 0.28886189\n",
      "Iteration 588, loss = 0.28857183\n",
      "Iteration 589, loss = 0.28828215\n",
      "Iteration 590, loss = 0.28799298\n",
      "Iteration 591, loss = 0.28770425\n",
      "Iteration 592, loss = 0.28741595\n",
      "Iteration 593, loss = 0.28712814\n",
      "Iteration 594, loss = 0.28684075\n",
      "Iteration 595, loss = 0.28655381\n",
      "Iteration 596, loss = 0.28626738\n",
      "Iteration 597, loss = 0.28598135\n",
      "Iteration 598, loss = 0.28569575\n",
      "Iteration 599, loss = 0.28541068\n",
      "Iteration 600, loss = 0.28512597\n",
      "Iteration 601, loss = 0.28484179\n",
      "Iteration 602, loss = 0.28455801\n",
      "Iteration 603, loss = 0.28427467\n",
      "Iteration 604, loss = 0.28399182\n",
      "Iteration 605, loss = 0.28370939\n",
      "Iteration 606, loss = 0.28342742\n",
      "Iteration 607, loss = 0.28314595\n",
      "Iteration 608, loss = 0.28286489\n",
      "Iteration 609, loss = 0.28258427\n",
      "Iteration 610, loss = 0.28230411\n",
      "Iteration 611, loss = 0.28202438\n",
      "Iteration 612, loss = 0.28174516\n",
      "Iteration 613, loss = 0.28146631\n",
      "Iteration 614, loss = 0.28118798\n",
      "Iteration 615, loss = 0.28091006\n",
      "Iteration 616, loss = 0.28063258\n",
      "Iteration 617, loss = 0.28035556\n",
      "Iteration 618, loss = 0.28007900\n",
      "Iteration 619, loss = 0.27980285\n",
      "Iteration 620, loss = 0.27952721\n",
      "Iteration 621, loss = 0.27925195\n",
      "Iteration 622, loss = 0.27897716\n",
      "Iteration 623, loss = 0.27870283\n",
      "Iteration 624, loss = 0.27842892\n",
      "Iteration 625, loss = 0.27815547\n",
      "Iteration 626, loss = 0.27788247\n",
      "Iteration 627, loss = 0.27760989\n",
      "Iteration 628, loss = 0.27733779\n",
      "Iteration 629, loss = 0.27706609\n",
      "Iteration 630, loss = 0.27679486\n",
      "Iteration 631, loss = 0.27652409\n",
      "Iteration 632, loss = 0.27625373\n",
      "Iteration 633, loss = 0.27598381\n",
      "Iteration 634, loss = 0.27571438\n",
      "Iteration 635, loss = 0.27544533\n",
      "Iteration 636, loss = 0.27517676\n",
      "Iteration 637, loss = 0.27490862\n",
      "Iteration 638, loss = 0.27464091\n",
      "Iteration 639, loss = 0.27437369\n",
      "Iteration 640, loss = 0.27410683\n",
      "Iteration 641, loss = 0.27384048\n",
      "Iteration 642, loss = 0.27357453\n",
      "Iteration 643, loss = 0.27330902\n",
      "Iteration 644, loss = 0.27304397\n",
      "Iteration 645, loss = 0.27277935\n",
      "Iteration 646, loss = 0.27251516\n",
      "Iteration 647, loss = 0.27225143\n",
      "Iteration 648, loss = 0.27198811\n",
      "Iteration 649, loss = 0.27172523\n",
      "Iteration 650, loss = 0.27146282\n",
      "Iteration 651, loss = 0.27120082\n",
      "Iteration 652, loss = 0.27093922\n",
      "Iteration 653, loss = 0.27067816\n",
      "Iteration 654, loss = 0.27041742\n",
      "Iteration 655, loss = 0.27015719\n",
      "Iteration 656, loss = 0.26989737\n",
      "Iteration 657, loss = 0.26963796\n",
      "Iteration 658, loss = 0.26937913\n",
      "Iteration 659, loss = 0.26912067\n",
      "Iteration 660, loss = 0.26886264\n",
      "Iteration 661, loss = 0.26860504\n",
      "Iteration 662, loss = 0.26834784\n",
      "Iteration 663, loss = 0.26809110\n",
      "Iteration 664, loss = 0.26783484\n",
      "Iteration 665, loss = 0.26757897\n",
      "Iteration 666, loss = 0.26732356\n",
      "Iteration 667, loss = 0.26706856\n",
      "Iteration 668, loss = 0.26681407\n",
      "Iteration 669, loss = 0.26655991\n",
      "Iteration 670, loss = 0.26630623\n",
      "Iteration 671, loss = 0.26605297\n",
      "Iteration 672, loss = 0.26580014\n",
      "Iteration 673, loss = 0.26554776\n",
      "Iteration 674, loss = 0.26529577\n",
      "Iteration 675, loss = 0.26504425\n",
      "Iteration 676, loss = 0.26479310\n",
      "Iteration 677, loss = 0.26454248\n",
      "Iteration 678, loss = 0.26429219\n",
      "Iteration 679, loss = 0.26404236\n",
      "Iteration 680, loss = 0.26379297\n",
      "Iteration 681, loss = 0.26354400\n",
      "Iteration 682, loss = 0.26329546\n",
      "Iteration 683, loss = 0.26304733\n",
      "Iteration 684, loss = 0.26279964\n",
      "Iteration 685, loss = 0.26255236\n",
      "Iteration 686, loss = 0.26230556\n",
      "Iteration 687, loss = 0.26205912\n",
      "Iteration 688, loss = 0.26181311\n",
      "Iteration 689, loss = 0.26156756\n",
      "Iteration 690, loss = 0.26132242\n",
      "Iteration 691, loss = 0.26107769\n",
      "Iteration 692, loss = 0.26083341\n",
      "Iteration 693, loss = 0.26058951\n",
      "Iteration 694, loss = 0.26034604\n",
      "Iteration 695, loss = 0.26010304\n",
      "Iteration 696, loss = 0.25986041\n",
      "Iteration 697, loss = 0.25961824\n",
      "Iteration 698, loss = 0.25937645\n",
      "Iteration 699, loss = 0.25913514\n",
      "Iteration 700, loss = 0.25889422\n",
      "Iteration 701, loss = 0.25865370\n",
      "Iteration 702, loss = 0.25841360\n",
      "Iteration 703, loss = 0.25817397\n",
      "Iteration 704, loss = 0.25793469\n",
      "Iteration 705, loss = 0.25769586\n",
      "Iteration 706, loss = 0.25745744\n",
      "Iteration 707, loss = 0.25721944\n",
      "Iteration 708, loss = 0.25698186\n",
      "Iteration 709, loss = 0.25674470\n",
      "Iteration 710, loss = 0.25650793\n",
      "Iteration 711, loss = 0.25627158\n",
      "Iteration 712, loss = 0.25603567\n",
      "Iteration 713, loss = 0.25580015\n",
      "Iteration 714, loss = 0.25556505\n",
      "Iteration 715, loss = 0.25533040\n",
      "Iteration 716, loss = 0.25509617\n",
      "Iteration 717, loss = 0.25486240\n",
      "Iteration 718, loss = 0.25462903\n",
      "Iteration 719, loss = 0.25439611\n",
      "Iteration 720, loss = 0.25416354\n",
      "Iteration 721, loss = 0.25393144\n",
      "Iteration 722, loss = 0.25369976\n",
      "Iteration 723, loss = 0.25346843\n",
      "Iteration 724, loss = 0.25323757\n",
      "Iteration 725, loss = 0.25300707\n",
      "Iteration 726, loss = 0.25277704\n",
      "Iteration 727, loss = 0.25254732\n",
      "Iteration 728, loss = 0.25231813\n",
      "Iteration 729, loss = 0.25208921\n",
      "Iteration 730, loss = 0.25186077\n",
      "Iteration 731, loss = 0.25163275\n",
      "Iteration 732, loss = 0.25140506\n",
      "Iteration 733, loss = 0.25117786\n",
      "Iteration 734, loss = 0.25095102\n",
      "Iteration 735, loss = 0.25072457\n",
      "Iteration 736, loss = 0.25049851\n",
      "Iteration 737, loss = 0.25027289\n",
      "Iteration 738, loss = 0.25004764\n",
      "Iteration 739, loss = 0.24982278\n",
      "Iteration 740, loss = 0.24959838\n",
      "Iteration 741, loss = 0.24937430\n",
      "Iteration 742, loss = 0.24915068\n",
      "Iteration 743, loss = 0.24892740\n",
      "Iteration 744, loss = 0.24870458\n",
      "Iteration 745, loss = 0.24848209\n",
      "Iteration 746, loss = 0.24826001\n",
      "Iteration 747, loss = 0.24803843\n",
      "Iteration 748, loss = 0.24781710\n",
      "Iteration 749, loss = 0.24759619\n",
      "Iteration 750, loss = 0.24737577\n",
      "Iteration 751, loss = 0.24715566\n",
      "Iteration 752, loss = 0.24693597\n",
      "Iteration 753, loss = 0.24671662\n",
      "Iteration 754, loss = 0.24649771\n",
      "Iteration 755, loss = 0.24627921\n",
      "Iteration 756, loss = 0.24606104\n",
      "Iteration 757, loss = 0.24584331\n",
      "Iteration 758, loss = 0.24562596\n",
      "Iteration 759, loss = 0.24540896\n",
      "Iteration 760, loss = 0.24519242\n",
      "Iteration 761, loss = 0.24497619\n",
      "Iteration 762, loss = 0.24476040\n",
      "Iteration 763, loss = 0.24454497\n",
      "Iteration 764, loss = 0.24432991\n",
      "Iteration 765, loss = 0.24411531\n",
      "Iteration 766, loss = 0.24390102\n",
      "Iteration 767, loss = 0.24368712\n",
      "Iteration 768, loss = 0.24347366\n",
      "Iteration 769, loss = 0.24326054\n",
      "Iteration 770, loss = 0.24304778\n",
      "Iteration 771, loss = 0.24283546\n",
      "Iteration 772, loss = 0.24262348\n",
      "Iteration 773, loss = 0.24241192\n",
      "Iteration 774, loss = 0.24220070\n",
      "Iteration 775, loss = 0.24198987\n",
      "Iteration 776, loss = 0.24177942\n",
      "Iteration 777, loss = 0.24156935\n",
      "Iteration 778, loss = 0.24135962\n",
      "Iteration 779, loss = 0.24115040\n",
      "Iteration 780, loss = 0.24094138\n",
      "Iteration 781, loss = 0.24073285\n",
      "Iteration 782, loss = 0.24052469\n",
      "Iteration 783, loss = 0.24031686\n",
      "Iteration 784, loss = 0.24010943\n",
      "Iteration 785, loss = 0.23990238\n",
      "Iteration 786, loss = 0.23969572\n",
      "Iteration 787, loss = 0.23948938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 788, loss = 0.23928346\n",
      "Iteration 789, loss = 0.23907790\n",
      "Iteration 790, loss = 0.23887271\n",
      "Iteration 791, loss = 0.23866791\n",
      "Iteration 792, loss = 0.23846344\n",
      "Iteration 793, loss = 0.23825939\n",
      "Iteration 794, loss = 0.23805567\n",
      "Iteration 795, loss = 0.23785232\n",
      "Iteration 796, loss = 0.23764936\n",
      "Iteration 797, loss = 0.23744679\n",
      "Iteration 798, loss = 0.23724453\n",
      "Iteration 799, loss = 0.23704266\n",
      "Iteration 800, loss = 0.23684118\n",
      "Iteration 801, loss = 0.23664005\n",
      "Iteration 802, loss = 0.23643927\n",
      "Iteration 803, loss = 0.23623889\n",
      "Iteration 804, loss = 0.23603884\n",
      "Iteration 805, loss = 0.23583917\n",
      "Iteration 806, loss = 0.23563989\n",
      "Iteration 807, loss = 0.23544091\n",
      "Iteration 808, loss = 0.23524232\n",
      "Iteration 809, loss = 0.23504414\n",
      "Iteration 810, loss = 0.23484627\n",
      "Iteration 811, loss = 0.23464875\n",
      "Iteration 812, loss = 0.23445160\n",
      "Iteration 813, loss = 0.23425485\n",
      "Iteration 814, loss = 0.23405841\n",
      "Iteration 815, loss = 0.23386232\n",
      "Iteration 816, loss = 0.23366669\n",
      "Iteration 817, loss = 0.23347129\n",
      "Iteration 818, loss = 0.23327629\n",
      "Iteration 819, loss = 0.23308168\n",
      "Iteration 820, loss = 0.23288741\n",
      "Iteration 821, loss = 0.23269348\n",
      "Iteration 822, loss = 0.23249989\n",
      "Iteration 823, loss = 0.23230672\n",
      "Iteration 824, loss = 0.23211382\n",
      "Iteration 825, loss = 0.23192131\n",
      "Iteration 826, loss = 0.23172917\n",
      "Iteration 827, loss = 0.23153736\n",
      "Iteration 828, loss = 0.23134592\n",
      "Iteration 829, loss = 0.23115481\n",
      "Iteration 830, loss = 0.23096404\n",
      "Iteration 831, loss = 0.23077368\n",
      "Iteration 832, loss = 0.23058360\n",
      "Iteration 833, loss = 0.23039388\n",
      "Iteration 834, loss = 0.23020455\n",
      "Iteration 835, loss = 0.23001553\n",
      "Iteration 836, loss = 0.22982687\n",
      "Iteration 837, loss = 0.22963855\n",
      "Iteration 838, loss = 0.22945057\n",
      "Iteration 839, loss = 0.22926296\n",
      "Iteration 840, loss = 0.22907567\n",
      "Iteration 841, loss = 0.22888876\n",
      "Iteration 842, loss = 0.22870213\n",
      "Iteration 843, loss = 0.22851589\n",
      "Iteration 844, loss = 0.22832999\n",
      "Iteration 845, loss = 0.22814442\n",
      "Iteration 846, loss = 0.22795919\n",
      "Iteration 847, loss = 0.22777431\n",
      "Iteration 848, loss = 0.22758976\n",
      "Iteration 849, loss = 0.22740555\n",
      "Iteration 850, loss = 0.22722169\n",
      "Iteration 851, loss = 0.22703815\n",
      "Iteration 852, loss = 0.22685501\n",
      "Iteration 853, loss = 0.22667222\n",
      "Iteration 854, loss = 0.22648978\n",
      "Iteration 855, loss = 0.22630768\n",
      "Iteration 856, loss = 0.22612596\n",
      "Iteration 857, loss = 0.22594455\n",
      "Iteration 858, loss = 0.22576348\n",
      "Iteration 859, loss = 0.22558273\n",
      "Iteration 860, loss = 0.22540232\n",
      "Iteration 861, loss = 0.22522227\n",
      "Iteration 862, loss = 0.22504253\n",
      "Iteration 863, loss = 0.22486313\n",
      "Iteration 864, loss = 0.22468409\n",
      "Iteration 865, loss = 0.22450534\n",
      "Iteration 866, loss = 0.22432692\n",
      "Iteration 867, loss = 0.22414886\n",
      "Iteration 868, loss = 0.22397112\n",
      "Iteration 869, loss = 0.22379370\n",
      "Iteration 870, loss = 0.22361661\n",
      "Iteration 871, loss = 0.22343985\n",
      "Iteration 872, loss = 0.22326342\n",
      "Iteration 873, loss = 0.22308730\n",
      "Iteration 874, loss = 0.22291152\n",
      "Iteration 875, loss = 0.22273607\n",
      "Iteration 876, loss = 0.22256093\n",
      "Iteration 877, loss = 0.22238613\n",
      "Iteration 878, loss = 0.22221164\n",
      "Iteration 879, loss = 0.22203746\n",
      "Iteration 880, loss = 0.22186362\n",
      "Iteration 881, loss = 0.22169010\n",
      "Iteration 882, loss = 0.22151692\n",
      "Iteration 883, loss = 0.22134402\n",
      "Iteration 884, loss = 0.22117146\n",
      "Iteration 885, loss = 0.22099921\n",
      "Iteration 886, loss = 0.22082728\n",
      "Iteration 887, loss = 0.22065568\n",
      "Iteration 888, loss = 0.22048437\n",
      "Iteration 889, loss = 0.22031345\n",
      "Iteration 890, loss = 0.22014274\n",
      "Iteration 891, loss = 0.21997242\n",
      "Iteration 892, loss = 0.21980238\n",
      "Iteration 893, loss = 0.21963266\n",
      "Iteration 894, loss = 0.21946327\n",
      "Iteration 895, loss = 0.21929417\n",
      "Iteration 896, loss = 0.21912539\n",
      "Iteration 897, loss = 0.21895693\n",
      "Iteration 898, loss = 0.21878878\n",
      "Iteration 899, loss = 0.21862094\n",
      "Iteration 900, loss = 0.21845340\n",
      "Iteration 901, loss = 0.21828620\n",
      "Iteration 902, loss = 0.21811927\n",
      "Iteration 903, loss = 0.21795266\n",
      "Iteration 904, loss = 0.21778637\n",
      "Iteration 905, loss = 0.21762037\n",
      "Iteration 906, loss = 0.21745469\n",
      "Iteration 907, loss = 0.21728930\n",
      "Iteration 908, loss = 0.21712425\n",
      "Iteration 909, loss = 0.21695948\n",
      "Iteration 910, loss = 0.21679502\n",
      "Iteration 911, loss = 0.21663085\n",
      "Iteration 912, loss = 0.21646700\n",
      "Iteration 913, loss = 0.21630344\n",
      "Iteration 914, loss = 0.21614020\n",
      "Iteration 915, loss = 0.21597725\n",
      "Iteration 916, loss = 0.21581459\n",
      "Iteration 917, loss = 0.21565227\n",
      "Iteration 918, loss = 0.21549020\n",
      "Iteration 919, loss = 0.21532846\n",
      "Iteration 920, loss = 0.21516700\n",
      "Iteration 921, loss = 0.21500587\n",
      "Iteration 922, loss = 0.21484499\n",
      "Iteration 923, loss = 0.21468447\n",
      "Iteration 924, loss = 0.21452420\n",
      "Iteration 925, loss = 0.21436422\n",
      "Iteration 926, loss = 0.21420459\n",
      "Iteration 927, loss = 0.21404521\n",
      "Iteration 928, loss = 0.21388610\n",
      "Iteration 929, loss = 0.21372734\n",
      "Iteration 930, loss = 0.21356885\n",
      "Iteration 931, loss = 0.21341064\n",
      "Iteration 932, loss = 0.21325276\n",
      "Iteration 933, loss = 0.21309513\n",
      "Iteration 934, loss = 0.21293782\n",
      "Iteration 935, loss = 0.21278078\n",
      "Iteration 936, loss = 0.21262405\n",
      "Iteration 937, loss = 0.21246758\n",
      "Iteration 938, loss = 0.21231144\n",
      "Iteration 939, loss = 0.21215555\n",
      "Iteration 940, loss = 0.21199997\n",
      "Iteration 941, loss = 0.21184467\n",
      "Iteration 942, loss = 0.21168966\n",
      "Iteration 943, loss = 0.21153494\n",
      "Iteration 944, loss = 0.21138048\n",
      "Iteration 945, loss = 0.21122636\n",
      "Iteration 946, loss = 0.21107247\n",
      "Iteration 947, loss = 0.21091888\n",
      "Iteration 948, loss = 0.21076557\n",
      "Iteration 949, loss = 0.21061257\n",
      "Iteration 950, loss = 0.21045982\n",
      "Iteration 951, loss = 0.21030738\n",
      "Iteration 952, loss = 0.21015519\n",
      "Iteration 953, loss = 0.21000331\n",
      "Iteration 954, loss = 0.20985168\n",
      "Iteration 955, loss = 0.20970038\n",
      "Iteration 956, loss = 0.20954930\n",
      "Iteration 957, loss = 0.20939853\n",
      "Iteration 958, loss = 0.20924802\n",
      "Iteration 959, loss = 0.20909782\n",
      "Iteration 960, loss = 0.20894787\n",
      "Iteration 961, loss = 0.20879821\n",
      "Iteration 962, loss = 0.20864881\n",
      "Iteration 963, loss = 0.20849971\n",
      "Iteration 964, loss = 0.20835085\n",
      "Iteration 965, loss = 0.20820231\n",
      "Iteration 966, loss = 0.20805400\n",
      "Iteration 967, loss = 0.20790599\n",
      "Iteration 968, loss = 0.20775823\n",
      "Iteration 969, loss = 0.20761078\n",
      "Iteration 970, loss = 0.20746356\n",
      "Iteration 971, loss = 0.20731663\n",
      "Iteration 972, loss = 0.20716996\n",
      "Iteration 973, loss = 0.20702359\n",
      "Iteration 974, loss = 0.20687745\n",
      "Iteration 975, loss = 0.20673161\n",
      "Iteration 976, loss = 0.20658601\n",
      "Iteration 977, loss = 0.20644071\n",
      "Iteration 978, loss = 0.20629565\n",
      "Iteration 979, loss = 0.20615087\n",
      "Iteration 980, loss = 0.20600635\n",
      "Iteration 981, loss = 0.20586211\n",
      "Iteration 982, loss = 0.20571811\n",
      "Iteration 983, loss = 0.20557440\n",
      "Iteration 984, loss = 0.20543093\n",
      "Iteration 985, loss = 0.20528776\n",
      "Iteration 986, loss = 0.20514482\n",
      "Iteration 987, loss = 0.20500215\n",
      "Iteration 988, loss = 0.20485973\n",
      "Iteration 989, loss = 0.20471761\n",
      "Iteration 990, loss = 0.20457571\n",
      "Iteration 991, loss = 0.20443411\n",
      "Iteration 992, loss = 0.20429273\n",
      "Iteration 993, loss = 0.20415163\n",
      "Iteration 994, loss = 0.20401078\n",
      "Iteration 995, loss = 0.20387020\n",
      "Iteration 996, loss = 0.20372986\n",
      "Iteration 997, loss = 0.20358981\n",
      "Iteration 998, loss = 0.20344997\n",
      "Iteration 999, loss = 0.20331044\n",
      "Iteration 1000, loss = 0.20317113\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 3.39215154\n",
      "Iteration 3, loss = 1.37237638\n",
      "Iteration 4, loss = 0.80971506\n",
      "Iteration 5, loss = 1.11163849\n",
      "Iteration 6, loss = 0.57488434\n",
      "Iteration 7, loss = 1.05671239\n",
      "Iteration 8, loss = 0.54395369\n",
      "Iteration 9, loss = 0.48320272\n",
      "Iteration 10, loss = 0.57156945\n",
      "Iteration 11, loss = 0.48509864\n",
      "Iteration 12, loss = 0.38156776\n",
      "Iteration 13, loss = 0.40736377\n",
      "Iteration 14, loss = 0.39741170\n",
      "Iteration 15, loss = 0.31752559\n",
      "Iteration 16, loss = 0.30074655\n",
      "Iteration 17, loss = 0.35040065\n",
      "Iteration 18, loss = 0.32585837\n",
      "Iteration 19, loss = 0.27017888\n",
      "Iteration 20, loss = 0.29877149\n",
      "Iteration 21, loss = 0.28827944\n",
      "Iteration 22, loss = 0.23413981\n",
      "Iteration 23, loss = 0.25227144\n",
      "Iteration 24, loss = 0.24927558\n",
      "Iteration 25, loss = 0.20648486\n",
      "Iteration 26, loss = 0.21837762\n",
      "Iteration 27, loss = 0.21506476\n",
      "Iteration 28, loss = 0.18102812\n",
      "Iteration 29, loss = 0.19352201\n",
      "Iteration 30, loss = 0.18547734\n",
      "Iteration 31, loss = 0.16171823\n",
      "Iteration 32, loss = 0.17417181\n",
      "Iteration 33, loss = 0.16051747\n",
      "Iteration 34, loss = 0.14910349\n",
      "Iteration 35, loss = 0.15793909\n",
      "Iteration 36, loss = 0.14267958\n",
      "Iteration 37, loss = 0.14026109\n",
      "Iteration 38, loss = 0.14352291\n",
      "Iteration 39, loss = 0.13005165\n",
      "Iteration 40, loss = 0.13481996\n",
      "Iteration 41, loss = 0.13045064\n",
      "Iteration 42, loss = 0.12372569\n",
      "Iteration 43, loss = 0.12843853\n",
      "Iteration 44, loss = 0.12020360\n",
      "Iteration 45, loss = 0.12129326\n",
      "Iteration 46, loss = 0.12049450\n",
      "Iteration 47, loss = 0.11516498\n",
      "Iteration 48, loss = 0.11822924\n",
      "Iteration 49, loss = 0.11328362\n",
      "Iteration 50, loss = 0.11367251\n",
      "Iteration 51, loss = 0.11306239\n",
      "Iteration 52, loss = 0.10982830\n",
      "Iteration 53, loss = 0.11158692\n",
      "Iteration 54, loss = 0.10815023\n",
      "Iteration 55, loss = 0.10888817\n",
      "Iteration 56, loss = 0.10757443\n",
      "Iteration 57, loss = 0.10623232\n",
      "Iteration 58, loss = 0.10669944\n",
      "Iteration 59, loss = 0.10451892\n",
      "Iteration 60, loss = 0.10525136\n",
      "Iteration 61, loss = 0.10351370\n",
      "Iteration 62, loss = 0.10357214\n",
      "Iteration 63, loss = 0.10273791\n",
      "Iteration 64, loss = 0.10204908\n",
      "Iteration 65, loss = 0.10194639\n",
      "Iteration 66, loss = 0.10079542\n",
      "Iteration 67, loss = 0.10100101\n",
      "Iteration 68, loss = 0.09984853\n",
      "Iteration 69, loss = 0.09995490\n",
      "Iteration 70, loss = 0.09906990\n",
      "Iteration 71, loss = 0.09893652\n",
      "Iteration 72, loss = 0.09833716\n",
      "Iteration 73, loss = 0.09799328\n",
      "Iteration 74, loss = 0.09763248\n",
      "Iteration 75, loss = 0.09713049\n",
      "Iteration 76, loss = 0.09691480\n",
      "Iteration 77, loss = 0.09636024\n",
      "Iteration 78, loss = 0.09619844\n",
      "Iteration 79, loss = 0.09564275\n",
      "Iteration 80, loss = 0.09549697\n",
      "Iteration 81, loss = 0.09497389\n",
      "Iteration 82, loss = 0.09481112\n",
      "Iteration 83, loss = 0.09433550\n",
      "Iteration 84, loss = 0.09415419\n",
      "Iteration 85, loss = 0.09372039\n",
      "Iteration 86, loss = 0.09352026\n",
      "Iteration 87, loss = 0.09312692\n",
      "Iteration 88, loss = 0.09291415\n",
      "Iteration 89, loss = 0.09255015\n",
      "Iteration 90, loss = 0.09233021\n",
      "Iteration 91, loss = 0.09199189\n",
      "Iteration 92, loss = 0.09176905\n",
      "Iteration 93, loss = 0.09144907\n",
      "Iteration 94, loss = 0.09122645\n",
      "Iteration 95, loss = 0.09092308\n",
      "Iteration 96, loss = 0.09070275\n",
      "Iteration 97, loss = 0.09041252\n",
      "Iteration 98, loss = 0.09019469\n",
      "Iteration 99, loss = 0.08991734\n",
      "Iteration 100, loss = 0.08970262\n",
      "Iteration 101, loss = 0.08943722\n",
      "Iteration 102, loss = 0.08922431\n",
      "Iteration 103, loss = 0.08897098\n",
      "Iteration 104, loss = 0.08875947\n",
      "Iteration 105, loss = 0.08851822\n",
      "Iteration 106, loss = 0.08830707\n",
      "Iteration 107, loss = 0.08807779\n",
      "Iteration 108, loss = 0.08786696\n",
      "Iteration 109, loss = 0.08764921\n",
      "Iteration 110, loss = 0.08743887\n",
      "Iteration 111, loss = 0.08723149\n",
      "Iteration 112, loss = 0.08702263\n",
      "Iteration 113, loss = 0.08682392\n",
      "Iteration 114, loss = 0.08661807\n",
      "Iteration 115, loss = 0.08642585\n",
      "Iteration 116, loss = 0.08622468\n",
      "Iteration 117, loss = 0.08603685\n",
      "Iteration 118, loss = 0.08584206\n",
      "Iteration 119, loss = 0.08565703\n",
      "Iteration 120, loss = 0.08546937\n",
      "Iteration 121, loss = 0.08528648\n",
      "Iteration 122, loss = 0.08510584\n",
      "Iteration 123, loss = 0.08492541\n",
      "Iteration 124, loss = 0.08475061\n",
      "Iteration 125, loss = 0.08457373\n",
      "Iteration 126, loss = 0.08440326\n",
      "Iteration 127, loss = 0.08423109\n",
      "Iteration 128, loss = 0.08406370\n",
      "Iteration 129, loss = 0.08389683\n",
      "Iteration 130, loss = 0.08373210\n",
      "Iteration 131, loss = 0.08357026\n",
      "Iteration 132, loss = 0.08340860\n",
      "Iteration 133, loss = 0.08325082\n",
      "Iteration 134, loss = 0.08309299\n",
      "Iteration 135, loss = 0.08293835\n",
      "Iteration 136, loss = 0.08278481\n",
      "Iteration 137, loss = 0.08263292\n",
      "Iteration 138, loss = 0.08248341\n",
      "Iteration 139, loss = 0.08233458\n",
      "Iteration 140, loss = 0.08218842\n",
      "Iteration 141, loss = 0.08204309\n",
      "Iteration 142, loss = 0.08189972\n",
      "Iteration 143, loss = 0.08175800\n",
      "Iteration 144, loss = 0.08161739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 145, loss = 0.08147889\n",
      "Iteration 146, loss = 0.08134135\n",
      "Iteration 147, loss = 0.08120561\n",
      "Iteration 148, loss = 0.08107130\n",
      "Iteration 149, loss = 0.08093826\n",
      "Iteration 150, loss = 0.08080695\n",
      "Iteration 151, loss = 0.08067672\n",
      "Iteration 152, loss = 0.08054806\n",
      "Iteration 153, loss = 0.08042075\n",
      "Iteration 154, loss = 0.08029461\n",
      "Iteration 155, loss = 0.08017001\n",
      "Iteration 156, loss = 0.08004651\n",
      "Iteration 157, loss = 0.07992434\n",
      "Iteration 158, loss = 0.07980348\n",
      "Iteration 159, loss = 0.07968377\n",
      "Iteration 160, loss = 0.07956541\n",
      "Iteration 161, loss = 0.07944816\n",
      "Iteration 162, loss = 0.07933207\n",
      "Iteration 163, loss = 0.07921721\n",
      "Iteration 164, loss = 0.07910341\n",
      "Iteration 165, loss = 0.07899077\n",
      "Iteration 166, loss = 0.07887923\n",
      "Iteration 167, loss = 0.07876873\n",
      "Iteration 168, loss = 0.07865935\n",
      "Iteration 169, loss = 0.07855100\n",
      "Iteration 170, loss = 0.07844367\n",
      "Iteration 171, loss = 0.07833740\n",
      "Iteration 172, loss = 0.07823214\n",
      "Iteration 173, loss = 0.07812787\n",
      "Iteration 174, loss = 0.07802461\n",
      "Iteration 175, loss = 0.07792228\n",
      "Iteration 176, loss = 0.07782095\n",
      "Iteration 177, loss = 0.07772055\n",
      "Iteration 178, loss = 0.07762110\n",
      "Iteration 179, loss = 0.07752260\n",
      "Iteration 180, loss = 0.07742501\n",
      "Iteration 181, loss = 0.07732831\n",
      "Iteration 182, loss = 0.07723250\n",
      "Iteration 183, loss = 0.07713755\n",
      "Iteration 184, loss = 0.07704345\n",
      "Iteration 185, loss = 0.07695020\n",
      "Iteration 186, loss = 0.07685778\n",
      "Iteration 187, loss = 0.07676618\n",
      "Iteration 188, loss = 0.07667540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 3.39945134\n",
      "Iteration 3, loss = 1.36052596\n",
      "Iteration 4, loss = 0.81661730\n",
      "Iteration 5, loss = 1.07290492\n",
      "Iteration 6, loss = 0.52744380\n",
      "Iteration 7, loss = 1.28750612\n",
      "Iteration 8, loss = 0.55091271\n",
      "Iteration 9, loss = 0.53286401\n",
      "Iteration 10, loss = 0.67541372\n",
      "Iteration 11, loss = 0.55715154\n",
      "Iteration 12, loss = 0.43408793\n",
      "Iteration 13, loss = 0.42581179\n",
      "Iteration 14, loss = 0.37872221\n",
      "Iteration 15, loss = 0.30151316\n",
      "Iteration 16, loss = 0.31350157\n",
      "Iteration 17, loss = 0.36937715\n",
      "Iteration 18, loss = 0.31200681\n",
      "Iteration 19, loss = 0.26581165\n",
      "Iteration 20, loss = 0.29488372\n",
      "Iteration 21, loss = 0.25463304\n",
      "Iteration 22, loss = 0.22994012\n",
      "Iteration 23, loss = 0.25717399\n",
      "Iteration 24, loss = 0.22526134\n",
      "Iteration 25, loss = 0.19999361\n",
      "Iteration 26, loss = 0.21736197\n",
      "Iteration 27, loss = 0.18906662\n",
      "Iteration 28, loss = 0.17406340\n",
      "Iteration 29, loss = 0.18368034\n",
      "Iteration 30, loss = 0.16192629\n",
      "Iteration 31, loss = 0.15650364\n",
      "Iteration 32, loss = 0.16214818\n",
      "Iteration 33, loss = 0.14249252\n",
      "Iteration 34, loss = 0.14479728\n",
      "Iteration 35, loss = 0.14307341\n",
      "Iteration 36, loss = 0.12962335\n",
      "Iteration 37, loss = 0.13486506\n",
      "Iteration 38, loss = 0.12800484\n",
      "Iteration 39, loss = 0.12200719\n",
      "Iteration 40, loss = 0.12578025\n",
      "Iteration 41, loss = 0.11747226\n",
      "Iteration 42, loss = 0.11716234\n",
      "Iteration 43, loss = 0.11719227\n",
      "Iteration 44, loss = 0.11084815\n",
      "Iteration 45, loss = 0.11338119\n",
      "Iteration 46, loss = 0.10970263\n",
      "Iteration 47, loss = 0.10757308\n",
      "Iteration 48, loss = 0.10883794\n",
      "Iteration 49, loss = 0.10453111\n",
      "Iteration 50, loss = 0.10566914\n",
      "Iteration 51, loss = 0.10395522\n",
      "Iteration 52, loss = 0.10211692\n",
      "Iteration 53, loss = 0.10302197\n",
      "Iteration 54, loss = 0.10028855\n",
      "Iteration 55, loss = 0.10089276\n",
      "Iteration 56, loss = 0.09970169\n",
      "Iteration 57, loss = 0.09862207\n",
      "Iteration 58, loss = 0.09896271\n",
      "Iteration 59, loss = 0.09721032\n",
      "Iteration 60, loss = 0.09762286\n",
      "Iteration 61, loss = 0.09649889\n",
      "Iteration 62, loss = 0.09608596\n",
      "Iteration 63, loss = 0.09587422\n",
      "Iteration 64, loss = 0.09486106\n",
      "Iteration 65, loss = 0.09502282\n",
      "Iteration 66, loss = 0.09401727\n",
      "Iteration 67, loss = 0.09398948\n",
      "Iteration 68, loss = 0.09337907\n",
      "Iteration 69, loss = 0.09297704\n",
      "Iteration 70, loss = 0.09276121\n",
      "Iteration 71, loss = 0.09211162\n",
      "Iteration 72, loss = 0.09206617\n",
      "Iteration 73, loss = 0.09141949\n",
      "Iteration 74, loss = 0.09131832\n",
      "Iteration 75, loss = 0.09083105\n",
      "Iteration 76, loss = 0.09058595\n",
      "Iteration 77, loss = 0.09027189\n",
      "Iteration 78, loss = 0.08991297\n",
      "Iteration 79, loss = 0.08971445\n",
      "Iteration 80, loss = 0.08930335\n",
      "Iteration 81, loss = 0.08915014\n",
      "Iteration 82, loss = 0.08875183\n",
      "Iteration 83, loss = 0.08858685\n",
      "Iteration 84, loss = 0.08823820\n",
      "Iteration 85, loss = 0.08803983\n",
      "Iteration 86, loss = 0.08774532\n",
      "Iteration 87, loss = 0.08751512\n",
      "Iteration 88, loss = 0.08726806\n",
      "Iteration 89, loss = 0.08701780\n",
      "Iteration 90, loss = 0.08680498\n",
      "Iteration 91, loss = 0.08654727\n",
      "Iteration 92, loss = 0.08635251\n",
      "Iteration 93, loss = 0.08609681\n",
      "Iteration 94, loss = 0.08591163\n",
      "Iteration 95, loss = 0.08566478\n",
      "Iteration 96, loss = 0.08548392\n",
      "Iteration 97, loss = 0.08524951\n",
      "Iteration 98, loss = 0.08507018\n",
      "Iteration 99, loss = 0.08484761\n",
      "Iteration 100, loss = 0.08466898\n",
      "Iteration 101, loss = 0.08445786\n",
      "Iteration 102, loss = 0.08428046\n",
      "Iteration 103, loss = 0.08407922\n",
      "Iteration 104, loss = 0.08390375\n",
      "Iteration 105, loss = 0.08371133\n",
      "Iteration 106, loss = 0.08353861\n",
      "Iteration 107, loss = 0.08335363\n",
      "Iteration 108, loss = 0.08318413\n",
      "Iteration 109, loss = 0.08300626\n",
      "Iteration 110, loss = 0.08284091\n",
      "Iteration 111, loss = 0.08266873\n",
      "Iteration 112, loss = 0.08250761\n",
      "Iteration 113, loss = 0.08234109\n",
      "Iteration 114, loss = 0.08218381\n",
      "Iteration 115, loss = 0.08202197\n",
      "Iteration 116, loss = 0.08186842\n",
      "Iteration 117, loss = 0.08171096\n",
      "Iteration 118, loss = 0.08156109\n",
      "Iteration 119, loss = 0.08140773\n",
      "Iteration 120, loss = 0.08126126\n",
      "Iteration 121, loss = 0.08111183\n",
      "Iteration 122, loss = 0.08096860\n",
      "Iteration 123, loss = 0.08082292\n",
      "Iteration 124, loss = 0.08068273\n",
      "Iteration 125, loss = 0.08054085\n",
      "Iteration 126, loss = 0.08040373\n",
      "Iteration 127, loss = 0.08026551\n",
      "Iteration 128, loss = 0.08013106\n",
      "Iteration 129, loss = 0.07999629\n",
      "Iteration 130, loss = 0.07986440\n",
      "Iteration 131, loss = 0.07973303\n",
      "Iteration 132, loss = 0.07960381\n",
      "Iteration 133, loss = 0.07947575\n",
      "Iteration 134, loss = 0.07934899\n",
      "Iteration 135, loss = 0.07922393\n",
      "Iteration 136, loss = 0.07909972\n",
      "Iteration 137, loss = 0.07897753\n",
      "Iteration 138, loss = 0.07885589\n",
      "Iteration 139, loss = 0.07873631\n",
      "Iteration 140, loss = 0.07861720\n",
      "Iteration 141, loss = 0.07850001\n",
      "Iteration 142, loss = 0.07838345\n",
      "Iteration 143, loss = 0.07826847\n",
      "Iteration 144, loss = 0.07815438\n",
      "Iteration 145, loss = 0.07804156\n",
      "Iteration 146, loss = 0.07792996\n",
      "Iteration 147, loss = 0.07781932\n",
      "Iteration 148, loss = 0.07770997\n",
      "Iteration 149, loss = 0.07760150\n",
      "Iteration 150, loss = 0.07749432\n",
      "Iteration 151, loss = 0.07738796\n",
      "Iteration 152, loss = 0.07728279\n",
      "Iteration 153, loss = 0.07717853\n",
      "Iteration 154, loss = 0.07707531\n",
      "Iteration 155, loss = 0.07697306\n",
      "Iteration 156, loss = 0.07687166\n",
      "Iteration 157, loss = 0.07677130\n",
      "Iteration 158, loss = 0.07667184\n",
      "Iteration 159, loss = 0.07657345\n",
      "Iteration 160, loss = 0.07647589\n",
      "Iteration 161, loss = 0.07637926\n",
      "Iteration 162, loss = 0.07628348\n",
      "Iteration 163, loss = 0.07618853\n",
      "Iteration 164, loss = 0.07609446\n",
      "Iteration 165, loss = 0.07600115\n",
      "Iteration 166, loss = 0.07590870\n",
      "Iteration 167, loss = 0.07581701\n",
      "Iteration 168, loss = 0.07572612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 3.39276275\n",
      "Iteration 3, loss = 1.34522771\n",
      "Iteration 4, loss = 0.81488990\n",
      "Iteration 5, loss = 1.06036669\n",
      "Iteration 6, loss = 0.53607746\n",
      "Iteration 7, loss = 1.19393819\n",
      "Iteration 8, loss = 0.52687136\n",
      "Iteration 9, loss = 0.50956823\n",
      "Iteration 10, loss = 0.63456444\n",
      "Iteration 11, loss = 0.52209688\n",
      "Iteration 12, loss = 0.41025866\n",
      "Iteration 13, loss = 0.41054959\n",
      "Iteration 14, loss = 0.35796388\n",
      "Iteration 15, loss = 0.28413752\n",
      "Iteration 16, loss = 0.29126634\n",
      "Iteration 17, loss = 0.33984079\n",
      "Iteration 18, loss = 0.28770589\n",
      "Iteration 19, loss = 0.24858502\n",
      "Iteration 20, loss = 0.27374706\n",
      "Iteration 21, loss = 0.23093901\n",
      "Iteration 22, loss = 0.21387634\n",
      "Iteration 23, loss = 0.23081759\n",
      "Iteration 24, loss = 0.19194479\n",
      "Iteration 25, loss = 0.17621483\n",
      "Iteration 26, loss = 0.18709930\n",
      "Iteration 27, loss = 0.15950465\n",
      "Iteration 28, loss = 0.15298195\n",
      "Iteration 29, loss = 0.15776612\n",
      "Iteration 30, loss = 0.13676201\n",
      "Iteration 31, loss = 0.13318051\n",
      "Iteration 32, loss = 0.13556111\n",
      "Iteration 33, loss = 0.11873393\n",
      "Iteration 34, loss = 0.11900326\n",
      "Iteration 35, loss = 0.11857372\n",
      "Iteration 36, loss = 0.10666203\n",
      "Iteration 37, loss = 0.10782019\n",
      "Iteration 38, loss = 0.10635201\n",
      "Iteration 39, loss = 0.09791792\n",
      "Iteration 40, loss = 0.09983614\n",
      "Iteration 41, loss = 0.09759171\n",
      "Iteration 42, loss = 0.09187956\n",
      "Iteration 43, loss = 0.09363788\n",
      "Iteration 44, loss = 0.09123527\n",
      "Iteration 45, loss = 0.08756631\n",
      "Iteration 46, loss = 0.08897809\n",
      "Iteration 47, loss = 0.08674779\n",
      "Iteration 48, loss = 0.08428836\n",
      "Iteration 49, loss = 0.08535955\n",
      "Iteration 50, loss = 0.08327494\n",
      "Iteration 51, loss = 0.08184266\n",
      "Iteration 52, loss = 0.08247075\n",
      "Iteration 53, loss = 0.08071662\n",
      "Iteration 54, loss = 0.07978760\n",
      "Iteration 55, loss = 0.08015016\n",
      "Iteration 56, loss = 0.07861048\n",
      "Iteration 57, loss = 0.07815391\n",
      "Iteration 58, loss = 0.07819609\n",
      "Iteration 59, loss = 0.07695660\n",
      "Iteration 60, loss = 0.07669652\n",
      "Iteration 61, loss = 0.07655159\n",
      "Iteration 62, loss = 0.07554842\n",
      "Iteration 63, loss = 0.07545075\n",
      "Iteration 64, loss = 0.07513290\n",
      "Iteration 65, loss = 0.07437016\n",
      "Iteration 66, loss = 0.07431026\n",
      "Iteration 67, loss = 0.07389276\n",
      "Iteration 68, loss = 0.07334099\n",
      "Iteration 69, loss = 0.07326757\n",
      "Iteration 70, loss = 0.07280886\n",
      "Iteration 71, loss = 0.07241738\n",
      "Iteration 72, loss = 0.07229733\n",
      "Iteration 73, loss = 0.07183956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74, loss = 0.07157672\n",
      "Iteration 75, loss = 0.07138605\n",
      "Iteration 76, loss = 0.07097659\n",
      "Iteration 77, loss = 0.07078060\n",
      "Iteration 78, loss = 0.07053366\n",
      "Iteration 79, loss = 0.07019122\n",
      "Iteration 80, loss = 0.07002245\n",
      "Iteration 81, loss = 0.06974059\n",
      "Iteration 82, loss = 0.06946801\n",
      "Iteration 83, loss = 0.06929174\n",
      "Iteration 84, loss = 0.06900486\n",
      "Iteration 85, loss = 0.06878920\n",
      "Iteration 86, loss = 0.06858893\n",
      "Iteration 87, loss = 0.06832446\n",
      "Iteration 88, loss = 0.06813881\n",
      "Iteration 89, loss = 0.06791885\n",
      "Iteration 90, loss = 0.06768907\n",
      "Iteration 91, loss = 0.06751079\n",
      "Iteration 92, loss = 0.06728782\n",
      "Iteration 93, loss = 0.06709228\n",
      "Iteration 94, loss = 0.06690932\n",
      "Iteration 95, loss = 0.06669750\n",
      "Iteration 96, loss = 0.06652235\n",
      "Iteration 97, loss = 0.06633211\n",
      "Iteration 98, loss = 0.06614058\n",
      "Iteration 99, loss = 0.06597158\n",
      "Iteration 100, loss = 0.06578223\n",
      "Iteration 101, loss = 0.06560961\n",
      "Iteration 102, loss = 0.06543911\n",
      "Iteration 103, loss = 0.06525981\n",
      "Iteration 104, loss = 0.06509802\n",
      "Iteration 105, loss = 0.06492707\n",
      "Iteration 106, loss = 0.06476139\n",
      "Iteration 107, loss = 0.06460294\n",
      "Iteration 108, loss = 0.06443714\n",
      "Iteration 109, loss = 0.06428214\n",
      "Iteration 110, loss = 0.06412491\n",
      "Iteration 111, loss = 0.06396921\n",
      "Iteration 112, loss = 0.06382058\n",
      "Iteration 113, loss = 0.06366783\n",
      "Iteration 114, loss = 0.06352185\n",
      "Iteration 115, loss = 0.06337640\n",
      "Iteration 116, loss = 0.06323093\n",
      "Iteration 117, loss = 0.06309083\n",
      "Iteration 118, loss = 0.06294878\n",
      "Iteration 119, loss = 0.06281068\n",
      "Iteration 120, loss = 0.06267397\n",
      "Iteration 121, loss = 0.06253741\n",
      "Iteration 122, loss = 0.06240474\n",
      "Iteration 123, loss = 0.06227151\n",
      "Iteration 124, loss = 0.06214108\n",
      "Iteration 125, loss = 0.06201209\n",
      "Iteration 126, loss = 0.06188365\n",
      "Iteration 127, loss = 0.06175818\n",
      "Iteration 128, loss = 0.06163282\n",
      "Iteration 129, loss = 0.06150962\n",
      "Iteration 130, loss = 0.06138780\n",
      "Iteration 131, loss = 0.06126686\n",
      "Iteration 132, loss = 0.06114817\n",
      "Iteration 133, loss = 0.06102999\n",
      "Iteration 134, loss = 0.06091371\n",
      "Iteration 135, loss = 0.06079856\n",
      "Iteration 136, loss = 0.06068443\n",
      "Iteration 137, loss = 0.06057199\n",
      "Iteration 138, loss = 0.06046031\n",
      "Iteration 139, loss = 0.06035018\n",
      "Iteration 140, loss = 0.06024097\n",
      "Iteration 141, loss = 0.06013286\n",
      "Iteration 142, loss = 0.06002600\n",
      "Iteration 143, loss = 0.05991994\n",
      "Iteration 144, loss = 0.05981523\n",
      "Iteration 145, loss = 0.05971137\n",
      "Iteration 146, loss = 0.05960860\n",
      "Iteration 147, loss = 0.05950697\n",
      "Iteration 148, loss = 0.05940639\n",
      "Iteration 149, loss = 0.05930707\n",
      "Iteration 150, loss = 0.05920863\n",
      "Iteration 151, loss = 0.05911122\n",
      "Iteration 152, loss = 0.05901467\n",
      "Iteration 153, loss = 0.05891900\n",
      "Iteration 154, loss = 0.05882430\n",
      "Iteration 155, loss = 0.05873041\n",
      "Iteration 156, loss = 0.05863744\n",
      "Iteration 157, loss = 0.05854527\n",
      "Iteration 158, loss = 0.05845395\n",
      "Iteration 159, loss = 0.05836342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 3.39060795\n",
      "Iteration 3, loss = 1.36145023\n",
      "Iteration 4, loss = 0.80647857\n",
      "Iteration 5, loss = 1.03637442\n",
      "Iteration 6, loss = 0.51452343\n",
      "Iteration 7, loss = 1.28941989\n",
      "Iteration 8, loss = 0.51563688\n",
      "Iteration 9, loss = 0.55275498\n",
      "Iteration 10, loss = 0.68952431\n",
      "Iteration 11, loss = 0.54466334\n",
      "Iteration 12, loss = 0.42181185\n",
      "Iteration 13, loss = 0.42402007\n",
      "Iteration 14, loss = 0.36968324\n",
      "Iteration 15, loss = 0.28392398\n",
      "Iteration 16, loss = 0.30431124\n",
      "Iteration 17, loss = 0.36108143\n",
      "Iteration 18, loss = 0.29644516\n",
      "Iteration 19, loss = 0.24733976\n",
      "Iteration 20, loss = 0.27960419\n",
      "Iteration 21, loss = 0.24411606\n",
      "Iteration 22, loss = 0.21188827\n",
      "Iteration 23, loss = 0.24112535\n",
      "Iteration 24, loss = 0.21441579\n",
      "Iteration 25, loss = 0.17840493\n",
      "Iteration 26, loss = 0.19405089\n",
      "Iteration 27, loss = 0.18003532\n",
      "Iteration 28, loss = 0.15352856\n",
      "Iteration 29, loss = 0.16393203\n",
      "Iteration 30, loss = 0.15596332\n",
      "Iteration 31, loss = 0.13407503\n",
      "Iteration 32, loss = 0.14065696\n",
      "Iteration 33, loss = 0.13551273\n",
      "Iteration 34, loss = 0.11874432\n",
      "Iteration 35, loss = 0.12359749\n",
      "Iteration 36, loss = 0.11984631\n",
      "Iteration 37, loss = 0.10719776\n",
      "Iteration 38, loss = 0.11059247\n",
      "Iteration 39, loss = 0.10766185\n",
      "Iteration 40, loss = 0.09842528\n",
      "Iteration 41, loss = 0.10125306\n",
      "Iteration 42, loss = 0.09856327\n",
      "Iteration 43, loss = 0.09184830\n",
      "Iteration 44, loss = 0.09409114\n",
      "Iteration 45, loss = 0.09146480\n",
      "Iteration 46, loss = 0.08688854\n",
      "Iteration 47, loss = 0.08868495\n",
      "Iteration 48, loss = 0.08607401\n",
      "Iteration 49, loss = 0.08355985\n",
      "Iteration 50, loss = 0.08298731\n",
      "Iteration 51, loss = 0.08064711\n",
      "Iteration 52, loss = 0.08051212\n",
      "Iteration 53, loss = 0.07964240\n",
      "Iteration 54, loss = 0.07798966\n",
      "Iteration 55, loss = 0.07836219\n",
      "Iteration 56, loss = 0.07614078\n",
      "Iteration 57, loss = 0.07518265\n",
      "Iteration 58, loss = 0.07429040\n",
      "Iteration 59, loss = 0.07352006\n",
      "Iteration 60, loss = 0.07284965\n",
      "Iteration 61, loss = 0.07213833\n",
      "Iteration 62, loss = 0.07152835\n",
      "Iteration 63, loss = 0.07082279\n",
      "Iteration 64, loss = 0.07023083\n",
      "Iteration 65, loss = 0.06956113\n",
      "Iteration 66, loss = 0.06886349\n",
      "Iteration 67, loss = 0.06822906\n",
      "Iteration 68, loss = 0.06754471\n",
      "Iteration 69, loss = 0.06684525\n",
      "Iteration 70, loss = 0.06622498\n",
      "Iteration 71, loss = 0.06640605\n",
      "Iteration 72, loss = 0.06944316\n",
      "Iteration 73, loss = 0.06947463\n",
      "Iteration 74, loss = 0.06384110\n",
      "Iteration 75, loss = 0.06565838\n",
      "Iteration 76, loss = 0.06535195\n",
      "Iteration 77, loss = 0.06212836\n",
      "Iteration 78, loss = 0.06428805\n",
      "Iteration 79, loss = 0.06190426\n",
      "Iteration 80, loss = 0.06154547\n",
      "Iteration 81, loss = 0.06247290\n",
      "Iteration 82, loss = 0.05995910\n",
      "Iteration 83, loss = 0.06065987\n",
      "Iteration 84, loss = 0.06088395\n",
      "Iteration 85, loss = 0.05860954\n",
      "Iteration 86, loss = 0.05902756\n",
      "Iteration 87, loss = 0.05980616\n",
      "Iteration 88, loss = 0.05791857\n",
      "Iteration 89, loss = 0.05680813\n",
      "Iteration 90, loss = 0.05759054\n",
      "Iteration 91, loss = 0.05819839\n",
      "Iteration 92, loss = 0.05780802\n",
      "Iteration 93, loss = 0.05592978\n",
      "Iteration 94, loss = 0.05477316\n",
      "Iteration 95, loss = 0.05481406\n",
      "Iteration 96, loss = 0.05576018\n",
      "Iteration 97, loss = 0.05784093\n",
      "Iteration 98, loss = 0.05860706\n",
      "Iteration 99, loss = 0.05867904\n",
      "Iteration 100, loss = 0.05414309\n",
      "Iteration 101, loss = 0.05271652\n",
      "Iteration 102, loss = 0.05497102\n",
      "Iteration 103, loss = 0.05574286\n",
      "Iteration 104, loss = 0.05429505\n",
      "Iteration 105, loss = 0.05170376\n",
      "Iteration 106, loss = 0.05234730\n",
      "Iteration 107, loss = 0.05432927\n",
      "Iteration 108, loss = 0.05303318\n",
      "Iteration 109, loss = 0.05108209\n",
      "Iteration 110, loss = 0.05075582\n",
      "Iteration 111, loss = 0.05187337\n",
      "Iteration 112, loss = 0.05247795\n",
      "Iteration 113, loss = 0.05090351\n",
      "Iteration 114, loss = 0.04977033\n",
      "Iteration 115, loss = 0.05000804\n",
      "Iteration 116, loss = 0.05070004\n",
      "Iteration 117, loss = 0.05094203\n",
      "Iteration 118, loss = 0.04988647\n",
      "Iteration 119, loss = 0.04898574\n",
      "Iteration 120, loss = 0.04876700\n",
      "Iteration 121, loss = 0.04912862\n",
      "Iteration 122, loss = 0.04963961\n",
      "Iteration 123, loss = 0.04951184\n",
      "Iteration 124, loss = 0.04919915\n",
      "Iteration 125, loss = 0.04844705\n",
      "Iteration 126, loss = 0.04789851\n",
      "Iteration 127, loss = 0.04761909\n",
      "Iteration 128, loss = 0.04761011\n",
      "Iteration 129, loss = 0.04779898\n",
      "Iteration 130, loss = 0.04804710\n",
      "Iteration 131, loss = 0.04855633\n",
      "Iteration 132, loss = 0.04883595\n",
      "Iteration 133, loss = 0.04962921\n",
      "Iteration 134, loss = 0.04934323\n",
      "Iteration 135, loss = 0.04935506\n",
      "Iteration 136, loss = 0.04797330\n",
      "Iteration 137, loss = 0.04692966\n",
      "Iteration 138, loss = 0.04625334\n",
      "Iteration 139, loss = 0.04627654\n",
      "Iteration 140, loss = 0.04676216\n",
      "Iteration 141, loss = 0.04710489\n",
      "Iteration 142, loss = 0.04735031\n",
      "Iteration 143, loss = 0.04676065\n",
      "Iteration 144, loss = 0.04616015\n",
      "Iteration 145, loss = 0.04558254\n",
      "Iteration 146, loss = 0.04538556\n",
      "Iteration 147, loss = 0.04551626\n",
      "Iteration 148, loss = 0.04572995\n",
      "Iteration 149, loss = 0.04592192\n",
      "Iteration 150, loss = 0.04574598\n",
      "Iteration 151, loss = 0.04550539\n",
      "Iteration 152, loss = 0.04508819\n",
      "Iteration 153, loss = 0.04477402\n",
      "Iteration 154, loss = 0.04459705\n",
      "Iteration 155, loss = 0.04456148\n",
      "Iteration 156, loss = 0.04461545\n",
      "Iteration 157, loss = 0.04467113\n",
      "Iteration 158, loss = 0.04474433\n",
      "Iteration 159, loss = 0.04469596\n",
      "Iteration 160, loss = 0.04466862\n",
      "Iteration 161, loss = 0.04449834\n",
      "Iteration 162, loss = 0.04436699\n",
      "Iteration 163, loss = 0.04415426\n",
      "Iteration 164, loss = 0.04398593\n",
      "Iteration 165, loss = 0.04380399\n",
      "Iteration 166, loss = 0.04365863\n",
      "Iteration 167, loss = 0.04352629\n",
      "Iteration 168, loss = 0.04341628\n",
      "Iteration 169, loss = 0.04331807\n",
      "Iteration 170, loss = 0.04322935\n",
      "Iteration 171, loss = 0.04314672\n",
      "Iteration 172, loss = 0.04306829\n",
      "Iteration 173, loss = 0.04299317\n",
      "Iteration 174, loss = 0.04292156\n",
      "Iteration 175, loss = 0.04285565\n",
      "Iteration 176, loss = 0.04279906\n",
      "Iteration 177, loss = 0.04276247\n",
      "Iteration 178, loss = 0.04276014\n",
      "Iteration 179, loss = 0.04284740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 3.40464561\n",
      "Iteration 3, loss = 1.36994469\n",
      "Iteration 4, loss = 0.81471570\n",
      "Iteration 5, loss = 1.09537369\n",
      "Iteration 6, loss = 0.52868664\n",
      "Iteration 7, loss = 1.26647530\n",
      "Iteration 8, loss = 0.56935444\n",
      "Iteration 9, loss = 0.51282026\n",
      "Iteration 10, loss = 0.66449759\n",
      "Iteration 11, loss = 0.56280234\n",
      "Iteration 12, loss = 0.43245892\n",
      "Iteration 13, loss = 0.43952956\n",
      "Iteration 14, loss = 0.39926065\n",
      "Iteration 15, loss = 0.32118186\n",
      "Iteration 16, loss = 0.31283931\n",
      "Iteration 17, loss = 0.36276022\n",
      "Iteration 18, loss = 0.35291155\n",
      "Iteration 19, loss = 0.29023516\n",
      "Iteration 20, loss = 0.29369215\n",
      "Iteration 21, loss = 0.29038631\n",
      "Iteration 22, loss = 0.23783109\n",
      "Iteration 23, loss = 0.25459140\n",
      "Iteration 24, loss = 0.25312474\n",
      "Iteration 25, loss = 0.20999864\n",
      "Iteration 26, loss = 0.22393928\n",
      "Iteration 27, loss = 0.21245456\n",
      "Iteration 28, loss = 0.18379435\n",
      "Iteration 29, loss = 0.19837799\n",
      "Iteration 30, loss = 0.18044927\n",
      "Iteration 31, loss = 0.16635194\n",
      "Iteration 32, loss = 0.17740928\n",
      "Iteration 33, loss = 0.15546173\n",
      "Iteration 34, loss = 0.15632704\n",
      "Iteration 35, loss = 0.15633208\n",
      "Iteration 36, loss = 0.14034041\n",
      "Iteration 37, loss = 0.14753941\n",
      "Iteration 38, loss = 0.13788813\n",
      "Iteration 39, loss = 0.13345396\n",
      "Iteration 40, loss = 0.13666002\n",
      "Iteration 41, loss = 0.12593442\n",
      "Iteration 42, loss = 0.12937812\n",
      "Iteration 43, loss = 0.12471311\n",
      "Iteration 44, loss = 0.12101962\n",
      "Iteration 45, loss = 0.12324702\n",
      "Iteration 46, loss = 0.11648391\n",
      "Iteration 47, loss = 0.11877201\n",
      "Iteration 48, loss = 0.11520879\n",
      "Iteration 49, loss = 0.11366634\n",
      "Iteration 50, loss = 0.11424456\n",
      "Iteration 51, loss = 0.11018743\n",
      "Iteration 52, loss = 0.11193680\n",
      "Iteration 53, loss = 0.10862836\n",
      "Iteration 54, loss = 0.10888656\n",
      "Iteration 55, loss = 0.10783040\n",
      "Iteration 56, loss = 0.10617897\n",
      "Iteration 57, loss = 0.10669907\n",
      "Iteration 58, loss = 0.10440105\n",
      "Iteration 59, loss = 0.10511223\n",
      "Iteration 60, loss = 0.10326385\n",
      "Iteration 61, loss = 0.10338542\n",
      "Iteration 62, loss = 0.10236190\n",
      "Iteration 63, loss = 0.10179930\n",
      "Iteration 64, loss = 0.10150040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 0.10045064\n",
      "Iteration 66, loss = 0.10053567\n",
      "Iteration 67, loss = 0.09938698\n",
      "Iteration 68, loss = 0.09949830\n",
      "Iteration 69, loss = 0.09848898\n",
      "Iteration 70, loss = 0.09847843\n",
      "Iteration 71, loss = 0.09767930\n",
      "Iteration 72, loss = 0.09750044\n",
      "Iteration 73, loss = 0.09692682\n",
      "Iteration 74, loss = 0.09659385\n",
      "Iteration 75, loss = 0.09618266\n",
      "Iteration 76, loss = 0.09576612\n",
      "Iteration 77, loss = 0.09545963\n",
      "Iteration 78, loss = 0.09499520\n",
      "Iteration 79, loss = 0.09475047\n",
      "Iteration 80, loss = 0.09428247\n",
      "Iteration 81, loss = 0.09406259\n",
      "Iteration 82, loss = 0.09360949\n",
      "Iteration 83, loss = 0.09340176\n",
      "Iteration 84, loss = 0.09297332\n",
      "Iteration 85, loss = 0.09276353\n",
      "Iteration 86, loss = 0.09236278\n",
      "Iteration 87, loss = 0.09215147\n",
      "Iteration 88, loss = 0.09177720\n",
      "Iteration 89, loss = 0.09156260\n",
      "Iteration 90, loss = 0.09121567\n",
      "Iteration 91, loss = 0.09099951\n",
      "Iteration 92, loss = 0.09067475\n",
      "Iteration 93, loss = 0.09045566\n",
      "Iteration 94, loss = 0.09015149\n",
      "Iteration 95, loss = 0.08993139\n",
      "Iteration 96, loss = 0.08964580\n",
      "Iteration 97, loss = 0.08942420\n",
      "Iteration 98, loss = 0.08915598\n",
      "Iteration 99, loss = 0.08893392\n",
      "Iteration 100, loss = 0.08868225\n",
      "Iteration 101, loss = 0.08846001\n",
      "Iteration 102, loss = 0.08822333\n",
      "Iteration 103, loss = 0.08800142\n",
      "Iteration 104, loss = 0.08777877\n",
      "Iteration 105, loss = 0.08755856\n",
      "Iteration 106, loss = 0.08734786\n",
      "Iteration 107, loss = 0.08713012\n",
      "Iteration 108, loss = 0.08692930\n",
      "Iteration 109, loss = 0.08671602\n",
      "Iteration 110, loss = 0.08652258\n",
      "Iteration 111, loss = 0.08631518\n",
      "Iteration 112, loss = 0.08612673\n",
      "Iteration 113, loss = 0.08592684\n",
      "Iteration 114, loss = 0.08574178\n",
      "Iteration 115, loss = 0.08555055\n",
      "Iteration 116, loss = 0.08536807\n",
      "Iteration 117, loss = 0.08518542\n",
      "Iteration 118, loss = 0.08500518\n",
      "Iteration 119, loss = 0.08482990\n",
      "Iteration 120, loss = 0.08465269\n",
      "Iteration 121, loss = 0.08448314\n",
      "Iteration 122, loss = 0.08431017\n",
      "Iteration 123, loss = 0.08414462\n",
      "Iteration 124, loss = 0.08397700\n",
      "Iteration 125, loss = 0.08381439\n",
      "Iteration 126, loss = 0.08365260\n",
      "Iteration 127, loss = 0.08349263\n",
      "Iteration 128, loss = 0.08333612\n",
      "Iteration 129, loss = 0.08317948\n",
      "Iteration 130, loss = 0.08302730\n",
      "Iteration 131, loss = 0.08287480\n",
      "Iteration 132, loss = 0.08272587\n",
      "Iteration 133, loss = 0.08257778\n",
      "Iteration 134, loss = 0.08243149\n",
      "Iteration 135, loss = 0.08228755\n",
      "Iteration 136, loss = 0.08214417\n",
      "Iteration 137, loss = 0.08200371\n",
      "Iteration 138, loss = 0.08186380\n",
      "Iteration 139, loss = 0.08172609\n",
      "Iteration 140, loss = 0.08158974\n",
      "Iteration 141, loss = 0.08145460\n",
      "Iteration 142, loss = 0.08132152\n",
      "Iteration 143, loss = 0.08118922\n",
      "Iteration 144, loss = 0.08105888\n",
      "Iteration 145, loss = 0.08092966\n",
      "Iteration 146, loss = 0.08080178\n",
      "Iteration 147, loss = 0.08067549\n",
      "Iteration 148, loss = 0.08055016\n",
      "Iteration 149, loss = 0.08042647\n",
      "Iteration 150, loss = 0.08030388\n",
      "Iteration 151, loss = 0.08018256\n",
      "Iteration 152, loss = 0.08006263\n",
      "Iteration 153, loss = 0.07994369\n",
      "Iteration 154, loss = 0.07982615\n",
      "Iteration 155, loss = 0.07970968\n",
      "Iteration 156, loss = 0.07959436\n",
      "Iteration 157, loss = 0.07948029\n",
      "Iteration 158, loss = 0.07936723\n",
      "Iteration 159, loss = 0.07925539\n",
      "Iteration 160, loss = 0.07914462\n",
      "Iteration 161, loss = 0.07903487\n",
      "Iteration 162, loss = 0.07892627\n",
      "Iteration 163, loss = 0.07881871\n",
      "Iteration 164, loss = 0.07871234\n",
      "Iteration 165, loss = 0.07860702\n",
      "Iteration 166, loss = 0.07850265\n",
      "Iteration 167, loss = 0.07839933\n",
      "Iteration 168, loss = 0.07829698\n",
      "Iteration 169, loss = 0.07819558\n",
      "Iteration 170, loss = 0.07809516\n",
      "Iteration 171, loss = 0.07799564\n",
      "Iteration 172, loss = 0.07789706\n",
      "Iteration 173, loss = 0.07779940\n",
      "Iteration 174, loss = 0.07770262\n",
      "Iteration 175, loss = 0.07760685\n",
      "Iteration 176, loss = 0.07751201\n",
      "Iteration 177, loss = 0.07741803\n",
      "Iteration 178, loss = 0.07732491\n",
      "Iteration 179, loss = 0.07723265\n",
      "Iteration 180, loss = 0.07714120\n",
      "Iteration 181, loss = 0.07705060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 3.92262608\n",
      "Iteration 3, loss = 1.63140018\n",
      "Iteration 4, loss = 1.12082820\n",
      "Iteration 5, loss = 0.89317501\n",
      "Iteration 6, loss = 0.81107945\n",
      "Iteration 7, loss = 0.70362097\n",
      "Iteration 8, loss = 0.58092223\n",
      "Iteration 9, loss = 0.50059998\n",
      "Iteration 10, loss = 0.45201741\n",
      "Iteration 11, loss = 0.41441154\n",
      "Iteration 12, loss = 0.38046211\n",
      "Iteration 13, loss = 0.34847094\n",
      "Iteration 14, loss = 0.31948239\n",
      "Iteration 15, loss = 0.29877254\n",
      "Iteration 16, loss = 0.40416144\n",
      "Iteration 17, loss = 1.78539580\n",
      "Iteration 18, loss = 1.50960723\n",
      "Iteration 19, loss = 1.14093166\n",
      "Iteration 20, loss = 0.47512657\n",
      "Iteration 21, loss = 0.40394162\n",
      "Iteration 22, loss = 0.41025548\n",
      "Iteration 23, loss = 0.40828609\n",
      "Iteration 24, loss = 0.40264580\n",
      "Iteration 25, loss = 0.39673264\n",
      "Iteration 26, loss = 0.38923500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 3.98811238\n",
      "Iteration 3, loss = 1.57678317\n",
      "Iteration 4, loss = 1.13301148\n",
      "Iteration 5, loss = 0.90165079\n",
      "Iteration 6, loss = 0.81668391\n",
      "Iteration 7, loss = 0.70095571\n",
      "Iteration 8, loss = 0.58036275\n",
      "Iteration 9, loss = 0.52531272\n",
      "Iteration 10, loss = 0.47608085\n",
      "Iteration 11, loss = 0.43419746\n",
      "Iteration 12, loss = 0.39618291\n",
      "Iteration 13, loss = 0.36001834\n",
      "Iteration 14, loss = 0.32697861\n",
      "Iteration 15, loss = 0.31473550\n",
      "Iteration 16, loss = 0.66104554\n",
      "Iteration 17, loss = 2.77017829\n",
      "Iteration 18, loss = 0.40186633\n",
      "Iteration 19, loss = 0.40979744\n",
      "Iteration 20, loss = 0.64637644\n",
      "Iteration 21, loss = 0.46102109\n",
      "Iteration 22, loss = 0.40501950\n",
      "Iteration 23, loss = 0.39851353\n",
      "Iteration 24, loss = 0.35890127\n",
      "Iteration 25, loss = 0.34311364\n",
      "Iteration 26, loss = 0.32762961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 3.88334905\n",
      "Iteration 3, loss = 1.55055618\n",
      "Iteration 4, loss = 1.20847257\n",
      "Iteration 5, loss = 0.86140393\n",
      "Iteration 6, loss = 0.77620518\n",
      "Iteration 7, loss = 0.65289900\n",
      "Iteration 8, loss = 0.52595094\n",
      "Iteration 9, loss = 0.46332285\n",
      "Iteration 10, loss = 0.42144554\n",
      "Iteration 11, loss = 0.38699866\n",
      "Iteration 12, loss = 0.35474222\n",
      "Iteration 13, loss = 0.32444759\n",
      "Iteration 14, loss = 0.33700231\n",
      "Iteration 15, loss = 0.94670810\n",
      "Iteration 16, loss = 2.44463851\n",
      "Iteration 17, loss = 0.84304569\n",
      "Iteration 18, loss = 0.57260691\n",
      "Iteration 19, loss = 0.52266630\n",
      "Iteration 20, loss = 0.44793684\n",
      "Iteration 21, loss = 0.43493594\n",
      "Iteration 22, loss = 0.42107903\n",
      "Iteration 23, loss = 0.39962187\n",
      "Iteration 24, loss = 0.36801417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 3.95536278\n",
      "Iteration 3, loss = 1.59658496\n",
      "Iteration 4, loss = 1.14667995\n",
      "Iteration 5, loss = 0.89258341\n",
      "Iteration 6, loss = 0.82073742\n",
      "Iteration 7, loss = 0.69262824\n",
      "Iteration 8, loss = 0.58531147\n",
      "Iteration 9, loss = 0.53678470\n",
      "Iteration 10, loss = 0.47828444\n",
      "Iteration 11, loss = 0.43534095\n",
      "Iteration 12, loss = 0.39626689\n",
      "Iteration 13, loss = 0.37104392\n",
      "Iteration 14, loss = 0.49280152\n",
      "Iteration 15, loss = 1.63043071\n",
      "Iteration 16, loss = 1.57705873\n",
      "Iteration 17, loss = 0.57289714\n",
      "Iteration 18, loss = 0.65495342\n",
      "Iteration 19, loss = 0.57581588\n",
      "Iteration 20, loss = 0.48865364\n",
      "Iteration 21, loss = 0.46354407\n",
      "Iteration 22, loss = 0.44845427\n",
      "Iteration 23, loss = 0.42719853\n",
      "Iteration 24, loss = 0.40797328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 4.02333274\n",
      "Iteration 3, loss = 1.61379911\n",
      "Iteration 4, loss = 1.10258556\n",
      "Iteration 5, loss = 0.88934632\n",
      "Iteration 6, loss = 0.80640042\n",
      "Iteration 7, loss = 0.69353957\n",
      "Iteration 8, loss = 0.57847618\n",
      "Iteration 9, loss = 0.52126322\n",
      "Iteration 10, loss = 0.47024348\n",
      "Iteration 11, loss = 0.42932641\n",
      "Iteration 12, loss = 0.39290556\n",
      "Iteration 13, loss = 0.35832700\n",
      "Iteration 14, loss = 0.34107606\n",
      "Iteration 15, loss = 0.53362812\n",
      "Iteration 16, loss = 1.95606564\n",
      "Iteration 17, loss = 1.41272778\n",
      "Iteration 18, loss = 0.61147971\n",
      "Iteration 19, loss = 0.70125365\n",
      "Iteration 20, loss = 0.59965875\n",
      "Iteration 21, loss = 0.51181351\n",
      "Iteration 22, loss = 0.48223858\n",
      "Iteration 23, loss = 0.46586854\n",
      "Iteration 24, loss = 0.44204123\n",
      "Iteration 25, loss = 0.42010713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.53072926\n",
      "Iteration 3, loss = 1.24135840\n",
      "Iteration 4, loss = 1.06807408\n",
      "Iteration 5, loss = 1.03265039\n",
      "Iteration 6, loss = 1.04575491\n",
      "Iteration 7, loss = 1.00772329\n",
      "Iteration 8, loss = 0.92434942\n",
      "Iteration 9, loss = 0.83035834\n",
      "Iteration 10, loss = 0.75790882\n",
      "Iteration 11, loss = 0.72253314\n",
      "Iteration 12, loss = 0.70485697\n",
      "Iteration 13, loss = 0.68606728\n",
      "Iteration 14, loss = 0.66092068\n",
      "Iteration 15, loss = 0.63296253\n",
      "Iteration 16, loss = 0.60668663\n",
      "Iteration 17, loss = 0.58274988\n",
      "Iteration 18, loss = 0.55862055\n",
      "Iteration 19, loss = 0.53302607\n",
      "Iteration 20, loss = 0.50902140\n",
      "Iteration 21, loss = 0.48814000\n",
      "Iteration 22, loss = 0.47058633\n",
      "Iteration 23, loss = 0.45760176\n",
      "Iteration 24, loss = 0.44884260\n",
      "Iteration 25, loss = 0.44177655\n",
      "Iteration 26, loss = 0.43309863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.42097580\n",
      "Iteration 28, loss = 0.40649051\n",
      "Iteration 29, loss = 0.39219834\n",
      "Iteration 30, loss = 0.38026424\n",
      "Iteration 31, loss = 0.37091523\n",
      "Iteration 32, loss = 0.36276409\n",
      "Iteration 33, loss = 0.35452869\n",
      "Iteration 34, loss = 0.34626963\n",
      "Iteration 35, loss = 0.33912949\n",
      "Iteration 36, loss = 0.33319711\n",
      "Iteration 37, loss = 0.32681341\n",
      "Iteration 38, loss = 0.31916672\n",
      "Iteration 39, loss = 0.31234174\n",
      "Iteration 40, loss = 0.30463349\n",
      "Iteration 41, loss = 0.29720380\n",
      "Iteration 42, loss = 0.29082011\n",
      "Iteration 43, loss = 0.28467694\n",
      "Iteration 44, loss = 0.27869548\n",
      "Iteration 45, loss = 0.27296260\n",
      "Iteration 46, loss = 0.26719452\n",
      "Iteration 47, loss = 0.26138230\n",
      "Iteration 48, loss = 0.25583131\n",
      "Iteration 49, loss = 0.25004521\n",
      "Iteration 50, loss = 0.24445718\n",
      "Iteration 51, loss = 0.23884846\n",
      "Iteration 52, loss = 0.23322700\n",
      "Iteration 53, loss = 0.22756136\n",
      "Iteration 54, loss = 0.22185237\n",
      "Iteration 55, loss = 0.21616014\n",
      "Iteration 56, loss = 0.21052647\n",
      "Iteration 57, loss = 0.20493552\n",
      "Iteration 58, loss = 0.19944235\n",
      "Iteration 59, loss = 0.19402622\n",
      "Iteration 60, loss = 0.18859398\n",
      "Iteration 61, loss = 0.18323798\n",
      "Iteration 62, loss = 0.17818791\n",
      "Iteration 63, loss = 0.17323665\n",
      "Iteration 64, loss = 0.16846181\n",
      "Iteration 65, loss = 0.16372529\n",
      "Iteration 66, loss = 0.15927939\n",
      "Iteration 67, loss = 0.15491079\n",
      "Iteration 68, loss = 0.15075371\n",
      "Iteration 69, loss = 0.14674908\n",
      "Iteration 70, loss = 0.14292811\n",
      "Iteration 71, loss = 0.13928147\n",
      "Iteration 72, loss = 0.13582740\n",
      "Iteration 73, loss = 0.13252822\n",
      "Iteration 74, loss = 0.12945945\n",
      "Iteration 75, loss = 0.12649472\n",
      "Iteration 76, loss = 0.12380730\n",
      "Iteration 77, loss = 0.12117246\n",
      "Iteration 78, loss = 0.11875649\n",
      "Iteration 79, loss = 0.11646518\n",
      "Iteration 80, loss = 0.11427358\n",
      "Iteration 81, loss = 0.11229305\n",
      "Iteration 82, loss = 0.11037740\n",
      "Iteration 83, loss = 0.10858283\n",
      "Iteration 84, loss = 0.10694864\n",
      "Iteration 85, loss = 0.10536588\n",
      "Iteration 86, loss = 0.10388463\n",
      "Iteration 87, loss = 0.10253971\n",
      "Iteration 88, loss = 0.10124511\n",
      "Iteration 89, loss = 0.10002004\n",
      "Iteration 90, loss = 0.09890709\n",
      "Iteration 91, loss = 0.09784793\n",
      "Iteration 92, loss = 0.09683261\n",
      "Iteration 93, loss = 0.09590136\n",
      "Iteration 94, loss = 0.09503198\n",
      "Iteration 95, loss = 0.09419315\n",
      "Iteration 96, loss = 0.09340465\n",
      "Iteration 97, loss = 0.09267894\n",
      "Iteration 98, loss = 0.09199275\n",
      "Iteration 99, loss = 0.09133253\n",
      "Iteration 100, loss = 0.09070839\n",
      "Iteration 101, loss = 0.09012809\n",
      "Iteration 102, loss = 0.08958265\n",
      "Iteration 103, loss = 0.08906102\n",
      "Iteration 104, loss = 0.08856067\n",
      "Iteration 105, loss = 0.08808681\n",
      "Iteration 106, loss = 0.08764138\n",
      "Iteration 107, loss = 0.08722052\n",
      "Iteration 108, loss = 0.08681950\n",
      "Iteration 109, loss = 0.08643523\n",
      "Iteration 110, loss = 0.08606584\n",
      "Iteration 111, loss = 0.08571213\n",
      "Iteration 112, loss = 0.08537372\n",
      "Iteration 113, loss = 0.08505049\n",
      "Iteration 114, loss = 0.08474161\n",
      "Iteration 115, loss = 0.08444618\n",
      "Iteration 116, loss = 0.08416362\n",
      "Iteration 117, loss = 0.08389323\n",
      "Iteration 118, loss = 0.08363456\n",
      "Iteration 119, loss = 0.08338863\n",
      "Iteration 120, loss = 0.08315649\n",
      "Iteration 121, loss = 0.08294207\n",
      "Iteration 122, loss = 0.08274619\n",
      "Iteration 123, loss = 0.08256165\n",
      "Iteration 124, loss = 0.08236720\n",
      "Iteration 125, loss = 0.08213159\n",
      "Iteration 126, loss = 0.08187791\n",
      "Iteration 127, loss = 0.08165889\n",
      "Iteration 128, loss = 0.08149651\n",
      "Iteration 129, loss = 0.08135709\n",
      "Iteration 130, loss = 0.08119247\n",
      "Iteration 131, loss = 0.08099499\n",
      "Iteration 132, loss = 0.08079970\n",
      "Iteration 133, loss = 0.08064246\n",
      "Iteration 134, loss = 0.08051225\n",
      "Iteration 135, loss = 0.08037354\n",
      "Iteration 136, loss = 0.08021235\n",
      "Iteration 137, loss = 0.08004594\n",
      "Iteration 138, loss = 0.07990021\n",
      "Iteration 139, loss = 0.07977576\n",
      "Iteration 140, loss = 0.07965332\n",
      "Iteration 141, loss = 0.07951916\n",
      "Iteration 142, loss = 0.07937654\n",
      "Iteration 143, loss = 0.07924046\n",
      "Iteration 144, loss = 0.07911825\n",
      "Iteration 145, loss = 0.07900531\n",
      "Iteration 146, loss = 0.07889185\n",
      "Iteration 147, loss = 0.07877269\n",
      "Iteration 148, loss = 0.07865037\n",
      "Iteration 149, loss = 0.07853126\n",
      "Iteration 150, loss = 0.07841950\n",
      "Iteration 151, loss = 0.07831415\n",
      "Iteration 152, loss = 0.07821142\n",
      "Iteration 153, loss = 0.07810801\n",
      "Iteration 154, loss = 0.07800277\n",
      "Iteration 155, loss = 0.07789703\n",
      "Iteration 156, loss = 0.07779281\n",
      "Iteration 157, loss = 0.07769167\n",
      "Iteration 158, loss = 0.07759401\n",
      "Iteration 159, loss = 0.07749929\n",
      "Iteration 160, loss = 0.07740661\n",
      "Iteration 161, loss = 0.07731526\n",
      "Iteration 162, loss = 0.07722476\n",
      "Iteration 163, loss = 0.07713509\n",
      "Iteration 164, loss = 0.07704622\n",
      "Iteration 165, loss = 0.07695827\n",
      "Iteration 166, loss = 0.07687157\n",
      "Iteration 167, loss = 0.07678603\n",
      "Iteration 168, loss = 0.07670190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.53593022\n",
      "Iteration 3, loss = 1.24185233\n",
      "Iteration 4, loss = 1.06571155\n",
      "Iteration 5, loss = 1.03232224\n",
      "Iteration 6, loss = 1.04888826\n",
      "Iteration 7, loss = 1.01284221\n",
      "Iteration 8, loss = 0.92981193\n",
      "Iteration 9, loss = 0.83610920\n",
      "Iteration 10, loss = 0.76363602\n",
      "Iteration 11, loss = 0.72712302\n",
      "Iteration 12, loss = 0.70908490\n",
      "Iteration 13, loss = 0.68953059\n",
      "Iteration 14, loss = 0.66365528\n",
      "Iteration 15, loss = 0.63574884\n",
      "Iteration 16, loss = 0.61011563\n",
      "Iteration 17, loss = 0.58646735\n",
      "Iteration 18, loss = 0.56198541\n",
      "Iteration 19, loss = 0.53623665\n",
      "Iteration 20, loss = 0.51276559\n",
      "Iteration 21, loss = 0.49125206\n",
      "Iteration 22, loss = 0.47318603\n",
      "Iteration 23, loss = 0.45996093\n",
      "Iteration 24, loss = 0.45101912\n",
      "Iteration 25, loss = 0.44342171\n",
      "Iteration 26, loss = 0.43401055\n",
      "Iteration 27, loss = 0.42117723\n",
      "Iteration 28, loss = 0.40623987\n",
      "Iteration 29, loss = 0.39181825\n",
      "Iteration 30, loss = 0.37984878\n",
      "Iteration 31, loss = 0.37033364\n",
      "Iteration 32, loss = 0.36197327\n",
      "Iteration 33, loss = 0.35356962\n",
      "Iteration 34, loss = 0.34551168\n",
      "Iteration 35, loss = 0.33860594\n",
      "Iteration 36, loss = 0.33244359\n",
      "Iteration 37, loss = 0.32543909\n",
      "Iteration 38, loss = 0.31738837\n",
      "Iteration 39, loss = 0.30930068\n",
      "Iteration 40, loss = 0.30153611\n",
      "Iteration 41, loss = 0.29448418\n",
      "Iteration 42, loss = 0.28780068\n",
      "Iteration 43, loss = 0.28125638\n",
      "Iteration 44, loss = 0.27482794\n",
      "Iteration 45, loss = 0.26861943\n",
      "Iteration 46, loss = 0.26251583\n",
      "Iteration 47, loss = 0.25637177\n",
      "Iteration 48, loss = 0.25013778\n",
      "Iteration 49, loss = 0.24397125\n",
      "Iteration 50, loss = 0.23806035\n",
      "Iteration 51, loss = 0.23187856\n",
      "Iteration 52, loss = 0.22610430\n",
      "Iteration 53, loss = 0.22021504\n",
      "Iteration 54, loss = 0.21402521\n",
      "Iteration 55, loss = 0.20820510\n",
      "Iteration 56, loss = 0.20238322\n",
      "Iteration 57, loss = 0.19658982\n",
      "Iteration 58, loss = 0.19107256\n",
      "Iteration 59, loss = 0.18551572\n",
      "Iteration 60, loss = 0.18003389\n",
      "Iteration 61, loss = 0.17477745\n",
      "Iteration 62, loss = 0.16953099\n",
      "Iteration 63, loss = 0.16453101\n",
      "Iteration 64, loss = 0.15964408\n",
      "Iteration 65, loss = 0.15489374\n",
      "Iteration 66, loss = 0.15029683\n",
      "Iteration 67, loss = 0.14581337\n",
      "Iteration 68, loss = 0.14155706\n",
      "Iteration 69, loss = 0.13746813\n",
      "Iteration 70, loss = 0.13358499\n",
      "Iteration 71, loss = 0.12985687\n",
      "Iteration 72, loss = 0.12633706\n",
      "Iteration 73, loss = 0.12300377\n",
      "Iteration 74, loss = 0.11986809\n",
      "Iteration 75, loss = 0.11690781\n",
      "Iteration 76, loss = 0.11412311\n",
      "Iteration 77, loss = 0.11151066\n",
      "Iteration 78, loss = 0.10905153\n",
      "Iteration 79, loss = 0.10676363\n",
      "Iteration 80, loss = 0.10461552\n",
      "Iteration 81, loss = 0.10261375\n",
      "Iteration 82, loss = 0.10075300\n",
      "Iteration 83, loss = 0.09901077\n",
      "Iteration 84, loss = 0.09739191\n",
      "Iteration 85, loss = 0.09589102\n",
      "Iteration 86, loss = 0.09449063\n",
      "Iteration 87, loss = 0.09319073\n",
      "Iteration 88, loss = 0.09198851\n",
      "Iteration 89, loss = 0.09086981\n",
      "Iteration 90, loss = 0.08982692\n",
      "Iteration 91, loss = 0.08886035\n",
      "Iteration 92, loss = 0.08796480\n",
      "Iteration 93, loss = 0.08713036\n",
      "Iteration 94, loss = 0.08635313\n",
      "Iteration 95, loss = 0.08563052\n",
      "Iteration 96, loss = 0.08496021\n",
      "Iteration 97, loss = 0.08433872\n",
      "Iteration 98, loss = 0.08375999\n",
      "Iteration 99, loss = 0.08322049\n",
      "Iteration 100, loss = 0.08271687\n",
      "Iteration 101, loss = 0.08224659\n",
      "Iteration 102, loss = 0.08180730\n",
      "Iteration 103, loss = 0.08139688\n",
      "Iteration 104, loss = 0.08101251\n",
      "Iteration 105, loss = 0.08065226\n",
      "Iteration 106, loss = 0.08031459\n",
      "Iteration 107, loss = 0.07999760\n",
      "Iteration 108, loss = 0.07970038\n",
      "Iteration 109, loss = 0.07942257\n",
      "Iteration 110, loss = 0.07916609\n",
      "Iteration 111, loss = 0.07893358\n",
      "Iteration 112, loss = 0.07872788\n",
      "Iteration 113, loss = 0.07853620\n",
      "Iteration 114, loss = 0.07831688\n",
      "Iteration 115, loss = 0.07806323\n",
      "Iteration 116, loss = 0.07783027\n",
      "Iteration 117, loss = 0.07766437\n",
      "Iteration 118, loss = 0.07752601\n",
      "Iteration 119, loss = 0.07735403\n",
      "Iteration 120, loss = 0.07715701\n",
      "Iteration 121, loss = 0.07698955\n",
      "Iteration 122, loss = 0.07686303\n",
      "Iteration 123, loss = 0.07673315\n",
      "Iteration 124, loss = 0.07657663\n",
      "Iteration 125, loss = 0.07642361\n",
      "Iteration 126, loss = 0.07629982\n",
      "Iteration 127, loss = 0.07618802\n",
      "Iteration 128, loss = 0.07606260\n",
      "Iteration 129, loss = 0.07592791\n",
      "Iteration 130, loss = 0.07580715\n",
      "Iteration 131, loss = 0.07570247\n",
      "Iteration 132, loss = 0.07559793\n",
      "Iteration 133, loss = 0.07548447\n",
      "Iteration 134, loss = 0.07537098\n",
      "Iteration 135, loss = 0.07526835\n",
      "Iteration 136, loss = 0.07517344\n",
      "Iteration 137, loss = 0.07507659\n",
      "Iteration 138, loss = 0.07497563\n",
      "Iteration 139, loss = 0.07487529\n",
      "Iteration 140, loss = 0.07478135\n",
      "Iteration 141, loss = 0.07469341\n",
      "Iteration 142, loss = 0.07460595\n",
      "Iteration 143, loss = 0.07451656\n",
      "Iteration 144, loss = 0.07442632\n",
      "Iteration 145, loss = 0.07433854\n",
      "Iteration 146, loss = 0.07425435\n",
      "Iteration 147, loss = 0.07417288\n",
      "Iteration 148, loss = 0.07409221\n",
      "Iteration 149, loss = 0.07401166\n",
      "Iteration 150, loss = 0.07393106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.52143197\n",
      "Iteration 3, loss = 1.23316583\n",
      "Iteration 4, loss = 1.06138341\n",
      "Iteration 5, loss = 1.02630559\n",
      "Iteration 6, loss = 1.03711330\n",
      "Iteration 7, loss = 0.99680069\n",
      "Iteration 8, loss = 0.91191110\n",
      "Iteration 9, loss = 0.81925558\n",
      "Iteration 10, loss = 0.74820288\n",
      "Iteration 11, loss = 0.71100643\n",
      "Iteration 12, loss = 0.69322792\n",
      "Iteration 13, loss = 0.67359237\n",
      "Iteration 14, loss = 0.64773417\n",
      "Iteration 15, loss = 0.61934004\n",
      "Iteration 16, loss = 0.59252018\n",
      "Iteration 17, loss = 0.56749074\n",
      "Iteration 18, loss = 0.54264357\n",
      "Iteration 19, loss = 0.51906265\n",
      "Iteration 20, loss = 0.49672686\n",
      "Iteration 21, loss = 0.47540530\n",
      "Iteration 22, loss = 0.45694991\n",
      "Iteration 23, loss = 0.44306282\n",
      "Iteration 24, loss = 0.43343214\n",
      "Iteration 25, loss = 0.42529680\n",
      "Iteration 26, loss = 0.41554897\n",
      "Iteration 27, loss = 0.40266446\n",
      "Iteration 28, loss = 0.38775647\n",
      "Iteration 29, loss = 0.37332241\n",
      "Iteration 30, loss = 0.36127291\n",
      "Iteration 31, loss = 0.35167533\n",
      "Iteration 32, loss = 0.34312154\n",
      "Iteration 33, loss = 0.33451584\n",
      "Iteration 34, loss = 0.32625617\n",
      "Iteration 35, loss = 0.31902676\n",
      "Iteration 36, loss = 0.31230941\n",
      "Iteration 37, loss = 0.30475358\n",
      "Iteration 38, loss = 0.29614110\n",
      "Iteration 39, loss = 0.28839504\n",
      "Iteration 40, loss = 0.28044754\n",
      "Iteration 41, loss = 0.27314661\n",
      "Iteration 42, loss = 0.26636964\n",
      "Iteration 43, loss = 0.25981527\n",
      "Iteration 44, loss = 0.25340746\n",
      "Iteration 45, loss = 0.24717951\n",
      "Iteration 46, loss = 0.24095703\n",
      "Iteration 47, loss = 0.23471212\n",
      "Iteration 48, loss = 0.22857671\n",
      "Iteration 49, loss = 0.22237470\n",
      "Iteration 50, loss = 0.21625662\n",
      "Iteration 51, loss = 0.21014036\n",
      "Iteration 52, loss = 0.20403361\n",
      "Iteration 53, loss = 0.19790485\n",
      "Iteration 54, loss = 0.19177012\n",
      "Iteration 55, loss = 0.18569287\n",
      "Iteration 56, loss = 0.17977817\n",
      "Iteration 57, loss = 0.17392964\n",
      "Iteration 58, loss = 0.16818748\n",
      "Iteration 59, loss = 0.16263396\n",
      "Iteration 60, loss = 0.15711779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 0.15164146\n",
      "Iteration 62, loss = 0.14644605\n",
      "Iteration 63, loss = 0.14144498\n",
      "Iteration 64, loss = 0.13659237\n",
      "Iteration 65, loss = 0.13188689\n",
      "Iteration 66, loss = 0.12733609\n",
      "Iteration 67, loss = 0.12293246\n",
      "Iteration 68, loss = 0.11871193\n",
      "Iteration 69, loss = 0.11467461\n",
      "Iteration 70, loss = 0.11084750\n",
      "Iteration 71, loss = 0.10723629\n",
      "Iteration 72, loss = 0.10383940\n",
      "Iteration 73, loss = 0.10061981\n",
      "Iteration 74, loss = 0.09759294\n",
      "Iteration 75, loss = 0.09477748\n",
      "Iteration 76, loss = 0.09216202\n",
      "Iteration 77, loss = 0.08971554\n",
      "Iteration 78, loss = 0.08741908\n",
      "Iteration 79, loss = 0.08526597\n",
      "Iteration 80, loss = 0.08325806\n",
      "Iteration 81, loss = 0.08140102\n",
      "Iteration 82, loss = 0.07968322\n",
      "Iteration 83, loss = 0.07809212\n",
      "Iteration 84, loss = 0.07661496\n",
      "Iteration 85, loss = 0.07524729\n",
      "Iteration 86, loss = 0.07398078\n",
      "Iteration 87, loss = 0.07280887\n",
      "Iteration 88, loss = 0.07172479\n",
      "Iteration 89, loss = 0.07072121\n",
      "Iteration 90, loss = 0.06979410\n",
      "Iteration 91, loss = 0.06893524\n",
      "Iteration 92, loss = 0.06813835\n",
      "Iteration 93, loss = 0.06739974\n",
      "Iteration 94, loss = 0.06671443\n",
      "Iteration 95, loss = 0.06607746\n",
      "Iteration 96, loss = 0.06548582\n",
      "Iteration 97, loss = 0.06493508\n",
      "Iteration 98, loss = 0.06442172\n",
      "Iteration 99, loss = 0.06394380\n",
      "Iteration 100, loss = 0.06349804\n",
      "Iteration 101, loss = 0.06308087\n",
      "Iteration 102, loss = 0.06269079\n",
      "Iteration 103, loss = 0.06232567\n",
      "Iteration 104, loss = 0.06198298\n",
      "Iteration 105, loss = 0.06166102\n",
      "Iteration 106, loss = 0.06135865\n",
      "Iteration 107, loss = 0.06107396\n",
      "Iteration 108, loss = 0.06080548\n",
      "Iteration 109, loss = 0.06055211\n",
      "Iteration 110, loss = 0.06031278\n",
      "Iteration 111, loss = 0.06008616\n",
      "Iteration 112, loss = 0.05987124\n",
      "Iteration 113, loss = 0.05966726\n",
      "Iteration 114, loss = 0.05947338\n",
      "Iteration 115, loss = 0.05928873\n",
      "Iteration 116, loss = 0.05911272\n",
      "Iteration 117, loss = 0.05894473\n",
      "Iteration 118, loss = 0.05878416\n",
      "Iteration 119, loss = 0.05863051\n",
      "Iteration 120, loss = 0.05848334\n",
      "Iteration 121, loss = 0.05834216\n",
      "Iteration 122, loss = 0.05820656\n",
      "Iteration 123, loss = 0.05807613\n",
      "Iteration 124, loss = 0.05795054\n",
      "Iteration 125, loss = 0.05782946\n",
      "Iteration 126, loss = 0.05771258\n",
      "Iteration 127, loss = 0.05759960\n",
      "Iteration 128, loss = 0.05749028\n",
      "Iteration 129, loss = 0.05738439\n",
      "Iteration 130, loss = 0.05728169\n",
      "Iteration 131, loss = 0.05718199\n",
      "Iteration 132, loss = 0.05708509\n",
      "Iteration 133, loss = 0.05699081\n",
      "Iteration 134, loss = 0.05689898\n",
      "Iteration 135, loss = 0.05680947\n",
      "Iteration 136, loss = 0.05672209\n",
      "Iteration 137, loss = 0.05663673\n",
      "Iteration 138, loss = 0.05655326\n",
      "Iteration 139, loss = 0.05647158\n",
      "Iteration 140, loss = 0.05639159\n",
      "Iteration 141, loss = 0.05631318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.53939357\n",
      "Iteration 3, loss = 1.24584935\n",
      "Iteration 4, loss = 1.07053841\n",
      "Iteration 5, loss = 1.03578649\n",
      "Iteration 6, loss = 1.04999023\n",
      "Iteration 7, loss = 1.01255598\n",
      "Iteration 8, loss = 0.92877748\n",
      "Iteration 9, loss = 0.83484490\n",
      "Iteration 10, loss = 0.76128978\n",
      "Iteration 11, loss = 0.71973032\n",
      "Iteration 12, loss = 0.70163299\n",
      "Iteration 13, loss = 0.68322067\n",
      "Iteration 14, loss = 0.65858110\n",
      "Iteration 15, loss = 0.63110817\n",
      "Iteration 16, loss = 0.60472320\n",
      "Iteration 17, loss = 0.57924346\n",
      "Iteration 18, loss = 0.55321576\n",
      "Iteration 19, loss = 0.52749294\n",
      "Iteration 20, loss = 0.50417866\n",
      "Iteration 21, loss = 0.48250338\n",
      "Iteration 22, loss = 0.46395711\n",
      "Iteration 23, loss = 0.44987838\n",
      "Iteration 24, loss = 0.44002261\n",
      "Iteration 25, loss = 0.43171033\n",
      "Iteration 26, loss = 0.42193557\n",
      "Iteration 27, loss = 0.40895628\n",
      "Iteration 28, loss = 0.39368580\n",
      "Iteration 29, loss = 0.37856219\n",
      "Iteration 30, loss = 0.36569372\n",
      "Iteration 31, loss = 0.35553402\n",
      "Iteration 32, loss = 0.34672715\n",
      "Iteration 33, loss = 0.33810719\n",
      "Iteration 34, loss = 0.32981165\n",
      "Iteration 35, loss = 0.32225981\n",
      "Iteration 36, loss = 0.31530762\n",
      "Iteration 37, loss = 0.30774424\n",
      "Iteration 38, loss = 0.29994901\n",
      "Iteration 39, loss = 0.29088333\n",
      "Iteration 40, loss = 0.28283228\n",
      "Iteration 41, loss = 0.27549589\n",
      "Iteration 42, loss = 0.26855393\n",
      "Iteration 43, loss = 0.26181973\n",
      "Iteration 44, loss = 0.25514807\n",
      "Iteration 45, loss = 0.24862362\n",
      "Iteration 46, loss = 0.24228789\n",
      "Iteration 47, loss = 0.23600693\n",
      "Iteration 48, loss = 0.22969668\n",
      "Iteration 49, loss = 0.22333882\n",
      "Iteration 50, loss = 0.21700868\n",
      "Iteration 51, loss = 0.21071286\n",
      "Iteration 52, loss = 0.20451286\n",
      "Iteration 53, loss = 0.19841253\n",
      "Iteration 54, loss = 0.19230868\n",
      "Iteration 55, loss = 0.18626086\n",
      "Iteration 56, loss = 0.18032794\n",
      "Iteration 57, loss = 0.17434735\n",
      "Iteration 58, loss = 0.16859166\n",
      "Iteration 59, loss = 0.16300167\n",
      "Iteration 60, loss = 0.15752405\n",
      "Iteration 61, loss = 0.15214525\n",
      "Iteration 62, loss = 0.14688236\n",
      "Iteration 63, loss = 0.14173764\n",
      "Iteration 64, loss = 0.13674667\n",
      "Iteration 65, loss = 0.13191462\n",
      "Iteration 66, loss = 0.12723128\n",
      "Iteration 67, loss = 0.12272532\n",
      "Iteration 68, loss = 0.11841179\n",
      "Iteration 69, loss = 0.11426581\n",
      "Iteration 70, loss = 0.11031194\n",
      "Iteration 71, loss = 0.10657123\n",
      "Iteration 72, loss = 0.10301456\n",
      "Iteration 73, loss = 0.09964299\n",
      "Iteration 74, loss = 0.09644853\n",
      "Iteration 75, loss = 0.09343464\n",
      "Iteration 76, loss = 0.09060009\n",
      "Iteration 77, loss = 0.08793207\n",
      "Iteration 78, loss = 0.08541662\n",
      "Iteration 79, loss = 0.08304698\n",
      "Iteration 80, loss = 0.08081965\n",
      "Iteration 81, loss = 0.07872713\n",
      "Iteration 82, loss = 0.07676398\n",
      "Iteration 83, loss = 0.07492218\n",
      "Iteration 84, loss = 0.07319331\n",
      "Iteration 85, loss = 0.07157180\n",
      "Iteration 86, loss = 0.07005366\n",
      "Iteration 87, loss = 0.06862976\n",
      "Iteration 88, loss = 0.06729608\n",
      "Iteration 89, loss = 0.06604470\n",
      "Iteration 90, loss = 0.06486949\n",
      "Iteration 91, loss = 0.06376823\n",
      "Iteration 92, loss = 0.06273453\n",
      "Iteration 93, loss = 0.06176242\n",
      "Iteration 94, loss = 0.06084919\n",
      "Iteration 95, loss = 0.05999111\n",
      "Iteration 96, loss = 0.05918390\n",
      "Iteration 97, loss = 0.05842327\n",
      "Iteration 98, loss = 0.05770778\n",
      "Iteration 99, loss = 0.05703289\n",
      "Iteration 100, loss = 0.05639491\n",
      "Iteration 101, loss = 0.05579177\n",
      "Iteration 102, loss = 0.05522172\n",
      "Iteration 103, loss = 0.05468261\n",
      "Iteration 104, loss = 0.05417172\n",
      "Iteration 105, loss = 0.05368716\n",
      "Iteration 106, loss = 0.05322735\n",
      "Iteration 107, loss = 0.05279074\n",
      "Iteration 108, loss = 0.05237584\n",
      "Iteration 109, loss = 0.05198101\n",
      "Iteration 110, loss = 0.05160476\n",
      "Iteration 111, loss = 0.05124574\n",
      "Iteration 112, loss = 0.05090287\n",
      "Iteration 113, loss = 0.05057518\n",
      "Iteration 114, loss = 0.05026177\n",
      "Iteration 115, loss = 0.04996177\n",
      "Iteration 116, loss = 0.04967435\n",
      "Iteration 117, loss = 0.04939880\n",
      "Iteration 118, loss = 0.04913434\n",
      "Iteration 119, loss = 0.04888059\n",
      "Iteration 120, loss = 0.04863678\n",
      "Iteration 121, loss = 0.04840253\n",
      "Iteration 122, loss = 0.04817784\n",
      "Iteration 123, loss = 0.04796346\n",
      "Iteration 124, loss = 0.04776228\n",
      "Iteration 125, loss = 0.04758081\n",
      "Iteration 126, loss = 0.04742864\n",
      "Iteration 127, loss = 0.04729763\n",
      "Iteration 128, loss = 0.04711465\n",
      "Iteration 129, loss = 0.04686849\n",
      "Iteration 130, loss = 0.04666541\n",
      "Iteration 131, loss = 0.04655970\n",
      "Iteration 132, loss = 0.04643738\n",
      "Iteration 133, loss = 0.04623489\n",
      "Iteration 134, loss = 0.04607041\n",
      "Iteration 135, loss = 0.04597417\n",
      "Iteration 136, loss = 0.04584009\n",
      "Iteration 137, loss = 0.04566938\n",
      "Iteration 138, loss = 0.04554668\n",
      "Iteration 139, loss = 0.04544898\n",
      "Iteration 140, loss = 0.04531250\n",
      "Iteration 141, loss = 0.04517264\n",
      "Iteration 142, loss = 0.04507330\n",
      "Iteration 143, loss = 0.04497063\n",
      "Iteration 144, loss = 0.04484070\n",
      "Iteration 145, loss = 0.04472728\n",
      "Iteration 146, loss = 0.04463796\n",
      "Iteration 147, loss = 0.04453427\n",
      "Iteration 148, loss = 0.04441759\n",
      "Iteration 149, loss = 0.04431930\n",
      "Iteration 150, loss = 0.04423290\n",
      "Iteration 151, loss = 0.04413415\n",
      "Iteration 152, loss = 0.04403011\n",
      "Iteration 153, loss = 0.04394072\n",
      "Iteration 154, loss = 0.04385814\n",
      "Iteration 155, loss = 0.04376597\n",
      "Iteration 156, loss = 0.04367160\n",
      "Iteration 157, loss = 0.04358718\n",
      "Iteration 158, loss = 0.04350807\n",
      "Iteration 159, loss = 0.04342407\n",
      "Iteration 160, loss = 0.04333760\n",
      "Iteration 161, loss = 0.04325713\n",
      "Iteration 162, loss = 0.04318179\n",
      "Iteration 163, loss = 0.04310491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.54802883\n",
      "Iteration 3, loss = 1.24985438\n",
      "Iteration 4, loss = 1.06867523\n",
      "Iteration 5, loss = 1.03134124\n",
      "Iteration 6, loss = 1.04843548\n",
      "Iteration 7, loss = 1.01330630\n",
      "Iteration 8, loss = 0.92975774\n",
      "Iteration 9, loss = 0.83412512\n",
      "Iteration 10, loss = 0.75916778\n",
      "Iteration 11, loss = 0.71939948\n",
      "Iteration 12, loss = 0.70163541\n",
      "Iteration 13, loss = 0.68325086\n",
      "Iteration 14, loss = 0.65797465\n",
      "Iteration 15, loss = 0.62958630\n",
      "Iteration 16, loss = 0.60325424\n",
      "Iteration 17, loss = 0.57951096\n",
      "Iteration 18, loss = 0.55580589\n",
      "Iteration 19, loss = 0.53139525\n",
      "Iteration 20, loss = 0.50872980\n",
      "Iteration 21, loss = 0.48750263\n",
      "Iteration 22, loss = 0.46923634\n",
      "Iteration 23, loss = 0.45560874\n",
      "Iteration 24, loss = 0.44656260\n",
      "Iteration 25, loss = 0.43967349\n",
      "Iteration 26, loss = 0.43156418\n",
      "Iteration 27, loss = 0.42017702\n",
      "Iteration 28, loss = 0.40631075\n",
      "Iteration 29, loss = 0.39232512\n",
      "Iteration 30, loss = 0.38038372\n",
      "Iteration 31, loss = 0.37098583\n",
      "Iteration 32, loss = 0.36285311\n",
      "Iteration 33, loss = 0.35463784\n",
      "Iteration 34, loss = 0.34649886\n",
      "Iteration 35, loss = 0.33948397\n",
      "Iteration 36, loss = 0.33353170\n",
      "Iteration 37, loss = 0.32724702\n",
      "Iteration 38, loss = 0.31980087\n",
      "Iteration 39, loss = 0.31296197\n",
      "Iteration 40, loss = 0.30497777\n",
      "Iteration 41, loss = 0.29796236\n",
      "Iteration 42, loss = 0.29163421\n",
      "Iteration 43, loss = 0.28551319\n",
      "Iteration 44, loss = 0.27953269\n",
      "Iteration 45, loss = 0.27372498\n",
      "Iteration 46, loss = 0.26790787\n",
      "Iteration 47, loss = 0.26211554\n",
      "Iteration 48, loss = 0.25636863\n",
      "Iteration 49, loss = 0.25068271\n",
      "Iteration 50, loss = 0.24496538\n",
      "Iteration 51, loss = 0.23919312\n",
      "Iteration 52, loss = 0.23336649\n",
      "Iteration 53, loss = 0.22750570\n",
      "Iteration 54, loss = 0.22162189\n",
      "Iteration 55, loss = 0.21576849\n",
      "Iteration 56, loss = 0.21000541\n",
      "Iteration 57, loss = 0.20439188\n",
      "Iteration 58, loss = 0.19889414\n",
      "Iteration 59, loss = 0.19326584\n",
      "Iteration 60, loss = 0.18768679\n",
      "Iteration 61, loss = 0.18228005\n",
      "Iteration 62, loss = 0.17705299\n",
      "Iteration 63, loss = 0.17198673\n",
      "Iteration 64, loss = 0.16707448\n",
      "Iteration 65, loss = 0.16229677\n",
      "Iteration 66, loss = 0.15764511\n",
      "Iteration 67, loss = 0.15313807\n",
      "Iteration 68, loss = 0.14876809\n",
      "Iteration 69, loss = 0.14459139\n",
      "Iteration 70, loss = 0.14058923\n",
      "Iteration 71, loss = 0.13681082\n",
      "Iteration 72, loss = 0.13322554\n",
      "Iteration 73, loss = 0.12982313\n",
      "Iteration 74, loss = 0.12663834\n",
      "Iteration 75, loss = 0.12362916\n",
      "Iteration 76, loss = 0.12080256\n",
      "Iteration 77, loss = 0.11814106\n",
      "Iteration 78, loss = 0.11564433\n",
      "Iteration 79, loss = 0.11331367\n",
      "Iteration 80, loss = 0.11112960\n",
      "Iteration 81, loss = 0.10908880\n",
      "Iteration 82, loss = 0.10718490\n",
      "Iteration 83, loss = 0.10540648\n",
      "Iteration 84, loss = 0.10374581\n",
      "Iteration 85, loss = 0.10219592\n",
      "Iteration 86, loss = 0.10075219\n",
      "Iteration 87, loss = 0.09940896\n",
      "Iteration 88, loss = 0.09815813\n",
      "Iteration 89, loss = 0.09699341\n",
      "Iteration 90, loss = 0.09590818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91, loss = 0.09489592\n",
      "Iteration 92, loss = 0.09395271\n",
      "Iteration 93, loss = 0.09307425\n",
      "Iteration 94, loss = 0.09225414\n",
      "Iteration 95, loss = 0.09148813\n",
      "Iteration 96, loss = 0.09077236\n",
      "Iteration 97, loss = 0.09010447\n",
      "Iteration 98, loss = 0.08948401\n",
      "Iteration 99, loss = 0.08891902\n",
      "Iteration 100, loss = 0.08842812\n",
      "Iteration 101, loss = 0.08803091\n",
      "Iteration 102, loss = 0.08764188\n",
      "Iteration 103, loss = 0.08705531\n",
      "Iteration 104, loss = 0.08652613\n",
      "Iteration 105, loss = 0.08625530\n",
      "Iteration 106, loss = 0.08592493\n",
      "Iteration 107, loss = 0.08545456\n",
      "Iteration 108, loss = 0.08514614\n",
      "Iteration 109, loss = 0.08491256\n",
      "Iteration 110, loss = 0.08454193\n",
      "Iteration 111, loss = 0.08423241\n",
      "Iteration 112, loss = 0.08403235\n",
      "Iteration 113, loss = 0.08373906\n",
      "Iteration 114, loss = 0.08344901\n",
      "Iteration 115, loss = 0.08326329\n",
      "Iteration 116, loss = 0.08302563\n",
      "Iteration 117, loss = 0.08276122\n",
      "Iteration 118, loss = 0.08258124\n",
      "Iteration 119, loss = 0.08238435\n",
      "Iteration 120, loss = 0.08214828\n",
      "Iteration 121, loss = 0.08197090\n",
      "Iteration 122, loss = 0.08180283\n",
      "Iteration 123, loss = 0.08159562\n",
      "Iteration 124, loss = 0.08141978\n",
      "Iteration 125, loss = 0.08126990\n",
      "Iteration 126, loss = 0.08109151\n",
      "Iteration 127, loss = 0.08091937\n",
      "Iteration 128, loss = 0.08077762\n",
      "Iteration 129, loss = 0.08062547\n",
      "Iteration 130, loss = 0.08046281\n",
      "Iteration 131, loss = 0.08032187\n",
      "Iteration 132, loss = 0.08018867\n",
      "Iteration 133, loss = 0.08004232\n",
      "Iteration 134, loss = 0.07990135\n",
      "Iteration 135, loss = 0.07977584\n",
      "Iteration 136, loss = 0.07964775\n",
      "Iteration 137, loss = 0.07951365\n",
      "Iteration 138, loss = 0.07938826\n",
      "Iteration 139, loss = 0.07927120\n",
      "Iteration 140, loss = 0.07915060\n",
      "Iteration 141, loss = 0.07902816\n",
      "Iteration 142, loss = 0.07891292\n",
      "Iteration 143, loss = 0.07880275\n",
      "Iteration 144, loss = 0.07869045\n",
      "Iteration 145, loss = 0.07857750\n",
      "Iteration 146, loss = 0.07846940\n",
      "Iteration 147, loss = 0.07836539\n",
      "Iteration 148, loss = 0.07826100\n",
      "Iteration 149, loss = 0.07815590\n",
      "Iteration 150, loss = 0.07805349\n",
      "Iteration 151, loss = 0.07795469\n",
      "Iteration 152, loss = 0.07785711\n",
      "Iteration 153, loss = 0.07775920\n",
      "Iteration 154, loss = 0.07766209\n",
      "Iteration 155, loss = 0.07756739\n",
      "Iteration 156, loss = 0.07747496\n",
      "Iteration 157, loss = 0.07738339\n",
      "Iteration 158, loss = 0.07729206\n",
      "Iteration 159, loss = 0.07720152\n",
      "Iteration 160, loss = 0.07711261\n",
      "Iteration 161, loss = 0.07702542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.54449948\n",
      "Iteration 3, loss = 1.23237610\n",
      "Iteration 4, loss = 1.09098040\n",
      "Iteration 5, loss = 1.10732441\n",
      "Iteration 6, loss = 1.06279520\n",
      "Iteration 7, loss = 0.96961789\n",
      "Iteration 8, loss = 0.89028003\n",
      "Iteration 9, loss = 0.82194205\n",
      "Iteration 10, loss = 0.76330471\n",
      "Iteration 11, loss = 0.71742607\n",
      "Iteration 12, loss = 0.68683111\n",
      "Iteration 13, loss = 0.65886270\n",
      "Iteration 14, loss = 0.62981888\n",
      "Iteration 15, loss = 0.60038999\n",
      "Iteration 16, loss = 0.57265714\n",
      "Iteration 17, loss = 0.54972856\n",
      "Iteration 18, loss = 0.53014365\n",
      "Iteration 19, loss = 0.51241853\n",
      "Iteration 20, loss = 0.49599805\n",
      "Iteration 21, loss = 0.48065016\n",
      "Iteration 22, loss = 0.46647363\n",
      "Iteration 23, loss = 0.45346021\n",
      "Iteration 24, loss = 0.44160491\n",
      "Iteration 25, loss = 0.43082153\n",
      "Iteration 26, loss = 0.42097086\n",
      "Iteration 27, loss = 0.41175679\n",
      "Iteration 28, loss = 0.40296185\n",
      "Iteration 29, loss = 0.39474035\n",
      "Iteration 30, loss = 0.38703280\n",
      "Iteration 31, loss = 0.37980684\n",
      "Iteration 32, loss = 0.37295980\n",
      "Iteration 33, loss = 0.36646925\n",
      "Iteration 34, loss = 0.36028634\n",
      "Iteration 35, loss = 0.35437524\n",
      "Iteration 36, loss = 0.34871121\n",
      "Iteration 37, loss = 0.34327186\n",
      "Iteration 38, loss = 0.33803266\n",
      "Iteration 39, loss = 0.33297781\n",
      "Iteration 40, loss = 0.32809311\n",
      "Iteration 41, loss = 0.32336723\n",
      "Iteration 42, loss = 0.31879115\n",
      "Iteration 43, loss = 0.31435465\n",
      "Iteration 44, loss = 0.31004765\n",
      "Iteration 45, loss = 0.30586394\n",
      "Iteration 46, loss = 0.30179779\n",
      "Iteration 47, loss = 0.29784104\n",
      "Iteration 48, loss = 0.29398849\n",
      "Iteration 49, loss = 0.29023575\n",
      "Iteration 50, loss = 0.28657935\n",
      "Iteration 51, loss = 0.28301721\n",
      "Iteration 52, loss = 0.27954134\n",
      "Iteration 53, loss = 0.27615186\n",
      "Iteration 54, loss = 0.27284501\n",
      "Iteration 55, loss = 0.26961871\n",
      "Iteration 56, loss = 0.26647185\n",
      "Iteration 57, loss = 0.26340080\n",
      "Iteration 58, loss = 0.26040357\n",
      "Iteration 59, loss = 0.25747761\n",
      "Iteration 60, loss = 0.25462197\n",
      "Iteration 61, loss = 0.25183332\n",
      "Iteration 62, loss = 0.24911128\n",
      "Iteration 63, loss = 0.24645367\n",
      "Iteration 64, loss = 0.24385781\n",
      "Iteration 65, loss = 0.24132242\n",
      "Iteration 66, loss = 0.23884603\n",
      "Iteration 67, loss = 0.23642736\n",
      "Iteration 68, loss = 0.23406619\n",
      "Iteration 69, loss = 0.23175858\n",
      "Iteration 70, loss = 0.22950524\n",
      "Iteration 71, loss = 0.22730318\n",
      "Iteration 72, loss = 0.22515150\n",
      "Iteration 73, loss = 0.22304951\n",
      "Iteration 74, loss = 0.22099521\n",
      "Iteration 75, loss = 0.21898762\n",
      "Iteration 76, loss = 0.21702630\n",
      "Iteration 77, loss = 0.21510946\n",
      "Iteration 78, loss = 0.21323600\n",
      "Iteration 79, loss = 0.21140389\n",
      "Iteration 80, loss = 0.20961349\n",
      "Iteration 81, loss = 0.20786204\n",
      "Iteration 82, loss = 0.20614933\n",
      "Iteration 83, loss = 0.20447491\n",
      "Iteration 84, loss = 0.20283704\n",
      "Iteration 85, loss = 0.20123518\n",
      "Iteration 86, loss = 0.19966780\n",
      "Iteration 87, loss = 0.19813529\n",
      "Iteration 88, loss = 0.19663544\n",
      "Iteration 89, loss = 0.19516857\n",
      "Iteration 90, loss = 0.19373312\n",
      "Iteration 91, loss = 0.19232770\n",
      "Iteration 92, loss = 0.19095167\n",
      "Iteration 93, loss = 0.18960495\n",
      "Iteration 94, loss = 0.18828652\n",
      "Iteration 95, loss = 0.18699575\n",
      "Iteration 96, loss = 0.18573212\n",
      "Iteration 97, loss = 0.18449458\n",
      "Iteration 98, loss = 0.18328229\n",
      "Iteration 99, loss = 0.18209490\n",
      "Iteration 100, loss = 0.18093182\n",
      "Iteration 101, loss = 0.17979322\n",
      "Iteration 102, loss = 0.17867760\n",
      "Iteration 103, loss = 0.17758470\n",
      "Iteration 104, loss = 0.17651347\n",
      "Iteration 105, loss = 0.17546325\n",
      "Iteration 106, loss = 0.17443351\n",
      "Iteration 107, loss = 0.17342389\n",
      "Iteration 108, loss = 0.17243396\n",
      "Iteration 109, loss = 0.17146315\n",
      "Iteration 110, loss = 0.17051131\n",
      "Iteration 111, loss = 0.16957751\n",
      "Iteration 112, loss = 0.16866130\n",
      "Iteration 113, loss = 0.16776233\n",
      "Iteration 114, loss = 0.16688036\n",
      "Iteration 115, loss = 0.16601493\n",
      "Iteration 116, loss = 0.16516545\n",
      "Iteration 117, loss = 0.16433152\n",
      "Iteration 118, loss = 0.16351296\n",
      "Iteration 119, loss = 0.16270896\n",
      "Iteration 120, loss = 0.16191950\n",
      "Iteration 121, loss = 0.16114418\n",
      "Iteration 122, loss = 0.16038256\n",
      "Iteration 123, loss = 0.15963429\n",
      "Iteration 124, loss = 0.15889903\n",
      "Iteration 125, loss = 0.15817651\n",
      "Iteration 126, loss = 0.15746639\n",
      "Iteration 127, loss = 0.15676842\n",
      "Iteration 128, loss = 0.15608230\n",
      "Iteration 129, loss = 0.15540774\n",
      "Iteration 130, loss = 0.15474463\n",
      "Iteration 131, loss = 0.15409263\n",
      "Iteration 132, loss = 0.15345142\n",
      "Iteration 133, loss = 0.15282080\n",
      "Iteration 134, loss = 0.15220054\n",
      "Iteration 135, loss = 0.15159036\n",
      "Iteration 136, loss = 0.15099006\n",
      "Iteration 137, loss = 0.15039941\n",
      "Iteration 138, loss = 0.14981818\n",
      "Iteration 139, loss = 0.14924615\n",
      "Iteration 140, loss = 0.14868312\n",
      "Iteration 141, loss = 0.14812888\n",
      "Iteration 142, loss = 0.14758326\n",
      "Iteration 143, loss = 0.14704606\n",
      "Iteration 144, loss = 0.14651713\n",
      "Iteration 145, loss = 0.14599627\n",
      "Iteration 146, loss = 0.14548330\n",
      "Iteration 147, loss = 0.14497806\n",
      "Iteration 148, loss = 0.14448043\n",
      "Iteration 149, loss = 0.14399022\n",
      "Iteration 150, loss = 0.14350726\n",
      "Iteration 151, loss = 0.14303140\n",
      "Iteration 152, loss = 0.14256249\n",
      "Iteration 153, loss = 0.14210043\n",
      "Iteration 154, loss = 0.14164506\n",
      "Iteration 155, loss = 0.14119622\n",
      "Iteration 156, loss = 0.14075379\n",
      "Iteration 157, loss = 0.14031770\n",
      "Iteration 158, loss = 0.13988778\n",
      "Iteration 159, loss = 0.13946390\n",
      "Iteration 160, loss = 0.13904598\n",
      "Iteration 161, loss = 0.13863382\n",
      "Iteration 162, loss = 0.13822738\n",
      "Iteration 163, loss = 0.13782652\n",
      "Iteration 164, loss = 0.13743115\n",
      "Iteration 165, loss = 0.13704113\n",
      "Iteration 166, loss = 0.13665637\n",
      "Iteration 167, loss = 0.13627680\n",
      "Iteration 168, loss = 0.13590226\n",
      "Iteration 169, loss = 0.13553271\n",
      "Iteration 170, loss = 0.13516802\n",
      "Iteration 171, loss = 0.13480812\n",
      "Iteration 172, loss = 0.13445290\n",
      "Iteration 173, loss = 0.13410231\n",
      "Iteration 174, loss = 0.13375622\n",
      "Iteration 175, loss = 0.13341461\n",
      "Iteration 176, loss = 0.13307738\n",
      "Iteration 177, loss = 0.13274442\n",
      "Iteration 178, loss = 0.13241566\n",
      "Iteration 179, loss = 0.13209111\n",
      "Iteration 180, loss = 0.13177050\n",
      "Iteration 181, loss = 0.13145392\n",
      "Iteration 182, loss = 0.13114124\n",
      "Iteration 183, loss = 0.13083240\n",
      "Iteration 184, loss = 0.13052738\n",
      "Iteration 185, loss = 0.13022610\n",
      "Iteration 186, loss = 0.12992848\n",
      "Iteration 187, loss = 0.12963444\n",
      "Iteration 188, loss = 0.12934392\n",
      "Iteration 189, loss = 0.12905685\n",
      "Iteration 190, loss = 0.12877317\n",
      "Iteration 191, loss = 0.12849284\n",
      "Iteration 192, loss = 0.12821581\n",
      "Iteration 193, loss = 0.12794200\n",
      "Iteration 194, loss = 0.12767137\n",
      "Iteration 195, loss = 0.12740386\n",
      "Iteration 196, loss = 0.12713941\n",
      "Iteration 197, loss = 0.12687799\n",
      "Iteration 198, loss = 0.12661952\n",
      "Iteration 199, loss = 0.12636399\n",
      "Iteration 200, loss = 0.12611132\n",
      "Iteration 201, loss = 0.12586147\n",
      "Iteration 202, loss = 0.12561441\n",
      "Iteration 203, loss = 0.12537008\n",
      "Iteration 204, loss = 0.12512844\n",
      "Iteration 205, loss = 0.12488945\n",
      "Iteration 206, loss = 0.12465306\n",
      "Iteration 207, loss = 0.12441924\n",
      "Iteration 208, loss = 0.12418795\n",
      "Iteration 209, loss = 0.12395913\n",
      "Iteration 210, loss = 0.12373278\n",
      "Iteration 211, loss = 0.12350885\n",
      "Iteration 212, loss = 0.12328730\n",
      "Iteration 213, loss = 0.12306808\n",
      "Iteration 214, loss = 0.12285118\n",
      "Iteration 215, loss = 0.12263653\n",
      "Iteration 216, loss = 0.12242411\n",
      "Iteration 217, loss = 0.12221392\n",
      "Iteration 218, loss = 0.12200585\n",
      "Iteration 219, loss = 0.12179993\n",
      "Iteration 220, loss = 0.12159610\n",
      "Iteration 221, loss = 0.12139434\n",
      "Iteration 222, loss = 0.12119464\n",
      "Iteration 223, loss = 0.12099693\n",
      "Iteration 224, loss = 0.12080119\n",
      "Iteration 225, loss = 0.12060740\n",
      "Iteration 226, loss = 0.12041553\n",
      "Iteration 227, loss = 0.12022555\n",
      "Iteration 228, loss = 0.12003743\n",
      "Iteration 229, loss = 0.11985109\n",
      "Iteration 230, loss = 0.11966658\n",
      "Iteration 231, loss = 0.11948386\n",
      "Iteration 232, loss = 0.11930292\n",
      "Iteration 233, loss = 0.11912369\n",
      "Iteration 234, loss = 0.11894616\n",
      "Iteration 235, loss = 0.11877031\n",
      "Iteration 236, loss = 0.11859612\n",
      "Iteration 237, loss = 0.11842357\n",
      "Iteration 238, loss = 0.11825263\n",
      "Iteration 239, loss = 0.11808327\n",
      "Iteration 240, loss = 0.11791547\n",
      "Iteration 241, loss = 0.11774921\n",
      "Iteration 242, loss = 0.11758449\n",
      "Iteration 243, loss = 0.11742130\n",
      "Iteration 244, loss = 0.11725960\n",
      "Iteration 245, loss = 0.11709938\n",
      "Iteration 246, loss = 0.11694060\n",
      "Iteration 247, loss = 0.11678326\n",
      "Iteration 248, loss = 0.11662733\n",
      "Iteration 249, loss = 0.11647280\n",
      "Iteration 250, loss = 0.11631964\n",
      "Iteration 251, loss = 0.11616785\n",
      "Iteration 252, loss = 0.11601740\n",
      "Iteration 253, loss = 0.11586827\n",
      "Iteration 254, loss = 0.11572046\n",
      "Iteration 255, loss = 0.11557390\n",
      "Iteration 256, loss = 0.11542866\n",
      "Iteration 257, loss = 0.11528467\n",
      "Iteration 258, loss = 0.11514191\n",
      "Iteration 259, loss = 0.11500040\n",
      "Iteration 260, loss = 0.11486010\n",
      "Iteration 261, loss = 0.11472094\n",
      "Iteration 262, loss = 0.11458293\n",
      "Iteration 263, loss = 0.11444605\n",
      "Iteration 264, loss = 0.11431029\n",
      "Iteration 265, loss = 0.11417566\n",
      "Iteration 266, loss = 0.11404218\n",
      "Iteration 267, loss = 0.11390979\n",
      "Iteration 268, loss = 0.11377846\n",
      "Iteration 269, loss = 0.11364821\n",
      "Iteration 270, loss = 0.11351902\n",
      "Iteration 271, loss = 0.11339087\n",
      "Iteration 272, loss = 0.11326375\n",
      "Iteration 273, loss = 0.11313766\n",
      "Iteration 274, loss = 0.11301257\n",
      "Iteration 275, loss = 0.11288849\n",
      "Iteration 276, loss = 0.11276540\n",
      "Iteration 277, loss = 0.11264327\n",
      "Iteration 278, loss = 0.11252210\n",
      "Iteration 279, loss = 0.11240188\n",
      "Iteration 280, loss = 0.11228257\n",
      "Iteration 281, loss = 0.11216420\n",
      "Iteration 282, loss = 0.11204675\n",
      "Iteration 283, loss = 0.11193022\n",
      "Iteration 284, loss = 0.11181457\n",
      "Iteration 285, loss = 0.11169982\n",
      "Iteration 286, loss = 0.11158593\n",
      "Iteration 287, loss = 0.11147292\n",
      "Iteration 288, loss = 0.11136077\n",
      "Iteration 289, loss = 0.11124946\n",
      "Iteration 290, loss = 0.11113893\n",
      "Iteration 291, loss = 0.11102918\n",
      "Iteration 292, loss = 0.11092025\n",
      "Iteration 293, loss = 0.11081213\n",
      "Iteration 294, loss = 0.11070480\n",
      "Iteration 295, loss = 0.11059826\n",
      "Iteration 296, loss = 0.11049249\n",
      "Iteration 297, loss = 0.11038750\n",
      "Iteration 298, loss = 0.11028327\n",
      "Iteration 299, loss = 0.11017981\n",
      "Iteration 300, loss = 0.11007709\n",
      "Iteration 301, loss = 0.10997512\n",
      "Iteration 302, loss = 0.10987389\n",
      "Iteration 303, loss = 0.10977339\n",
      "Iteration 304, loss = 0.10967361\n",
      "Iteration 305, loss = 0.10957455\n",
      "Iteration 306, loss = 0.10947620\n",
      "Iteration 307, loss = 0.10937863\n",
      "Iteration 308, loss = 0.10928174\n",
      "Iteration 309, loss = 0.10918549\n",
      "Iteration 310, loss = 0.10908971\n",
      "Iteration 311, loss = 0.10899503\n",
      "Iteration 312, loss = 0.10890102\n",
      "Iteration 313, loss = 0.10880766\n",
      "Iteration 314, loss = 0.10871497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.54689016\n",
      "Iteration 3, loss = 1.22799555\n",
      "Iteration 4, loss = 1.08946023\n",
      "Iteration 5, loss = 1.10903032\n",
      "Iteration 6, loss = 1.06231227\n",
      "Iteration 7, loss = 0.96958677\n",
      "Iteration 8, loss = 0.89313197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.82435853\n",
      "Iteration 10, loss = 0.76481728\n",
      "Iteration 11, loss = 0.71836439\n",
      "Iteration 12, loss = 0.68831001\n",
      "Iteration 13, loss = 0.66109213\n",
      "Iteration 14, loss = 0.63250285\n",
      "Iteration 15, loss = 0.60320478\n",
      "Iteration 16, loss = 0.57525406\n",
      "Iteration 17, loss = 0.55189417\n",
      "Iteration 18, loss = 0.53170241\n",
      "Iteration 19, loss = 0.51349891\n",
      "Iteration 20, loss = 0.49682465\n",
      "Iteration 21, loss = 0.48139152\n",
      "Iteration 22, loss = 0.46711996\n",
      "Iteration 23, loss = 0.45384379\n",
      "Iteration 24, loss = 0.44150929\n",
      "Iteration 25, loss = 0.43012629\n",
      "Iteration 26, loss = 0.41970458\n",
      "Iteration 27, loss = 0.41003811\n",
      "Iteration 28, loss = 0.40097262\n",
      "Iteration 29, loss = 0.39248954\n",
      "Iteration 30, loss = 0.38452532\n",
      "Iteration 31, loss = 0.37700428\n",
      "Iteration 32, loss = 0.36989180\n",
      "Iteration 33, loss = 0.36313268\n",
      "Iteration 34, loss = 0.35667289\n",
      "Iteration 35, loss = 0.35049627\n",
      "Iteration 36, loss = 0.34457387\n",
      "Iteration 37, loss = 0.33888525\n",
      "Iteration 38, loss = 0.33341324\n",
      "Iteration 39, loss = 0.32813641\n",
      "Iteration 40, loss = 0.32304657\n",
      "Iteration 41, loss = 0.31812376\n",
      "Iteration 42, loss = 0.31335527\n",
      "Iteration 43, loss = 0.30872996\n",
      "Iteration 44, loss = 0.30423888\n",
      "Iteration 45, loss = 0.29987391\n",
      "Iteration 46, loss = 0.29563021\n",
      "Iteration 47, loss = 0.29150329\n",
      "Iteration 48, loss = 0.28748656\n",
      "Iteration 49, loss = 0.28357318\n",
      "Iteration 50, loss = 0.27976075\n",
      "Iteration 51, loss = 0.27604526\n",
      "Iteration 52, loss = 0.27242499\n",
      "Iteration 53, loss = 0.26889694\n",
      "Iteration 54, loss = 0.26545612\n",
      "Iteration 55, loss = 0.26210054\n",
      "Iteration 56, loss = 0.25882799\n",
      "Iteration 57, loss = 0.25563630\n",
      "Iteration 58, loss = 0.25252219\n",
      "Iteration 59, loss = 0.24948381\n",
      "Iteration 60, loss = 0.24651910\n",
      "Iteration 61, loss = 0.24362572\n",
      "Iteration 62, loss = 0.24080366\n",
      "Iteration 63, loss = 0.23804836\n",
      "Iteration 64, loss = 0.23536039\n",
      "Iteration 65, loss = 0.23273619\n",
      "Iteration 66, loss = 0.23017513\n",
      "Iteration 67, loss = 0.22767609\n",
      "Iteration 68, loss = 0.22523627\n",
      "Iteration 69, loss = 0.22285616\n",
      "Iteration 70, loss = 0.22053249\n",
      "Iteration 71, loss = 0.21826429\n",
      "Iteration 72, loss = 0.21604946\n",
      "Iteration 73, loss = 0.21388779\n",
      "Iteration 74, loss = 0.21177626\n",
      "Iteration 75, loss = 0.20971506\n",
      "Iteration 76, loss = 0.20770142\n",
      "Iteration 77, loss = 0.20573510\n",
      "Iteration 78, loss = 0.20381464\n",
      "Iteration 79, loss = 0.20193871\n",
      "Iteration 80, loss = 0.20010609\n",
      "Iteration 81, loss = 0.19831639\n",
      "Iteration 82, loss = 0.19656740\n",
      "Iteration 83, loss = 0.19485888\n",
      "Iteration 84, loss = 0.19318953\n",
      "Iteration 85, loss = 0.19155781\n",
      "Iteration 86, loss = 0.18996296\n",
      "Iteration 87, loss = 0.18840409\n",
      "Iteration 88, loss = 0.18688018\n",
      "Iteration 89, loss = 0.18539023\n",
      "Iteration 90, loss = 0.18393347\n",
      "Iteration 91, loss = 0.18250913\n",
      "Iteration 92, loss = 0.18111624\n",
      "Iteration 93, loss = 0.17975401\n",
      "Iteration 94, loss = 0.17842129\n",
      "Iteration 95, loss = 0.17711795\n",
      "Iteration 96, loss = 0.17584487\n",
      "Iteration 97, loss = 0.17459979\n",
      "Iteration 98, loss = 0.17338143\n",
      "Iteration 99, loss = 0.17218902\n",
      "Iteration 100, loss = 0.17102237\n",
      "Iteration 101, loss = 0.16988012\n",
      "Iteration 102, loss = 0.16876201\n",
      "Iteration 103, loss = 0.16766724\n",
      "Iteration 104, loss = 0.16659518\n",
      "Iteration 105, loss = 0.16554507\n",
      "Iteration 106, loss = 0.16451639\n",
      "Iteration 107, loss = 0.16350869\n",
      "Iteration 108, loss = 0.16252135\n",
      "Iteration 109, loss = 0.16155413\n",
      "Iteration 110, loss = 0.16060608\n",
      "Iteration 111, loss = 0.15967687\n",
      "Iteration 112, loss = 0.15876588\n",
      "Iteration 113, loss = 0.15787270\n",
      "Iteration 114, loss = 0.15699683\n",
      "Iteration 115, loss = 0.15613779\n",
      "Iteration 116, loss = 0.15529518\n",
      "Iteration 117, loss = 0.15446850\n",
      "Iteration 118, loss = 0.15365741\n",
      "Iteration 119, loss = 0.15286146\n",
      "Iteration 120, loss = 0.15208025\n",
      "Iteration 121, loss = 0.15131353\n",
      "Iteration 122, loss = 0.15056083\n",
      "Iteration 123, loss = 0.14982189\n",
      "Iteration 124, loss = 0.14909635\n",
      "Iteration 125, loss = 0.14838391\n",
      "Iteration 126, loss = 0.14768417\n",
      "Iteration 127, loss = 0.14699681\n",
      "Iteration 128, loss = 0.14632177\n",
      "Iteration 129, loss = 0.14565848\n",
      "Iteration 130, loss = 0.14500669\n",
      "Iteration 131, loss = 0.14436612\n",
      "Iteration 132, loss = 0.14373652\n",
      "Iteration 133, loss = 0.14311768\n",
      "Iteration 134, loss = 0.14250936\n",
      "Iteration 135, loss = 0.14191141\n",
      "Iteration 136, loss = 0.14132342\n",
      "Iteration 137, loss = 0.14074519\n",
      "Iteration 138, loss = 0.14017650\n",
      "Iteration 139, loss = 0.13961714\n",
      "Iteration 140, loss = 0.13906692\n",
      "Iteration 141, loss = 0.13852562\n",
      "Iteration 142, loss = 0.13799305\n",
      "Iteration 143, loss = 0.13746901\n",
      "Iteration 144, loss = 0.13695335\n",
      "Iteration 145, loss = 0.13644582\n",
      "Iteration 146, loss = 0.13594629\n",
      "Iteration 147, loss = 0.13545453\n",
      "Iteration 148, loss = 0.13497040\n",
      "Iteration 149, loss = 0.13449376\n",
      "Iteration 150, loss = 0.13402441\n",
      "Iteration 151, loss = 0.13356221\n",
      "Iteration 152, loss = 0.13310701\n",
      "Iteration 153, loss = 0.13265867\n",
      "Iteration 154, loss = 0.13221703\n",
      "Iteration 155, loss = 0.13178195\n",
      "Iteration 156, loss = 0.13135334\n",
      "Iteration 157, loss = 0.13093100\n",
      "Iteration 158, loss = 0.13051485\n",
      "Iteration 159, loss = 0.13010476\n",
      "Iteration 160, loss = 0.12970059\n",
      "Iteration 161, loss = 0.12930221\n",
      "Iteration 162, loss = 0.12890954\n",
      "Iteration 163, loss = 0.12852243\n",
      "Iteration 164, loss = 0.12814079\n",
      "Iteration 165, loss = 0.12776449\n",
      "Iteration 166, loss = 0.12739344\n",
      "Iteration 167, loss = 0.12702754\n",
      "Iteration 168, loss = 0.12666667\n",
      "Iteration 169, loss = 0.12631081\n",
      "Iteration 170, loss = 0.12595972\n",
      "Iteration 171, loss = 0.12561343\n",
      "Iteration 172, loss = 0.12527182\n",
      "Iteration 173, loss = 0.12493481\n",
      "Iteration 174, loss = 0.12460222\n",
      "Iteration 175, loss = 0.12427405\n",
      "Iteration 176, loss = 0.12395020\n",
      "Iteration 177, loss = 0.12363064\n",
      "Iteration 178, loss = 0.12331518\n",
      "Iteration 179, loss = 0.12300382\n",
      "Iteration 180, loss = 0.12269648\n",
      "Iteration 181, loss = 0.12239308\n",
      "Iteration 182, loss = 0.12209354\n",
      "Iteration 183, loss = 0.12179778\n",
      "Iteration 184, loss = 0.12150578\n",
      "Iteration 185, loss = 0.12121748\n",
      "Iteration 186, loss = 0.12093281\n",
      "Iteration 187, loss = 0.12065163\n",
      "Iteration 188, loss = 0.12037393\n",
      "Iteration 189, loss = 0.12009951\n",
      "Iteration 190, loss = 0.11982843\n",
      "Iteration 191, loss = 0.11956060\n",
      "Iteration 192, loss = 0.11929601\n",
      "Iteration 193, loss = 0.11903458\n",
      "Iteration 194, loss = 0.11877626\n",
      "Iteration 195, loss = 0.11852101\n",
      "Iteration 196, loss = 0.11826874\n",
      "Iteration 197, loss = 0.11801944\n",
      "Iteration 198, loss = 0.11777305\n",
      "Iteration 199, loss = 0.11752952\n",
      "Iteration 200, loss = 0.11728883\n",
      "Iteration 201, loss = 0.11705089\n",
      "Iteration 202, loss = 0.11681568\n",
      "Iteration 203, loss = 0.11658315\n",
      "Iteration 204, loss = 0.11635325\n",
      "Iteration 205, loss = 0.11612594\n",
      "Iteration 206, loss = 0.11590119\n",
      "Iteration 207, loss = 0.11567894\n",
      "Iteration 208, loss = 0.11545916\n",
      "Iteration 209, loss = 0.11524179\n",
      "Iteration 210, loss = 0.11502681\n",
      "Iteration 211, loss = 0.11481416\n",
      "Iteration 212, loss = 0.11460382\n",
      "Iteration 213, loss = 0.11439572\n",
      "Iteration 214, loss = 0.11418980\n",
      "Iteration 215, loss = 0.11398604\n",
      "Iteration 216, loss = 0.11378446\n",
      "Iteration 217, loss = 0.11358499\n",
      "Iteration 218, loss = 0.11338762\n",
      "Iteration 219, loss = 0.11319230\n",
      "Iteration 220, loss = 0.11299899\n",
      "Iteration 221, loss = 0.11280770\n",
      "Iteration 222, loss = 0.11261836\n",
      "Iteration 223, loss = 0.11243094\n",
      "Iteration 224, loss = 0.11224541\n",
      "Iteration 225, loss = 0.11206177\n",
      "Iteration 226, loss = 0.11187999\n",
      "Iteration 227, loss = 0.11170005\n",
      "Iteration 228, loss = 0.11152191\n",
      "Iteration 229, loss = 0.11134551\n",
      "Iteration 230, loss = 0.11117088\n",
      "Iteration 231, loss = 0.11099798\n",
      "Iteration 232, loss = 0.11082681\n",
      "Iteration 233, loss = 0.11065727\n",
      "Iteration 234, loss = 0.11048927\n",
      "Iteration 235, loss = 0.11032293\n",
      "Iteration 236, loss = 0.11015820\n",
      "Iteration 237, loss = 0.10999506\n",
      "Iteration 238, loss = 0.10983344\n",
      "Iteration 239, loss = 0.10967331\n",
      "Iteration 240, loss = 0.10951471\n",
      "Iteration 241, loss = 0.10935761\n",
      "Iteration 242, loss = 0.10920202\n",
      "Iteration 243, loss = 0.10904775\n",
      "Iteration 244, loss = 0.10889462\n",
      "Iteration 245, loss = 0.10874291\n",
      "Iteration 246, loss = 0.10859316\n",
      "Iteration 247, loss = 0.10844507\n",
      "Iteration 248, loss = 0.10829833\n",
      "Iteration 249, loss = 0.10815294\n",
      "Iteration 250, loss = 0.10800889\n",
      "Iteration 251, loss = 0.10786594\n",
      "Iteration 252, loss = 0.10771781\n",
      "Iteration 253, loss = 0.10756961\n",
      "Iteration 254, loss = 0.10742143\n",
      "Iteration 255, loss = 0.10727349\n",
      "Iteration 256, loss = 0.10712612\n",
      "Iteration 257, loss = 0.10697855\n",
      "Iteration 258, loss = 0.10683288\n",
      "Iteration 259, loss = 0.10669120\n",
      "Iteration 260, loss = 0.10654126\n",
      "Iteration 261, loss = 0.10637584\n",
      "Iteration 262, loss = 0.10620433\n",
      "Iteration 263, loss = 0.10603823\n",
      "Iteration 264, loss = 0.10589728\n",
      "Iteration 265, loss = 0.10576188\n",
      "Iteration 266, loss = 0.10565054\n",
      "Iteration 267, loss = 0.10554004\n",
      "Iteration 268, loss = 0.10542394\n",
      "Iteration 269, loss = 0.10530534\n",
      "Iteration 270, loss = 0.10518096\n",
      "Iteration 271, loss = 0.10505586\n",
      "Iteration 272, loss = 0.10492661\n",
      "Iteration 273, loss = 0.10479476\n",
      "Iteration 274, loss = 0.10467963\n",
      "Iteration 275, loss = 0.10456705\n",
      "Iteration 276, loss = 0.10445526\n",
      "Iteration 277, loss = 0.10434330\n",
      "Iteration 278, loss = 0.10423089\n",
      "Iteration 279, loss = 0.10411818\n",
      "Iteration 280, loss = 0.10400541\n",
      "Iteration 281, loss = 0.10389407\n",
      "Iteration 282, loss = 0.10378327\n",
      "Iteration 283, loss = 0.10367355\n",
      "Iteration 284, loss = 0.10356462\n",
      "Iteration 285, loss = 0.10345731\n",
      "Iteration 286, loss = 0.10335165\n",
      "Iteration 287, loss = 0.10324683\n",
      "Iteration 288, loss = 0.10314369\n",
      "Iteration 289, loss = 0.10304045\n",
      "Iteration 290, loss = 0.10293775\n",
      "Iteration 291, loss = 0.10283557\n",
      "Iteration 292, loss = 0.10273560\n",
      "Iteration 293, loss = 0.10263557\n",
      "Iteration 294, loss = 0.10253622\n",
      "Iteration 295, loss = 0.10243728\n",
      "Iteration 296, loss = 0.10233888\n",
      "Iteration 297, loss = 0.10224454\n",
      "Iteration 298, loss = 0.10214628\n",
      "Iteration 299, loss = 0.10205057\n",
      "Iteration 300, loss = 0.10195529\n",
      "Iteration 301, loss = 0.10186255\n",
      "Iteration 302, loss = 0.10176887\n",
      "Iteration 303, loss = 0.10167607\n",
      "Iteration 304, loss = 0.10158365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.53884643\n",
      "Iteration 3, loss = 1.22814779\n",
      "Iteration 4, loss = 1.08434876\n",
      "Iteration 5, loss = 1.09690740\n",
      "Iteration 6, loss = 1.05141210\n",
      "Iteration 7, loss = 0.95725883\n",
      "Iteration 8, loss = 0.87783884\n",
      "Iteration 9, loss = 0.80809506\n",
      "Iteration 10, loss = 0.74807259\n",
      "Iteration 11, loss = 0.70200312\n",
      "Iteration 12, loss = 0.67110800\n",
      "Iteration 13, loss = 0.64365590\n",
      "Iteration 14, loss = 0.61529623\n",
      "Iteration 15, loss = 0.58628224\n",
      "Iteration 16, loss = 0.55930867\n",
      "Iteration 17, loss = 0.53667188\n",
      "Iteration 18, loss = 0.51634736\n",
      "Iteration 19, loss = 0.49787482\n",
      "Iteration 20, loss = 0.48093673\n",
      "Iteration 21, loss = 0.46542150\n",
      "Iteration 22, loss = 0.45117980\n",
      "Iteration 23, loss = 0.43805681\n",
      "Iteration 24, loss = 0.42586504\n",
      "Iteration 25, loss = 0.41456030\n",
      "Iteration 26, loss = 0.40406880\n",
      "Iteration 27, loss = 0.39425079\n",
      "Iteration 28, loss = 0.38506528\n",
      "Iteration 29, loss = 0.37646919\n",
      "Iteration 30, loss = 0.36837077\n",
      "Iteration 31, loss = 0.36071878\n",
      "Iteration 32, loss = 0.35346836\n",
      "Iteration 33, loss = 0.34656253\n",
      "Iteration 34, loss = 0.33995385\n",
      "Iteration 35, loss = 0.33361217\n",
      "Iteration 36, loss = 0.32751644\n",
      "Iteration 37, loss = 0.32165206\n",
      "Iteration 38, loss = 0.31599548\n",
      "Iteration 39, loss = 0.31053433\n",
      "Iteration 40, loss = 0.30525109\n",
      "Iteration 41, loss = 0.30013975\n",
      "Iteration 42, loss = 0.29518355\n",
      "Iteration 43, loss = 0.29037045\n",
      "Iteration 44, loss = 0.28569566\n",
      "Iteration 45, loss = 0.28115149\n",
      "Iteration 46, loss = 0.27673553\n",
      "Iteration 47, loss = 0.27244075\n",
      "Iteration 48, loss = 0.26826210\n",
      "Iteration 49, loss = 0.26419255\n",
      "Iteration 50, loss = 0.26023023\n",
      "Iteration 51, loss = 0.25636910\n",
      "Iteration 52, loss = 0.25260781\n",
      "Iteration 53, loss = 0.24894333\n",
      "Iteration 54, loss = 0.24537175\n",
      "Iteration 55, loss = 0.24189180\n",
      "Iteration 56, loss = 0.23849855\n",
      "Iteration 57, loss = 0.23519245\n",
      "Iteration 58, loss = 0.23197003\n",
      "Iteration 59, loss = 0.22882864\n",
      "Iteration 60, loss = 0.22576710\n",
      "Iteration 61, loss = 0.22278149\n",
      "Iteration 62, loss = 0.21987147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63, loss = 0.21703384\n",
      "Iteration 64, loss = 0.21426843\n",
      "Iteration 65, loss = 0.21157138\n",
      "Iteration 66, loss = 0.20894239\n",
      "Iteration 67, loss = 0.20637836\n",
      "Iteration 68, loss = 0.20387877\n",
      "Iteration 69, loss = 0.20144091\n",
      "Iteration 70, loss = 0.19906394\n",
      "Iteration 71, loss = 0.19674532\n",
      "Iteration 72, loss = 0.19448484\n",
      "Iteration 73, loss = 0.19227997\n",
      "Iteration 74, loss = 0.19012924\n",
      "Iteration 75, loss = 0.18803107\n",
      "Iteration 76, loss = 0.18598455\n",
      "Iteration 77, loss = 0.18398763\n",
      "Iteration 78, loss = 0.18203906\n",
      "Iteration 79, loss = 0.18013801\n",
      "Iteration 80, loss = 0.17828271\n",
      "Iteration 81, loss = 0.17647189\n",
      "Iteration 82, loss = 0.17470427\n",
      "Iteration 83, loss = 0.17297906\n",
      "Iteration 84, loss = 0.17129481\n",
      "Iteration 85, loss = 0.16965021\n",
      "Iteration 86, loss = 0.16804403\n",
      "Iteration 87, loss = 0.16647518\n",
      "Iteration 88, loss = 0.16494261\n",
      "Iteration 89, loss = 0.16344576\n",
      "Iteration 90, loss = 0.16198316\n",
      "Iteration 91, loss = 0.16055413\n",
      "Iteration 92, loss = 0.15915784\n",
      "Iteration 93, loss = 0.15779273\n",
      "Iteration 94, loss = 0.15645860\n",
      "Iteration 95, loss = 0.15515482\n",
      "Iteration 96, loss = 0.15388026\n",
      "Iteration 97, loss = 0.15263543\n",
      "Iteration 98, loss = 0.15141899\n",
      "Iteration 99, loss = 0.15022936\n",
      "Iteration 100, loss = 0.14906551\n",
      "Iteration 101, loss = 0.14792692\n",
      "Iteration 102, loss = 0.14681280\n",
      "Iteration 103, loss = 0.14572252\n",
      "Iteration 104, loss = 0.14465570\n",
      "Iteration 105, loss = 0.14361148\n",
      "Iteration 106, loss = 0.14258918\n",
      "Iteration 107, loss = 0.14158807\n",
      "Iteration 108, loss = 0.14060759\n",
      "Iteration 109, loss = 0.13964723\n",
      "Iteration 110, loss = 0.13870654\n",
      "Iteration 111, loss = 0.13778475\n",
      "Iteration 112, loss = 0.13688151\n",
      "Iteration 113, loss = 0.13599633\n",
      "Iteration 114, loss = 0.13512861\n",
      "Iteration 115, loss = 0.13427811\n",
      "Iteration 116, loss = 0.13344430\n",
      "Iteration 117, loss = 0.13262667\n",
      "Iteration 118, loss = 0.13182480\n",
      "Iteration 119, loss = 0.13103825\n",
      "Iteration 120, loss = 0.13026667\n",
      "Iteration 121, loss = 0.12950964\n",
      "Iteration 122, loss = 0.12876678\n",
      "Iteration 123, loss = 0.12803775\n",
      "Iteration 124, loss = 0.12732216\n",
      "Iteration 125, loss = 0.12661973\n",
      "Iteration 126, loss = 0.12593012\n",
      "Iteration 127, loss = 0.12525296\n",
      "Iteration 128, loss = 0.12458798\n",
      "Iteration 129, loss = 0.12393492\n",
      "Iteration 130, loss = 0.12329343\n",
      "Iteration 131, loss = 0.12266328\n",
      "Iteration 132, loss = 0.12204423\n",
      "Iteration 133, loss = 0.12143593\n",
      "Iteration 134, loss = 0.12083812\n",
      "Iteration 135, loss = 0.12025057\n",
      "Iteration 136, loss = 0.11967303\n",
      "Iteration 137, loss = 0.11910526\n",
      "Iteration 138, loss = 0.11854705\n",
      "Iteration 139, loss = 0.11799817\n",
      "Iteration 140, loss = 0.11745844\n",
      "Iteration 141, loss = 0.11692763\n",
      "Iteration 142, loss = 0.11640554\n",
      "Iteration 143, loss = 0.11589195\n",
      "Iteration 144, loss = 0.11538670\n",
      "Iteration 145, loss = 0.11488957\n",
      "Iteration 146, loss = 0.11440039\n",
      "Iteration 147, loss = 0.11391899\n",
      "Iteration 148, loss = 0.11344520\n",
      "Iteration 149, loss = 0.11297885\n",
      "Iteration 150, loss = 0.11251977\n",
      "Iteration 151, loss = 0.11206782\n",
      "Iteration 152, loss = 0.11162284\n",
      "Iteration 153, loss = 0.11118468\n",
      "Iteration 154, loss = 0.11075319\n",
      "Iteration 155, loss = 0.11032823\n",
      "Iteration 156, loss = 0.10990967\n",
      "Iteration 157, loss = 0.10949737\n",
      "Iteration 158, loss = 0.10909122\n",
      "Iteration 159, loss = 0.10869107\n",
      "Iteration 160, loss = 0.10829680\n",
      "Iteration 161, loss = 0.10790829\n",
      "Iteration 162, loss = 0.10752542\n",
      "Iteration 163, loss = 0.10714807\n",
      "Iteration 164, loss = 0.10677613\n",
      "Iteration 165, loss = 0.10640950\n",
      "Iteration 166, loss = 0.10604807\n",
      "Iteration 167, loss = 0.10569173\n",
      "Iteration 168, loss = 0.10534038\n",
      "Iteration 169, loss = 0.10499394\n",
      "Iteration 170, loss = 0.10465229\n",
      "Iteration 171, loss = 0.10431534\n",
      "Iteration 172, loss = 0.10398301\n",
      "Iteration 173, loss = 0.10365524\n",
      "Iteration 174, loss = 0.10333194\n",
      "Iteration 175, loss = 0.10301304\n",
      "Iteration 176, loss = 0.10269840\n",
      "Iteration 177, loss = 0.10238795\n",
      "Iteration 178, loss = 0.10208161\n",
      "Iteration 179, loss = 0.10177930\n",
      "Iteration 180, loss = 0.10148095\n",
      "Iteration 181, loss = 0.10118648\n",
      "Iteration 182, loss = 0.10089583\n",
      "Iteration 183, loss = 0.10060896\n",
      "Iteration 184, loss = 0.10032574\n",
      "Iteration 185, loss = 0.10004614\n",
      "Iteration 186, loss = 0.09977007\n",
      "Iteration 187, loss = 0.09949748\n",
      "Iteration 188, loss = 0.09922831\n",
      "Iteration 189, loss = 0.09896250\n",
      "Iteration 190, loss = 0.09869999\n",
      "Iteration 191, loss = 0.09844073\n",
      "Iteration 192, loss = 0.09818461\n",
      "Iteration 193, loss = 0.09793157\n",
      "Iteration 194, loss = 0.09768163\n",
      "Iteration 195, loss = 0.09743469\n",
      "Iteration 196, loss = 0.09719073\n",
      "Iteration 197, loss = 0.09694968\n",
      "Iteration 198, loss = 0.09671149\n",
      "Iteration 199, loss = 0.09647612\n",
      "Iteration 200, loss = 0.09624352\n",
      "Iteration 201, loss = 0.09601365\n",
      "Iteration 202, loss = 0.09578645\n",
      "Iteration 203, loss = 0.09556189\n",
      "Iteration 204, loss = 0.09533989\n",
      "Iteration 205, loss = 0.09512045\n",
      "Iteration 206, loss = 0.09490350\n",
      "Iteration 207, loss = 0.09468902\n",
      "Iteration 208, loss = 0.09447697\n",
      "Iteration 209, loss = 0.09426730\n",
      "Iteration 210, loss = 0.09405998\n",
      "Iteration 211, loss = 0.09385499\n",
      "Iteration 212, loss = 0.09365228\n",
      "Iteration 213, loss = 0.09345182\n",
      "Iteration 214, loss = 0.09325355\n",
      "Iteration 215, loss = 0.09305744\n",
      "Iteration 216, loss = 0.09286346\n",
      "Iteration 217, loss = 0.09267158\n",
      "Iteration 218, loss = 0.09248176\n",
      "Iteration 219, loss = 0.09229396\n",
      "Iteration 220, loss = 0.09210818\n",
      "Iteration 221, loss = 0.09192439\n",
      "Iteration 222, loss = 0.09174253\n",
      "Iteration 223, loss = 0.09156254\n",
      "Iteration 224, loss = 0.09138440\n",
      "Iteration 225, loss = 0.09120810\n",
      "Iteration 226, loss = 0.09103361\n",
      "Iteration 227, loss = 0.09086090\n",
      "Iteration 228, loss = 0.09068996\n",
      "Iteration 229, loss = 0.09052076\n",
      "Iteration 230, loss = 0.09035327\n",
      "Iteration 231, loss = 0.09018745\n",
      "Iteration 232, loss = 0.09002331\n",
      "Iteration 233, loss = 0.08986079\n",
      "Iteration 234, loss = 0.08969988\n",
      "Iteration 235, loss = 0.08954056\n",
      "Iteration 236, loss = 0.08938280\n",
      "Iteration 237, loss = 0.08922656\n",
      "Iteration 238, loss = 0.08907185\n",
      "Iteration 239, loss = 0.08891865\n",
      "Iteration 240, loss = 0.08876693\n",
      "Iteration 241, loss = 0.08861667\n",
      "Iteration 242, loss = 0.08846784\n",
      "Iteration 243, loss = 0.08832041\n",
      "Iteration 244, loss = 0.08817442\n",
      "Iteration 245, loss = 0.08802982\n",
      "Iteration 246, loss = 0.08788659\n",
      "Iteration 247, loss = 0.08774468\n",
      "Iteration 248, loss = 0.08760406\n",
      "Iteration 249, loss = 0.08746472\n",
      "Iteration 250, loss = 0.08732668\n",
      "Iteration 251, loss = 0.08718989\n",
      "Iteration 252, loss = 0.08705434\n",
      "Iteration 253, loss = 0.08691999\n",
      "Iteration 254, loss = 0.08678682\n",
      "Iteration 255, loss = 0.08665484\n",
      "Iteration 256, loss = 0.08652404\n",
      "Iteration 257, loss = 0.08639438\n",
      "Iteration 258, loss = 0.08626587\n",
      "Iteration 259, loss = 0.08613852\n",
      "Iteration 260, loss = 0.08601234\n",
      "Iteration 261, loss = 0.08588729\n",
      "Iteration 262, loss = 0.08576334\n",
      "Iteration 263, loss = 0.08564055\n",
      "Iteration 264, loss = 0.08551886\n",
      "Iteration 265, loss = 0.08539822\n",
      "Iteration 266, loss = 0.08527869\n",
      "Iteration 267, loss = 0.08516022\n",
      "Iteration 268, loss = 0.08504274\n",
      "Iteration 269, loss = 0.08492586\n",
      "Iteration 270, loss = 0.08480998\n",
      "Iteration 271, loss = 0.08469505\n",
      "Iteration 272, loss = 0.08458107\n",
      "Iteration 273, loss = 0.08446804\n",
      "Iteration 274, loss = 0.08435595\n",
      "Iteration 275, loss = 0.08424477\n",
      "Iteration 276, loss = 0.08413451\n",
      "Iteration 277, loss = 0.08402516\n",
      "Iteration 278, loss = 0.08391670\n",
      "Iteration 279, loss = 0.08380903\n",
      "Iteration 280, loss = 0.08370215\n",
      "Iteration 281, loss = 0.08359619\n",
      "Iteration 282, loss = 0.08349117\n",
      "Iteration 283, loss = 0.08338700\n",
      "Iteration 284, loss = 0.08328367\n",
      "Iteration 285, loss = 0.08318120\n",
      "Iteration 286, loss = 0.08307935\n",
      "Iteration 287, loss = 0.08297830\n",
      "Iteration 288, loss = 0.08287804\n",
      "Iteration 289, loss = 0.08277857\n",
      "Iteration 290, loss = 0.08267987\n",
      "Iteration 291, loss = 0.08258305\n",
      "Iteration 292, loss = 0.08248764\n",
      "Iteration 293, loss = 0.08239305\n",
      "Iteration 294, loss = 0.08229926\n",
      "Iteration 295, loss = 0.08220629\n",
      "Iteration 296, loss = 0.08211396\n",
      "Iteration 297, loss = 0.08202237\n",
      "Iteration 298, loss = 0.08193133\n",
      "Iteration 299, loss = 0.08184085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.55133863\n",
      "Iteration 3, loss = 1.23387661\n",
      "Iteration 4, loss = 1.09300117\n",
      "Iteration 5, loss = 1.10904151\n",
      "Iteration 6, loss = 1.06091533\n",
      "Iteration 7, loss = 0.96497161\n",
      "Iteration 8, loss = 0.88549126\n",
      "Iteration 9, loss = 0.81567207\n",
      "Iteration 10, loss = 0.75577350\n",
      "Iteration 11, loss = 0.70934962\n",
      "Iteration 12, loss = 0.67892114\n",
      "Iteration 13, loss = 0.65144880\n",
      "Iteration 14, loss = 0.62301505\n",
      "Iteration 15, loss = 0.59368115\n",
      "Iteration 16, loss = 0.56660536\n",
      "Iteration 17, loss = 0.54386884\n",
      "Iteration 18, loss = 0.52345294\n",
      "Iteration 19, loss = 0.50481608\n",
      "Iteration 20, loss = 0.48763599\n",
      "Iteration 21, loss = 0.47166200\n",
      "Iteration 22, loss = 0.45677610\n",
      "Iteration 23, loss = 0.44294881\n",
      "Iteration 24, loss = 0.43020078\n",
      "Iteration 25, loss = 0.41847086\n",
      "Iteration 26, loss = 0.40749865\n",
      "Iteration 27, loss = 0.39721639\n",
      "Iteration 28, loss = 0.38759025\n",
      "Iteration 29, loss = 0.37862091\n",
      "Iteration 30, loss = 0.37030501\n",
      "Iteration 31, loss = 0.36248945\n",
      "Iteration 32, loss = 0.35508752\n",
      "Iteration 33, loss = 0.34797543\n",
      "Iteration 34, loss = 0.34112594\n",
      "Iteration 35, loss = 0.33456084\n",
      "Iteration 36, loss = 0.32827760\n",
      "Iteration 37, loss = 0.32224274\n",
      "Iteration 38, loss = 0.31644145\n",
      "Iteration 39, loss = 0.31085125\n",
      "Iteration 40, loss = 0.30546115\n",
      "Iteration 41, loss = 0.30024126\n",
      "Iteration 42, loss = 0.29517948\n",
      "Iteration 43, loss = 0.29026792\n",
      "Iteration 44, loss = 0.28549901\n",
      "Iteration 45, loss = 0.28086939\n",
      "Iteration 46, loss = 0.27636801\n",
      "Iteration 47, loss = 0.27199943\n",
      "Iteration 48, loss = 0.26774976\n",
      "Iteration 49, loss = 0.26361479\n",
      "Iteration 50, loss = 0.25959049\n",
      "Iteration 51, loss = 0.25567238\n",
      "Iteration 52, loss = 0.25185871\n",
      "Iteration 53, loss = 0.24814475\n",
      "Iteration 54, loss = 0.24452889\n",
      "Iteration 55, loss = 0.24100878\n",
      "Iteration 56, loss = 0.23757785\n",
      "Iteration 57, loss = 0.23423623\n",
      "Iteration 58, loss = 0.23098044\n",
      "Iteration 59, loss = 0.22780816\n",
      "Iteration 60, loss = 0.22471674\n",
      "Iteration 61, loss = 0.22170469\n",
      "Iteration 62, loss = 0.21876956\n",
      "Iteration 63, loss = 0.21590857\n",
      "Iteration 64, loss = 0.21312045\n",
      "Iteration 65, loss = 0.21040283\n",
      "Iteration 66, loss = 0.20775398\n",
      "Iteration 67, loss = 0.20517227\n",
      "Iteration 68, loss = 0.20265548\n",
      "Iteration 69, loss = 0.20020135\n",
      "Iteration 70, loss = 0.19780799\n",
      "Iteration 71, loss = 0.19547385\n",
      "Iteration 72, loss = 0.19319741\n",
      "Iteration 73, loss = 0.19097683\n",
      "Iteration 74, loss = 0.18881043\n",
      "Iteration 75, loss = 0.18669655\n",
      "Iteration 76, loss = 0.18463377\n",
      "Iteration 77, loss = 0.18262050\n",
      "Iteration 78, loss = 0.18065531\n",
      "Iteration 79, loss = 0.17873738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80, loss = 0.17686488\n",
      "Iteration 81, loss = 0.17503651\n",
      "Iteration 82, loss = 0.17325084\n",
      "Iteration 83, loss = 0.17150697\n",
      "Iteration 84, loss = 0.16980348\n",
      "Iteration 85, loss = 0.16813960\n",
      "Iteration 86, loss = 0.16651387\n",
      "Iteration 87, loss = 0.16492582\n",
      "Iteration 88, loss = 0.16337380\n",
      "Iteration 89, loss = 0.16185691\n",
      "Iteration 90, loss = 0.16037430\n",
      "Iteration 91, loss = 0.15892517\n",
      "Iteration 92, loss = 0.15750957\n",
      "Iteration 93, loss = 0.15612570\n",
      "Iteration 94, loss = 0.15477193\n",
      "Iteration 95, loss = 0.15344758\n",
      "Iteration 96, loss = 0.15215200\n",
      "Iteration 97, loss = 0.15088427\n",
      "Iteration 98, loss = 0.14964383\n",
      "Iteration 99, loss = 0.14842979\n",
      "Iteration 100, loss = 0.14724082\n",
      "Iteration 101, loss = 0.14607675\n",
      "Iteration 102, loss = 0.14493686\n",
      "Iteration 103, loss = 0.14382056\n",
      "Iteration 104, loss = 0.14272717\n",
      "Iteration 105, loss = 0.14165598\n",
      "Iteration 106, loss = 0.14060617\n",
      "Iteration 107, loss = 0.13957725\n",
      "Iteration 108, loss = 0.13856865\n",
      "Iteration 109, loss = 0.13757978\n",
      "Iteration 110, loss = 0.13661019\n",
      "Iteration 111, loss = 0.13565921\n",
      "Iteration 112, loss = 0.13472660\n",
      "Iteration 113, loss = 0.13381156\n",
      "Iteration 114, loss = 0.13291398\n",
      "Iteration 115, loss = 0.13203306\n",
      "Iteration 116, loss = 0.13116860\n",
      "Iteration 117, loss = 0.13031992\n",
      "Iteration 118, loss = 0.12948669\n",
      "Iteration 119, loss = 0.12866861\n",
      "Iteration 120, loss = 0.12786514\n",
      "Iteration 121, loss = 0.12707601\n",
      "Iteration 122, loss = 0.12630102\n",
      "Iteration 123, loss = 0.12553966\n",
      "Iteration 124, loss = 0.12479157\n",
      "Iteration 125, loss = 0.12405657\n",
      "Iteration 126, loss = 0.12333402\n",
      "Iteration 127, loss = 0.12262391\n",
      "Iteration 128, loss = 0.12192590\n",
      "Iteration 129, loss = 0.12123961\n",
      "Iteration 130, loss = 0.12056483\n",
      "Iteration 131, loss = 0.11990127\n",
      "Iteration 132, loss = 0.11924862\n",
      "Iteration 133, loss = 0.11860665\n",
      "Iteration 134, loss = 0.11797515\n",
      "Iteration 135, loss = 0.11735392\n",
      "Iteration 136, loss = 0.11674263\n",
      "Iteration 137, loss = 0.11614107\n",
      "Iteration 138, loss = 0.11554902\n",
      "Iteration 139, loss = 0.11496629\n",
      "Iteration 140, loss = 0.11439265\n",
      "Iteration 141, loss = 0.11382793\n",
      "Iteration 142, loss = 0.11327189\n",
      "Iteration 143, loss = 0.11272436\n",
      "Iteration 144, loss = 0.11218514\n",
      "Iteration 145, loss = 0.11165406\n",
      "Iteration 146, loss = 0.11113094\n",
      "Iteration 147, loss = 0.11061561\n",
      "Iteration 148, loss = 0.11010790\n",
      "Iteration 149, loss = 0.10960766\n",
      "Iteration 150, loss = 0.10911471\n",
      "Iteration 151, loss = 0.10862893\n",
      "Iteration 152, loss = 0.10815013\n",
      "Iteration 153, loss = 0.10767825\n",
      "Iteration 154, loss = 0.10721301\n",
      "Iteration 155, loss = 0.10675439\n",
      "Iteration 156, loss = 0.10630222\n",
      "Iteration 157, loss = 0.10585635\n",
      "Iteration 158, loss = 0.10541666\n",
      "Iteration 159, loss = 0.10498305\n",
      "Iteration 160, loss = 0.10455536\n",
      "Iteration 161, loss = 0.10413349\n",
      "Iteration 162, loss = 0.10371737\n",
      "Iteration 163, loss = 0.10330686\n",
      "Iteration 164, loss = 0.10290186\n",
      "Iteration 165, loss = 0.10250227\n",
      "Iteration 166, loss = 0.10210797\n",
      "Iteration 167, loss = 0.10171877\n",
      "Iteration 168, loss = 0.10133468\n",
      "Iteration 169, loss = 0.10095555\n",
      "Iteration 170, loss = 0.10058129\n",
      "Iteration 171, loss = 0.10021180\n",
      "Iteration 172, loss = 0.09984702\n",
      "Iteration 173, loss = 0.09948689\n",
      "Iteration 174, loss = 0.09913131\n",
      "Iteration 175, loss = 0.09878013\n",
      "Iteration 176, loss = 0.09843329\n",
      "Iteration 177, loss = 0.09809073\n",
      "Iteration 178, loss = 0.09775239\n",
      "Iteration 179, loss = 0.09741818\n",
      "Iteration 180, loss = 0.09708805\n",
      "Iteration 181, loss = 0.09676191\n",
      "Iteration 182, loss = 0.09643969\n",
      "Iteration 183, loss = 0.09612127\n",
      "Iteration 184, loss = 0.09580657\n",
      "Iteration 185, loss = 0.09549553\n",
      "Iteration 186, loss = 0.09518811\n",
      "Iteration 187, loss = 0.09488425\n",
      "Iteration 188, loss = 0.09458389\n",
      "Iteration 189, loss = 0.09428695\n",
      "Iteration 190, loss = 0.09399328\n",
      "Iteration 191, loss = 0.09370292\n",
      "Iteration 192, loss = 0.09341581\n",
      "Iteration 193, loss = 0.09313192\n",
      "Iteration 194, loss = 0.09285117\n",
      "Iteration 195, loss = 0.09257348\n",
      "Iteration 196, loss = 0.09229886\n",
      "Iteration 197, loss = 0.09202715\n",
      "Iteration 198, loss = 0.09175839\n",
      "Iteration 199, loss = 0.09149255\n",
      "Iteration 200, loss = 0.09122959\n",
      "Iteration 201, loss = 0.09096941\n",
      "Iteration 202, loss = 0.09071197\n",
      "Iteration 203, loss = 0.09045727\n",
      "Iteration 204, loss = 0.09020524\n",
      "Iteration 205, loss = 0.08995580\n",
      "Iteration 206, loss = 0.08970891\n",
      "Iteration 207, loss = 0.08946454\n",
      "Iteration 208, loss = 0.08922266\n",
      "Iteration 209, loss = 0.08898324\n",
      "Iteration 210, loss = 0.08874619\n",
      "Iteration 211, loss = 0.08851156\n",
      "Iteration 212, loss = 0.08827947\n",
      "Iteration 213, loss = 0.08804988\n",
      "Iteration 214, loss = 0.08782266\n",
      "Iteration 215, loss = 0.08759770\n",
      "Iteration 216, loss = 0.08737547\n",
      "Iteration 217, loss = 0.08715579\n",
      "Iteration 218, loss = 0.08693831\n",
      "Iteration 219, loss = 0.08672298\n",
      "Iteration 220, loss = 0.08650977\n",
      "Iteration 221, loss = 0.08629660\n",
      "Iteration 222, loss = 0.08607916\n",
      "Iteration 223, loss = 0.08586180\n",
      "Iteration 224, loss = 0.08564471\n",
      "Iteration 225, loss = 0.08542822\n",
      "Iteration 226, loss = 0.08521173\n",
      "Iteration 227, loss = 0.08499624\n",
      "Iteration 228, loss = 0.08478157\n",
      "Iteration 229, loss = 0.08457121\n",
      "Iteration 230, loss = 0.08436120\n",
      "Iteration 231, loss = 0.08415521\n",
      "Iteration 232, loss = 0.08396138\n",
      "Iteration 233, loss = 0.08377373\n",
      "Iteration 234, loss = 0.08359498\n",
      "Iteration 235, loss = 0.08341537\n",
      "Iteration 236, loss = 0.08323488\n",
      "Iteration 237, loss = 0.08305371\n",
      "Iteration 238, loss = 0.08287213\n",
      "Iteration 239, loss = 0.08269043\n",
      "Iteration 240, loss = 0.08250921\n",
      "Iteration 241, loss = 0.08233109\n",
      "Iteration 242, loss = 0.08215884\n",
      "Iteration 243, loss = 0.08198805\n",
      "Iteration 244, loss = 0.08181869\n",
      "Iteration 245, loss = 0.08165073\n",
      "Iteration 246, loss = 0.08148416\n",
      "Iteration 247, loss = 0.08131894\n",
      "Iteration 248, loss = 0.08115506\n",
      "Iteration 249, loss = 0.08099251\n",
      "Iteration 250, loss = 0.08083127\n",
      "Iteration 251, loss = 0.08067131\n",
      "Iteration 252, loss = 0.08051262\n",
      "Iteration 253, loss = 0.08035520\n",
      "Iteration 254, loss = 0.08019901\n",
      "Iteration 255, loss = 0.08004402\n",
      "Iteration 256, loss = 0.07989023\n",
      "Iteration 257, loss = 0.07974043\n",
      "Iteration 258, loss = 0.07958903\n",
      "Iteration 259, loss = 0.07943881\n",
      "Iteration 260, loss = 0.07929065\n",
      "Iteration 261, loss = 0.07914361\n",
      "Iteration 262, loss = 0.07899882\n",
      "Iteration 263, loss = 0.07885440\n",
      "Iteration 264, loss = 0.07871101\n",
      "Iteration 265, loss = 0.07856860\n",
      "Iteration 266, loss = 0.07842767\n",
      "Iteration 267, loss = 0.07828813\n",
      "Iteration 268, loss = 0.07814928\n",
      "Iteration 269, loss = 0.07801156\n",
      "Iteration 270, loss = 0.07787580\n",
      "Iteration 271, loss = 0.07774037\n",
      "Iteration 272, loss = 0.07760581\n",
      "Iteration 273, loss = 0.07747318\n",
      "Iteration 274, loss = 0.07734082\n",
      "Iteration 275, loss = 0.07720952\n",
      "Iteration 276, loss = 0.07708022\n",
      "Iteration 277, loss = 0.07695087\n",
      "Iteration 278, loss = 0.07682263\n",
      "Iteration 279, loss = 0.07669609\n",
      "Iteration 280, loss = 0.07657002\n",
      "Iteration 281, loss = 0.07644475\n",
      "Iteration 282, loss = 0.07632075\n",
      "Iteration 283, loss = 0.07619794\n",
      "Iteration 284, loss = 0.07607547\n",
      "Iteration 285, loss = 0.07595402\n",
      "Iteration 286, loss = 0.07583427\n",
      "Iteration 287, loss = 0.07571451\n",
      "Iteration 288, loss = 0.07559541\n",
      "Iteration 289, loss = 0.07547867\n",
      "Iteration 290, loss = 0.07536153\n",
      "Iteration 291, loss = 0.07524500\n",
      "Iteration 292, loss = 0.07513136\n",
      "Iteration 293, loss = 0.07501562\n",
      "Iteration 294, loss = 0.07490346\n",
      "Iteration 295, loss = 0.07479076\n",
      "Iteration 296, loss = 0.07467853\n",
      "Iteration 297, loss = 0.07456695\n",
      "Iteration 298, loss = 0.07445795\n",
      "Iteration 299, loss = 0.07434723\n",
      "Iteration 300, loss = 0.07423973\n",
      "Iteration 301, loss = 0.07413181\n",
      "Iteration 302, loss = 0.07402431\n",
      "Iteration 303, loss = 0.07391729\n",
      "Iteration 304, loss = 0.07381336\n",
      "Iteration 305, loss = 0.07370736\n",
      "Iteration 306, loss = 0.07360362\n",
      "Iteration 307, loss = 0.07350007\n",
      "Iteration 308, loss = 0.07339689\n",
      "Iteration 309, loss = 0.07329583\n",
      "Iteration 310, loss = 0.07319366\n",
      "Iteration 311, loss = 0.07309365\n",
      "Iteration 312, loss = 0.07299370\n",
      "Iteration 313, loss = 0.07289412\n",
      "Iteration 314, loss = 0.07279622\n",
      "Iteration 315, loss = 0.07269793\n",
      "Iteration 316, loss = 0.07260056\n",
      "Iteration 317, loss = 0.07250483\n",
      "Iteration 318, loss = 0.07240860\n",
      "Iteration 319, loss = 0.07231301\n",
      "Iteration 320, loss = 0.07221906\n",
      "Iteration 321, loss = 0.07212458\n",
      "Iteration 322, loss = 0.07203217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.55656928\n",
      "Iteration 3, loss = 1.23077489\n",
      "Iteration 4, loss = 1.08707879\n",
      "Iteration 5, loss = 1.10665413\n",
      "Iteration 6, loss = 1.05743140\n",
      "Iteration 7, loss = 0.96236176\n",
      "Iteration 8, loss = 0.88383765\n",
      "Iteration 9, loss = 0.81304633\n",
      "Iteration 10, loss = 0.75228554\n",
      "Iteration 11, loss = 0.70640234\n",
      "Iteration 12, loss = 0.67603132\n",
      "Iteration 13, loss = 0.64802874\n",
      "Iteration 14, loss = 0.61885596\n",
      "Iteration 15, loss = 0.58917954\n",
      "Iteration 16, loss = 0.56180670\n",
      "Iteration 17, loss = 0.53892058\n",
      "Iteration 18, loss = 0.51929404\n",
      "Iteration 19, loss = 0.50150706\n",
      "Iteration 20, loss = 0.48527051\n",
      "Iteration 21, loss = 0.47039213\n",
      "Iteration 22, loss = 0.45669076\n",
      "Iteration 23, loss = 0.44406306\n",
      "Iteration 24, loss = 0.43237979\n",
      "Iteration 25, loss = 0.42166356\n",
      "Iteration 26, loss = 0.41177216\n",
      "Iteration 27, loss = 0.40251411\n",
      "Iteration 28, loss = 0.39386869\n",
      "Iteration 29, loss = 0.38579749\n",
      "Iteration 30, loss = 0.37825263\n",
      "Iteration 31, loss = 0.37112773\n",
      "Iteration 32, loss = 0.36437761\n",
      "Iteration 33, loss = 0.35795075\n",
      "Iteration 34, loss = 0.35181058\n",
      "Iteration 35, loss = 0.34592827\n",
      "Iteration 36, loss = 0.34028113\n",
      "Iteration 37, loss = 0.33485416\n",
      "Iteration 38, loss = 0.32962975\n",
      "Iteration 39, loss = 0.32459343\n",
      "Iteration 40, loss = 0.31972840\n",
      "Iteration 41, loss = 0.31501980\n",
      "Iteration 42, loss = 0.31045618\n",
      "Iteration 43, loss = 0.30602816\n",
      "Iteration 44, loss = 0.30172763\n",
      "Iteration 45, loss = 0.29754762\n",
      "Iteration 46, loss = 0.29348518\n",
      "Iteration 47, loss = 0.28953375\n",
      "Iteration 48, loss = 0.28568687\n",
      "Iteration 49, loss = 0.28194009\n",
      "Iteration 50, loss = 0.27828972\n",
      "Iteration 51, loss = 0.27473371\n",
      "Iteration 52, loss = 0.27126923\n",
      "Iteration 53, loss = 0.26789258\n",
      "Iteration 54, loss = 0.26460027\n",
      "Iteration 55, loss = 0.26138979\n",
      "Iteration 56, loss = 0.25825915\n",
      "Iteration 57, loss = 0.25520552\n",
      "Iteration 58, loss = 0.25222709\n",
      "Iteration 59, loss = 0.24932198\n",
      "Iteration 60, loss = 0.24648814\n",
      "Iteration 61, loss = 0.24372340\n",
      "Iteration 62, loss = 0.24102606\n",
      "Iteration 63, loss = 0.23839416\n",
      "Iteration 64, loss = 0.23582663\n",
      "Iteration 65, loss = 0.23332118\n",
      "Iteration 66, loss = 0.23087656\n",
      "Iteration 67, loss = 0.22849149\n",
      "Iteration 68, loss = 0.22616449\n",
      "Iteration 69, loss = 0.22389292\n",
      "Iteration 70, loss = 0.22167714\n",
      "Iteration 71, loss = 0.21951349\n",
      "Iteration 72, loss = 0.21740191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.21534056\n",
      "Iteration 74, loss = 0.21332806\n",
      "Iteration 75, loss = 0.21136343\n",
      "Iteration 76, loss = 0.20944520\n",
      "Iteration 77, loss = 0.20757199\n",
      "Iteration 78, loss = 0.20574263\n",
      "Iteration 79, loss = 0.20395629\n",
      "Iteration 80, loss = 0.20221138\n",
      "Iteration 81, loss = 0.20050693\n",
      "Iteration 82, loss = 0.19884171\n",
      "Iteration 83, loss = 0.19721542\n",
      "Iteration 84, loss = 0.19562641\n",
      "Iteration 85, loss = 0.19407370\n",
      "Iteration 86, loss = 0.19255586\n",
      "Iteration 87, loss = 0.19107228\n",
      "Iteration 88, loss = 0.18962169\n",
      "Iteration 89, loss = 0.18820360\n",
      "Iteration 90, loss = 0.18681701\n",
      "Iteration 91, loss = 0.18546138\n",
      "Iteration 92, loss = 0.18413577\n",
      "Iteration 93, loss = 0.18283921\n",
      "Iteration 94, loss = 0.18157081\n",
      "Iteration 95, loss = 0.18033006\n",
      "Iteration 96, loss = 0.17911606\n",
      "Iteration 97, loss = 0.17792890\n",
      "Iteration 98, loss = 0.17676732\n",
      "Iteration 99, loss = 0.17563028\n",
      "Iteration 100, loss = 0.17451708\n",
      "Iteration 101, loss = 0.17342742\n",
      "Iteration 102, loss = 0.17236052\n",
      "Iteration 103, loss = 0.17131559\n",
      "Iteration 104, loss = 0.17029217\n",
      "Iteration 105, loss = 0.16928960\n",
      "Iteration 106, loss = 0.16830740\n",
      "Iteration 107, loss = 0.16734480\n",
      "Iteration 108, loss = 0.16640174\n",
      "Iteration 109, loss = 0.16547781\n",
      "Iteration 110, loss = 0.16457207\n",
      "Iteration 111, loss = 0.16368409\n",
      "Iteration 112, loss = 0.16281331\n",
      "Iteration 113, loss = 0.16195939\n",
      "Iteration 114, loss = 0.16112209\n",
      "Iteration 115, loss = 0.16030083\n",
      "Iteration 116, loss = 0.15949508\n",
      "Iteration 117, loss = 0.15870472\n",
      "Iteration 118, loss = 0.15792908\n",
      "Iteration 119, loss = 0.15716776\n",
      "Iteration 120, loss = 0.15642040\n",
      "Iteration 121, loss = 0.15568674\n",
      "Iteration 122, loss = 0.15496636\n",
      "Iteration 123, loss = 0.15425894\n",
      "Iteration 124, loss = 0.15356417\n",
      "Iteration 125, loss = 0.15288178\n",
      "Iteration 126, loss = 0.15221142\n",
      "Iteration 127, loss = 0.15155282\n",
      "Iteration 128, loss = 0.15090570\n",
      "Iteration 129, loss = 0.15026977\n",
      "Iteration 130, loss = 0.14964478\n",
      "Iteration 131, loss = 0.14903046\n",
      "Iteration 132, loss = 0.14842657\n",
      "Iteration 133, loss = 0.14783299\n",
      "Iteration 134, loss = 0.14724939\n",
      "Iteration 135, loss = 0.14667552\n",
      "Iteration 136, loss = 0.14611114\n",
      "Iteration 137, loss = 0.14555605\n",
      "Iteration 138, loss = 0.14501006\n",
      "Iteration 139, loss = 0.14447294\n",
      "Iteration 140, loss = 0.14394448\n",
      "Iteration 141, loss = 0.14342451\n",
      "Iteration 142, loss = 0.14291282\n",
      "Iteration 143, loss = 0.14240924\n",
      "Iteration 144, loss = 0.14191359\n",
      "Iteration 145, loss = 0.14142572\n",
      "Iteration 146, loss = 0.14094549\n",
      "Iteration 147, loss = 0.14047270\n",
      "Iteration 148, loss = 0.14000716\n",
      "Iteration 149, loss = 0.13954873\n",
      "Iteration 150, loss = 0.13909736\n",
      "Iteration 151, loss = 0.13865271\n",
      "Iteration 152, loss = 0.13821481\n",
      "Iteration 153, loss = 0.13778344\n",
      "Iteration 154, loss = 0.13735844\n",
      "Iteration 155, loss = 0.13693970\n",
      "Iteration 156, loss = 0.13652709\n",
      "Iteration 157, loss = 0.13612050\n",
      "Iteration 158, loss = 0.13571977\n",
      "Iteration 159, loss = 0.13532479\n",
      "Iteration 160, loss = 0.13493551\n",
      "Iteration 161, loss = 0.13455169\n",
      "Iteration 162, loss = 0.13417339\n",
      "Iteration 163, loss = 0.13380039\n",
      "Iteration 164, loss = 0.13343261\n",
      "Iteration 165, loss = 0.13306990\n",
      "Iteration 166, loss = 0.13271219\n",
      "Iteration 167, loss = 0.13235939\n",
      "Iteration 168, loss = 0.13201141\n",
      "Iteration 169, loss = 0.13166815\n",
      "Iteration 170, loss = 0.13132953\n",
      "Iteration 171, loss = 0.13099549\n",
      "Iteration 172, loss = 0.13066586\n",
      "Iteration 173, loss = 0.13034062\n",
      "Iteration 174, loss = 0.13001967\n",
      "Iteration 175, loss = 0.12970293\n",
      "Iteration 176, loss = 0.12939031\n",
      "Iteration 177, loss = 0.12908176\n",
      "Iteration 178, loss = 0.12877720\n",
      "Iteration 179, loss = 0.12847658\n",
      "Iteration 180, loss = 0.12817980\n",
      "Iteration 181, loss = 0.12788678\n",
      "Iteration 182, loss = 0.12759746\n",
      "Iteration 183, loss = 0.12731178\n",
      "Iteration 184, loss = 0.12702967\n",
      "Iteration 185, loss = 0.12675106\n",
      "Iteration 186, loss = 0.12647589\n",
      "Iteration 187, loss = 0.12620410\n",
      "Iteration 188, loss = 0.12593569\n",
      "Iteration 189, loss = 0.12567056\n",
      "Iteration 190, loss = 0.12540866\n",
      "Iteration 191, loss = 0.12514989\n",
      "Iteration 192, loss = 0.12489423\n",
      "Iteration 193, loss = 0.12464161\n",
      "Iteration 194, loss = 0.12439199\n",
      "Iteration 195, loss = 0.12414530\n",
      "Iteration 196, loss = 0.12390151\n",
      "Iteration 197, loss = 0.12366056\n",
      "Iteration 198, loss = 0.12342241\n",
      "Iteration 199, loss = 0.12318701\n",
      "Iteration 200, loss = 0.12295430\n",
      "Iteration 201, loss = 0.12272427\n",
      "Iteration 202, loss = 0.12249682\n",
      "Iteration 203, loss = 0.12227196\n",
      "Iteration 204, loss = 0.12204961\n",
      "Iteration 205, loss = 0.12182976\n",
      "Iteration 206, loss = 0.12161235\n",
      "Iteration 207, loss = 0.12139736\n",
      "Iteration 208, loss = 0.12118473\n",
      "Iteration 209, loss = 0.12097444\n",
      "Iteration 210, loss = 0.12076638\n",
      "Iteration 211, loss = 0.12056058\n",
      "Iteration 212, loss = 0.12035700\n",
      "Iteration 213, loss = 0.12015562\n",
      "Iteration 214, loss = 0.11995637\n",
      "Iteration 215, loss = 0.11975921\n",
      "Iteration 216, loss = 0.11956412\n",
      "Iteration 217, loss = 0.11937109\n",
      "Iteration 218, loss = 0.11918010\n",
      "Iteration 219, loss = 0.11899109\n",
      "Iteration 220, loss = 0.11880404\n",
      "Iteration 221, loss = 0.11861893\n",
      "Iteration 222, loss = 0.11843572\n",
      "Iteration 223, loss = 0.11825438\n",
      "Iteration 224, loss = 0.11807488\n",
      "Iteration 225, loss = 0.11789720\n",
      "Iteration 226, loss = 0.11772132\n",
      "Iteration 227, loss = 0.11754719\n",
      "Iteration 228, loss = 0.11737481\n",
      "Iteration 229, loss = 0.11720414\n",
      "Iteration 230, loss = 0.11703517\n",
      "Iteration 231, loss = 0.11686786\n",
      "Iteration 232, loss = 0.11670218\n",
      "Iteration 233, loss = 0.11653812\n",
      "Iteration 234, loss = 0.11637566\n",
      "Iteration 235, loss = 0.11621475\n",
      "Iteration 236, loss = 0.11605538\n",
      "Iteration 237, loss = 0.11589752\n",
      "Iteration 238, loss = 0.11574118\n",
      "Iteration 239, loss = 0.11558631\n",
      "Iteration 240, loss = 0.11543286\n",
      "Iteration 241, loss = 0.11528081\n",
      "Iteration 242, loss = 0.11513018\n",
      "Iteration 243, loss = 0.11498091\n",
      "Iteration 244, loss = 0.11483302\n",
      "Iteration 245, loss = 0.11468648\n",
      "Iteration 246, loss = 0.11454128\n",
      "Iteration 247, loss = 0.11439739\n",
      "Iteration 248, loss = 0.11425481\n",
      "Iteration 249, loss = 0.11411351\n",
      "Iteration 250, loss = 0.11397348\n",
      "Iteration 251, loss = 0.11383471\n",
      "Iteration 252, loss = 0.11369718\n",
      "Iteration 253, loss = 0.11356085\n",
      "Iteration 254, loss = 0.11342573\n",
      "Iteration 255, loss = 0.11329180\n",
      "Iteration 256, loss = 0.11315905\n",
      "Iteration 257, loss = 0.11302746\n",
      "Iteration 258, loss = 0.11289702\n",
      "Iteration 259, loss = 0.11276770\n",
      "Iteration 260, loss = 0.11263949\n",
      "Iteration 261, loss = 0.11251239\n",
      "Iteration 262, loss = 0.11238636\n",
      "Iteration 263, loss = 0.11226140\n",
      "Iteration 264, loss = 0.11213747\n",
      "Iteration 265, loss = 0.11201456\n",
      "Iteration 266, loss = 0.11189270\n",
      "Iteration 267, loss = 0.11177185\n",
      "Iteration 268, loss = 0.11165204\n",
      "Iteration 269, loss = 0.11153329\n",
      "Iteration 270, loss = 0.11141549\n",
      "Iteration 271, loss = 0.11129866\n",
      "Iteration 272, loss = 0.11118282\n",
      "Iteration 273, loss = 0.11106797\n",
      "Iteration 274, loss = 0.11095406\n",
      "Iteration 275, loss = 0.11084116\n",
      "Iteration 276, loss = 0.11072917\n",
      "Iteration 277, loss = 0.11061791\n",
      "Iteration 278, loss = 0.11050789\n",
      "Iteration 279, loss = 0.11039895\n",
      "Iteration 280, loss = 0.11029088\n",
      "Iteration 281, loss = 0.11018368\n",
      "Iteration 282, loss = 0.11007732\n",
      "Iteration 283, loss = 0.10997180\n",
      "Iteration 284, loss = 0.10986611\n",
      "Iteration 285, loss = 0.10975862\n",
      "Iteration 286, loss = 0.10965105\n",
      "Iteration 287, loss = 0.10954348\n",
      "Iteration 288, loss = 0.10943603\n",
      "Iteration 289, loss = 0.10932886\n",
      "Iteration 290, loss = 0.10922200\n",
      "Iteration 291, loss = 0.10911535\n",
      "Iteration 292, loss = 0.10900957\n",
      "Iteration 293, loss = 0.10890431\n",
      "Iteration 294, loss = 0.10879947\n",
      "Iteration 295, loss = 0.10868541\n",
      "Iteration 296, loss = 0.10856089\n",
      "Iteration 297, loss = 0.10842784\n",
      "Iteration 298, loss = 0.10830549\n",
      "Iteration 299, loss = 0.10819423\n",
      "Iteration 300, loss = 0.10808425\n",
      "Iteration 301, loss = 0.10799125\n",
      "Iteration 302, loss = 0.10791003\n",
      "Iteration 303, loss = 0.10782948\n",
      "Iteration 304, loss = 0.10774620\n",
      "Iteration 305, loss = 0.10765830\n",
      "Iteration 306, loss = 0.10756553\n",
      "Iteration 307, loss = 0.10746952\n",
      "Iteration 308, loss = 0.10737593\n",
      "Iteration 309, loss = 0.10728336\n",
      "Iteration 310, loss = 0.10719124\n",
      "Iteration 311, loss = 0.10710347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.87334245\n",
      "Iteration 3, loss = 1.83059730\n",
      "Iteration 4, loss = 1.78895149\n",
      "Iteration 5, loss = 1.74844199\n",
      "Iteration 6, loss = 1.70887170\n",
      "Iteration 7, loss = 1.67011197\n",
      "Iteration 8, loss = 1.63220233\n",
      "Iteration 9, loss = 1.59505713\n",
      "Iteration 10, loss = 1.55863695\n",
      "Iteration 11, loss = 1.52301999\n",
      "Iteration 12, loss = 1.48820494\n",
      "Iteration 13, loss = 1.45421145\n",
      "Iteration 14, loss = 1.42107357\n",
      "Iteration 15, loss = 1.38885147\n",
      "Iteration 16, loss = 1.35753189\n",
      "Iteration 17, loss = 1.32715513\n",
      "Iteration 18, loss = 1.29780456\n",
      "Iteration 19, loss = 1.26955973\n",
      "Iteration 20, loss = 1.24247067\n",
      "Iteration 21, loss = 1.21659262\n",
      "Iteration 22, loss = 1.19197727\n",
      "Iteration 23, loss = 1.16867053\n",
      "Iteration 24, loss = 1.14675410\n",
      "Iteration 25, loss = 1.12620913\n",
      "Iteration 26, loss = 1.10707051\n",
      "Iteration 27, loss = 1.08932924\n",
      "Iteration 28, loss = 1.07296118\n",
      "Iteration 29, loss = 1.05791723\n",
      "Iteration 30, loss = 1.04413942\n",
      "Iteration 31, loss = 1.03152723\n",
      "Iteration 32, loss = 1.01999108\n",
      "Iteration 33, loss = 1.00942610\n",
      "Iteration 34, loss = 0.99967829\n",
      "Iteration 35, loss = 0.99062044\n",
      "Iteration 36, loss = 0.98212755\n",
      "Iteration 37, loss = 0.97407238\n",
      "Iteration 38, loss = 0.96635081\n",
      "Iteration 39, loss = 0.95880866\n",
      "Iteration 40, loss = 0.95135886\n",
      "Iteration 41, loss = 0.94390437\n",
      "Iteration 42, loss = 0.93638573\n",
      "Iteration 43, loss = 0.92879247\n",
      "Iteration 44, loss = 0.92113263\n",
      "Iteration 45, loss = 0.91351996\n",
      "Iteration 46, loss = 0.90592757\n",
      "Iteration 47, loss = 0.89837393\n",
      "Iteration 48, loss = 0.89089124\n",
      "Iteration 49, loss = 0.88343903\n",
      "Iteration 50, loss = 0.87613975\n",
      "Iteration 51, loss = 0.86891115\n",
      "Iteration 52, loss = 0.86170857\n",
      "Iteration 53, loss = 0.85455230\n",
      "Iteration 54, loss = 0.84743492\n",
      "Iteration 55, loss = 0.84036780\n",
      "Iteration 56, loss = 0.83339222\n",
      "Iteration 57, loss = 0.82658845\n",
      "Iteration 58, loss = 0.81997477\n",
      "Iteration 59, loss = 0.81361831\n",
      "Iteration 60, loss = 0.80752907\n",
      "Iteration 61, loss = 0.80164713\n",
      "Iteration 62, loss = 0.79597935\n",
      "Iteration 63, loss = 0.79059641\n",
      "Iteration 64, loss = 0.78538992\n",
      "Iteration 65, loss = 0.78033241\n",
      "Iteration 66, loss = 0.77539218\n",
      "Iteration 67, loss = 0.77060406\n",
      "Iteration 68, loss = 0.76590987\n",
      "Iteration 69, loss = 0.76127647\n",
      "Iteration 70, loss = 0.75672134\n",
      "Iteration 71, loss = 0.75222895\n",
      "Iteration 72, loss = 0.74778206\n",
      "Iteration 73, loss = 0.74335415\n",
      "Iteration 74, loss = 0.73894833\n",
      "Iteration 75, loss = 0.73456061\n",
      "Iteration 76, loss = 0.73020053\n",
      "Iteration 77, loss = 0.72587129\n",
      "Iteration 78, loss = 0.72157773\n",
      "Iteration 79, loss = 0.71732352\n",
      "Iteration 80, loss = 0.71310656\n",
      "Iteration 81, loss = 0.70892657\n",
      "Iteration 82, loss = 0.70478273\n",
      "Iteration 83, loss = 0.70067705\n",
      "Iteration 84, loss = 0.69661586\n",
      "Iteration 85, loss = 0.69259175\n",
      "Iteration 86, loss = 0.68861284\n",
      "Iteration 87, loss = 0.68467598\n",
      "Iteration 88, loss = 0.68077386\n",
      "Iteration 89, loss = 0.67691175\n",
      "Iteration 90, loss = 0.67309302\n",
      "Iteration 91, loss = 0.66932770\n",
      "Iteration 92, loss = 0.66559388\n",
      "Iteration 93, loss = 0.66189158\n",
      "Iteration 94, loss = 0.65822565\n",
      "Iteration 95, loss = 0.65459593\n",
      "Iteration 96, loss = 0.65100427\n",
      "Iteration 97, loss = 0.64744595\n",
      "Iteration 98, loss = 0.64392319\n",
      "Iteration 99, loss = 0.64043430\n",
      "Iteration 100, loss = 0.63697424\n",
      "Iteration 101, loss = 0.63354768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 102, loss = 0.63015503\n",
      "Iteration 103, loss = 0.62679706\n",
      "Iteration 104, loss = 0.62346683\n",
      "Iteration 105, loss = 0.62015244\n",
      "Iteration 106, loss = 0.61686336\n",
      "Iteration 107, loss = 0.61359929\n",
      "Iteration 108, loss = 0.61036564\n",
      "Iteration 109, loss = 0.60717014\n",
      "Iteration 110, loss = 0.60401279\n",
      "Iteration 111, loss = 0.60087755\n",
      "Iteration 112, loss = 0.59777103\n",
      "Iteration 113, loss = 0.59468789\n",
      "Iteration 114, loss = 0.59163945\n",
      "Iteration 115, loss = 0.58864700\n",
      "Iteration 116, loss = 0.58568883\n",
      "Iteration 117, loss = 0.58277211\n",
      "Iteration 118, loss = 0.57987733\n",
      "Iteration 119, loss = 0.57701081\n",
      "Iteration 120, loss = 0.57420560\n",
      "Iteration 121, loss = 0.57144271\n",
      "Iteration 122, loss = 0.56872770\n",
      "Iteration 123, loss = 0.56606409\n",
      "Iteration 124, loss = 0.56342948\n",
      "Iteration 125, loss = 0.56082662\n",
      "Iteration 126, loss = 0.55825474\n",
      "Iteration 127, loss = 0.55570244\n",
      "Iteration 128, loss = 0.55317569\n",
      "Iteration 129, loss = 0.55067489\n",
      "Iteration 130, loss = 0.54819988\n",
      "Iteration 131, loss = 0.54576076\n",
      "Iteration 132, loss = 0.54334362\n",
      "Iteration 133, loss = 0.54095492\n",
      "Iteration 134, loss = 0.53859166\n",
      "Iteration 135, loss = 0.53627186\n",
      "Iteration 136, loss = 0.53398109\n",
      "Iteration 137, loss = 0.53170900\n",
      "Iteration 138, loss = 0.52946657\n",
      "Iteration 139, loss = 0.52725158\n",
      "Iteration 140, loss = 0.52507096\n",
      "Iteration 141, loss = 0.52291270\n",
      "Iteration 142, loss = 0.52077563\n",
      "Iteration 143, loss = 0.51866607\n",
      "Iteration 144, loss = 0.51657778\n",
      "Iteration 145, loss = 0.51451110\n",
      "Iteration 146, loss = 0.51246629\n",
      "Iteration 147, loss = 0.51044650\n",
      "Iteration 148, loss = 0.50844622\n",
      "Iteration 149, loss = 0.50646447\n",
      "Iteration 150, loss = 0.50450166\n",
      "Iteration 151, loss = 0.50255722\n",
      "Iteration 152, loss = 0.50063118\n",
      "Iteration 153, loss = 0.49872605\n",
      "Iteration 154, loss = 0.49683773\n",
      "Iteration 155, loss = 0.49496593\n",
      "Iteration 156, loss = 0.49311062\n",
      "Iteration 157, loss = 0.49127141\n",
      "Iteration 158, loss = 0.48944810\n",
      "Iteration 159, loss = 0.48764048\n",
      "Iteration 160, loss = 0.48584913\n",
      "Iteration 161, loss = 0.48407350\n",
      "Iteration 162, loss = 0.48231272\n",
      "Iteration 163, loss = 0.48056646\n",
      "Iteration 164, loss = 0.47883451\n",
      "Iteration 165, loss = 0.47711652\n",
      "Iteration 166, loss = 0.47541216\n",
      "Iteration 167, loss = 0.47372115\n",
      "Iteration 168, loss = 0.47204339\n",
      "Iteration 169, loss = 0.47038040\n",
      "Iteration 170, loss = 0.46873125\n",
      "Iteration 171, loss = 0.46709524\n",
      "Iteration 172, loss = 0.46547166\n",
      "Iteration 173, loss = 0.46386032\n",
      "Iteration 174, loss = 0.46226100\n",
      "Iteration 175, loss = 0.46067351\n",
      "Iteration 176, loss = 0.45909765\n",
      "Iteration 177, loss = 0.45753310\n",
      "Iteration 178, loss = 0.45597965\n",
      "Iteration 179, loss = 0.45443751\n",
      "Iteration 180, loss = 0.45290668\n",
      "Iteration 181, loss = 0.45138694\n",
      "Iteration 182, loss = 0.44987807\n",
      "Iteration 183, loss = 0.44837950\n",
      "Iteration 184, loss = 0.44689145\n",
      "Iteration 185, loss = 0.44541361\n",
      "Iteration 186, loss = 0.44394546\n",
      "Iteration 187, loss = 0.44248692\n",
      "Iteration 188, loss = 0.44103789\n",
      "Iteration 189, loss = 0.43959834\n",
      "Iteration 190, loss = 0.43816788\n",
      "Iteration 191, loss = 0.43674665\n",
      "Iteration 192, loss = 0.43533445\n",
      "Iteration 193, loss = 0.43393113\n",
      "Iteration 194, loss = 0.43253657\n",
      "Iteration 195, loss = 0.43115098\n",
      "Iteration 196, loss = 0.42977375\n",
      "Iteration 197, loss = 0.42840457\n",
      "Iteration 198, loss = 0.42704344\n",
      "Iteration 199, loss = 0.42569068\n",
      "Iteration 200, loss = 0.42434575\n",
      "Iteration 201, loss = 0.42300848\n",
      "Iteration 202, loss = 0.42167900\n",
      "Iteration 203, loss = 0.42035706\n",
      "Iteration 204, loss = 0.41904252\n",
      "Iteration 205, loss = 0.41773527\n",
      "Iteration 206, loss = 0.41643531\n",
      "Iteration 207, loss = 0.41514288\n",
      "Iteration 208, loss = 0.41385740\n",
      "Iteration 209, loss = 0.41257875\n",
      "Iteration 210, loss = 0.41130684\n",
      "Iteration 211, loss = 0.41004156\n",
      "Iteration 212, loss = 0.40878281\n",
      "Iteration 213, loss = 0.40753047\n",
      "Iteration 214, loss = 0.40628446\n",
      "Iteration 215, loss = 0.40504468\n",
      "Iteration 216, loss = 0.40381101\n",
      "Iteration 217, loss = 0.40258336\n",
      "Iteration 218, loss = 0.40136144\n",
      "Iteration 219, loss = 0.40014530\n",
      "Iteration 220, loss = 0.39893489\n",
      "Iteration 221, loss = 0.39773109\n",
      "Iteration 222, loss = 0.39653301\n",
      "Iteration 223, loss = 0.39534065\n",
      "Iteration 224, loss = 0.39415383\n",
      "Iteration 225, loss = 0.39297248\n",
      "Iteration 226, loss = 0.39179652\n",
      "Iteration 227, loss = 0.39062588\n",
      "Iteration 228, loss = 0.38946050\n",
      "Iteration 229, loss = 0.38830030\n",
      "Iteration 230, loss = 0.38714539\n",
      "Iteration 231, loss = 0.38599615\n",
      "Iteration 232, loss = 0.38485179\n",
      "Iteration 233, loss = 0.38371246\n",
      "Iteration 234, loss = 0.38257811\n",
      "Iteration 235, loss = 0.38144877\n",
      "Iteration 236, loss = 0.38032480\n",
      "Iteration 237, loss = 0.37920592\n",
      "Iteration 238, loss = 0.37809338\n",
      "Iteration 239, loss = 0.37698559\n",
      "Iteration 240, loss = 0.37588240\n",
      "Iteration 241, loss = 0.37478372\n",
      "Iteration 242, loss = 0.37368952\n",
      "Iteration 243, loss = 0.37260001\n",
      "Iteration 244, loss = 0.37151508\n",
      "Iteration 245, loss = 0.37043467\n",
      "Iteration 246, loss = 0.36935873\n",
      "Iteration 247, loss = 0.36828724\n",
      "Iteration 248, loss = 0.36722016\n",
      "Iteration 249, loss = 0.36615746\n",
      "Iteration 250, loss = 0.36509911\n",
      "Iteration 251, loss = 0.36404507\n",
      "Iteration 252, loss = 0.36299533\n",
      "Iteration 253, loss = 0.36194984\n",
      "Iteration 254, loss = 0.36090856\n",
      "Iteration 255, loss = 0.35987147\n",
      "Iteration 256, loss = 0.35883853\n",
      "Iteration 257, loss = 0.35780969\n",
      "Iteration 258, loss = 0.35678493\n",
      "Iteration 259, loss = 0.35576420\n",
      "Iteration 260, loss = 0.35474750\n",
      "Iteration 261, loss = 0.35373500\n",
      "Iteration 262, loss = 0.35272639\n",
      "Iteration 263, loss = 0.35172165\n",
      "Iteration 264, loss = 0.35072086\n",
      "Iteration 265, loss = 0.34972367\n",
      "Iteration 266, loss = 0.34873024\n",
      "Iteration 267, loss = 0.34774056\n",
      "Iteration 268, loss = 0.34675462\n",
      "Iteration 269, loss = 0.34577236\n",
      "Iteration 270, loss = 0.34479394\n",
      "Iteration 271, loss = 0.34381919\n",
      "Iteration 272, loss = 0.34284803\n",
      "Iteration 273, loss = 0.34188045\n",
      "Iteration 274, loss = 0.34091644\n",
      "Iteration 275, loss = 0.33995616\n",
      "Iteration 276, loss = 0.33899950\n",
      "Iteration 277, loss = 0.33804634\n",
      "Iteration 278, loss = 0.33709676\n",
      "Iteration 279, loss = 0.33615068\n",
      "Iteration 280, loss = 0.33520803\n",
      "Iteration 281, loss = 0.33426880\n",
      "Iteration 282, loss = 0.33333305\n",
      "Iteration 283, loss = 0.33240075\n",
      "Iteration 284, loss = 0.33147186\n",
      "Iteration 285, loss = 0.33054645\n",
      "Iteration 286, loss = 0.32962442\n",
      "Iteration 287, loss = 0.32870579\n",
      "Iteration 288, loss = 0.32779051\n",
      "Iteration 289, loss = 0.32687860\n",
      "Iteration 290, loss = 0.32597007\n",
      "Iteration 291, loss = 0.32506490\n",
      "Iteration 292, loss = 0.32416292\n",
      "Iteration 293, loss = 0.32326425\n",
      "Iteration 294, loss = 0.32236881\n",
      "Iteration 295, loss = 0.32147658\n",
      "Iteration 296, loss = 0.32058760\n",
      "Iteration 297, loss = 0.31970183\n",
      "Iteration 298, loss = 0.31881921\n",
      "Iteration 299, loss = 0.31793973\n",
      "Iteration 300, loss = 0.31706354\n",
      "Iteration 301, loss = 0.31619048\n",
      "Iteration 302, loss = 0.31532052\n",
      "Iteration 303, loss = 0.31445367\n",
      "Iteration 304, loss = 0.31358991\n",
      "Iteration 305, loss = 0.31272923\n",
      "Iteration 306, loss = 0.31187172\n",
      "Iteration 307, loss = 0.31101727\n",
      "Iteration 308, loss = 0.31016681\n",
      "Iteration 309, loss = 0.30931966\n",
      "Iteration 310, loss = 0.30847531\n",
      "Iteration 311, loss = 0.30763359\n",
      "Iteration 312, loss = 0.30679476\n",
      "Iteration 313, loss = 0.30595879\n",
      "Iteration 314, loss = 0.30512572\n",
      "Iteration 315, loss = 0.30429554\n",
      "Iteration 316, loss = 0.30346906\n",
      "Iteration 317, loss = 0.30264571\n",
      "Iteration 318, loss = 0.30182532\n",
      "Iteration 319, loss = 0.30100775\n",
      "Iteration 320, loss = 0.30019299\n",
      "Iteration 321, loss = 0.29938104\n",
      "Iteration 322, loss = 0.29857190\n",
      "Iteration 323, loss = 0.29776558\n",
      "Iteration 324, loss = 0.29696208\n",
      "Iteration 325, loss = 0.29616225\n",
      "Iteration 326, loss = 0.29536526\n",
      "Iteration 327, loss = 0.29457098\n",
      "Iteration 328, loss = 0.29377944\n",
      "Iteration 329, loss = 0.29299068\n",
      "Iteration 330, loss = 0.29220444\n",
      "Iteration 331, loss = 0.29142086\n",
      "Iteration 332, loss = 0.29064010\n",
      "Iteration 333, loss = 0.28986261\n",
      "Iteration 334, loss = 0.28908777\n",
      "Iteration 335, loss = 0.28831561\n",
      "Iteration 336, loss = 0.28754610\n",
      "Iteration 337, loss = 0.28677979\n",
      "Iteration 338, loss = 0.28601616\n",
      "Iteration 339, loss = 0.28525519\n",
      "Iteration 340, loss = 0.28449693\n",
      "Iteration 341, loss = 0.28374132\n",
      "Iteration 342, loss = 0.28298837\n",
      "Iteration 343, loss = 0.28223871\n",
      "Iteration 344, loss = 0.28149167\n",
      "Iteration 345, loss = 0.28074751\n",
      "Iteration 346, loss = 0.28000605\n",
      "Iteration 347, loss = 0.27926710\n",
      "Iteration 348, loss = 0.27853115\n",
      "Iteration 349, loss = 0.27779797\n",
      "Iteration 350, loss = 0.27706737\n",
      "Iteration 351, loss = 0.27633940\n",
      "Iteration 352, loss = 0.27561408\n",
      "Iteration 353, loss = 0.27489133\n",
      "Iteration 354, loss = 0.27417117\n",
      "Iteration 355, loss = 0.27345432\n",
      "Iteration 356, loss = 0.27273998\n",
      "Iteration 357, loss = 0.27202813\n",
      "Iteration 358, loss = 0.27131984\n",
      "Iteration 359, loss = 0.27061288\n",
      "Iteration 360, loss = 0.26990883\n",
      "Iteration 361, loss = 0.26920798\n",
      "Iteration 362, loss = 0.26850961\n",
      "Iteration 363, loss = 0.26781374\n",
      "Iteration 364, loss = 0.26712039\n",
      "Iteration 365, loss = 0.26642963\n",
      "Iteration 366, loss = 0.26574145\n",
      "Iteration 367, loss = 0.26505621\n",
      "Iteration 368, loss = 0.26437368\n",
      "Iteration 369, loss = 0.26369364\n",
      "Iteration 370, loss = 0.26301599\n",
      "Iteration 371, loss = 0.26234108\n",
      "Iteration 372, loss = 0.26166884\n",
      "Iteration 373, loss = 0.26099907\n",
      "Iteration 374, loss = 0.26033181\n",
      "Iteration 375, loss = 0.25966703\n",
      "Iteration 376, loss = 0.25900474\n",
      "Iteration 377, loss = 0.25834534\n",
      "Iteration 378, loss = 0.25768836\n",
      "Iteration 379, loss = 0.25703370\n",
      "Iteration 380, loss = 0.25638142\n",
      "Iteration 381, loss = 0.25573193\n",
      "Iteration 382, loss = 0.25508468\n",
      "Iteration 383, loss = 0.25444035\n",
      "Iteration 384, loss = 0.25379875\n",
      "Iteration 385, loss = 0.25315905\n",
      "Iteration 386, loss = 0.25252136\n",
      "Iteration 387, loss = 0.25188667\n",
      "Iteration 388, loss = 0.25125496\n",
      "Iteration 389, loss = 0.25062537\n",
      "Iteration 390, loss = 0.24999798\n",
      "Iteration 391, loss = 0.24937260\n",
      "Iteration 392, loss = 0.24875002\n",
      "Iteration 393, loss = 0.24813015\n",
      "Iteration 394, loss = 0.24751189\n",
      "Iteration 395, loss = 0.24689455\n",
      "Iteration 396, loss = 0.24627881\n",
      "Iteration 397, loss = 0.24566363\n",
      "Iteration 398, loss = 0.24504410\n",
      "Iteration 399, loss = 0.24442715\n",
      "Iteration 400, loss = 0.24380938\n",
      "Iteration 401, loss = 0.24318885\n",
      "Iteration 402, loss = 0.24256694\n",
      "Iteration 403, loss = 0.24194934\n",
      "Iteration 404, loss = 0.24132846\n",
      "Iteration 405, loss = 0.24070718\n",
      "Iteration 406, loss = 0.24008685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 407, loss = 0.23946485\n",
      "Iteration 408, loss = 0.23884124\n",
      "Iteration 409, loss = 0.23821639\n",
      "Iteration 410, loss = 0.23759036\n",
      "Iteration 411, loss = 0.23696421\n",
      "Iteration 412, loss = 0.23633655\n",
      "Iteration 413, loss = 0.23570833\n",
      "Iteration 414, loss = 0.23508040\n",
      "Iteration 415, loss = 0.23445077\n",
      "Iteration 416, loss = 0.23381958\n",
      "Iteration 417, loss = 0.23318977\n",
      "Iteration 418, loss = 0.23255874\n",
      "Iteration 419, loss = 0.23192741\n",
      "Iteration 420, loss = 0.23129722\n",
      "Iteration 421, loss = 0.23066581\n",
      "Iteration 422, loss = 0.23003418\n",
      "Iteration 423, loss = 0.22940306\n",
      "Iteration 424, loss = 0.22877228\n",
      "Iteration 425, loss = 0.22814133\n",
      "Iteration 426, loss = 0.22751116\n",
      "Iteration 427, loss = 0.22688144\n",
      "Iteration 428, loss = 0.22625211\n",
      "Iteration 429, loss = 0.22562340\n",
      "Iteration 430, loss = 0.22499562\n",
      "Iteration 431, loss = 0.22436868\n",
      "Iteration 432, loss = 0.22374251\n",
      "Iteration 433, loss = 0.22311757\n",
      "Iteration 434, loss = 0.22249372\n",
      "Iteration 435, loss = 0.22187092\n",
      "Iteration 436, loss = 0.22124946\n",
      "Iteration 437, loss = 0.22062934\n",
      "Iteration 438, loss = 0.22001074\n",
      "Iteration 439, loss = 0.21939365\n",
      "Iteration 440, loss = 0.21877786\n",
      "Iteration 441, loss = 0.21816401\n",
      "Iteration 442, loss = 0.21755196\n",
      "Iteration 443, loss = 0.21694173\n",
      "Iteration 444, loss = 0.21633338\n",
      "Iteration 445, loss = 0.21572698\n",
      "Iteration 446, loss = 0.21512307\n",
      "Iteration 447, loss = 0.21452091\n",
      "Iteration 448, loss = 0.21392073\n",
      "Iteration 449, loss = 0.21332312\n",
      "Iteration 450, loss = 0.21272774\n",
      "Iteration 451, loss = 0.21213460\n",
      "Iteration 452, loss = 0.21154384\n",
      "Iteration 453, loss = 0.21095559\n",
      "Iteration 454, loss = 0.21036975\n",
      "Iteration 455, loss = 0.20978641\n",
      "Iteration 456, loss = 0.20920573\n",
      "Iteration 457, loss = 0.20862749\n",
      "Iteration 458, loss = 0.20805179\n",
      "Iteration 459, loss = 0.20747866\n",
      "Iteration 460, loss = 0.20690817\n",
      "Iteration 461, loss = 0.20634038\n",
      "Iteration 462, loss = 0.20577520\n",
      "Iteration 463, loss = 0.20521260\n",
      "Iteration 464, loss = 0.20465286\n",
      "Iteration 465, loss = 0.20409580\n",
      "Iteration 466, loss = 0.20354151\n",
      "Iteration 467, loss = 0.20299030\n",
      "Iteration 468, loss = 0.20244242\n",
      "Iteration 469, loss = 0.20189726\n",
      "Iteration 470, loss = 0.20135496\n",
      "Iteration 471, loss = 0.20081551\n",
      "Iteration 472, loss = 0.20027872\n",
      "Iteration 473, loss = 0.19974490\n",
      "Iteration 474, loss = 0.19921387\n",
      "Iteration 475, loss = 0.19868580\n",
      "Iteration 476, loss = 0.19816084\n",
      "Iteration 477, loss = 0.19763849\n",
      "Iteration 478, loss = 0.19711897\n",
      "Iteration 479, loss = 0.19660249\n",
      "Iteration 480, loss = 0.19608852\n",
      "Iteration 481, loss = 0.19557718\n",
      "Iteration 482, loss = 0.19506905\n",
      "Iteration 483, loss = 0.19456373\n",
      "Iteration 484, loss = 0.19406071\n",
      "Iteration 485, loss = 0.19356081\n",
      "Iteration 486, loss = 0.19306393\n",
      "Iteration 487, loss = 0.19256974\n",
      "Iteration 488, loss = 0.19207796\n",
      "Iteration 489, loss = 0.19158977\n",
      "Iteration 490, loss = 0.19110391\n",
      "Iteration 491, loss = 0.19062030\n",
      "Iteration 492, loss = 0.19013957\n",
      "Iteration 493, loss = 0.18966231\n",
      "Iteration 494, loss = 0.18918696\n",
      "Iteration 495, loss = 0.18871475\n",
      "Iteration 496, loss = 0.18824552\n",
      "Iteration 497, loss = 0.18777858\n",
      "Iteration 498, loss = 0.18731405\n",
      "Iteration 499, loss = 0.18685244\n",
      "Iteration 500, loss = 0.18639388\n",
      "Iteration 501, loss = 0.18593802\n",
      "Iteration 502, loss = 0.18548460\n",
      "Iteration 503, loss = 0.18503396\n",
      "Iteration 504, loss = 0.18458591\n",
      "Iteration 505, loss = 0.18414007\n",
      "Iteration 506, loss = 0.18369685\n",
      "Iteration 507, loss = 0.18325601\n",
      "Iteration 508, loss = 0.18281774\n",
      "Iteration 509, loss = 0.18238172\n",
      "Iteration 510, loss = 0.18194792\n",
      "Iteration 511, loss = 0.18151651\n",
      "Iteration 512, loss = 0.18108735\n",
      "Iteration 513, loss = 0.18066054\n",
      "Iteration 514, loss = 0.18023605\n",
      "Iteration 515, loss = 0.17981363\n",
      "Iteration 516, loss = 0.17939371\n",
      "Iteration 517, loss = 0.17897611\n",
      "Iteration 518, loss = 0.17856088\n",
      "Iteration 519, loss = 0.17814755\n",
      "Iteration 520, loss = 0.17773665\n",
      "Iteration 521, loss = 0.17732803\n",
      "Iteration 522, loss = 0.17692112\n",
      "Iteration 523, loss = 0.17651638\n",
      "Iteration 524, loss = 0.17611377\n",
      "Iteration 525, loss = 0.17571347\n",
      "Iteration 526, loss = 0.17531545\n",
      "Iteration 527, loss = 0.17491962\n",
      "Iteration 528, loss = 0.17452541\n",
      "Iteration 529, loss = 0.17413301\n",
      "Iteration 530, loss = 0.17374269\n",
      "Iteration 531, loss = 0.17335453\n",
      "Iteration 532, loss = 0.17296802\n",
      "Iteration 533, loss = 0.17258304\n",
      "Iteration 534, loss = 0.17219974\n",
      "Iteration 535, loss = 0.17181830\n",
      "Iteration 536, loss = 0.17143886\n",
      "Iteration 537, loss = 0.17106259\n",
      "Iteration 538, loss = 0.17068830\n",
      "Iteration 539, loss = 0.17031635\n",
      "Iteration 540, loss = 0.16994668\n",
      "Iteration 541, loss = 0.16957906\n",
      "Iteration 542, loss = 0.16921345\n",
      "Iteration 543, loss = 0.16885007\n",
      "Iteration 544, loss = 0.16848875\n",
      "Iteration 545, loss = 0.16812945\n",
      "Iteration 546, loss = 0.16777218\n",
      "Iteration 547, loss = 0.16741692\n",
      "Iteration 548, loss = 0.16706364\n",
      "Iteration 549, loss = 0.16671261\n",
      "Iteration 550, loss = 0.16636351\n",
      "Iteration 551, loss = 0.16601667\n",
      "Iteration 552, loss = 0.16567178\n",
      "Iteration 553, loss = 0.16532837\n",
      "Iteration 554, loss = 0.16498703\n",
      "Iteration 555, loss = 0.16464755\n",
      "Iteration 556, loss = 0.16431022\n",
      "Iteration 557, loss = 0.16397505\n",
      "Iteration 558, loss = 0.16364226\n",
      "Iteration 559, loss = 0.16331209\n",
      "Iteration 560, loss = 0.16298391\n",
      "Iteration 561, loss = 0.16265747\n",
      "Iteration 562, loss = 0.16233292\n",
      "Iteration 563, loss = 0.16201034\n",
      "Iteration 564, loss = 0.16168926\n",
      "Iteration 565, loss = 0.16136981\n",
      "Iteration 566, loss = 0.16105195\n",
      "Iteration 567, loss = 0.16073603\n",
      "Iteration 568, loss = 0.16042190\n",
      "Iteration 569, loss = 0.16010948\n",
      "Iteration 570, loss = 0.15979875\n",
      "Iteration 571, loss = 0.15948971\n",
      "Iteration 572, loss = 0.15918249\n",
      "Iteration 573, loss = 0.15887698\n",
      "Iteration 574, loss = 0.15857449\n",
      "Iteration 575, loss = 0.15827378\n",
      "Iteration 576, loss = 0.15797477\n",
      "Iteration 577, loss = 0.15767763\n",
      "Iteration 578, loss = 0.15738216\n",
      "Iteration 579, loss = 0.15708833\n",
      "Iteration 580, loss = 0.15679611\n",
      "Iteration 581, loss = 0.15650533\n",
      "Iteration 582, loss = 0.15621596\n",
      "Iteration 583, loss = 0.15592816\n",
      "Iteration 584, loss = 0.15564202\n",
      "Iteration 585, loss = 0.15535760\n",
      "Iteration 586, loss = 0.15507481\n",
      "Iteration 587, loss = 0.15479360\n",
      "Iteration 588, loss = 0.15451388\n",
      "Iteration 589, loss = 0.15423563\n",
      "Iteration 590, loss = 0.15395893\n",
      "Iteration 591, loss = 0.15368375\n",
      "Iteration 592, loss = 0.15341005\n",
      "Iteration 593, loss = 0.15313821\n",
      "Iteration 594, loss = 0.15286779\n",
      "Iteration 595, loss = 0.15259864\n",
      "Iteration 596, loss = 0.15233091\n",
      "Iteration 597, loss = 0.15206461\n",
      "Iteration 598, loss = 0.15179968\n",
      "Iteration 599, loss = 0.15153621\n",
      "Iteration 600, loss = 0.15127426\n",
      "Iteration 601, loss = 0.15101365\n",
      "Iteration 602, loss = 0.15075432\n",
      "Iteration 603, loss = 0.15049615\n",
      "Iteration 604, loss = 0.15023939\n",
      "Iteration 605, loss = 0.14998394\n",
      "Iteration 606, loss = 0.14972985\n",
      "Iteration 607, loss = 0.14947705\n",
      "Iteration 608, loss = 0.14922563\n",
      "Iteration 609, loss = 0.14897547\n",
      "Iteration 610, loss = 0.14872658\n",
      "Iteration 611, loss = 0.14847895\n",
      "Iteration 612, loss = 0.14823256\n",
      "Iteration 613, loss = 0.14798740\n",
      "Iteration 614, loss = 0.14774346\n",
      "Iteration 615, loss = 0.14750073\n",
      "Iteration 616, loss = 0.14725911\n",
      "Iteration 617, loss = 0.14701848\n",
      "Iteration 618, loss = 0.14677901\n",
      "Iteration 619, loss = 0.14654068\n",
      "Iteration 620, loss = 0.14630366\n",
      "Iteration 621, loss = 0.14606767\n",
      "Iteration 622, loss = 0.14583270\n",
      "Iteration 623, loss = 0.14559894\n",
      "Iteration 624, loss = 0.14536638\n",
      "Iteration 625, loss = 0.14513493\n",
      "Iteration 626, loss = 0.14490456\n",
      "Iteration 627, loss = 0.14467528\n",
      "Iteration 628, loss = 0.14444700\n",
      "Iteration 629, loss = 0.14421988\n",
      "Iteration 630, loss = 0.14399365\n",
      "Iteration 631, loss = 0.14376863\n",
      "Iteration 632, loss = 0.14354505\n",
      "Iteration 633, loss = 0.14332257\n",
      "Iteration 634, loss = 0.14310126\n",
      "Iteration 635, loss = 0.14288096\n",
      "Iteration 636, loss = 0.14266183\n",
      "Iteration 637, loss = 0.14244373\n",
      "Iteration 638, loss = 0.14222671\n",
      "Iteration 639, loss = 0.14201074\n",
      "Iteration 640, loss = 0.14179588\n",
      "Iteration 641, loss = 0.14158206\n",
      "Iteration 642, loss = 0.14136929\n",
      "Iteration 643, loss = 0.14115754\n",
      "Iteration 644, loss = 0.14094684\n",
      "Iteration 645, loss = 0.14073711\n",
      "Iteration 646, loss = 0.14052840\n",
      "Iteration 647, loss = 0.14032069\n",
      "Iteration 648, loss = 0.14011395\n",
      "Iteration 649, loss = 0.13990820\n",
      "Iteration 650, loss = 0.13970341\n",
      "Iteration 651, loss = 0.13949959\n",
      "Iteration 652, loss = 0.13929674\n",
      "Iteration 653, loss = 0.13909483\n",
      "Iteration 654, loss = 0.13889388\n",
      "Iteration 655, loss = 0.13869386\n",
      "Iteration 656, loss = 0.13849481\n",
      "Iteration 657, loss = 0.13829666\n",
      "Iteration 658, loss = 0.13809941\n",
      "Iteration 659, loss = 0.13790302\n",
      "Iteration 660, loss = 0.13770752\n",
      "Iteration 661, loss = 0.13751285\n",
      "Iteration 662, loss = 0.13731911\n",
      "Iteration 663, loss = 0.13712612\n",
      "Iteration 664, loss = 0.13693440\n",
      "Iteration 665, loss = 0.13674337\n",
      "Iteration 666, loss = 0.13655273\n",
      "Iteration 667, loss = 0.13636241\n",
      "Iteration 668, loss = 0.13617308\n",
      "Iteration 669, loss = 0.13598705\n",
      "Iteration 670, loss = 0.13579793\n",
      "Iteration 671, loss = 0.13561379\n",
      "Iteration 672, loss = 0.13542958\n",
      "Iteration 673, loss = 0.13524501\n",
      "Iteration 674, loss = 0.13505987\n",
      "Iteration 675, loss = 0.13487405\n",
      "Iteration 676, loss = 0.13468795\n",
      "Iteration 677, loss = 0.13450517\n",
      "Iteration 678, loss = 0.13432503\n",
      "Iteration 679, loss = 0.13414130\n",
      "Iteration 680, loss = 0.13396314\n",
      "Iteration 681, loss = 0.13378509\n",
      "Iteration 682, loss = 0.13360561\n",
      "Iteration 683, loss = 0.13342476\n",
      "Iteration 684, loss = 0.13324364\n",
      "Iteration 685, loss = 0.13306429\n",
      "Iteration 686, loss = 0.13289244\n",
      "Iteration 687, loss = 0.13271050\n",
      "Iteration 688, loss = 0.13253443\n",
      "Iteration 689, loss = 0.13236066\n",
      "Iteration 690, loss = 0.13218665\n",
      "Iteration 691, loss = 0.13201171\n",
      "Iteration 692, loss = 0.13183615\n",
      "Iteration 693, loss = 0.13166124\n",
      "Iteration 694, loss = 0.13148747\n",
      "Iteration 695, loss = 0.13131669\n",
      "Iteration 696, loss = 0.13114388\n",
      "Iteration 697, loss = 0.13097096\n",
      "Iteration 698, loss = 0.13080068\n",
      "Iteration 699, loss = 0.13063043\n",
      "Iteration 700, loss = 0.13046003\n",
      "Iteration 701, loss = 0.13028963\n",
      "Iteration 702, loss = 0.13011987\n",
      "Iteration 703, loss = 0.12995133\n",
      "Iteration 704, loss = 0.12978258\n",
      "Iteration 705, loss = 0.12961489\n",
      "Iteration 706, loss = 0.12944770\n",
      "Iteration 707, loss = 0.12928070\n",
      "Iteration 708, loss = 0.12911390\n",
      "Iteration 709, loss = 0.12894735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 710, loss = 0.12878270\n",
      "Iteration 711, loss = 0.12861768\n",
      "Iteration 712, loss = 0.12845212\n",
      "Iteration 713, loss = 0.12828786\n",
      "Iteration 714, loss = 0.12812461\n",
      "Iteration 715, loss = 0.12796155\n",
      "Iteration 716, loss = 0.12779836\n",
      "Iteration 717, loss = 0.12763554\n",
      "Iteration 718, loss = 0.12747459\n",
      "Iteration 719, loss = 0.12731369\n",
      "Iteration 720, loss = 0.12715218\n",
      "Iteration 721, loss = 0.12699199\n",
      "Iteration 722, loss = 0.12683299\n",
      "Iteration 723, loss = 0.12667399\n",
      "Iteration 724, loss = 0.12651495\n",
      "Iteration 725, loss = 0.12635639\n",
      "Iteration 726, loss = 0.12619948\n",
      "Iteration 727, loss = 0.12604267\n",
      "Iteration 728, loss = 0.12588553\n",
      "Iteration 729, loss = 0.12573004\n",
      "Iteration 730, loss = 0.12557479\n",
      "Iteration 731, loss = 0.12541968\n",
      "Iteration 732, loss = 0.12526579\n",
      "Iteration 733, loss = 0.12511144\n",
      "Iteration 734, loss = 0.12495763\n",
      "Iteration 735, loss = 0.12480364\n",
      "Iteration 736, loss = 0.12464908\n",
      "Iteration 737, loss = 0.12449382\n",
      "Iteration 738, loss = 0.12433780\n",
      "Iteration 739, loss = 0.12418145\n",
      "Iteration 740, loss = 0.12402359\n",
      "Iteration 741, loss = 0.12386511\n",
      "Iteration 742, loss = 0.12370538\n",
      "Iteration 743, loss = 0.12354389\n",
      "Iteration 744, loss = 0.12338047\n",
      "Iteration 745, loss = 0.12321512\n",
      "Iteration 746, loss = 0.12304794\n",
      "Iteration 747, loss = 0.12287953\n",
      "Iteration 748, loss = 0.12270870\n",
      "Iteration 749, loss = 0.12253596\n",
      "Iteration 750, loss = 0.12236121\n",
      "Iteration 751, loss = 0.12218450\n",
      "Iteration 752, loss = 0.12200581\n",
      "Iteration 753, loss = 0.12182516\n",
      "Iteration 754, loss = 0.12164289\n",
      "Iteration 755, loss = 0.12145869\n",
      "Iteration 756, loss = 0.12127262\n",
      "Iteration 757, loss = 0.12108490\n",
      "Iteration 758, loss = 0.12089554\n",
      "Iteration 759, loss = 0.12070466\n",
      "Iteration 760, loss = 0.12051227\n",
      "Iteration 761, loss = 0.12031844\n",
      "Iteration 762, loss = 0.12012326\n",
      "Iteration 763, loss = 0.11992710\n",
      "Iteration 764, loss = 0.11972955\n",
      "Iteration 765, loss = 0.11953097\n",
      "Iteration 766, loss = 0.11933146\n",
      "Iteration 767, loss = 0.11913098\n",
      "Iteration 768, loss = 0.11892964\n",
      "Iteration 769, loss = 0.11872752\n",
      "Iteration 770, loss = 0.11852471\n",
      "Iteration 771, loss = 0.11832136\n",
      "Iteration 772, loss = 0.11811740\n",
      "Iteration 773, loss = 0.11791316\n",
      "Iteration 774, loss = 0.11770849\n",
      "Iteration 775, loss = 0.11750354\n",
      "Iteration 776, loss = 0.11729833\n",
      "Iteration 777, loss = 0.11709292\n",
      "Iteration 778, loss = 0.11688739\n",
      "Iteration 779, loss = 0.11668179\n",
      "Iteration 780, loss = 0.11647619\n",
      "Iteration 781, loss = 0.11627065\n",
      "Iteration 782, loss = 0.11606522\n",
      "Iteration 783, loss = 0.11585997\n",
      "Iteration 784, loss = 0.11565498\n",
      "Iteration 785, loss = 0.11545025\n",
      "Iteration 786, loss = 0.11524587\n",
      "Iteration 787, loss = 0.11504185\n",
      "Iteration 788, loss = 0.11483829\n",
      "Iteration 789, loss = 0.11463518\n",
      "Iteration 790, loss = 0.11443257\n",
      "Iteration 791, loss = 0.11423054\n",
      "Iteration 792, loss = 0.11402909\n",
      "Iteration 793, loss = 0.11382827\n",
      "Iteration 794, loss = 0.11362812\n",
      "Iteration 795, loss = 0.11342865\n",
      "Iteration 796, loss = 0.11322998\n",
      "Iteration 797, loss = 0.11303210\n",
      "Iteration 798, loss = 0.11283499\n",
      "Iteration 799, loss = 0.11263872\n",
      "Iteration 800, loss = 0.11244343\n",
      "Iteration 801, loss = 0.11224889\n",
      "Iteration 802, loss = 0.11205526\n",
      "Iteration 803, loss = 0.11186248\n",
      "Iteration 804, loss = 0.11167063\n",
      "Iteration 805, loss = 0.11147976\n",
      "Iteration 806, loss = 0.11128988\n",
      "Iteration 807, loss = 0.11110106\n",
      "Iteration 808, loss = 0.11091319\n",
      "Iteration 809, loss = 0.11072628\n",
      "Iteration 810, loss = 0.11054039\n",
      "Iteration 811, loss = 0.11035556\n",
      "Iteration 812, loss = 0.11017169\n",
      "Iteration 813, loss = 0.10998897\n",
      "Iteration 814, loss = 0.10980731\n",
      "Iteration 815, loss = 0.10962669\n",
      "Iteration 816, loss = 0.10944712\n",
      "Iteration 817, loss = 0.10926863\n",
      "Iteration 818, loss = 0.10909123\n",
      "Iteration 819, loss = 0.10891492\n",
      "Iteration 820, loss = 0.10873979\n",
      "Iteration 821, loss = 0.10856562\n",
      "Iteration 822, loss = 0.10839264\n",
      "Iteration 823, loss = 0.10822076\n",
      "Iteration 824, loss = 0.10805000\n",
      "Iteration 825, loss = 0.10788035\n",
      "Iteration 826, loss = 0.10771180\n",
      "Iteration 827, loss = 0.10754436\n",
      "Iteration 828, loss = 0.10737805\n",
      "Iteration 829, loss = 0.10721284\n",
      "Iteration 830, loss = 0.10704874\n",
      "Iteration 831, loss = 0.10688579\n",
      "Iteration 832, loss = 0.10672390\n",
      "Iteration 833, loss = 0.10656313\n",
      "Iteration 834, loss = 0.10640348\n",
      "Iteration 835, loss = 0.10624497\n",
      "Iteration 836, loss = 0.10608761\n",
      "Iteration 837, loss = 0.10593135\n",
      "Iteration 838, loss = 0.10577618\n",
      "Iteration 839, loss = 0.10562210\n",
      "Iteration 840, loss = 0.10546911\n",
      "Iteration 841, loss = 0.10531719\n",
      "Iteration 842, loss = 0.10516634\n",
      "Iteration 843, loss = 0.10501656\n",
      "Iteration 844, loss = 0.10486792\n",
      "Iteration 845, loss = 0.10472023\n",
      "Iteration 846, loss = 0.10457366\n",
      "Iteration 847, loss = 0.10442814\n",
      "Iteration 848, loss = 0.10428365\n",
      "Iteration 849, loss = 0.10414020\n",
      "Iteration 850, loss = 0.10399780\n",
      "Iteration 851, loss = 0.10385641\n",
      "Iteration 852, loss = 0.10371602\n",
      "Iteration 853, loss = 0.10357665\n",
      "Iteration 854, loss = 0.10343830\n",
      "Iteration 855, loss = 0.10330098\n",
      "Iteration 856, loss = 0.10316468\n",
      "Iteration 857, loss = 0.10302931\n",
      "Iteration 858, loss = 0.10289500\n",
      "Iteration 859, loss = 0.10276168\n",
      "Iteration 860, loss = 0.10262932\n",
      "Iteration 861, loss = 0.10249792\n",
      "Iteration 862, loss = 0.10236746\n",
      "Iteration 863, loss = 0.10223795\n",
      "Iteration 864, loss = 0.10210939\n",
      "Iteration 865, loss = 0.10198176\n",
      "Iteration 866, loss = 0.10185505\n",
      "Iteration 867, loss = 0.10172927\n",
      "Iteration 868, loss = 0.10160440\n",
      "Iteration 869, loss = 0.10148044\n",
      "Iteration 870, loss = 0.10135747\n",
      "Iteration 871, loss = 0.10123539\n",
      "Iteration 872, loss = 0.10111439\n",
      "Iteration 873, loss = 0.10099408\n",
      "Iteration 874, loss = 0.10087477\n",
      "Iteration 875, loss = 0.10075631\n",
      "Iteration 876, loss = 0.10063864\n",
      "Iteration 877, loss = 0.10052188\n",
      "Iteration 878, loss = 0.10040596\n",
      "Iteration 879, loss = 0.10029082\n",
      "Iteration 880, loss = 0.10017659\n",
      "Iteration 881, loss = 0.10006315\n",
      "Iteration 882, loss = 0.09995054\n",
      "Iteration 883, loss = 0.09983881\n",
      "Iteration 884, loss = 0.09972783\n",
      "Iteration 885, loss = 0.09961767\n",
      "Iteration 886, loss = 0.09950836\n",
      "Iteration 887, loss = 0.09939979\n",
      "Iteration 888, loss = 0.09929210\n",
      "Iteration 889, loss = 0.09918519\n",
      "Iteration 890, loss = 0.09907905\n",
      "Iteration 891, loss = 0.09897368\n",
      "Iteration 892, loss = 0.09886907\n",
      "Iteration 893, loss = 0.09876521\n",
      "Iteration 894, loss = 0.09866213\n",
      "Iteration 895, loss = 0.09855976\n",
      "Iteration 896, loss = 0.09845816\n",
      "Iteration 897, loss = 0.09835729\n",
      "Iteration 898, loss = 0.09825717\n",
      "Iteration 899, loss = 0.09815774\n",
      "Iteration 900, loss = 0.09805903\n",
      "Iteration 901, loss = 0.09796103\n",
      "Iteration 902, loss = 0.09786380\n",
      "Iteration 903, loss = 0.09776728\n",
      "Iteration 904, loss = 0.09767144\n",
      "Iteration 905, loss = 0.09757628\n",
      "Iteration 906, loss = 0.09748180\n",
      "Iteration 907, loss = 0.09738798\n",
      "Iteration 908, loss = 0.09729483\n",
      "Iteration 909, loss = 0.09720234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.88148122\n",
      "Iteration 3, loss = 1.83837827\n",
      "Iteration 4, loss = 1.79628362\n",
      "Iteration 5, loss = 1.75523019\n",
      "Iteration 6, loss = 1.71506751\n",
      "Iteration 7, loss = 1.67572137\n",
      "Iteration 8, loss = 1.63722320\n",
      "Iteration 9, loss = 1.59949272\n",
      "Iteration 10, loss = 1.56254813\n",
      "Iteration 11, loss = 1.52641572\n",
      "Iteration 12, loss = 1.49113087\n",
      "Iteration 13, loss = 1.45670356\n",
      "Iteration 14, loss = 1.42313988\n",
      "Iteration 15, loss = 1.39052110\n",
      "Iteration 16, loss = 1.35883948\n",
      "Iteration 17, loss = 1.32810859\n",
      "Iteration 18, loss = 1.29840192\n",
      "Iteration 19, loss = 1.26980437\n",
      "Iteration 20, loss = 1.24235491\n",
      "Iteration 21, loss = 1.21613247\n",
      "Iteration 22, loss = 1.19121166\n",
      "Iteration 23, loss = 1.16764370\n",
      "Iteration 24, loss = 1.14549120\n",
      "Iteration 25, loss = 1.12474202\n",
      "Iteration 26, loss = 1.10543115\n",
      "Iteration 27, loss = 1.08754993\n",
      "Iteration 28, loss = 1.07107021\n",
      "Iteration 29, loss = 1.05594721\n",
      "Iteration 30, loss = 1.04212449\n",
      "Iteration 31, loss = 1.02950265\n",
      "Iteration 32, loss = 1.01799437\n",
      "Iteration 33, loss = 1.00749752\n",
      "Iteration 34, loss = 0.99785490\n",
      "Iteration 35, loss = 0.98891799\n",
      "Iteration 36, loss = 0.98055688\n",
      "Iteration 37, loss = 0.97263658\n",
      "Iteration 38, loss = 0.96504376\n",
      "Iteration 39, loss = 0.95766065\n",
      "Iteration 40, loss = 0.95037629\n",
      "Iteration 41, loss = 0.94309053\n",
      "Iteration 42, loss = 0.93578678\n",
      "Iteration 43, loss = 0.92843973\n",
      "Iteration 44, loss = 0.92107514\n",
      "Iteration 45, loss = 0.91371982\n",
      "Iteration 46, loss = 0.90640126\n",
      "Iteration 47, loss = 0.89905454\n",
      "Iteration 48, loss = 0.89177467\n",
      "Iteration 49, loss = 0.88460110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.87742646\n",
      "Iteration 51, loss = 0.87027775\n",
      "Iteration 52, loss = 0.86314673\n",
      "Iteration 53, loss = 0.85604554\n",
      "Iteration 54, loss = 0.84894680\n",
      "Iteration 55, loss = 0.84190128\n",
      "Iteration 56, loss = 0.83495320\n",
      "Iteration 57, loss = 0.82817101\n",
      "Iteration 58, loss = 0.82155907\n",
      "Iteration 59, loss = 0.81519320\n",
      "Iteration 60, loss = 0.80905525\n",
      "Iteration 61, loss = 0.80312367\n",
      "Iteration 62, loss = 0.79736906\n",
      "Iteration 63, loss = 0.79189391\n",
      "Iteration 64, loss = 0.78658862\n",
      "Iteration 65, loss = 0.78149276\n",
      "Iteration 66, loss = 0.77656010\n",
      "Iteration 67, loss = 0.77183246\n",
      "Iteration 68, loss = 0.76719757\n",
      "Iteration 69, loss = 0.76264320\n",
      "Iteration 70, loss = 0.75818564\n",
      "Iteration 71, loss = 0.75376062\n",
      "Iteration 72, loss = 0.74934265\n",
      "Iteration 73, loss = 0.74494199\n",
      "Iteration 74, loss = 0.74055398\n",
      "Iteration 75, loss = 0.73618085\n",
      "Iteration 76, loss = 0.73182939\n",
      "Iteration 77, loss = 0.72750733\n",
      "Iteration 78, loss = 0.72321408\n",
      "Iteration 79, loss = 0.71895623\n",
      "Iteration 80, loss = 0.71474014\n",
      "Iteration 81, loss = 0.71056105\n",
      "Iteration 82, loss = 0.70641861\n",
      "Iteration 83, loss = 0.70232072\n",
      "Iteration 84, loss = 0.69825897\n",
      "Iteration 85, loss = 0.69423912\n",
      "Iteration 86, loss = 0.69025753\n",
      "Iteration 87, loss = 0.68630520\n",
      "Iteration 88, loss = 0.68238574\n",
      "Iteration 89, loss = 0.67849809\n",
      "Iteration 90, loss = 0.67464817\n",
      "Iteration 91, loss = 0.67082535\n",
      "Iteration 92, loss = 0.66703504\n",
      "Iteration 93, loss = 0.66327987\n",
      "Iteration 94, loss = 0.65956802\n",
      "Iteration 95, loss = 0.65589995\n",
      "Iteration 96, loss = 0.65227227\n",
      "Iteration 97, loss = 0.64868225\n",
      "Iteration 98, loss = 0.64512196\n",
      "Iteration 99, loss = 0.64159770\n",
      "Iteration 100, loss = 0.63810739\n",
      "Iteration 101, loss = 0.63464989\n",
      "Iteration 102, loss = 0.63122620\n",
      "Iteration 103, loss = 0.62782446\n",
      "Iteration 104, loss = 0.62444884\n",
      "Iteration 105, loss = 0.62110836\n",
      "Iteration 106, loss = 0.61779306\n",
      "Iteration 107, loss = 0.61450772\n",
      "Iteration 108, loss = 0.61125937\n",
      "Iteration 109, loss = 0.60804847\n",
      "Iteration 110, loss = 0.60486992\n",
      "Iteration 111, loss = 0.60170726\n",
      "Iteration 112, loss = 0.59856655\n",
      "Iteration 113, loss = 0.59548146\n",
      "Iteration 114, loss = 0.59243123\n",
      "Iteration 115, loss = 0.58942336\n",
      "Iteration 116, loss = 0.58644368\n",
      "Iteration 117, loss = 0.58349746\n",
      "Iteration 118, loss = 0.58062541\n",
      "Iteration 119, loss = 0.57779583\n",
      "Iteration 120, loss = 0.57501312\n",
      "Iteration 121, loss = 0.57227809\n",
      "Iteration 122, loss = 0.56957306\n",
      "Iteration 123, loss = 0.56688825\n",
      "Iteration 124, loss = 0.56424762\n",
      "Iteration 125, loss = 0.56162790\n",
      "Iteration 126, loss = 0.55903648\n",
      "Iteration 127, loss = 0.55647286\n",
      "Iteration 128, loss = 0.55393497\n",
      "Iteration 129, loss = 0.55142748\n",
      "Iteration 130, loss = 0.54895642\n",
      "Iteration 131, loss = 0.54650899\n",
      "Iteration 132, loss = 0.54408313\n",
      "Iteration 133, loss = 0.54168103\n",
      "Iteration 134, loss = 0.53931752\n",
      "Iteration 135, loss = 0.53699628\n",
      "Iteration 136, loss = 0.53469455\n",
      "Iteration 137, loss = 0.53241388\n",
      "Iteration 138, loss = 0.53016390\n",
      "Iteration 139, loss = 0.52794054\n",
      "Iteration 140, loss = 0.52574429\n",
      "Iteration 141, loss = 0.52357169\n",
      "Iteration 142, loss = 0.52142052\n",
      "Iteration 143, loss = 0.51929080\n",
      "Iteration 144, loss = 0.51718171\n",
      "Iteration 145, loss = 0.51509427\n",
      "Iteration 146, loss = 0.51302824\n",
      "Iteration 147, loss = 0.51098148\n",
      "Iteration 148, loss = 0.50895366\n",
      "Iteration 149, loss = 0.50694444\n",
      "Iteration 150, loss = 0.50495371\n",
      "Iteration 151, loss = 0.50298112\n",
      "Iteration 152, loss = 0.50102610\n",
      "Iteration 153, loss = 0.49908849\n",
      "Iteration 154, loss = 0.49716960\n",
      "Iteration 155, loss = 0.49526803\n",
      "Iteration 156, loss = 0.49338303\n",
      "Iteration 157, loss = 0.49151439\n",
      "Iteration 158, loss = 0.48966190\n",
      "Iteration 159, loss = 0.48782534\n",
      "Iteration 160, loss = 0.48600451\n",
      "Iteration 161, loss = 0.48419926\n",
      "Iteration 162, loss = 0.48240916\n",
      "Iteration 163, loss = 0.48063385\n",
      "Iteration 164, loss = 0.47887310\n",
      "Iteration 165, loss = 0.47712671\n",
      "Iteration 166, loss = 0.47539429\n",
      "Iteration 167, loss = 0.47367585\n",
      "Iteration 168, loss = 0.47197106\n",
      "Iteration 169, loss = 0.47027968\n",
      "Iteration 170, loss = 0.46860143\n",
      "Iteration 171, loss = 0.46693618\n",
      "Iteration 172, loss = 0.46528360\n",
      "Iteration 173, loss = 0.46364344\n",
      "Iteration 174, loss = 0.46201546\n",
      "Iteration 175, loss = 0.46039942\n",
      "Iteration 176, loss = 0.45879509\n",
      "Iteration 177, loss = 0.45720222\n",
      "Iteration 178, loss = 0.45562121\n",
      "Iteration 179, loss = 0.45405197\n",
      "Iteration 180, loss = 0.45249357\n",
      "Iteration 181, loss = 0.45094587\n",
      "Iteration 182, loss = 0.44940883\n",
      "Iteration 183, loss = 0.44788300\n",
      "Iteration 184, loss = 0.44636710\n",
      "Iteration 185, loss = 0.44486098\n",
      "Iteration 186, loss = 0.44336478\n",
      "Iteration 187, loss = 0.44187832\n",
      "Iteration 188, loss = 0.44040135\n",
      "Iteration 189, loss = 0.43893365\n",
      "Iteration 190, loss = 0.43747663\n",
      "Iteration 191, loss = 0.43602880\n",
      "Iteration 192, loss = 0.43458851\n",
      "Iteration 193, loss = 0.43315742\n",
      "Iteration 194, loss = 0.43173505\n",
      "Iteration 195, loss = 0.43032099\n",
      "Iteration 196, loss = 0.42891506\n",
      "Iteration 197, loss = 0.42751709\n",
      "Iteration 198, loss = 0.42612709\n",
      "Iteration 199, loss = 0.42474596\n",
      "Iteration 200, loss = 0.42337384\n",
      "Iteration 201, loss = 0.42200990\n",
      "Iteration 202, loss = 0.42065377\n",
      "Iteration 203, loss = 0.41930450\n",
      "Iteration 204, loss = 0.41796210\n",
      "Iteration 205, loss = 0.41662867\n",
      "Iteration 206, loss = 0.41530296\n",
      "Iteration 207, loss = 0.41398404\n",
      "Iteration 208, loss = 0.41267184\n",
      "Iteration 209, loss = 0.41136627\n",
      "Iteration 210, loss = 0.41006798\n",
      "Iteration 211, loss = 0.40877685\n",
      "Iteration 212, loss = 0.40749228\n",
      "Iteration 213, loss = 0.40621422\n",
      "Iteration 214, loss = 0.40494261\n",
      "Iteration 215, loss = 0.40367748\n",
      "Iteration 216, loss = 0.40241843\n",
      "Iteration 217, loss = 0.40116580\n",
      "Iteration 218, loss = 0.39991934\n",
      "Iteration 219, loss = 0.39867893\n",
      "Iteration 220, loss = 0.39744448\n",
      "Iteration 221, loss = 0.39621604\n",
      "Iteration 222, loss = 0.39499342\n",
      "Iteration 223, loss = 0.39377654\n",
      "Iteration 224, loss = 0.39256556\n",
      "Iteration 225, loss = 0.39136015\n",
      "Iteration 226, loss = 0.39016029\n",
      "Iteration 227, loss = 0.38896602\n",
      "Iteration 228, loss = 0.38777709\n",
      "Iteration 229, loss = 0.38659357\n",
      "Iteration 230, loss = 0.38541541\n",
      "Iteration 231, loss = 0.38424249\n",
      "Iteration 232, loss = 0.38307484\n",
      "Iteration 233, loss = 0.38191230\n",
      "Iteration 234, loss = 0.38075487\n",
      "Iteration 235, loss = 0.37960247\n",
      "Iteration 236, loss = 0.37845488\n",
      "Iteration 237, loss = 0.37731208\n",
      "Iteration 238, loss = 0.37617411\n",
      "Iteration 239, loss = 0.37504085\n",
      "Iteration 240, loss = 0.37391242\n",
      "Iteration 241, loss = 0.37278859\n",
      "Iteration 242, loss = 0.37166937\n",
      "Iteration 243, loss = 0.37055486\n",
      "Iteration 244, loss = 0.36944485\n",
      "Iteration 245, loss = 0.36833935\n",
      "Iteration 246, loss = 0.36723823\n",
      "Iteration 247, loss = 0.36614158\n",
      "Iteration 248, loss = 0.36504940\n",
      "Iteration 249, loss = 0.36396178\n",
      "Iteration 250, loss = 0.36287879\n",
      "Iteration 251, loss = 0.36179998\n",
      "Iteration 252, loss = 0.36072538\n",
      "Iteration 253, loss = 0.35965500\n",
      "Iteration 254, loss = 0.35858879\n",
      "Iteration 255, loss = 0.35752670\n",
      "Iteration 256, loss = 0.35646877\n",
      "Iteration 257, loss = 0.35541493\n",
      "Iteration 258, loss = 0.35436505\n",
      "Iteration 259, loss = 0.35331920\n",
      "Iteration 260, loss = 0.35227732\n",
      "Iteration 261, loss = 0.35123949\n",
      "Iteration 262, loss = 0.35020540\n",
      "Iteration 263, loss = 0.34917539\n",
      "Iteration 264, loss = 0.34814923\n",
      "Iteration 265, loss = 0.34712699\n",
      "Iteration 266, loss = 0.34610854\n",
      "Iteration 267, loss = 0.34509391\n",
      "Iteration 268, loss = 0.34408303\n",
      "Iteration 269, loss = 0.34307589\n",
      "Iteration 270, loss = 0.34207255\n",
      "Iteration 271, loss = 0.34107292\n",
      "Iteration 272, loss = 0.34007695\n",
      "Iteration 273, loss = 0.33908475\n",
      "Iteration 274, loss = 0.33809635\n",
      "Iteration 275, loss = 0.33711155\n",
      "Iteration 276, loss = 0.33613026\n",
      "Iteration 277, loss = 0.33515250\n",
      "Iteration 278, loss = 0.33417835\n",
      "Iteration 279, loss = 0.33320772\n",
      "Iteration 280, loss = 0.33224061\n",
      "Iteration 281, loss = 0.33127695\n",
      "Iteration 282, loss = 0.33031672\n",
      "Iteration 283, loss = 0.32935994\n",
      "Iteration 284, loss = 0.32840658\n",
      "Iteration 285, loss = 0.32745661\n",
      "Iteration 286, loss = 0.32651000\n",
      "Iteration 287, loss = 0.32556674\n",
      "Iteration 288, loss = 0.32462689\n",
      "Iteration 289, loss = 0.32369037\n",
      "Iteration 290, loss = 0.32275713\n",
      "Iteration 291, loss = 0.32182719\n",
      "Iteration 292, loss = 0.32090053\n",
      "Iteration 293, loss = 0.31997711\n",
      "Iteration 294, loss = 0.31905693\n",
      "Iteration 295, loss = 0.31814002\n",
      "Iteration 296, loss = 0.31722629\n",
      "Iteration 297, loss = 0.31631573\n",
      "Iteration 298, loss = 0.31540838\n",
      "Iteration 299, loss = 0.31450420\n",
      "Iteration 300, loss = 0.31360370\n",
      "Iteration 301, loss = 0.31270640\n",
      "Iteration 302, loss = 0.31181206\n",
      "Iteration 303, loss = 0.31092069\n",
      "Iteration 304, loss = 0.31003229\n",
      "Iteration 305, loss = 0.30914696\n",
      "Iteration 306, loss = 0.30826459\n",
      "Iteration 307, loss = 0.30738593\n",
      "Iteration 308, loss = 0.30651033\n",
      "Iteration 309, loss = 0.30563773\n",
      "Iteration 310, loss = 0.30476812\n",
      "Iteration 311, loss = 0.30390148\n",
      "Iteration 312, loss = 0.30303863\n",
      "Iteration 313, loss = 0.30217877\n",
      "Iteration 314, loss = 0.30132181\n",
      "Iteration 315, loss = 0.30046776\n",
      "Iteration 316, loss = 0.29961672\n",
      "Iteration 317, loss = 0.29876876\n",
      "Iteration 318, loss = 0.29792364\n",
      "Iteration 319, loss = 0.29708138\n",
      "Iteration 320, loss = 0.29624198\n",
      "Iteration 321, loss = 0.29540545\n",
      "Iteration 322, loss = 0.29457239\n",
      "Iteration 323, loss = 0.29374204\n",
      "Iteration 324, loss = 0.29291437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 325, loss = 0.29208972\n",
      "Iteration 326, loss = 0.29126821\n",
      "Iteration 327, loss = 0.29044960\n",
      "Iteration 328, loss = 0.28963378\n",
      "Iteration 329, loss = 0.28882079\n",
      "Iteration 330, loss = 0.28801097\n",
      "Iteration 331, loss = 0.28720406\n",
      "Iteration 332, loss = 0.28639994\n",
      "Iteration 333, loss = 0.28559858\n",
      "Iteration 334, loss = 0.28480031\n",
      "Iteration 335, loss = 0.28400501\n",
      "Iteration 336, loss = 0.28321243\n",
      "Iteration 337, loss = 0.28242264\n",
      "Iteration 338, loss = 0.28163563\n",
      "Iteration 339, loss = 0.28085161\n",
      "Iteration 340, loss = 0.28007036\n",
      "Iteration 341, loss = 0.27929210\n",
      "Iteration 342, loss = 0.27851670\n",
      "Iteration 343, loss = 0.27774408\n",
      "Iteration 344, loss = 0.27697406\n",
      "Iteration 345, loss = 0.27620699\n",
      "Iteration 346, loss = 0.27544296\n",
      "Iteration 347, loss = 0.27468160\n",
      "Iteration 348, loss = 0.27392295\n",
      "Iteration 349, loss = 0.27316700\n",
      "Iteration 350, loss = 0.27241389\n",
      "Iteration 351, loss = 0.27166348\n",
      "Iteration 352, loss = 0.27091580\n",
      "Iteration 353, loss = 0.27017097\n",
      "Iteration 354, loss = 0.26942889\n",
      "Iteration 355, loss = 0.26868970\n",
      "Iteration 356, loss = 0.26795328\n",
      "Iteration 357, loss = 0.26721940\n",
      "Iteration 358, loss = 0.26648759\n",
      "Iteration 359, loss = 0.26575834\n",
      "Iteration 360, loss = 0.26503287\n",
      "Iteration 361, loss = 0.26430956\n",
      "Iteration 362, loss = 0.26358906\n",
      "Iteration 363, loss = 0.26287166\n",
      "Iteration 364, loss = 0.26215640\n",
      "Iteration 365, loss = 0.26144397\n",
      "Iteration 366, loss = 0.26073434\n",
      "Iteration 367, loss = 0.26002705\n",
      "Iteration 368, loss = 0.25932248\n",
      "Iteration 369, loss = 0.25862069\n",
      "Iteration 370, loss = 0.25792169\n",
      "Iteration 371, loss = 0.25722516\n",
      "Iteration 372, loss = 0.25653114\n",
      "Iteration 373, loss = 0.25583978\n",
      "Iteration 374, loss = 0.25515041\n",
      "Iteration 375, loss = 0.25446438\n",
      "Iteration 376, loss = 0.25377911\n",
      "Iteration 377, loss = 0.25309695\n",
      "Iteration 378, loss = 0.25241370\n",
      "Iteration 379, loss = 0.25172857\n",
      "Iteration 380, loss = 0.25104473\n",
      "Iteration 381, loss = 0.25036072\n",
      "Iteration 382, loss = 0.24967279\n",
      "Iteration 383, loss = 0.24898318\n",
      "Iteration 384, loss = 0.24829609\n",
      "Iteration 385, loss = 0.24760847\n",
      "Iteration 386, loss = 0.24691743\n",
      "Iteration 387, loss = 0.24622646\n",
      "Iteration 388, loss = 0.24553520\n",
      "Iteration 389, loss = 0.24484217\n",
      "Iteration 390, loss = 0.24414815\n",
      "Iteration 391, loss = 0.24345277\n",
      "Iteration 392, loss = 0.24275615\n",
      "Iteration 393, loss = 0.24205901\n",
      "Iteration 394, loss = 0.24136087\n",
      "Iteration 395, loss = 0.24066122\n",
      "Iteration 396, loss = 0.23996179\n",
      "Iteration 397, loss = 0.23926167\n",
      "Iteration 398, loss = 0.23856120\n",
      "Iteration 399, loss = 0.23786066\n",
      "Iteration 400, loss = 0.23715976\n",
      "Iteration 401, loss = 0.23645885\n",
      "Iteration 402, loss = 0.23575783\n",
      "Iteration 403, loss = 0.23505689\n",
      "Iteration 404, loss = 0.23435610\n",
      "Iteration 405, loss = 0.23365533\n",
      "Iteration 406, loss = 0.23295511\n",
      "Iteration 407, loss = 0.23225538\n",
      "Iteration 408, loss = 0.23155688\n",
      "Iteration 409, loss = 0.23085885\n",
      "Iteration 410, loss = 0.23016118\n",
      "Iteration 411, loss = 0.22946496\n",
      "Iteration 412, loss = 0.22876973\n",
      "Iteration 413, loss = 0.22807560\n",
      "Iteration 414, loss = 0.22738269\n",
      "Iteration 415, loss = 0.22669111\n",
      "Iteration 416, loss = 0.22600099\n",
      "Iteration 417, loss = 0.22531266\n",
      "Iteration 418, loss = 0.22462710\n",
      "Iteration 419, loss = 0.22394317\n",
      "Iteration 420, loss = 0.22326101\n",
      "Iteration 421, loss = 0.22258065\n",
      "Iteration 422, loss = 0.22190224\n",
      "Iteration 423, loss = 0.22122585\n",
      "Iteration 424, loss = 0.22055192\n",
      "Iteration 425, loss = 0.21987958\n",
      "Iteration 426, loss = 0.21920987\n",
      "Iteration 427, loss = 0.21854267\n",
      "Iteration 428, loss = 0.21787737\n",
      "Iteration 429, loss = 0.21721431\n",
      "Iteration 430, loss = 0.21655489\n",
      "Iteration 431, loss = 0.21589534\n",
      "Iteration 432, loss = 0.21523993\n",
      "Iteration 433, loss = 0.21458752\n",
      "Iteration 434, loss = 0.21393739\n",
      "Iteration 435, loss = 0.21329000\n",
      "Iteration 436, loss = 0.21264571\n",
      "Iteration 437, loss = 0.21200453\n",
      "Iteration 438, loss = 0.21136575\n",
      "Iteration 439, loss = 0.21072960\n",
      "Iteration 440, loss = 0.21009679\n",
      "Iteration 441, loss = 0.20946690\n",
      "Iteration 442, loss = 0.20884002\n",
      "Iteration 443, loss = 0.20821606\n",
      "Iteration 444, loss = 0.20759515\n",
      "Iteration 445, loss = 0.20697728\n",
      "Iteration 446, loss = 0.20636249\n",
      "Iteration 447, loss = 0.20575080\n",
      "Iteration 448, loss = 0.20514223\n",
      "Iteration 449, loss = 0.20453645\n",
      "Iteration 450, loss = 0.20393374\n",
      "Iteration 451, loss = 0.20333413\n",
      "Iteration 452, loss = 0.20273763\n",
      "Iteration 453, loss = 0.20214424\n",
      "Iteration 454, loss = 0.20155395\n",
      "Iteration 455, loss = 0.20096680\n",
      "Iteration 456, loss = 0.20038280\n",
      "Iteration 457, loss = 0.19980193\n",
      "Iteration 458, loss = 0.19922419\n",
      "Iteration 459, loss = 0.19864969\n",
      "Iteration 460, loss = 0.19807824\n",
      "Iteration 461, loss = 0.19750990\n",
      "Iteration 462, loss = 0.19694460\n",
      "Iteration 463, loss = 0.19638240\n",
      "Iteration 464, loss = 0.19582327\n",
      "Iteration 465, loss = 0.19526723\n",
      "Iteration 466, loss = 0.19471426\n",
      "Iteration 467, loss = 0.19416430\n",
      "Iteration 468, loss = 0.19361728\n",
      "Iteration 469, loss = 0.19307338\n",
      "Iteration 470, loss = 0.19253253\n",
      "Iteration 471, loss = 0.19199470\n",
      "Iteration 472, loss = 0.19145994\n",
      "Iteration 473, loss = 0.19092818\n",
      "Iteration 474, loss = 0.19039942\n",
      "Iteration 475, loss = 0.18987365\n",
      "Iteration 476, loss = 0.18935087\n",
      "Iteration 477, loss = 0.18883138\n",
      "Iteration 478, loss = 0.18831512\n",
      "Iteration 479, loss = 0.18780182\n",
      "Iteration 480, loss = 0.18729145\n",
      "Iteration 481, loss = 0.18678405\n",
      "Iteration 482, loss = 0.18627953\n",
      "Iteration 483, loss = 0.18577761\n",
      "Iteration 484, loss = 0.18527820\n",
      "Iteration 485, loss = 0.18478155\n",
      "Iteration 486, loss = 0.18428773\n",
      "Iteration 487, loss = 0.18379672\n",
      "Iteration 488, loss = 0.18330841\n",
      "Iteration 489, loss = 0.18282284\n",
      "Iteration 490, loss = 0.18233997\n",
      "Iteration 491, loss = 0.18185991\n",
      "Iteration 492, loss = 0.18138258\n",
      "Iteration 493, loss = 0.18090795\n",
      "Iteration 494, loss = 0.18043590\n",
      "Iteration 495, loss = 0.17996647\n",
      "Iteration 496, loss = 0.17950007\n",
      "Iteration 497, loss = 0.17903626\n",
      "Iteration 498, loss = 0.17857509\n",
      "Iteration 499, loss = 0.17811620\n",
      "Iteration 500, loss = 0.17765927\n",
      "Iteration 501, loss = 0.17720474\n",
      "Iteration 502, loss = 0.17675269\n",
      "Iteration 503, loss = 0.17630281\n",
      "Iteration 504, loss = 0.17585521\n",
      "Iteration 505, loss = 0.17540949\n",
      "Iteration 506, loss = 0.17496607\n",
      "Iteration 507, loss = 0.17452401\n",
      "Iteration 508, loss = 0.17408394\n",
      "Iteration 509, loss = 0.17364614\n",
      "Iteration 510, loss = 0.17321161\n",
      "Iteration 511, loss = 0.17277950\n",
      "Iteration 512, loss = 0.17234965\n",
      "Iteration 513, loss = 0.17192227\n",
      "Iteration 514, loss = 0.17149742\n",
      "Iteration 515, loss = 0.17107494\n",
      "Iteration 516, loss = 0.17065497\n",
      "Iteration 517, loss = 0.17023728\n",
      "Iteration 518, loss = 0.16982220\n",
      "Iteration 519, loss = 0.16940962\n",
      "Iteration 520, loss = 0.16899929\n",
      "Iteration 521, loss = 0.16859184\n",
      "Iteration 522, loss = 0.16818693\n",
      "Iteration 523, loss = 0.16778432\n",
      "Iteration 524, loss = 0.16738414\n",
      "Iteration 525, loss = 0.16698653\n",
      "Iteration 526, loss = 0.16659185\n",
      "Iteration 527, loss = 0.16619974\n",
      "Iteration 528, loss = 0.16581115\n",
      "Iteration 529, loss = 0.16542480\n",
      "Iteration 530, loss = 0.16504074\n",
      "Iteration 531, loss = 0.16465899\n",
      "Iteration 532, loss = 0.16427974\n",
      "Iteration 533, loss = 0.16390204\n",
      "Iteration 534, loss = 0.16352629\n",
      "Iteration 535, loss = 0.16315291\n",
      "Iteration 536, loss = 0.16278168\n",
      "Iteration 537, loss = 0.16241269\n",
      "Iteration 538, loss = 0.16204612\n",
      "Iteration 539, loss = 0.16168170\n",
      "Iteration 540, loss = 0.16131934\n",
      "Iteration 541, loss = 0.16095873\n",
      "Iteration 542, loss = 0.16060010\n",
      "Iteration 543, loss = 0.16024405\n",
      "Iteration 544, loss = 0.15989126\n",
      "Iteration 545, loss = 0.15954054\n",
      "Iteration 546, loss = 0.15919192\n",
      "Iteration 547, loss = 0.15884564\n",
      "Iteration 548, loss = 0.15850201\n",
      "Iteration 549, loss = 0.15816057\n",
      "Iteration 550, loss = 0.15782109\n",
      "Iteration 551, loss = 0.15748357\n",
      "Iteration 552, loss = 0.15714786\n",
      "Iteration 553, loss = 0.15681395\n",
      "Iteration 554, loss = 0.15648208\n",
      "Iteration 555, loss = 0.15615224\n",
      "Iteration 556, loss = 0.15582455\n",
      "Iteration 557, loss = 0.15549867\n",
      "Iteration 558, loss = 0.15517464\n",
      "Iteration 559, loss = 0.15485253\n",
      "Iteration 560, loss = 0.15453216\n",
      "Iteration 561, loss = 0.15421358\n",
      "Iteration 562, loss = 0.15389675\n",
      "Iteration 563, loss = 0.15358173\n",
      "Iteration 564, loss = 0.15326848\n",
      "Iteration 565, loss = 0.15295700\n",
      "Iteration 566, loss = 0.15264726\n",
      "Iteration 567, loss = 0.15233921\n",
      "Iteration 568, loss = 0.15203285\n",
      "Iteration 569, loss = 0.15172816\n",
      "Iteration 570, loss = 0.15142508\n",
      "Iteration 571, loss = 0.15112363\n",
      "Iteration 572, loss = 0.15082377\n",
      "Iteration 573, loss = 0.15052550\n",
      "Iteration 574, loss = 0.15022880\n",
      "Iteration 575, loss = 0.14993366\n",
      "Iteration 576, loss = 0.14964008\n",
      "Iteration 577, loss = 0.14934804\n",
      "Iteration 578, loss = 0.14905753\n",
      "Iteration 579, loss = 0.14876854\n",
      "Iteration 580, loss = 0.14848107\n",
      "Iteration 581, loss = 0.14819509\n",
      "Iteration 582, loss = 0.14791060\n",
      "Iteration 583, loss = 0.14762758\n",
      "Iteration 584, loss = 0.14734605\n",
      "Iteration 585, loss = 0.14706599\n",
      "Iteration 586, loss = 0.14678747\n",
      "Iteration 587, loss = 0.14651039\n",
      "Iteration 588, loss = 0.14623470\n",
      "Iteration 589, loss = 0.14596041\n",
      "Iteration 590, loss = 0.14568755\n",
      "Iteration 591, loss = 0.14541627\n",
      "Iteration 592, loss = 0.14514634\n",
      "Iteration 593, loss = 0.14487772\n",
      "Iteration 594, loss = 0.14461049\n",
      "Iteration 595, loss = 0.14434470\n",
      "Iteration 596, loss = 0.14408028\n",
      "Iteration 597, loss = 0.14381721\n",
      "Iteration 598, loss = 0.14355548\n",
      "Iteration 599, loss = 0.14329511\n",
      "Iteration 600, loss = 0.14303607\n",
      "Iteration 601, loss = 0.14277835\n",
      "Iteration 602, loss = 0.14252194\n",
      "Iteration 603, loss = 0.14226684\n",
      "Iteration 604, loss = 0.14201308\n",
      "Iteration 605, loss = 0.14176063\n",
      "Iteration 606, loss = 0.14150940\n",
      "Iteration 607, loss = 0.14125954\n",
      "Iteration 608, loss = 0.14101095\n",
      "Iteration 609, loss = 0.14076358\n",
      "Iteration 610, loss = 0.14051740\n",
      "Iteration 611, loss = 0.14027252\n",
      "Iteration 612, loss = 0.14002893\n",
      "Iteration 613, loss = 0.13978653\n",
      "Iteration 614, loss = 0.13954535\n",
      "Iteration 615, loss = 0.13930537\n",
      "Iteration 616, loss = 0.13906660\n",
      "Iteration 617, loss = 0.13882904\n",
      "Iteration 618, loss = 0.13859267\n",
      "Iteration 619, loss = 0.13835749\n",
      "Iteration 620, loss = 0.13812350\n",
      "Iteration 621, loss = 0.13789069\n",
      "Iteration 622, loss = 0.13765903\n",
      "Iteration 623, loss = 0.13742850\n",
      "Iteration 624, loss = 0.13719911\n",
      "Iteration 625, loss = 0.13697083\n",
      "Iteration 626, loss = 0.13674367\n",
      "Iteration 627, loss = 0.13651761\n",
      "Iteration 628, loss = 0.13629265\n",
      "Iteration 629, loss = 0.13606879\n",
      "Iteration 630, loss = 0.13584617\n",
      "Iteration 631, loss = 0.13562459\n",
      "Iteration 632, loss = 0.13540396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 633, loss = 0.13518433\n",
      "Iteration 634, loss = 0.13496597\n",
      "Iteration 635, loss = 0.13474867\n",
      "Iteration 636, loss = 0.13453237\n",
      "Iteration 637, loss = 0.13431703\n",
      "Iteration 638, loss = 0.13410263\n",
      "Iteration 639, loss = 0.13388922\n",
      "Iteration 640, loss = 0.13367699\n",
      "Iteration 641, loss = 0.13346571\n",
      "Iteration 642, loss = 0.13325537\n",
      "Iteration 643, loss = 0.13304597\n",
      "Iteration 644, loss = 0.13283751\n",
      "Iteration 645, loss = 0.13263000\n",
      "Iteration 646, loss = 0.13242343\n",
      "Iteration 647, loss = 0.13221772\n",
      "Iteration 648, loss = 0.13201295\n",
      "Iteration 649, loss = 0.13180906\n",
      "Iteration 650, loss = 0.13160601\n",
      "Iteration 651, loss = 0.13140376\n",
      "Iteration 652, loss = 0.13120231\n",
      "Iteration 653, loss = 0.13100161\n",
      "Iteration 654, loss = 0.13080165\n",
      "Iteration 655, loss = 0.13060240\n",
      "Iteration 656, loss = 0.13040381\n",
      "Iteration 657, loss = 0.13020586\n",
      "Iteration 658, loss = 0.13000853\n",
      "Iteration 659, loss = 0.12981173\n",
      "Iteration 660, loss = 0.12961544\n",
      "Iteration 661, loss = 0.12941963\n",
      "Iteration 662, loss = 0.12922425\n",
      "Iteration 663, loss = 0.12902930\n",
      "Iteration 664, loss = 0.12883469\n",
      "Iteration 665, loss = 0.12864046\n",
      "Iteration 666, loss = 0.12844648\n",
      "Iteration 667, loss = 0.12825271\n",
      "Iteration 668, loss = 0.12805913\n",
      "Iteration 669, loss = 0.12786558\n",
      "Iteration 670, loss = 0.12767245\n",
      "Iteration 671, loss = 0.12747915\n",
      "Iteration 672, loss = 0.12728615\n",
      "Iteration 673, loss = 0.12709261\n",
      "Iteration 674, loss = 0.12689975\n",
      "Iteration 675, loss = 0.12670654\n",
      "Iteration 676, loss = 0.12651291\n",
      "Iteration 677, loss = 0.12632016\n",
      "Iteration 678, loss = 0.12612642\n",
      "Iteration 679, loss = 0.12593356\n",
      "Iteration 680, loss = 0.12574034\n",
      "Iteration 681, loss = 0.12554695\n",
      "Iteration 682, loss = 0.12535505\n",
      "Iteration 683, loss = 0.12516122\n",
      "Iteration 684, loss = 0.12496885\n",
      "Iteration 685, loss = 0.12477625\n",
      "Iteration 686, loss = 0.12458358\n",
      "Iteration 687, loss = 0.12439144\n",
      "Iteration 688, loss = 0.12419967\n",
      "Iteration 689, loss = 0.12400785\n",
      "Iteration 690, loss = 0.12381663\n",
      "Iteration 691, loss = 0.12362553\n",
      "Iteration 692, loss = 0.12343535\n",
      "Iteration 693, loss = 0.12324481\n",
      "Iteration 694, loss = 0.12305505\n",
      "Iteration 695, loss = 0.12286543\n",
      "Iteration 696, loss = 0.12267560\n",
      "Iteration 697, loss = 0.12248705\n",
      "Iteration 698, loss = 0.12229868\n",
      "Iteration 699, loss = 0.12211201\n",
      "Iteration 700, loss = 0.12192575\n",
      "Iteration 701, loss = 0.12173991\n",
      "Iteration 702, loss = 0.12155417\n",
      "Iteration 703, loss = 0.12137057\n",
      "Iteration 704, loss = 0.12118649\n",
      "Iteration 705, loss = 0.12100343\n",
      "Iteration 706, loss = 0.12082181\n",
      "Iteration 707, loss = 0.12064039\n",
      "Iteration 708, loss = 0.12045995\n",
      "Iteration 709, loss = 0.12028050\n",
      "Iteration 710, loss = 0.12010230\n",
      "Iteration 711, loss = 0.11992440\n",
      "Iteration 712, loss = 0.11974777\n",
      "Iteration 713, loss = 0.11957226\n",
      "Iteration 714, loss = 0.11939751\n",
      "Iteration 715, loss = 0.11922351\n",
      "Iteration 716, loss = 0.11905033\n",
      "Iteration 717, loss = 0.11887796\n",
      "Iteration 718, loss = 0.11870673\n",
      "Iteration 719, loss = 0.11853653\n",
      "Iteration 720, loss = 0.11836751\n",
      "Iteration 721, loss = 0.11820027\n",
      "Iteration 722, loss = 0.11803289\n",
      "Iteration 723, loss = 0.11786710\n",
      "Iteration 724, loss = 0.11770233\n",
      "Iteration 725, loss = 0.11753854\n",
      "Iteration 726, loss = 0.11737564\n",
      "Iteration 727, loss = 0.11721364\n",
      "Iteration 728, loss = 0.11705254\n",
      "Iteration 729, loss = 0.11689236\n",
      "Iteration 730, loss = 0.11673311\n",
      "Iteration 731, loss = 0.11657475\n",
      "Iteration 732, loss = 0.11641762\n",
      "Iteration 733, loss = 0.11626147\n",
      "Iteration 734, loss = 0.11610617\n",
      "Iteration 735, loss = 0.11595153\n",
      "Iteration 736, loss = 0.11579766\n",
      "Iteration 737, loss = 0.11564468\n",
      "Iteration 738, loss = 0.11549325\n",
      "Iteration 739, loss = 0.11534276\n",
      "Iteration 740, loss = 0.11519271\n",
      "Iteration 741, loss = 0.11504303\n",
      "Iteration 742, loss = 0.11489357\n",
      "Iteration 743, loss = 0.11474419\n",
      "Iteration 744, loss = 0.11459482\n",
      "Iteration 745, loss = 0.11444507\n",
      "Iteration 746, loss = 0.11429476\n",
      "Iteration 747, loss = 0.11414370\n",
      "Iteration 748, loss = 0.11399188\n",
      "Iteration 749, loss = 0.11383910\n",
      "Iteration 750, loss = 0.11368536\n",
      "Iteration 751, loss = 0.11353099\n",
      "Iteration 752, loss = 0.11337493\n",
      "Iteration 753, loss = 0.11321703\n",
      "Iteration 754, loss = 0.11305735\n",
      "Iteration 755, loss = 0.11289580\n",
      "Iteration 756, loss = 0.11273307\n",
      "Iteration 757, loss = 0.11256867\n",
      "Iteration 758, loss = 0.11240213\n",
      "Iteration 759, loss = 0.11223403\n",
      "Iteration 760, loss = 0.11206421\n",
      "Iteration 761, loss = 0.11189253\n",
      "Iteration 762, loss = 0.11171908\n",
      "Iteration 763, loss = 0.11154391\n",
      "Iteration 764, loss = 0.11136725\n",
      "Iteration 765, loss = 0.11118966\n",
      "Iteration 766, loss = 0.11101052\n",
      "Iteration 767, loss = 0.11083017\n",
      "Iteration 768, loss = 0.11064836\n",
      "Iteration 769, loss = 0.11046519\n",
      "Iteration 770, loss = 0.11028078\n",
      "Iteration 771, loss = 0.11009566\n",
      "Iteration 772, loss = 0.10990950\n",
      "Iteration 773, loss = 0.10972274\n",
      "Iteration 774, loss = 0.10953508\n",
      "Iteration 775, loss = 0.10934646\n",
      "Iteration 776, loss = 0.10915697\n",
      "Iteration 777, loss = 0.10896704\n",
      "Iteration 778, loss = 0.10877715\n",
      "Iteration 779, loss = 0.10858672\n",
      "Iteration 780, loss = 0.10839576\n",
      "Iteration 781, loss = 0.10820436\n",
      "Iteration 782, loss = 0.10801259\n",
      "Iteration 783, loss = 0.10782080\n",
      "Iteration 784, loss = 0.10762943\n",
      "Iteration 785, loss = 0.10743790\n",
      "Iteration 786, loss = 0.10724633\n",
      "Iteration 787, loss = 0.10705475\n",
      "Iteration 788, loss = 0.10686345\n",
      "Iteration 789, loss = 0.10667223\n",
      "Iteration 790, loss = 0.10648130\n",
      "Iteration 791, loss = 0.10629068\n",
      "Iteration 792, loss = 0.10610035\n",
      "Iteration 793, loss = 0.10591047\n",
      "Iteration 794, loss = 0.10572123\n",
      "Iteration 795, loss = 0.10553249\n",
      "Iteration 796, loss = 0.10534411\n",
      "Iteration 797, loss = 0.10515615\n",
      "Iteration 798, loss = 0.10496883\n",
      "Iteration 799, loss = 0.10478239\n",
      "Iteration 800, loss = 0.10459644\n",
      "Iteration 801, loss = 0.10441136\n",
      "Iteration 802, loss = 0.10422685\n",
      "Iteration 803, loss = 0.10404289\n",
      "Iteration 804, loss = 0.10385987\n",
      "Iteration 805, loss = 0.10367778\n",
      "Iteration 806, loss = 0.10349649\n",
      "Iteration 807, loss = 0.10331597\n",
      "Iteration 808, loss = 0.10313636\n",
      "Iteration 809, loss = 0.10295764\n",
      "Iteration 810, loss = 0.10277984\n",
      "Iteration 811, loss = 0.10260300\n",
      "Iteration 812, loss = 0.10242707\n",
      "Iteration 813, loss = 0.10225227\n",
      "Iteration 814, loss = 0.10207852\n",
      "Iteration 815, loss = 0.10190569\n",
      "Iteration 816, loss = 0.10173383\n",
      "Iteration 817, loss = 0.10156308\n",
      "Iteration 818, loss = 0.10139332\n",
      "Iteration 819, loss = 0.10122454\n",
      "Iteration 820, loss = 0.10105701\n",
      "Iteration 821, loss = 0.10089045\n",
      "Iteration 822, loss = 0.10072486\n",
      "Iteration 823, loss = 0.10056034\n",
      "Iteration 824, loss = 0.10039707\n",
      "Iteration 825, loss = 0.10023487\n",
      "Iteration 826, loss = 0.10007363\n",
      "Iteration 827, loss = 0.09991354\n",
      "Iteration 828, loss = 0.09975459\n",
      "Iteration 829, loss = 0.09959659\n",
      "Iteration 830, loss = 0.09943953\n",
      "Iteration 831, loss = 0.09928355\n",
      "Iteration 832, loss = 0.09912861\n",
      "Iteration 833, loss = 0.09897472\n",
      "Iteration 834, loss = 0.09882187\n",
      "Iteration 835, loss = 0.09866991\n",
      "Iteration 836, loss = 0.09851888\n",
      "Iteration 837, loss = 0.09836878\n",
      "Iteration 838, loss = 0.09821951\n",
      "Iteration 839, loss = 0.09807086\n",
      "Iteration 840, loss = 0.09792296\n",
      "Iteration 841, loss = 0.09777625\n",
      "Iteration 842, loss = 0.09763040\n",
      "Iteration 843, loss = 0.09748507\n",
      "Iteration 844, loss = 0.09733998\n",
      "Iteration 845, loss = 0.09719498\n",
      "Iteration 846, loss = 0.09705025\n",
      "Iteration 847, loss = 0.09690563\n",
      "Iteration 848, loss = 0.09676111\n",
      "Iteration 849, loss = 0.09661693\n",
      "Iteration 850, loss = 0.09647335\n",
      "Iteration 851, loss = 0.09632936\n",
      "Iteration 852, loss = 0.09618583\n",
      "Iteration 853, loss = 0.09604196\n",
      "Iteration 854, loss = 0.09589828\n",
      "Iteration 855, loss = 0.09575452\n",
      "Iteration 856, loss = 0.09561038\n",
      "Iteration 857, loss = 0.09546654\n",
      "Iteration 858, loss = 0.09532260\n",
      "Iteration 859, loss = 0.09517837\n",
      "Iteration 860, loss = 0.09503428\n",
      "Iteration 861, loss = 0.09489009\n",
      "Iteration 862, loss = 0.09474623\n",
      "Iteration 863, loss = 0.09460228\n",
      "Iteration 864, loss = 0.09445822\n",
      "Iteration 865, loss = 0.09431415\n",
      "Iteration 866, loss = 0.09417006\n",
      "Iteration 867, loss = 0.09402610\n",
      "Iteration 868, loss = 0.09388246\n",
      "Iteration 869, loss = 0.09373899\n",
      "Iteration 870, loss = 0.09359553\n",
      "Iteration 871, loss = 0.09345237\n",
      "Iteration 872, loss = 0.09330953\n",
      "Iteration 873, loss = 0.09316696\n",
      "Iteration 874, loss = 0.09302469\n",
      "Iteration 875, loss = 0.09288290\n",
      "Iteration 876, loss = 0.09274154\n",
      "Iteration 877, loss = 0.09260059\n",
      "Iteration 878, loss = 0.09246014\n",
      "Iteration 879, loss = 0.09232020\n",
      "Iteration 880, loss = 0.09218079\n",
      "Iteration 881, loss = 0.09204212\n",
      "Iteration 882, loss = 0.09190392\n",
      "Iteration 883, loss = 0.09176633\n",
      "Iteration 884, loss = 0.09162951\n",
      "Iteration 885, loss = 0.09149340\n",
      "Iteration 886, loss = 0.09135800\n",
      "Iteration 887, loss = 0.09122334\n",
      "Iteration 888, loss = 0.09108945\n",
      "Iteration 889, loss = 0.09095633\n",
      "Iteration 890, loss = 0.09082404\n",
      "Iteration 891, loss = 0.09069258\n",
      "Iteration 892, loss = 0.09056196\n",
      "Iteration 893, loss = 0.09043222\n",
      "Iteration 894, loss = 0.09030340\n",
      "Iteration 895, loss = 0.09017560\n",
      "Iteration 896, loss = 0.09004854\n",
      "Iteration 897, loss = 0.08992246\n",
      "Iteration 898, loss = 0.08979739\n",
      "Iteration 899, loss = 0.08967331\n",
      "Iteration 900, loss = 0.08955026\n",
      "Iteration 901, loss = 0.08942814\n",
      "Iteration 902, loss = 0.08930692\n",
      "Iteration 903, loss = 0.08918684\n",
      "Iteration 904, loss = 0.08906757\n",
      "Iteration 905, loss = 0.08894947\n",
      "Iteration 906, loss = 0.08883225\n",
      "Iteration 907, loss = 0.08871618\n",
      "Iteration 908, loss = 0.08860115\n",
      "Iteration 909, loss = 0.08848713\n",
      "Iteration 910, loss = 0.08837420\n",
      "Iteration 911, loss = 0.08826243\n",
      "Iteration 912, loss = 0.08815161\n",
      "Iteration 913, loss = 0.08804239\n",
      "Iteration 914, loss = 0.08793381\n",
      "Iteration 915, loss = 0.08782675\n",
      "Iteration 916, loss = 0.08772056\n",
      "Iteration 917, loss = 0.08761531\n",
      "Iteration 918, loss = 0.08751149\n",
      "Iteration 919, loss = 0.08740818\n",
      "Iteration 920, loss = 0.08730611\n",
      "Iteration 921, loss = 0.08720453\n",
      "Iteration 922, loss = 0.08710506\n",
      "Iteration 923, loss = 0.08700541\n",
      "Iteration 924, loss = 0.08690790\n",
      "Iteration 925, loss = 0.08681024\n",
      "Iteration 926, loss = 0.08671480\n",
      "Iteration 927, loss = 0.08661912\n",
      "Iteration 928, loss = 0.08652548\n",
      "Iteration 929, loss = 0.08643196\n",
      "Iteration 930, loss = 0.08634012\n",
      "Iteration 931, loss = 0.08624866\n",
      "Iteration 932, loss = 0.08615848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.86268893\n",
      "Iteration 3, loss = 1.82009983\n",
      "Iteration 4, loss = 1.77857748\n",
      "Iteration 5, loss = 1.73817155\n",
      "Iteration 6, loss = 1.69870850\n",
      "Iteration 7, loss = 1.66007635\n",
      "Iteration 8, loss = 1.62229402\n",
      "Iteration 9, loss = 1.58525955\n",
      "Iteration 10, loss = 1.54896749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 1.51348519\n",
      "Iteration 12, loss = 1.47881304\n",
      "Iteration 13, loss = 1.44495592\n",
      "Iteration 14, loss = 1.41194128\n",
      "Iteration 15, loss = 1.37986711\n",
      "Iteration 16, loss = 1.34868867\n",
      "Iteration 17, loss = 1.31845986\n",
      "Iteration 18, loss = 1.28927478\n",
      "Iteration 19, loss = 1.26118443\n",
      "Iteration 20, loss = 1.23422756\n",
      "Iteration 21, loss = 1.20850335\n",
      "Iteration 22, loss = 1.18403321\n",
      "Iteration 23, loss = 1.16087069\n",
      "Iteration 24, loss = 1.13909160\n",
      "Iteration 25, loss = 1.11870063\n",
      "Iteration 26, loss = 1.09970191\n",
      "Iteration 27, loss = 1.08208445\n",
      "Iteration 28, loss = 1.06582252\n",
      "Iteration 29, loss = 1.05086589\n",
      "Iteration 30, loss = 1.03714558\n",
      "Iteration 31, loss = 1.02458091\n",
      "Iteration 32, loss = 1.01308393\n",
      "Iteration 33, loss = 1.00253346\n",
      "Iteration 34, loss = 0.99277924\n",
      "Iteration 35, loss = 0.98369793\n",
      "Iteration 36, loss = 0.97516377\n",
      "Iteration 37, loss = 0.96704699\n",
      "Iteration 38, loss = 0.95925234\n",
      "Iteration 39, loss = 0.95163579\n",
      "Iteration 40, loss = 0.94410727\n",
      "Iteration 41, loss = 0.93657847\n",
      "Iteration 42, loss = 0.92900779\n",
      "Iteration 43, loss = 0.92140475\n",
      "Iteration 44, loss = 0.91380368\n",
      "Iteration 45, loss = 0.90623561\n",
      "Iteration 46, loss = 0.89868240\n",
      "Iteration 47, loss = 0.89119948\n",
      "Iteration 48, loss = 0.88380127\n",
      "Iteration 49, loss = 0.87644825\n",
      "Iteration 50, loss = 0.86914982\n",
      "Iteration 51, loss = 0.86184892\n",
      "Iteration 52, loss = 0.85456740\n",
      "Iteration 53, loss = 0.84731311\n",
      "Iteration 54, loss = 0.84009087\n",
      "Iteration 55, loss = 0.83295373\n",
      "Iteration 56, loss = 0.82591031\n",
      "Iteration 57, loss = 0.81900035\n",
      "Iteration 58, loss = 0.81226083\n",
      "Iteration 59, loss = 0.80571827\n",
      "Iteration 60, loss = 0.79936963\n",
      "Iteration 61, loss = 0.79324942\n",
      "Iteration 62, loss = 0.78734730\n",
      "Iteration 63, loss = 0.78173048\n",
      "Iteration 64, loss = 0.77630439\n",
      "Iteration 65, loss = 0.77107826\n",
      "Iteration 66, loss = 0.76603554\n",
      "Iteration 67, loss = 0.76117148\n",
      "Iteration 68, loss = 0.75641359\n",
      "Iteration 69, loss = 0.75174223\n",
      "Iteration 70, loss = 0.74715493\n",
      "Iteration 71, loss = 0.74261886\n",
      "Iteration 72, loss = 0.73810508\n",
      "Iteration 73, loss = 0.73361604\n",
      "Iteration 74, loss = 0.72914101\n",
      "Iteration 75, loss = 0.72468199\n",
      "Iteration 76, loss = 0.72025085\n",
      "Iteration 77, loss = 0.71584872\n",
      "Iteration 78, loss = 0.71147773\n",
      "Iteration 79, loss = 0.70714781\n",
      "Iteration 80, loss = 0.70285139\n",
      "Iteration 81, loss = 0.69858921\n",
      "Iteration 82, loss = 0.69434827\n",
      "Iteration 83, loss = 0.69013792\n",
      "Iteration 84, loss = 0.68596919\n",
      "Iteration 85, loss = 0.68184292\n",
      "Iteration 86, loss = 0.67773857\n",
      "Iteration 87, loss = 0.67367134\n",
      "Iteration 88, loss = 0.66964201\n",
      "Iteration 89, loss = 0.66565939\n",
      "Iteration 90, loss = 0.66173772\n",
      "Iteration 91, loss = 0.65785896\n",
      "Iteration 92, loss = 0.65401512\n",
      "Iteration 93, loss = 0.65021336\n",
      "Iteration 94, loss = 0.64645095\n",
      "Iteration 95, loss = 0.64272742\n",
      "Iteration 96, loss = 0.63903762\n",
      "Iteration 97, loss = 0.63537844\n",
      "Iteration 98, loss = 0.63175768\n",
      "Iteration 99, loss = 0.62817976\n",
      "Iteration 100, loss = 0.62464050\n",
      "Iteration 101, loss = 0.62113823\n",
      "Iteration 102, loss = 0.61767797\n",
      "Iteration 103, loss = 0.61424061\n",
      "Iteration 104, loss = 0.61083433\n",
      "Iteration 105, loss = 0.60745891\n",
      "Iteration 106, loss = 0.60412089\n",
      "Iteration 107, loss = 0.60082894\n",
      "Iteration 108, loss = 0.59757780\n",
      "Iteration 109, loss = 0.59436442\n",
      "Iteration 110, loss = 0.59118411\n",
      "Iteration 111, loss = 0.58805482\n",
      "Iteration 112, loss = 0.58499033\n",
      "Iteration 113, loss = 0.58196451\n",
      "Iteration 114, loss = 0.57898087\n",
      "Iteration 115, loss = 0.57604834\n",
      "Iteration 116, loss = 0.57315389\n",
      "Iteration 117, loss = 0.57028890\n",
      "Iteration 118, loss = 0.56746791\n",
      "Iteration 119, loss = 0.56467192\n",
      "Iteration 120, loss = 0.56190272\n",
      "Iteration 121, loss = 0.55916082\n",
      "Iteration 122, loss = 0.55644891\n",
      "Iteration 123, loss = 0.55376483\n",
      "Iteration 124, loss = 0.55111867\n",
      "Iteration 125, loss = 0.54850243\n",
      "Iteration 126, loss = 0.54591118\n",
      "Iteration 127, loss = 0.54334604\n",
      "Iteration 128, loss = 0.54080866\n",
      "Iteration 129, loss = 0.53831432\n",
      "Iteration 130, loss = 0.53585035\n",
      "Iteration 131, loss = 0.53340687\n",
      "Iteration 132, loss = 0.53098515\n",
      "Iteration 133, loss = 0.52858655\n",
      "Iteration 134, loss = 0.52621749\n",
      "Iteration 135, loss = 0.52387493\n",
      "Iteration 136, loss = 0.52156171\n",
      "Iteration 137, loss = 0.51927118\n",
      "Iteration 138, loss = 0.51700298\n",
      "Iteration 139, loss = 0.51475631\n",
      "Iteration 140, loss = 0.51253124\n",
      "Iteration 141, loss = 0.51032717\n",
      "Iteration 142, loss = 0.50814406\n",
      "Iteration 143, loss = 0.50598156\n",
      "Iteration 144, loss = 0.50383958\n",
      "Iteration 145, loss = 0.50171802\n",
      "Iteration 146, loss = 0.49961634\n",
      "Iteration 147, loss = 0.49753481\n",
      "Iteration 148, loss = 0.49547415\n",
      "Iteration 149, loss = 0.49343308\n",
      "Iteration 150, loss = 0.49141014\n",
      "Iteration 151, loss = 0.48940519\n",
      "Iteration 152, loss = 0.48741786\n",
      "Iteration 153, loss = 0.48544782\n",
      "Iteration 154, loss = 0.48349498\n",
      "Iteration 155, loss = 0.48155860\n",
      "Iteration 156, loss = 0.47963858\n",
      "Iteration 157, loss = 0.47773602\n",
      "Iteration 158, loss = 0.47585124\n",
      "Iteration 159, loss = 0.47398221\n",
      "Iteration 160, loss = 0.47212810\n",
      "Iteration 161, loss = 0.47028870\n",
      "Iteration 162, loss = 0.46846385\n",
      "Iteration 163, loss = 0.46665330\n",
      "Iteration 164, loss = 0.46485678\n",
      "Iteration 165, loss = 0.46307371\n",
      "Iteration 166, loss = 0.46130418\n",
      "Iteration 167, loss = 0.45954796\n",
      "Iteration 168, loss = 0.45780459\n",
      "Iteration 169, loss = 0.45607379\n",
      "Iteration 170, loss = 0.45435633\n",
      "Iteration 171, loss = 0.45265153\n",
      "Iteration 172, loss = 0.45095901\n",
      "Iteration 173, loss = 0.44927851\n",
      "Iteration 174, loss = 0.44760982\n",
      "Iteration 175, loss = 0.44595270\n",
      "Iteration 176, loss = 0.44430841\n",
      "Iteration 177, loss = 0.44267591\n",
      "Iteration 178, loss = 0.44105445\n",
      "Iteration 179, loss = 0.43944504\n",
      "Iteration 180, loss = 0.43784581\n",
      "Iteration 181, loss = 0.43625675\n",
      "Iteration 182, loss = 0.43467786\n",
      "Iteration 183, loss = 0.43310877\n",
      "Iteration 184, loss = 0.43154939\n",
      "Iteration 185, loss = 0.42999994\n",
      "Iteration 186, loss = 0.42846089\n",
      "Iteration 187, loss = 0.42693158\n",
      "Iteration 188, loss = 0.42541164\n",
      "Iteration 189, loss = 0.42390085\n",
      "Iteration 190, loss = 0.42239857\n",
      "Iteration 191, loss = 0.42090511\n",
      "Iteration 192, loss = 0.41942019\n",
      "Iteration 193, loss = 0.41794365\n",
      "Iteration 194, loss = 0.41647535\n",
      "Iteration 195, loss = 0.41501526\n",
      "Iteration 196, loss = 0.41356500\n",
      "Iteration 197, loss = 0.41212284\n",
      "Iteration 198, loss = 0.41068874\n",
      "Iteration 199, loss = 0.40926307\n",
      "Iteration 200, loss = 0.40784496\n",
      "Iteration 201, loss = 0.40643423\n",
      "Iteration 202, loss = 0.40503147\n",
      "Iteration 203, loss = 0.40363618\n",
      "Iteration 204, loss = 0.40224827\n",
      "Iteration 205, loss = 0.40086776\n",
      "Iteration 206, loss = 0.39949442\n",
      "Iteration 207, loss = 0.39812806\n",
      "Iteration 208, loss = 0.39676863\n",
      "Iteration 209, loss = 0.39541610\n",
      "Iteration 210, loss = 0.39407034\n",
      "Iteration 211, loss = 0.39273127\n",
      "Iteration 212, loss = 0.39139881\n",
      "Iteration 213, loss = 0.39007291\n",
      "Iteration 214, loss = 0.38875347\n",
      "Iteration 215, loss = 0.38744047\n",
      "Iteration 216, loss = 0.38613386\n",
      "Iteration 217, loss = 0.38483343\n",
      "Iteration 218, loss = 0.38353915\n",
      "Iteration 219, loss = 0.38225109\n",
      "Iteration 220, loss = 0.38096901\n",
      "Iteration 221, loss = 0.37969286\n",
      "Iteration 222, loss = 0.37842261\n",
      "Iteration 223, loss = 0.37715807\n",
      "Iteration 224, loss = 0.37589924\n",
      "Iteration 225, loss = 0.37464614\n",
      "Iteration 226, loss = 0.37339864\n",
      "Iteration 227, loss = 0.37215663\n",
      "Iteration 228, loss = 0.37091973\n",
      "Iteration 229, loss = 0.36968811\n",
      "Iteration 230, loss = 0.36846184\n",
      "Iteration 231, loss = 0.36724082\n",
      "Iteration 232, loss = 0.36602497\n",
      "Iteration 233, loss = 0.36481421\n",
      "Iteration 234, loss = 0.36360848\n",
      "Iteration 235, loss = 0.36240774\n",
      "Iteration 236, loss = 0.36121193\n",
      "Iteration 237, loss = 0.36002099\n",
      "Iteration 238, loss = 0.35883489\n",
      "Iteration 239, loss = 0.35765363\n",
      "Iteration 240, loss = 0.35647707\n",
      "Iteration 241, loss = 0.35530516\n",
      "Iteration 242, loss = 0.35413790\n",
      "Iteration 243, loss = 0.35297543\n",
      "Iteration 244, loss = 0.35181751\n",
      "Iteration 245, loss = 0.35066411\n",
      "Iteration 246, loss = 0.34951518\n",
      "Iteration 247, loss = 0.34837067\n",
      "Iteration 248, loss = 0.34723056\n",
      "Iteration 249, loss = 0.34609483\n",
      "Iteration 250, loss = 0.34496359\n",
      "Iteration 251, loss = 0.34383665\n",
      "Iteration 252, loss = 0.34271403\n",
      "Iteration 253, loss = 0.34159571\n",
      "Iteration 254, loss = 0.34048157\n",
      "Iteration 255, loss = 0.33937159\n",
      "Iteration 256, loss = 0.33826574\n",
      "Iteration 257, loss = 0.33716472\n",
      "Iteration 258, loss = 0.33606869\n",
      "Iteration 259, loss = 0.33497660\n",
      "Iteration 260, loss = 0.33388848\n",
      "Iteration 261, loss = 0.33280430\n",
      "Iteration 262, loss = 0.33172405\n",
      "Iteration 263, loss = 0.33064798\n",
      "Iteration 264, loss = 0.32957721\n",
      "Iteration 265, loss = 0.32851101\n",
      "Iteration 266, loss = 0.32744906\n",
      "Iteration 267, loss = 0.32639075\n",
      "Iteration 268, loss = 0.32533599\n",
      "Iteration 269, loss = 0.32428484\n",
      "Iteration 270, loss = 0.32323742\n",
      "Iteration 271, loss = 0.32219362\n",
      "Iteration 272, loss = 0.32115394\n",
      "Iteration 273, loss = 0.32011831\n",
      "Iteration 274, loss = 0.31908653\n",
      "Iteration 275, loss = 0.31805855\n",
      "Iteration 276, loss = 0.31703436\n",
      "Iteration 277, loss = 0.31601443\n",
      "Iteration 278, loss = 0.31499823\n",
      "Iteration 279, loss = 0.31398585\n",
      "Iteration 280, loss = 0.31297693\n",
      "Iteration 281, loss = 0.31197165\n",
      "Iteration 282, loss = 0.31097024\n",
      "Iteration 283, loss = 0.30997263\n",
      "Iteration 284, loss = 0.30897842\n",
      "Iteration 285, loss = 0.30798802\n",
      "Iteration 286, loss = 0.30700119\n",
      "Iteration 287, loss = 0.30601806\n",
      "Iteration 288, loss = 0.30503853\n",
      "Iteration 289, loss = 0.30406249\n",
      "Iteration 290, loss = 0.30308996\n",
      "Iteration 291, loss = 0.30212091\n",
      "Iteration 292, loss = 0.30115540\n",
      "Iteration 293, loss = 0.30019342\n",
      "Iteration 294, loss = 0.29923506\n",
      "Iteration 295, loss = 0.29828023\n",
      "Iteration 296, loss = 0.29732883\n",
      "Iteration 297, loss = 0.29638085\n",
      "Iteration 298, loss = 0.29543626\n",
      "Iteration 299, loss = 0.29449531\n",
      "Iteration 300, loss = 0.29355782\n",
      "Iteration 301, loss = 0.29262361\n",
      "Iteration 302, loss = 0.29169264\n",
      "Iteration 303, loss = 0.29076526\n",
      "Iteration 304, loss = 0.28984127\n",
      "Iteration 305, loss = 0.28892063\n",
      "Iteration 306, loss = 0.28800332\n",
      "Iteration 307, loss = 0.28708942\n",
      "Iteration 308, loss = 0.28617881\n",
      "Iteration 309, loss = 0.28527179\n",
      "Iteration 310, loss = 0.28436814\n",
      "Iteration 311, loss = 0.28346772\n",
      "Iteration 312, loss = 0.28257054\n",
      "Iteration 313, loss = 0.28167674\n",
      "Iteration 314, loss = 0.28078625\n",
      "Iteration 315, loss = 0.27989895\n",
      "Iteration 316, loss = 0.27901495\n",
      "Iteration 317, loss = 0.27813421\n",
      "Iteration 318, loss = 0.27725668\n",
      "Iteration 319, loss = 0.27638236\n",
      "Iteration 320, loss = 0.27551129\n",
      "Iteration 321, loss = 0.27464346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 322, loss = 0.27377891\n",
      "Iteration 323, loss = 0.27291751\n",
      "Iteration 324, loss = 0.27205926\n",
      "Iteration 325, loss = 0.27120426\n",
      "Iteration 326, loss = 0.27035239\n",
      "Iteration 327, loss = 0.26950372\n",
      "Iteration 328, loss = 0.26865832\n",
      "Iteration 329, loss = 0.26781606\n",
      "Iteration 330, loss = 0.26697694\n",
      "Iteration 331, loss = 0.26614093\n",
      "Iteration 332, loss = 0.26530808\n",
      "Iteration 333, loss = 0.26447834\n",
      "Iteration 334, loss = 0.26365169\n",
      "Iteration 335, loss = 0.26282817\n",
      "Iteration 336, loss = 0.26200773\n",
      "Iteration 337, loss = 0.26119037\n",
      "Iteration 338, loss = 0.26037610\n",
      "Iteration 339, loss = 0.25956491\n",
      "Iteration 340, loss = 0.25875679\n",
      "Iteration 341, loss = 0.25795174\n",
      "Iteration 342, loss = 0.25714971\n",
      "Iteration 343, loss = 0.25635072\n",
      "Iteration 344, loss = 0.25555477\n",
      "Iteration 345, loss = 0.25476186\n",
      "Iteration 346, loss = 0.25397199\n",
      "Iteration 347, loss = 0.25318508\n",
      "Iteration 348, loss = 0.25240128\n",
      "Iteration 349, loss = 0.25162047\n",
      "Iteration 350, loss = 0.25084266\n",
      "Iteration 351, loss = 0.25006783\n",
      "Iteration 352, loss = 0.24929598\n",
      "Iteration 353, loss = 0.24852711\n",
      "Iteration 354, loss = 0.24776119\n",
      "Iteration 355, loss = 0.24699825\n",
      "Iteration 356, loss = 0.24623825\n",
      "Iteration 357, loss = 0.24548118\n",
      "Iteration 358, loss = 0.24472781\n",
      "Iteration 359, loss = 0.24397638\n",
      "Iteration 360, loss = 0.24322738\n",
      "Iteration 361, loss = 0.24248202\n",
      "Iteration 362, loss = 0.24173918\n",
      "Iteration 363, loss = 0.24100010\n",
      "Iteration 364, loss = 0.24026334\n",
      "Iteration 365, loss = 0.23952894\n",
      "Iteration 366, loss = 0.23879811\n",
      "Iteration 367, loss = 0.23807022\n",
      "Iteration 368, loss = 0.23734488\n",
      "Iteration 369, loss = 0.23662216\n",
      "Iteration 370, loss = 0.23590205\n",
      "Iteration 371, loss = 0.23518596\n",
      "Iteration 372, loss = 0.23447225\n",
      "Iteration 373, loss = 0.23376113\n",
      "Iteration 374, loss = 0.23305276\n",
      "Iteration 375, loss = 0.23234579\n",
      "Iteration 376, loss = 0.23163933\n",
      "Iteration 377, loss = 0.23093418\n",
      "Iteration 378, loss = 0.23022797\n",
      "Iteration 379, loss = 0.22951930\n",
      "Iteration 380, loss = 0.22881003\n",
      "Iteration 381, loss = 0.22809889\n",
      "Iteration 382, loss = 0.22738834\n",
      "Iteration 383, loss = 0.22667751\n",
      "Iteration 384, loss = 0.22596442\n",
      "Iteration 385, loss = 0.22525222\n",
      "Iteration 386, loss = 0.22453966\n",
      "Iteration 387, loss = 0.22382601\n",
      "Iteration 388, loss = 0.22311075\n",
      "Iteration 389, loss = 0.22239382\n",
      "Iteration 390, loss = 0.22167606\n",
      "Iteration 391, loss = 0.22095843\n",
      "Iteration 392, loss = 0.22023833\n",
      "Iteration 393, loss = 0.21951945\n",
      "Iteration 394, loss = 0.21880072\n",
      "Iteration 395, loss = 0.21807904\n",
      "Iteration 396, loss = 0.21735934\n",
      "Iteration 397, loss = 0.21663921\n",
      "Iteration 398, loss = 0.21591871\n",
      "Iteration 399, loss = 0.21519891\n",
      "Iteration 400, loss = 0.21447931\n",
      "Iteration 401, loss = 0.21375924\n",
      "Iteration 402, loss = 0.21303975\n",
      "Iteration 403, loss = 0.21232069\n",
      "Iteration 404, loss = 0.21160281\n",
      "Iteration 405, loss = 0.21088488\n",
      "Iteration 406, loss = 0.21016772\n",
      "Iteration 407, loss = 0.20945202\n",
      "Iteration 408, loss = 0.20873692\n",
      "Iteration 409, loss = 0.20802247\n",
      "Iteration 410, loss = 0.20730931\n",
      "Iteration 411, loss = 0.20659766\n",
      "Iteration 412, loss = 0.20588741\n",
      "Iteration 413, loss = 0.20517870\n",
      "Iteration 414, loss = 0.20447179\n",
      "Iteration 415, loss = 0.20376636\n",
      "Iteration 416, loss = 0.20306294\n",
      "Iteration 417, loss = 0.20236147\n",
      "Iteration 418, loss = 0.20166204\n",
      "Iteration 419, loss = 0.20096473\n",
      "Iteration 420, loss = 0.20026964\n",
      "Iteration 421, loss = 0.19957682\n",
      "Iteration 422, loss = 0.19888634\n",
      "Iteration 423, loss = 0.19819828\n",
      "Iteration 424, loss = 0.19751270\n",
      "Iteration 425, loss = 0.19682966\n",
      "Iteration 426, loss = 0.19614921\n",
      "Iteration 427, loss = 0.19547179\n",
      "Iteration 428, loss = 0.19479671\n",
      "Iteration 429, loss = 0.19412459\n",
      "Iteration 430, loss = 0.19345519\n",
      "Iteration 431, loss = 0.19278859\n",
      "Iteration 432, loss = 0.19212503\n",
      "Iteration 433, loss = 0.19146441\n",
      "Iteration 434, loss = 0.19080654\n",
      "Iteration 435, loss = 0.19015163\n",
      "Iteration 436, loss = 0.18949983\n",
      "Iteration 437, loss = 0.18885111\n",
      "Iteration 438, loss = 0.18820553\n",
      "Iteration 439, loss = 0.18756308\n",
      "Iteration 440, loss = 0.18692382\n",
      "Iteration 441, loss = 0.18628773\n",
      "Iteration 442, loss = 0.18565488\n",
      "Iteration 443, loss = 0.18502533\n",
      "Iteration 444, loss = 0.18439885\n",
      "Iteration 445, loss = 0.18377569\n",
      "Iteration 446, loss = 0.18315580\n",
      "Iteration 447, loss = 0.18253924\n",
      "Iteration 448, loss = 0.18192580\n",
      "Iteration 449, loss = 0.18131560\n",
      "Iteration 450, loss = 0.18070868\n",
      "Iteration 451, loss = 0.18010504\n",
      "Iteration 452, loss = 0.17950467\n",
      "Iteration 453, loss = 0.17890759\n",
      "Iteration 454, loss = 0.17831376\n",
      "Iteration 455, loss = 0.17772325\n",
      "Iteration 456, loss = 0.17713586\n",
      "Iteration 457, loss = 0.17655183\n",
      "Iteration 458, loss = 0.17597108\n",
      "Iteration 459, loss = 0.17539359\n",
      "Iteration 460, loss = 0.17481936\n",
      "Iteration 461, loss = 0.17424838\n",
      "Iteration 462, loss = 0.17368070\n",
      "Iteration 463, loss = 0.17311626\n",
      "Iteration 464, loss = 0.17255502\n",
      "Iteration 465, loss = 0.17199702\n",
      "Iteration 466, loss = 0.17144236\n",
      "Iteration 467, loss = 0.17089122\n",
      "Iteration 468, loss = 0.17034328\n",
      "Iteration 469, loss = 0.16979858\n",
      "Iteration 470, loss = 0.16925709\n",
      "Iteration 471, loss = 0.16871873\n",
      "Iteration 472, loss = 0.16818408\n",
      "Iteration 473, loss = 0.16765215\n",
      "Iteration 474, loss = 0.16712333\n",
      "Iteration 475, loss = 0.16659764\n",
      "Iteration 476, loss = 0.16607513\n",
      "Iteration 477, loss = 0.16555572\n",
      "Iteration 478, loss = 0.16503928\n",
      "Iteration 479, loss = 0.16452572\n",
      "Iteration 480, loss = 0.16401502\n",
      "Iteration 481, loss = 0.16350731\n",
      "Iteration 482, loss = 0.16300280\n",
      "Iteration 483, loss = 0.16250101\n",
      "Iteration 484, loss = 0.16200195\n",
      "Iteration 485, loss = 0.16150555\n",
      "Iteration 486, loss = 0.16101190\n",
      "Iteration 487, loss = 0.16052082\n",
      "Iteration 488, loss = 0.16003229\n",
      "Iteration 489, loss = 0.15954714\n",
      "Iteration 490, loss = 0.15906503\n",
      "Iteration 491, loss = 0.15858576\n",
      "Iteration 492, loss = 0.15810944\n",
      "Iteration 493, loss = 0.15763589\n",
      "Iteration 494, loss = 0.15716512\n",
      "Iteration 495, loss = 0.15669716\n",
      "Iteration 496, loss = 0.15623172\n",
      "Iteration 497, loss = 0.15576898\n",
      "Iteration 498, loss = 0.15530877\n",
      "Iteration 499, loss = 0.15485056\n",
      "Iteration 500, loss = 0.15439459\n",
      "Iteration 501, loss = 0.15394099\n",
      "Iteration 502, loss = 0.15348986\n",
      "Iteration 503, loss = 0.15304123\n",
      "Iteration 504, loss = 0.15259505\n",
      "Iteration 505, loss = 0.15215134\n",
      "Iteration 506, loss = 0.15171042\n",
      "Iteration 507, loss = 0.15127207\n",
      "Iteration 508, loss = 0.15083634\n",
      "Iteration 509, loss = 0.15040315\n",
      "Iteration 510, loss = 0.14997253\n",
      "Iteration 511, loss = 0.14954445\n",
      "Iteration 512, loss = 0.14911868\n",
      "Iteration 513, loss = 0.14869520\n",
      "Iteration 514, loss = 0.14827396\n",
      "Iteration 515, loss = 0.14785518\n",
      "Iteration 516, loss = 0.14743889\n",
      "Iteration 517, loss = 0.14702563\n",
      "Iteration 518, loss = 0.14661510\n",
      "Iteration 519, loss = 0.14620681\n",
      "Iteration 520, loss = 0.14580052\n",
      "Iteration 521, loss = 0.14539660\n",
      "Iteration 522, loss = 0.14499505\n",
      "Iteration 523, loss = 0.14459608\n",
      "Iteration 524, loss = 0.14419982\n",
      "Iteration 525, loss = 0.14380597\n",
      "Iteration 526, loss = 0.14341444\n",
      "Iteration 527, loss = 0.14302519\n",
      "Iteration 528, loss = 0.14263808\n",
      "Iteration 529, loss = 0.14225332\n",
      "Iteration 530, loss = 0.14187087\n",
      "Iteration 531, loss = 0.14149015\n",
      "Iteration 532, loss = 0.14111163\n",
      "Iteration 533, loss = 0.14073552\n",
      "Iteration 534, loss = 0.14036167\n",
      "Iteration 535, loss = 0.13999010\n",
      "Iteration 536, loss = 0.13962062\n",
      "Iteration 537, loss = 0.13925336\n",
      "Iteration 538, loss = 0.13888822\n",
      "Iteration 539, loss = 0.13852475\n",
      "Iteration 540, loss = 0.13816403\n",
      "Iteration 541, loss = 0.13780656\n",
      "Iteration 542, loss = 0.13745153\n",
      "Iteration 543, loss = 0.13709873\n",
      "Iteration 544, loss = 0.13674793\n",
      "Iteration 545, loss = 0.13639929\n",
      "Iteration 546, loss = 0.13605257\n",
      "Iteration 547, loss = 0.13570821\n",
      "Iteration 548, loss = 0.13536585\n",
      "Iteration 549, loss = 0.13502537\n",
      "Iteration 550, loss = 0.13468687\n",
      "Iteration 551, loss = 0.13435031\n",
      "Iteration 552, loss = 0.13401584\n",
      "Iteration 553, loss = 0.13368353\n",
      "Iteration 554, loss = 0.13335319\n",
      "Iteration 555, loss = 0.13302452\n",
      "Iteration 556, loss = 0.13269773\n",
      "Iteration 557, loss = 0.13237293\n",
      "Iteration 558, loss = 0.13204999\n",
      "Iteration 559, loss = 0.13172943\n",
      "Iteration 560, loss = 0.13141081\n",
      "Iteration 561, loss = 0.13109354\n",
      "Iteration 562, loss = 0.13077808\n",
      "Iteration 563, loss = 0.13046458\n",
      "Iteration 564, loss = 0.13015277\n",
      "Iteration 565, loss = 0.12984262\n",
      "Iteration 566, loss = 0.12953420\n",
      "Iteration 567, loss = 0.12922757\n",
      "Iteration 568, loss = 0.12892260\n",
      "Iteration 569, loss = 0.12861929\n",
      "Iteration 570, loss = 0.12831767\n",
      "Iteration 571, loss = 0.12801790\n",
      "Iteration 572, loss = 0.12771976\n",
      "Iteration 573, loss = 0.12742343\n",
      "Iteration 574, loss = 0.12712856\n",
      "Iteration 575, loss = 0.12683522\n",
      "Iteration 576, loss = 0.12654356\n",
      "Iteration 577, loss = 0.12625327\n",
      "Iteration 578, loss = 0.12596466\n",
      "Iteration 579, loss = 0.12567769\n",
      "Iteration 580, loss = 0.12539243\n",
      "Iteration 581, loss = 0.12510862\n",
      "Iteration 582, loss = 0.12482638\n",
      "Iteration 583, loss = 0.12454566\n",
      "Iteration 584, loss = 0.12426663\n",
      "Iteration 585, loss = 0.12398906\n",
      "Iteration 586, loss = 0.12371292\n",
      "Iteration 587, loss = 0.12343824\n",
      "Iteration 588, loss = 0.12316517\n",
      "Iteration 589, loss = 0.12289362\n",
      "Iteration 590, loss = 0.12262346\n",
      "Iteration 591, loss = 0.12235464\n",
      "Iteration 592, loss = 0.12208731\n",
      "Iteration 593, loss = 0.12182149\n",
      "Iteration 594, loss = 0.12155705\n",
      "Iteration 595, loss = 0.12129400\n",
      "Iteration 596, loss = 0.12103235\n",
      "Iteration 597, loss = 0.12077208\n",
      "Iteration 598, loss = 0.12051320\n",
      "Iteration 599, loss = 0.12025573\n",
      "Iteration 600, loss = 0.11999956\n",
      "Iteration 601, loss = 0.11974477\n",
      "Iteration 602, loss = 0.11949132\n",
      "Iteration 603, loss = 0.11923925\n",
      "Iteration 604, loss = 0.11898853\n",
      "Iteration 605, loss = 0.11873911\n",
      "Iteration 606, loss = 0.11849100\n",
      "Iteration 607, loss = 0.11824418\n",
      "Iteration 608, loss = 0.11799864\n",
      "Iteration 609, loss = 0.11775437\n",
      "Iteration 610, loss = 0.11751137\n",
      "Iteration 611, loss = 0.11726966\n",
      "Iteration 612, loss = 0.11702930\n",
      "Iteration 613, loss = 0.11679019\n",
      "Iteration 614, loss = 0.11655230\n",
      "Iteration 615, loss = 0.11631582\n",
      "Iteration 616, loss = 0.11608029\n",
      "Iteration 617, loss = 0.11584608\n",
      "Iteration 618, loss = 0.11561316\n",
      "Iteration 619, loss = 0.11538142\n",
      "Iteration 620, loss = 0.11515084\n",
      "Iteration 621, loss = 0.11492141\n",
      "Iteration 622, loss = 0.11469310\n",
      "Iteration 623, loss = 0.11446593\n",
      "Iteration 624, loss = 0.11423997\n",
      "Iteration 625, loss = 0.11401527\n",
      "Iteration 626, loss = 0.11379156\n",
      "Iteration 627, loss = 0.11356910\n",
      "Iteration 628, loss = 0.11334779\n",
      "Iteration 629, loss = 0.11312754\n",
      "Iteration 630, loss = 0.11290833\n",
      "Iteration 631, loss = 0.11269034\n",
      "Iteration 632, loss = 0.11247347\n",
      "Iteration 633, loss = 0.11225760\n",
      "Iteration 634, loss = 0.11204276\n",
      "Iteration 635, loss = 0.11182925\n",
      "Iteration 636, loss = 0.11161673\n",
      "Iteration 637, loss = 0.11140513\n",
      "Iteration 638, loss = 0.11119450\n",
      "Iteration 639, loss = 0.11098514\n",
      "Iteration 640, loss = 0.11077703\n",
      "Iteration 641, loss = 0.11056997\n",
      "Iteration 642, loss = 0.11036397\n",
      "Iteration 643, loss = 0.11015907\n",
      "Iteration 644, loss = 0.10995518\n",
      "Iteration 645, loss = 0.10975234\n",
      "Iteration 646, loss = 0.10955049\n",
      "Iteration 647, loss = 0.10934966\n",
      "Iteration 648, loss = 0.10914981\n",
      "Iteration 649, loss = 0.10895095\n",
      "Iteration 650, loss = 0.10875307\n",
      "Iteration 651, loss = 0.10855623\n",
      "Iteration 652, loss = 0.10836033\n",
      "Iteration 653, loss = 0.10816541\n",
      "Iteration 654, loss = 0.10797145\n",
      "Iteration 655, loss = 0.10777843\n",
      "Iteration 656, loss = 0.10758635\n",
      "Iteration 657, loss = 0.10739511\n",
      "Iteration 658, loss = 0.10720475\n",
      "Iteration 659, loss = 0.10701527\n",
      "Iteration 660, loss = 0.10682665\n",
      "Iteration 661, loss = 0.10663890\n",
      "Iteration 662, loss = 0.10645204\n",
      "Iteration 663, loss = 0.10626601\n",
      "Iteration 664, loss = 0.10608075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 665, loss = 0.10589620\n",
      "Iteration 666, loss = 0.10571244\n",
      "Iteration 667, loss = 0.10552922\n",
      "Iteration 668, loss = 0.10534685\n",
      "Iteration 669, loss = 0.10516485\n",
      "Iteration 670, loss = 0.10498359\n",
      "Iteration 671, loss = 0.10480266\n",
      "Iteration 672, loss = 0.10462246\n",
      "Iteration 673, loss = 0.10444227\n",
      "Iteration 674, loss = 0.10426268\n",
      "Iteration 675, loss = 0.10408326\n",
      "Iteration 676, loss = 0.10390403\n",
      "Iteration 677, loss = 0.10372549\n",
      "Iteration 678, loss = 0.10354681\n",
      "Iteration 679, loss = 0.10336859\n",
      "Iteration 680, loss = 0.10319106\n",
      "Iteration 681, loss = 0.10301362\n",
      "Iteration 682, loss = 0.10283674\n",
      "Iteration 683, loss = 0.10265964\n",
      "Iteration 684, loss = 0.10248416\n",
      "Iteration 685, loss = 0.10230788\n",
      "Iteration 686, loss = 0.10213296\n",
      "Iteration 687, loss = 0.10195750\n",
      "Iteration 688, loss = 0.10178243\n",
      "Iteration 689, loss = 0.10160830\n",
      "Iteration 690, loss = 0.10143443\n",
      "Iteration 691, loss = 0.10126053\n",
      "Iteration 692, loss = 0.10108705\n",
      "Iteration 693, loss = 0.10091438\n",
      "Iteration 694, loss = 0.10074155\n",
      "Iteration 695, loss = 0.10056947\n",
      "Iteration 696, loss = 0.10039774\n",
      "Iteration 697, loss = 0.10022628\n",
      "Iteration 698, loss = 0.10005550\n",
      "Iteration 699, loss = 0.09988481\n",
      "Iteration 700, loss = 0.09971474\n",
      "Iteration 701, loss = 0.09954541\n",
      "Iteration 702, loss = 0.09937645\n",
      "Iteration 703, loss = 0.09920822\n",
      "Iteration 704, loss = 0.09904014\n",
      "Iteration 705, loss = 0.09887284\n",
      "Iteration 706, loss = 0.09870604\n",
      "Iteration 707, loss = 0.09853976\n",
      "Iteration 708, loss = 0.09837397\n",
      "Iteration 709, loss = 0.09820933\n",
      "Iteration 710, loss = 0.09804439\n",
      "Iteration 711, loss = 0.09788064\n",
      "Iteration 712, loss = 0.09771748\n",
      "Iteration 713, loss = 0.09755447\n",
      "Iteration 714, loss = 0.09739224\n",
      "Iteration 715, loss = 0.09723076\n",
      "Iteration 716, loss = 0.09707016\n",
      "Iteration 717, loss = 0.09691044\n",
      "Iteration 718, loss = 0.09675125\n",
      "Iteration 719, loss = 0.09659261\n",
      "Iteration 720, loss = 0.09643456\n",
      "Iteration 721, loss = 0.09627739\n",
      "Iteration 722, loss = 0.09612095\n",
      "Iteration 723, loss = 0.09596526\n",
      "Iteration 724, loss = 0.09581029\n",
      "Iteration 725, loss = 0.09565621\n",
      "Iteration 726, loss = 0.09550288\n",
      "Iteration 727, loss = 0.09535036\n",
      "Iteration 728, loss = 0.09519856\n",
      "Iteration 729, loss = 0.09504752\n",
      "Iteration 730, loss = 0.09489715\n",
      "Iteration 731, loss = 0.09474736\n",
      "Iteration 732, loss = 0.09459818\n",
      "Iteration 733, loss = 0.09444938\n",
      "Iteration 734, loss = 0.09430088\n",
      "Iteration 735, loss = 0.09415249\n",
      "Iteration 736, loss = 0.09400403\n",
      "Iteration 737, loss = 0.09385582\n",
      "Iteration 738, loss = 0.09370710\n",
      "Iteration 739, loss = 0.09355824\n",
      "Iteration 740, loss = 0.09340867\n",
      "Iteration 741, loss = 0.09325853\n",
      "Iteration 742, loss = 0.09310769\n",
      "Iteration 743, loss = 0.09295561\n",
      "Iteration 744, loss = 0.09280215\n",
      "Iteration 745, loss = 0.09264720\n",
      "Iteration 746, loss = 0.09249064\n",
      "Iteration 747, loss = 0.09233270\n",
      "Iteration 748, loss = 0.09217309\n",
      "Iteration 749, loss = 0.09201198\n",
      "Iteration 750, loss = 0.09184952\n",
      "Iteration 751, loss = 0.09168549\n",
      "Iteration 752, loss = 0.09151980\n",
      "Iteration 753, loss = 0.09135259\n",
      "Iteration 754, loss = 0.09118349\n",
      "Iteration 755, loss = 0.09101293\n",
      "Iteration 756, loss = 0.09084078\n",
      "Iteration 757, loss = 0.09066707\n",
      "Iteration 758, loss = 0.09049198\n",
      "Iteration 759, loss = 0.09031558\n",
      "Iteration 760, loss = 0.09013748\n",
      "Iteration 761, loss = 0.08995839\n",
      "Iteration 762, loss = 0.08977806\n",
      "Iteration 763, loss = 0.08959672\n",
      "Iteration 764, loss = 0.08941427\n",
      "Iteration 765, loss = 0.08923097\n",
      "Iteration 766, loss = 0.08904673\n",
      "Iteration 767, loss = 0.08886158\n",
      "Iteration 768, loss = 0.08867593\n",
      "Iteration 769, loss = 0.08848949\n",
      "Iteration 770, loss = 0.08830241\n",
      "Iteration 771, loss = 0.08811498\n",
      "Iteration 772, loss = 0.08792697\n",
      "Iteration 773, loss = 0.08773864\n",
      "Iteration 774, loss = 0.08755015\n",
      "Iteration 775, loss = 0.08736137\n",
      "Iteration 776, loss = 0.08717253\n",
      "Iteration 777, loss = 0.08698375\n",
      "Iteration 778, loss = 0.08679471\n",
      "Iteration 779, loss = 0.08660585\n",
      "Iteration 780, loss = 0.08641716\n",
      "Iteration 781, loss = 0.08622859\n",
      "Iteration 782, loss = 0.08604012\n",
      "Iteration 783, loss = 0.08585200\n",
      "Iteration 784, loss = 0.08566422\n",
      "Iteration 785, loss = 0.08547669\n",
      "Iteration 786, loss = 0.08528965\n",
      "Iteration 787, loss = 0.08510321\n",
      "Iteration 788, loss = 0.08491718\n",
      "Iteration 789, loss = 0.08473160\n",
      "Iteration 790, loss = 0.08454658\n",
      "Iteration 791, loss = 0.08436229\n",
      "Iteration 792, loss = 0.08417868\n",
      "Iteration 793, loss = 0.08399573\n",
      "Iteration 794, loss = 0.08381352\n",
      "Iteration 795, loss = 0.08363219\n",
      "Iteration 796, loss = 0.08345155\n",
      "Iteration 797, loss = 0.08327162\n",
      "Iteration 798, loss = 0.08309263\n",
      "Iteration 799, loss = 0.08291452\n",
      "Iteration 800, loss = 0.08273730\n",
      "Iteration 801, loss = 0.08256098\n",
      "Iteration 802, loss = 0.08238558\n",
      "Iteration 803, loss = 0.08221113\n",
      "Iteration 804, loss = 0.08203765\n",
      "Iteration 805, loss = 0.08186515\n",
      "Iteration 806, loss = 0.08169365\n",
      "Iteration 807, loss = 0.08152317\n",
      "Iteration 808, loss = 0.08135371\n",
      "Iteration 809, loss = 0.08118530\n",
      "Iteration 810, loss = 0.08101794\n",
      "Iteration 811, loss = 0.08085165\n",
      "Iteration 812, loss = 0.08068642\n",
      "Iteration 813, loss = 0.08052229\n",
      "Iteration 814, loss = 0.08035924\n",
      "Iteration 815, loss = 0.08019729\n",
      "Iteration 816, loss = 0.08003646\n",
      "Iteration 817, loss = 0.07987670\n",
      "Iteration 818, loss = 0.07971808\n",
      "Iteration 819, loss = 0.07956060\n",
      "Iteration 820, loss = 0.07940424\n",
      "Iteration 821, loss = 0.07924901\n",
      "Iteration 822, loss = 0.07909489\n",
      "Iteration 823, loss = 0.07894190\n",
      "Iteration 824, loss = 0.07879005\n",
      "Iteration 825, loss = 0.07863932\n",
      "Iteration 826, loss = 0.07848972\n",
      "Iteration 827, loss = 0.07834126\n",
      "Iteration 828, loss = 0.07819395\n",
      "Iteration 829, loss = 0.07804771\n",
      "Iteration 830, loss = 0.07790264\n",
      "Iteration 831, loss = 0.07775869\n",
      "Iteration 832, loss = 0.07761585\n",
      "Iteration 833, loss = 0.07747413\n",
      "Iteration 834, loss = 0.07733353\n",
      "Iteration 835, loss = 0.07719406\n",
      "Iteration 836, loss = 0.07705568\n",
      "Iteration 837, loss = 0.07691838\n",
      "Iteration 838, loss = 0.07678218\n",
      "Iteration 839, loss = 0.07664710\n",
      "Iteration 840, loss = 0.07651312\n",
      "Iteration 841, loss = 0.07638022\n",
      "Iteration 842, loss = 0.07624839\n",
      "Iteration 843, loss = 0.07611763\n",
      "Iteration 844, loss = 0.07598792\n",
      "Iteration 845, loss = 0.07585929\n",
      "Iteration 846, loss = 0.07573173\n",
      "Iteration 847, loss = 0.07560521\n",
      "Iteration 848, loss = 0.07547973\n",
      "Iteration 849, loss = 0.07535530\n",
      "Iteration 850, loss = 0.07523191\n",
      "Iteration 851, loss = 0.07510955\n",
      "Iteration 852, loss = 0.07498820\n",
      "Iteration 853, loss = 0.07486785\n",
      "Iteration 854, loss = 0.07474855\n",
      "Iteration 855, loss = 0.07463018\n",
      "Iteration 856, loss = 0.07451293\n",
      "Iteration 857, loss = 0.07439682\n",
      "Iteration 858, loss = 0.07428164\n",
      "Iteration 859, loss = 0.07416747\n",
      "Iteration 860, loss = 0.07405423\n",
      "Iteration 861, loss = 0.07394190\n",
      "Iteration 862, loss = 0.07383048\n",
      "Iteration 863, loss = 0.07371997\n",
      "Iteration 864, loss = 0.07361037\n",
      "Iteration 865, loss = 0.07350167\n",
      "Iteration 866, loss = 0.07339387\n",
      "Iteration 867, loss = 0.07328697\n",
      "Iteration 868, loss = 0.07318094\n",
      "Iteration 869, loss = 0.07307579\n",
      "Iteration 870, loss = 0.07297151\n",
      "Iteration 871, loss = 0.07286817\n",
      "Iteration 872, loss = 0.07276568\n",
      "Iteration 873, loss = 0.07266406\n",
      "Iteration 874, loss = 0.07256341\n",
      "Iteration 875, loss = 0.07246371\n",
      "Iteration 876, loss = 0.07236478\n",
      "Iteration 877, loss = 0.07226664\n",
      "Iteration 878, loss = 0.07216923\n",
      "Iteration 879, loss = 0.07207259\n",
      "Iteration 880, loss = 0.07197674\n",
      "Iteration 881, loss = 0.07188177\n",
      "Iteration 882, loss = 0.07178764\n",
      "Iteration 883, loss = 0.07169411\n",
      "Iteration 884, loss = 0.07160167\n",
      "Iteration 885, loss = 0.07150977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.88558228\n",
      "Iteration 3, loss = 1.84243492\n",
      "Iteration 4, loss = 1.80036064\n",
      "Iteration 5, loss = 1.75937657\n",
      "Iteration 6, loss = 1.71927825\n",
      "Iteration 7, loss = 1.67999812\n",
      "Iteration 8, loss = 1.64159663\n",
      "Iteration 9, loss = 1.60399129\n",
      "Iteration 10, loss = 1.56711953\n",
      "Iteration 11, loss = 1.53104079\n",
      "Iteration 12, loss = 1.49577670\n",
      "Iteration 13, loss = 1.46134378\n",
      "Iteration 14, loss = 1.42777597\n",
      "Iteration 15, loss = 1.39513979\n",
      "Iteration 16, loss = 1.36342709\n",
      "Iteration 17, loss = 1.33269263\n",
      "Iteration 18, loss = 1.30298592\n",
      "Iteration 19, loss = 1.27440243\n",
      "Iteration 20, loss = 1.24697323\n",
      "Iteration 21, loss = 1.22077702\n",
      "Iteration 22, loss = 1.19587045\n",
      "Iteration 23, loss = 1.17231085\n",
      "Iteration 24, loss = 1.15015159\n",
      "Iteration 25, loss = 1.12937200\n",
      "Iteration 26, loss = 1.11000807\n",
      "Iteration 27, loss = 1.09204854\n",
      "Iteration 28, loss = 1.07547064\n",
      "Iteration 29, loss = 1.06023333\n",
      "Iteration 30, loss = 1.04626947\n",
      "Iteration 31, loss = 1.03348608\n",
      "Iteration 32, loss = 1.02178729\n",
      "Iteration 33, loss = 1.01108710\n",
      "Iteration 34, loss = 1.00120880\n",
      "Iteration 35, loss = 0.99203146\n",
      "Iteration 36, loss = 0.98344871\n",
      "Iteration 37, loss = 0.97531705\n",
      "Iteration 38, loss = 0.96753686\n",
      "Iteration 39, loss = 0.95995661\n",
      "Iteration 40, loss = 0.95248741\n",
      "Iteration 41, loss = 0.94503977\n",
      "Iteration 42, loss = 0.93757150\n",
      "Iteration 43, loss = 0.93006224\n",
      "Iteration 44, loss = 0.92252825\n",
      "Iteration 45, loss = 0.91495705\n",
      "Iteration 46, loss = 0.90740887\n",
      "Iteration 47, loss = 0.89994998\n",
      "Iteration 48, loss = 0.89253682\n",
      "Iteration 49, loss = 0.88525838\n",
      "Iteration 50, loss = 0.87800314\n",
      "Iteration 51, loss = 0.87076845\n",
      "Iteration 52, loss = 0.86357488\n",
      "Iteration 53, loss = 0.85638887\n",
      "Iteration 54, loss = 0.84921868\n",
      "Iteration 55, loss = 0.84212133\n",
      "Iteration 56, loss = 0.83510150\n",
      "Iteration 57, loss = 0.82824573\n",
      "Iteration 58, loss = 0.82154462\n",
      "Iteration 59, loss = 0.81508587\n",
      "Iteration 60, loss = 0.80886041\n",
      "Iteration 61, loss = 0.80284215\n",
      "Iteration 62, loss = 0.79701352\n",
      "Iteration 63, loss = 0.79139906\n",
      "Iteration 64, loss = 0.78599020\n",
      "Iteration 65, loss = 0.78079400\n",
      "Iteration 66, loss = 0.77576237\n",
      "Iteration 67, loss = 0.77092498\n",
      "Iteration 68, loss = 0.76617552\n",
      "Iteration 69, loss = 0.76152702\n",
      "Iteration 70, loss = 0.75695581\n",
      "Iteration 71, loss = 0.75241663\n",
      "Iteration 72, loss = 0.74790643\n",
      "Iteration 73, loss = 0.74341628\n",
      "Iteration 74, loss = 0.73893733\n",
      "Iteration 75, loss = 0.73447656\n",
      "Iteration 76, loss = 0.73004378\n",
      "Iteration 77, loss = 0.72563819\n",
      "Iteration 78, loss = 0.72126595\n",
      "Iteration 79, loss = 0.71692350\n",
      "Iteration 80, loss = 0.71261492\n",
      "Iteration 81, loss = 0.70835113\n",
      "Iteration 82, loss = 0.70412435\n",
      "Iteration 83, loss = 0.69993419\n",
      "Iteration 84, loss = 0.69577319\n",
      "Iteration 85, loss = 0.69163422\n",
      "Iteration 86, loss = 0.68752221\n",
      "Iteration 87, loss = 0.68344390\n",
      "Iteration 88, loss = 0.67939103\n",
      "Iteration 89, loss = 0.67537055\n",
      "Iteration 90, loss = 0.67138630\n",
      "Iteration 91, loss = 0.66743758\n",
      "Iteration 92, loss = 0.66352484\n",
      "Iteration 93, loss = 0.65965016\n",
      "Iteration 94, loss = 0.65581380\n",
      "Iteration 95, loss = 0.65201528\n",
      "Iteration 96, loss = 0.64826534\n",
      "Iteration 97, loss = 0.64455598\n",
      "Iteration 98, loss = 0.64088131\n",
      "Iteration 99, loss = 0.63723307\n",
      "Iteration 100, loss = 0.63361028\n",
      "Iteration 101, loss = 0.63002585\n",
      "Iteration 102, loss = 0.62647136\n",
      "Iteration 103, loss = 0.62295528\n",
      "Iteration 104, loss = 0.61947292\n",
      "Iteration 105, loss = 0.61601063\n",
      "Iteration 106, loss = 0.61257533\n",
      "Iteration 107, loss = 0.60915931\n",
      "Iteration 108, loss = 0.60577489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109, loss = 0.60242989\n",
      "Iteration 110, loss = 0.59912347\n",
      "Iteration 111, loss = 0.59585147\n",
      "Iteration 112, loss = 0.59262655\n",
      "Iteration 113, loss = 0.58948494\n",
      "Iteration 114, loss = 0.58639298\n",
      "Iteration 115, loss = 0.58335529\n",
      "Iteration 116, loss = 0.58036383\n",
      "Iteration 117, loss = 0.57740843\n",
      "Iteration 118, loss = 0.57448158\n",
      "Iteration 119, loss = 0.57159493\n",
      "Iteration 120, loss = 0.56873721\n",
      "Iteration 121, loss = 0.56590681\n",
      "Iteration 122, loss = 0.56310781\n",
      "Iteration 123, loss = 0.56034098\n",
      "Iteration 124, loss = 0.55760281\n",
      "Iteration 125, loss = 0.55488900\n",
      "Iteration 126, loss = 0.55220270\n",
      "Iteration 127, loss = 0.54955939\n",
      "Iteration 128, loss = 0.54696059\n",
      "Iteration 129, loss = 0.54438419\n",
      "Iteration 130, loss = 0.54183592\n",
      "Iteration 131, loss = 0.53932629\n",
      "Iteration 132, loss = 0.53685337\n",
      "Iteration 133, loss = 0.53440557\n",
      "Iteration 134, loss = 0.53198251\n",
      "Iteration 135, loss = 0.52958745\n",
      "Iteration 136, loss = 0.52721750\n",
      "Iteration 137, loss = 0.52487380\n",
      "Iteration 138, loss = 0.52255153\n",
      "Iteration 139, loss = 0.52025386\n",
      "Iteration 140, loss = 0.51797831\n",
      "Iteration 141, loss = 0.51572443\n",
      "Iteration 142, loss = 0.51349150\n",
      "Iteration 143, loss = 0.51127986\n",
      "Iteration 144, loss = 0.50909021\n",
      "Iteration 145, loss = 0.50692063\n",
      "Iteration 146, loss = 0.50477167\n",
      "Iteration 147, loss = 0.50264295\n",
      "Iteration 148, loss = 0.50053286\n",
      "Iteration 149, loss = 0.49844165\n",
      "Iteration 150, loss = 0.49636867\n",
      "Iteration 151, loss = 0.49431363\n",
      "Iteration 152, loss = 0.49227623\n",
      "Iteration 153, loss = 0.49025619\n",
      "Iteration 154, loss = 0.48825323\n",
      "Iteration 155, loss = 0.48626704\n",
      "Iteration 156, loss = 0.48429725\n",
      "Iteration 157, loss = 0.48234350\n",
      "Iteration 158, loss = 0.48040560\n",
      "Iteration 159, loss = 0.47848328\n",
      "Iteration 160, loss = 0.47657613\n",
      "Iteration 161, loss = 0.47468374\n",
      "Iteration 162, loss = 0.47280605\n",
      "Iteration 163, loss = 0.47094278\n",
      "Iteration 164, loss = 0.46909372\n",
      "Iteration 165, loss = 0.46725858\n",
      "Iteration 166, loss = 0.46543710\n",
      "Iteration 167, loss = 0.46363130\n",
      "Iteration 168, loss = 0.46184061\n",
      "Iteration 169, loss = 0.46006247\n",
      "Iteration 170, loss = 0.45829678\n",
      "Iteration 171, loss = 0.45654347\n",
      "Iteration 172, loss = 0.45480241\n",
      "Iteration 173, loss = 0.45307348\n",
      "Iteration 174, loss = 0.45135656\n",
      "Iteration 175, loss = 0.44965142\n",
      "Iteration 176, loss = 0.44795785\n",
      "Iteration 177, loss = 0.44627681\n",
      "Iteration 178, loss = 0.44460718\n",
      "Iteration 179, loss = 0.44294872\n",
      "Iteration 180, loss = 0.44130110\n",
      "Iteration 181, loss = 0.43966354\n",
      "Iteration 182, loss = 0.43803650\n",
      "Iteration 183, loss = 0.43641970\n",
      "Iteration 184, loss = 0.43481257\n",
      "Iteration 185, loss = 0.43321534\n",
      "Iteration 186, loss = 0.43162964\n",
      "Iteration 187, loss = 0.43005467\n",
      "Iteration 188, loss = 0.42849018\n",
      "Iteration 189, loss = 0.42693549\n",
      "Iteration 190, loss = 0.42538997\n",
      "Iteration 191, loss = 0.42385383\n",
      "Iteration 192, loss = 0.42232720\n",
      "Iteration 193, loss = 0.42080961\n",
      "Iteration 194, loss = 0.41930089\n",
      "Iteration 195, loss = 0.41780095\n",
      "Iteration 196, loss = 0.41630966\n",
      "Iteration 197, loss = 0.41482693\n",
      "Iteration 198, loss = 0.41335264\n",
      "Iteration 199, loss = 0.41188667\n",
      "Iteration 200, loss = 0.41042891\n",
      "Iteration 201, loss = 0.40897923\n",
      "Iteration 202, loss = 0.40753759\n",
      "Iteration 203, loss = 0.40610381\n",
      "Iteration 204, loss = 0.40467741\n",
      "Iteration 205, loss = 0.40325858\n",
      "Iteration 206, loss = 0.40184751\n",
      "Iteration 207, loss = 0.40044381\n",
      "Iteration 208, loss = 0.39904786\n",
      "Iteration 209, loss = 0.39765898\n",
      "Iteration 210, loss = 0.39627707\n",
      "Iteration 211, loss = 0.39490220\n",
      "Iteration 212, loss = 0.39353472\n",
      "Iteration 213, loss = 0.39217444\n",
      "Iteration 214, loss = 0.39082090\n",
      "Iteration 215, loss = 0.38947409\n",
      "Iteration 216, loss = 0.38813393\n",
      "Iteration 217, loss = 0.38680049\n",
      "Iteration 218, loss = 0.38547366\n",
      "Iteration 219, loss = 0.38415304\n",
      "Iteration 220, loss = 0.38283881\n",
      "Iteration 221, loss = 0.38153111\n",
      "Iteration 222, loss = 0.38022965\n",
      "Iteration 223, loss = 0.37893426\n",
      "Iteration 224, loss = 0.37764509\n",
      "Iteration 225, loss = 0.37636192\n",
      "Iteration 226, loss = 0.37508491\n",
      "Iteration 227, loss = 0.37381394\n",
      "Iteration 228, loss = 0.37254883\n",
      "Iteration 229, loss = 0.37128964\n",
      "Iteration 230, loss = 0.37003617\n",
      "Iteration 231, loss = 0.36878824\n",
      "Iteration 232, loss = 0.36754634\n",
      "Iteration 233, loss = 0.36630998\n",
      "Iteration 234, loss = 0.36507914\n",
      "Iteration 235, loss = 0.36385374\n",
      "Iteration 236, loss = 0.36263363\n",
      "Iteration 237, loss = 0.36141890\n",
      "Iteration 238, loss = 0.36020947\n",
      "Iteration 239, loss = 0.35900553\n",
      "Iteration 240, loss = 0.35780647\n",
      "Iteration 241, loss = 0.35661303\n",
      "Iteration 242, loss = 0.35542473\n",
      "Iteration 243, loss = 0.35424124\n",
      "Iteration 244, loss = 0.35306310\n",
      "Iteration 245, loss = 0.35189002\n",
      "Iteration 246, loss = 0.35072167\n",
      "Iteration 247, loss = 0.34955814\n",
      "Iteration 248, loss = 0.34839944\n",
      "Iteration 249, loss = 0.34724552\n",
      "Iteration 250, loss = 0.34609635\n",
      "Iteration 251, loss = 0.34495187\n",
      "Iteration 252, loss = 0.34381207\n",
      "Iteration 253, loss = 0.34267689\n",
      "Iteration 254, loss = 0.34154630\n",
      "Iteration 255, loss = 0.34042026\n",
      "Iteration 256, loss = 0.33929875\n",
      "Iteration 257, loss = 0.33818171\n",
      "Iteration 258, loss = 0.33706913\n",
      "Iteration 259, loss = 0.33596097\n",
      "Iteration 260, loss = 0.33485719\n",
      "Iteration 261, loss = 0.33375777\n",
      "Iteration 262, loss = 0.33266266\n",
      "Iteration 263, loss = 0.33157185\n",
      "Iteration 264, loss = 0.33048580\n",
      "Iteration 265, loss = 0.32940493\n",
      "Iteration 266, loss = 0.32832860\n",
      "Iteration 267, loss = 0.32725673\n",
      "Iteration 268, loss = 0.32618884\n",
      "Iteration 269, loss = 0.32512529\n",
      "Iteration 270, loss = 0.32406563\n",
      "Iteration 271, loss = 0.32300992\n",
      "Iteration 272, loss = 0.32195837\n",
      "Iteration 273, loss = 0.32091119\n",
      "Iteration 274, loss = 0.31986803\n",
      "Iteration 275, loss = 0.31882909\n",
      "Iteration 276, loss = 0.31779446\n",
      "Iteration 277, loss = 0.31676380\n",
      "Iteration 278, loss = 0.31573718\n",
      "Iteration 279, loss = 0.31471477\n",
      "Iteration 280, loss = 0.31369620\n",
      "Iteration 281, loss = 0.31268146\n",
      "Iteration 282, loss = 0.31167076\n",
      "Iteration 283, loss = 0.31066448\n",
      "Iteration 284, loss = 0.30966169\n",
      "Iteration 285, loss = 0.30866287\n",
      "Iteration 286, loss = 0.30766822\n",
      "Iteration 287, loss = 0.30667726\n",
      "Iteration 288, loss = 0.30569000\n",
      "Iteration 289, loss = 0.30470665\n",
      "Iteration 290, loss = 0.30372716\n",
      "Iteration 291, loss = 0.30275138\n",
      "Iteration 292, loss = 0.30177931\n",
      "Iteration 293, loss = 0.30081099\n",
      "Iteration 294, loss = 0.29984634\n",
      "Iteration 295, loss = 0.29888547\n",
      "Iteration 296, loss = 0.29792830\n",
      "Iteration 297, loss = 0.29697476\n",
      "Iteration 298, loss = 0.29602481\n",
      "Iteration 299, loss = 0.29507879\n",
      "Iteration 300, loss = 0.29413634\n",
      "Iteration 301, loss = 0.29319743\n",
      "Iteration 302, loss = 0.29226218\n",
      "Iteration 303, loss = 0.29133049\n",
      "Iteration 304, loss = 0.29040228\n",
      "Iteration 305, loss = 0.28947772\n",
      "Iteration 306, loss = 0.28855660\n",
      "Iteration 307, loss = 0.28763908\n",
      "Iteration 308, loss = 0.28672521\n",
      "Iteration 309, loss = 0.28581482\n",
      "Iteration 310, loss = 0.28490782\n",
      "Iteration 311, loss = 0.28400421\n",
      "Iteration 312, loss = 0.28310415\n",
      "Iteration 313, loss = 0.28220763\n",
      "Iteration 314, loss = 0.28131447\n",
      "Iteration 315, loss = 0.28042473\n",
      "Iteration 316, loss = 0.27953842\n",
      "Iteration 317, loss = 0.27865554\n",
      "Iteration 318, loss = 0.27777603\n",
      "Iteration 319, loss = 0.27689987\n",
      "Iteration 320, loss = 0.27602719\n",
      "Iteration 321, loss = 0.27515781\n",
      "Iteration 322, loss = 0.27429173\n",
      "Iteration 323, loss = 0.27342903\n",
      "Iteration 324, loss = 0.27256958\n",
      "Iteration 325, loss = 0.27171335\n",
      "Iteration 326, loss = 0.27086037\n",
      "Iteration 327, loss = 0.27001078\n",
      "Iteration 328, loss = 0.26916444\n",
      "Iteration 329, loss = 0.26832128\n",
      "Iteration 330, loss = 0.26748142\n",
      "Iteration 331, loss = 0.26664488\n",
      "Iteration 332, loss = 0.26581151\n",
      "Iteration 333, loss = 0.26498132\n",
      "Iteration 334, loss = 0.26415448\n",
      "Iteration 335, loss = 0.26333075\n",
      "Iteration 336, loss = 0.26251029\n",
      "Iteration 337, loss = 0.26169299\n",
      "Iteration 338, loss = 0.26087888\n",
      "Iteration 339, loss = 0.26006792\n",
      "Iteration 340, loss = 0.25926016\n",
      "Iteration 341, loss = 0.25845565\n",
      "Iteration 342, loss = 0.25765430\n",
      "Iteration 343, loss = 0.25685617\n",
      "Iteration 344, loss = 0.25606107\n",
      "Iteration 345, loss = 0.25526918\n",
      "Iteration 346, loss = 0.25448042\n",
      "Iteration 347, loss = 0.25369476\n",
      "Iteration 348, loss = 0.25291236\n",
      "Iteration 349, loss = 0.25213302\n",
      "Iteration 350, loss = 0.25135671\n",
      "Iteration 351, loss = 0.25058352\n",
      "Iteration 352, loss = 0.24981348\n",
      "Iteration 353, loss = 0.24904648\n",
      "Iteration 354, loss = 0.24828252\n",
      "Iteration 355, loss = 0.24752167\n",
      "Iteration 356, loss = 0.24676378\n",
      "Iteration 357, loss = 0.24600894\n",
      "Iteration 358, loss = 0.24525795\n",
      "Iteration 359, loss = 0.24450892\n",
      "Iteration 360, loss = 0.24376241\n",
      "Iteration 361, loss = 0.24301959\n",
      "Iteration 362, loss = 0.24227947\n",
      "Iteration 363, loss = 0.24154307\n",
      "Iteration 364, loss = 0.24080900\n",
      "Iteration 365, loss = 0.24007743\n",
      "Iteration 366, loss = 0.23934946\n",
      "Iteration 367, loss = 0.23862445\n",
      "Iteration 368, loss = 0.23790205\n",
      "Iteration 369, loss = 0.23718248\n",
      "Iteration 370, loss = 0.23646547\n",
      "Iteration 371, loss = 0.23575234\n",
      "Iteration 372, loss = 0.23504191\n",
      "Iteration 373, loss = 0.23433394\n",
      "Iteration 374, loss = 0.23362855\n",
      "Iteration 375, loss = 0.23292527\n",
      "Iteration 376, loss = 0.23222384\n",
      "Iteration 377, loss = 0.23152482\n",
      "Iteration 378, loss = 0.23082337\n",
      "Iteration 379, loss = 0.23011876\n",
      "Iteration 380, loss = 0.22941319\n",
      "Iteration 381, loss = 0.22870779\n",
      "Iteration 382, loss = 0.22800109\n",
      "Iteration 383, loss = 0.22729494\n",
      "Iteration 384, loss = 0.22658937\n",
      "Iteration 385, loss = 0.22588202\n",
      "Iteration 386, loss = 0.22517362\n",
      "Iteration 387, loss = 0.22446348\n",
      "Iteration 388, loss = 0.22375421\n",
      "Iteration 389, loss = 0.22304247\n",
      "Iteration 390, loss = 0.22233112\n",
      "Iteration 391, loss = 0.22161921\n",
      "Iteration 392, loss = 0.22090594\n",
      "Iteration 393, loss = 0.22019245\n",
      "Iteration 394, loss = 0.21947800\n",
      "Iteration 395, loss = 0.21876312\n",
      "Iteration 396, loss = 0.21804852\n",
      "Iteration 397, loss = 0.21733224\n",
      "Iteration 398, loss = 0.21661667\n",
      "Iteration 399, loss = 0.21590070\n",
      "Iteration 400, loss = 0.21518464\n",
      "Iteration 401, loss = 0.21446859\n",
      "Iteration 402, loss = 0.21375279\n",
      "Iteration 403, loss = 0.21303745\n",
      "Iteration 404, loss = 0.21232244\n",
      "Iteration 405, loss = 0.21160834\n",
      "Iteration 406, loss = 0.21089468\n",
      "Iteration 407, loss = 0.21018165\n",
      "Iteration 408, loss = 0.20946977\n",
      "Iteration 409, loss = 0.20875884\n",
      "Iteration 410, loss = 0.20804875\n",
      "Iteration 411, loss = 0.20733970\n",
      "Iteration 412, loss = 0.20663199\n",
      "Iteration 413, loss = 0.20592557\n",
      "Iteration 414, loss = 0.20522055\n",
      "Iteration 415, loss = 0.20451703\n",
      "Iteration 416, loss = 0.20381523\n",
      "Iteration 417, loss = 0.20311522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 418, loss = 0.20241709\n",
      "Iteration 419, loss = 0.20172093\n",
      "Iteration 420, loss = 0.20102683\n",
      "Iteration 421, loss = 0.20033544\n",
      "Iteration 422, loss = 0.19964633\n",
      "Iteration 423, loss = 0.19895952\n",
      "Iteration 424, loss = 0.19827511\n",
      "Iteration 425, loss = 0.19759313\n",
      "Iteration 426, loss = 0.19691363\n",
      "Iteration 427, loss = 0.19623675\n",
      "Iteration 428, loss = 0.19556281\n",
      "Iteration 429, loss = 0.19489145\n",
      "Iteration 430, loss = 0.19422309\n",
      "Iteration 431, loss = 0.19355756\n",
      "Iteration 432, loss = 0.19289490\n",
      "Iteration 433, loss = 0.19223501\n",
      "Iteration 434, loss = 0.19157839\n",
      "Iteration 435, loss = 0.19092429\n",
      "Iteration 436, loss = 0.19027362\n",
      "Iteration 437, loss = 0.18962592\n",
      "Iteration 438, loss = 0.18898123\n",
      "Iteration 439, loss = 0.18833980\n",
      "Iteration 440, loss = 0.18770188\n",
      "Iteration 441, loss = 0.18706711\n",
      "Iteration 442, loss = 0.18643550\n",
      "Iteration 443, loss = 0.18580705\n",
      "Iteration 444, loss = 0.18518175\n",
      "Iteration 445, loss = 0.18455966\n",
      "Iteration 446, loss = 0.18394086\n",
      "Iteration 447, loss = 0.18332527\n",
      "Iteration 448, loss = 0.18271312\n",
      "Iteration 449, loss = 0.18210443\n",
      "Iteration 450, loss = 0.18149893\n",
      "Iteration 451, loss = 0.18089659\n",
      "Iteration 452, loss = 0.18029762\n",
      "Iteration 453, loss = 0.17970178\n",
      "Iteration 454, loss = 0.17910927\n",
      "Iteration 455, loss = 0.17852019\n",
      "Iteration 456, loss = 0.17793437\n",
      "Iteration 457, loss = 0.17735173\n",
      "Iteration 458, loss = 0.17677234\n",
      "Iteration 459, loss = 0.17619627\n",
      "Iteration 460, loss = 0.17562344\n",
      "Iteration 461, loss = 0.17505393\n",
      "Iteration 462, loss = 0.17448765\n",
      "Iteration 463, loss = 0.17392460\n",
      "Iteration 464, loss = 0.17336481\n",
      "Iteration 465, loss = 0.17280826\n",
      "Iteration 466, loss = 0.17225486\n",
      "Iteration 467, loss = 0.17170475\n",
      "Iteration 468, loss = 0.17115780\n",
      "Iteration 469, loss = 0.17061400\n",
      "Iteration 470, loss = 0.17007337\n",
      "Iteration 471, loss = 0.16953583\n",
      "Iteration 472, loss = 0.16900147\n",
      "Iteration 473, loss = 0.16847024\n",
      "Iteration 474, loss = 0.16794211\n",
      "Iteration 475, loss = 0.16741702\n",
      "Iteration 476, loss = 0.16689505\n",
      "Iteration 477, loss = 0.16637614\n",
      "Iteration 478, loss = 0.16586027\n",
      "Iteration 479, loss = 0.16534708\n",
      "Iteration 480, loss = 0.16483673\n",
      "Iteration 481, loss = 0.16432929\n",
      "Iteration 482, loss = 0.16382448\n",
      "Iteration 483, loss = 0.16332229\n",
      "Iteration 484, loss = 0.16282292\n",
      "Iteration 485, loss = 0.16232639\n",
      "Iteration 486, loss = 0.16183242\n",
      "Iteration 487, loss = 0.16134110\n",
      "Iteration 488, loss = 0.16085254\n",
      "Iteration 489, loss = 0.16036677\n",
      "Iteration 490, loss = 0.15988359\n",
      "Iteration 491, loss = 0.15940296\n",
      "Iteration 492, loss = 0.15892473\n",
      "Iteration 493, loss = 0.15844888\n",
      "Iteration 494, loss = 0.15797546\n",
      "Iteration 495, loss = 0.15750479\n",
      "Iteration 496, loss = 0.15703682\n",
      "Iteration 497, loss = 0.15657190\n",
      "Iteration 498, loss = 0.15611020\n",
      "Iteration 499, loss = 0.15565090\n",
      "Iteration 500, loss = 0.15519332\n",
      "Iteration 501, loss = 0.15473813\n",
      "Iteration 502, loss = 0.15428547\n",
      "Iteration 503, loss = 0.15383529\n",
      "Iteration 504, loss = 0.15338718\n",
      "Iteration 505, loss = 0.15294105\n",
      "Iteration 506, loss = 0.15249676\n",
      "Iteration 507, loss = 0.15205468\n",
      "Iteration 508, loss = 0.15161598\n",
      "Iteration 509, loss = 0.15117993\n",
      "Iteration 510, loss = 0.15074647\n",
      "Iteration 511, loss = 0.15031588\n",
      "Iteration 512, loss = 0.14988782\n",
      "Iteration 513, loss = 0.14946231\n",
      "Iteration 514, loss = 0.14903906\n",
      "Iteration 515, loss = 0.14861821\n",
      "Iteration 516, loss = 0.14819983\n",
      "Iteration 517, loss = 0.14778385\n",
      "Iteration 518, loss = 0.14736998\n",
      "Iteration 519, loss = 0.14695848\n",
      "Iteration 520, loss = 0.14654919\n",
      "Iteration 521, loss = 0.14614226\n",
      "Iteration 522, loss = 0.14573818\n",
      "Iteration 523, loss = 0.14533634\n",
      "Iteration 524, loss = 0.14493708\n",
      "Iteration 525, loss = 0.14454020\n",
      "Iteration 526, loss = 0.14414566\n",
      "Iteration 527, loss = 0.14375344\n",
      "Iteration 528, loss = 0.14336404\n",
      "Iteration 529, loss = 0.14297772\n",
      "Iteration 530, loss = 0.14259349\n",
      "Iteration 531, loss = 0.14221146\n",
      "Iteration 532, loss = 0.14183182\n",
      "Iteration 533, loss = 0.14145443\n",
      "Iteration 534, loss = 0.14107926\n",
      "Iteration 535, loss = 0.14070569\n",
      "Iteration 536, loss = 0.14033478\n",
      "Iteration 537, loss = 0.13996607\n",
      "Iteration 538, loss = 0.13959948\n",
      "Iteration 539, loss = 0.13923489\n",
      "Iteration 540, loss = 0.13887201\n",
      "Iteration 541, loss = 0.13851110\n",
      "Iteration 542, loss = 0.13815225\n",
      "Iteration 543, loss = 0.13779568\n",
      "Iteration 544, loss = 0.13744090\n",
      "Iteration 545, loss = 0.13708815\n",
      "Iteration 546, loss = 0.13673774\n",
      "Iteration 547, loss = 0.13638926\n",
      "Iteration 548, loss = 0.13604216\n",
      "Iteration 549, loss = 0.13569677\n",
      "Iteration 550, loss = 0.13535340\n",
      "Iteration 551, loss = 0.13501249\n",
      "Iteration 552, loss = 0.13467351\n",
      "Iteration 553, loss = 0.13433644\n",
      "Iteration 554, loss = 0.13400133\n",
      "Iteration 555, loss = 0.13366875\n",
      "Iteration 556, loss = 0.13333775\n",
      "Iteration 557, loss = 0.13300860\n",
      "Iteration 558, loss = 0.13268170\n",
      "Iteration 559, loss = 0.13235701\n",
      "Iteration 560, loss = 0.13203479\n",
      "Iteration 561, loss = 0.13171459\n",
      "Iteration 562, loss = 0.13139651\n",
      "Iteration 563, loss = 0.13107968\n",
      "Iteration 564, loss = 0.13076436\n",
      "Iteration 565, loss = 0.13045089\n",
      "Iteration 566, loss = 0.13013910\n",
      "Iteration 567, loss = 0.12982907\n",
      "Iteration 568, loss = 0.12952058\n",
      "Iteration 569, loss = 0.12921379\n",
      "Iteration 570, loss = 0.12890901\n",
      "Iteration 571, loss = 0.12860643\n",
      "Iteration 572, loss = 0.12830562\n",
      "Iteration 573, loss = 0.12800642\n",
      "Iteration 574, loss = 0.12770893\n",
      "Iteration 575, loss = 0.12741305\n",
      "Iteration 576, loss = 0.12711892\n",
      "Iteration 577, loss = 0.12682636\n",
      "Iteration 578, loss = 0.12653535\n",
      "Iteration 579, loss = 0.12624589\n",
      "Iteration 580, loss = 0.12595795\n",
      "Iteration 581, loss = 0.12567151\n",
      "Iteration 582, loss = 0.12538657\n",
      "Iteration 583, loss = 0.12510310\n",
      "Iteration 584, loss = 0.12482110\n",
      "Iteration 585, loss = 0.12454055\n",
      "Iteration 586, loss = 0.12426145\n",
      "Iteration 587, loss = 0.12398383\n",
      "Iteration 588, loss = 0.12370798\n",
      "Iteration 589, loss = 0.12343326\n",
      "Iteration 590, loss = 0.12315972\n",
      "Iteration 591, loss = 0.12288800\n",
      "Iteration 592, loss = 0.12261774\n",
      "Iteration 593, loss = 0.12234882\n",
      "Iteration 594, loss = 0.12208117\n",
      "Iteration 595, loss = 0.12181478\n",
      "Iteration 596, loss = 0.12154966\n",
      "Iteration 597, loss = 0.12128598\n",
      "Iteration 598, loss = 0.12102392\n",
      "Iteration 599, loss = 0.12076308\n",
      "Iteration 600, loss = 0.12050346\n",
      "Iteration 601, loss = 0.12024508\n",
      "Iteration 602, loss = 0.11998837\n",
      "Iteration 603, loss = 0.11973285\n",
      "Iteration 604, loss = 0.11947841\n",
      "Iteration 605, loss = 0.11922511\n",
      "Iteration 606, loss = 0.11897344\n",
      "Iteration 607, loss = 0.11872302\n",
      "Iteration 608, loss = 0.11847381\n",
      "Iteration 609, loss = 0.11822582\n",
      "Iteration 610, loss = 0.11797908\n",
      "Iteration 611, loss = 0.11773372\n",
      "Iteration 612, loss = 0.11748937\n",
      "Iteration 613, loss = 0.11724631\n",
      "Iteration 614, loss = 0.11700450\n",
      "Iteration 615, loss = 0.11676386\n",
      "Iteration 616, loss = 0.11652439\n",
      "Iteration 617, loss = 0.11628606\n",
      "Iteration 618, loss = 0.11604890\n",
      "Iteration 619, loss = 0.11581288\n",
      "Iteration 620, loss = 0.11557802\n",
      "Iteration 621, loss = 0.11534431\n",
      "Iteration 622, loss = 0.11511175\n",
      "Iteration 623, loss = 0.11488032\n",
      "Iteration 624, loss = 0.11465001\n",
      "Iteration 625, loss = 0.11442083\n",
      "Iteration 626, loss = 0.11419274\n",
      "Iteration 627, loss = 0.11396576\n",
      "Iteration 628, loss = 0.11373985\n",
      "Iteration 629, loss = 0.11351503\n",
      "Iteration 630, loss = 0.11329127\n",
      "Iteration 631, loss = 0.11306859\n",
      "Iteration 632, loss = 0.11284696\n",
      "Iteration 633, loss = 0.11262639\n",
      "Iteration 634, loss = 0.11240687\n",
      "Iteration 635, loss = 0.11218843\n",
      "Iteration 636, loss = 0.11197104\n",
      "Iteration 637, loss = 0.11175468\n",
      "Iteration 638, loss = 0.11153938\n",
      "Iteration 639, loss = 0.11132509\n",
      "Iteration 640, loss = 0.11111180\n",
      "Iteration 641, loss = 0.11089949\n",
      "Iteration 642, loss = 0.11068817\n",
      "Iteration 643, loss = 0.11047784\n",
      "Iteration 644, loss = 0.11026847\n",
      "Iteration 645, loss = 0.11006008\n",
      "Iteration 646, loss = 0.10985266\n",
      "Iteration 647, loss = 0.10964623\n",
      "Iteration 648, loss = 0.10944076\n",
      "Iteration 649, loss = 0.10923624\n",
      "Iteration 650, loss = 0.10903266\n",
      "Iteration 651, loss = 0.10883003\n",
      "Iteration 652, loss = 0.10862833\n",
      "Iteration 653, loss = 0.10842756\n",
      "Iteration 654, loss = 0.10822771\n",
      "Iteration 655, loss = 0.10802878\n",
      "Iteration 656, loss = 0.10783076\n",
      "Iteration 657, loss = 0.10763363\n",
      "Iteration 658, loss = 0.10743737\n",
      "Iteration 659, loss = 0.10724196\n",
      "Iteration 660, loss = 0.10704741\n",
      "Iteration 661, loss = 0.10685370\n",
      "Iteration 662, loss = 0.10666091\n",
      "Iteration 663, loss = 0.10646879\n",
      "Iteration 664, loss = 0.10627753\n",
      "Iteration 665, loss = 0.10608705\n",
      "Iteration 666, loss = 0.10589719\n",
      "Iteration 667, loss = 0.10570814\n",
      "Iteration 668, loss = 0.10551951\n",
      "Iteration 669, loss = 0.10533159\n",
      "Iteration 670, loss = 0.10514427\n",
      "Iteration 671, loss = 0.10495708\n",
      "Iteration 672, loss = 0.10477033\n",
      "Iteration 673, loss = 0.10458395\n",
      "Iteration 674, loss = 0.10439798\n",
      "Iteration 675, loss = 0.10421248\n",
      "Iteration 676, loss = 0.10402687\n",
      "Iteration 677, loss = 0.10384188\n",
      "Iteration 678, loss = 0.10365710\n",
      "Iteration 679, loss = 0.10347264\n",
      "Iteration 680, loss = 0.10328832\n",
      "Iteration 681, loss = 0.10310450\n",
      "Iteration 682, loss = 0.10292071\n",
      "Iteration 683, loss = 0.10273687\n",
      "Iteration 684, loss = 0.10255412\n",
      "Iteration 685, loss = 0.10237075\n",
      "Iteration 686, loss = 0.10218832\n",
      "Iteration 687, loss = 0.10200579\n",
      "Iteration 688, loss = 0.10182351\n",
      "Iteration 689, loss = 0.10164192\n",
      "Iteration 690, loss = 0.10146020\n",
      "Iteration 691, loss = 0.10127940\n",
      "Iteration 692, loss = 0.10109868\n",
      "Iteration 693, loss = 0.10091803\n",
      "Iteration 694, loss = 0.10073790\n",
      "Iteration 695, loss = 0.10055824\n",
      "Iteration 696, loss = 0.10037887\n",
      "Iteration 697, loss = 0.10019971\n",
      "Iteration 698, loss = 0.10002088\n",
      "Iteration 699, loss = 0.09984263\n",
      "Iteration 700, loss = 0.09966498\n",
      "Iteration 701, loss = 0.09948752\n",
      "Iteration 702, loss = 0.09931050\n",
      "Iteration 703, loss = 0.09913414\n",
      "Iteration 704, loss = 0.09895826\n",
      "Iteration 705, loss = 0.09878287\n",
      "Iteration 706, loss = 0.09860793\n",
      "Iteration 707, loss = 0.09843343\n",
      "Iteration 708, loss = 0.09825984\n",
      "Iteration 709, loss = 0.09808656\n",
      "Iteration 710, loss = 0.09791348\n",
      "Iteration 711, loss = 0.09774122\n",
      "Iteration 712, loss = 0.09756958\n",
      "Iteration 713, loss = 0.09739834\n",
      "Iteration 714, loss = 0.09722754\n",
      "Iteration 715, loss = 0.09705732\n",
      "Iteration 716, loss = 0.09688845\n",
      "Iteration 717, loss = 0.09672006\n",
      "Iteration 718, loss = 0.09655194\n",
      "Iteration 719, loss = 0.09638516\n",
      "Iteration 720, loss = 0.09621901\n",
      "Iteration 721, loss = 0.09605332\n",
      "Iteration 722, loss = 0.09588814\n",
      "Iteration 723, loss = 0.09572356\n",
      "Iteration 724, loss = 0.09555986\n",
      "Iteration 725, loss = 0.09539732\n",
      "Iteration 726, loss = 0.09523525\n",
      "Iteration 727, loss = 0.09507387\n",
      "Iteration 728, loss = 0.09491357\n",
      "Iteration 729, loss = 0.09475388\n",
      "Iteration 730, loss = 0.09459458\n",
      "Iteration 731, loss = 0.09443620\n",
      "Iteration 732, loss = 0.09427821\n",
      "Iteration 733, loss = 0.09412044\n",
      "Iteration 734, loss = 0.09396273\n",
      "Iteration 735, loss = 0.09380511\n",
      "Iteration 736, loss = 0.09364715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 737, loss = 0.09348878\n",
      "Iteration 738, loss = 0.09333020\n",
      "Iteration 739, loss = 0.09317081\n",
      "Iteration 740, loss = 0.09301035\n",
      "Iteration 741, loss = 0.09284888\n",
      "Iteration 742, loss = 0.09268647\n",
      "Iteration 743, loss = 0.09252273\n",
      "Iteration 744, loss = 0.09235724\n",
      "Iteration 745, loss = 0.09219024\n",
      "Iteration 746, loss = 0.09202131\n",
      "Iteration 747, loss = 0.09185027\n",
      "Iteration 748, loss = 0.09167732\n",
      "Iteration 749, loss = 0.09150272\n",
      "Iteration 750, loss = 0.09132625\n",
      "Iteration 751, loss = 0.09114773\n",
      "Iteration 752, loss = 0.09096711\n",
      "Iteration 753, loss = 0.09078463\n",
      "Iteration 754, loss = 0.09060026\n",
      "Iteration 755, loss = 0.09041424\n",
      "Iteration 756, loss = 0.09022650\n",
      "Iteration 757, loss = 0.09003707\n",
      "Iteration 758, loss = 0.08984606\n",
      "Iteration 759, loss = 0.08965351\n",
      "Iteration 760, loss = 0.08945952\n",
      "Iteration 761, loss = 0.08926415\n",
      "Iteration 762, loss = 0.08906750\n",
      "Iteration 763, loss = 0.08886963\n",
      "Iteration 764, loss = 0.08867060\n",
      "Iteration 765, loss = 0.08847051\n",
      "Iteration 766, loss = 0.08826942\n",
      "Iteration 767, loss = 0.08806749\n",
      "Iteration 768, loss = 0.08786471\n",
      "Iteration 769, loss = 0.08766119\n",
      "Iteration 770, loss = 0.08745707\n",
      "Iteration 771, loss = 0.08725240\n",
      "Iteration 772, loss = 0.08704721\n",
      "Iteration 773, loss = 0.08684161\n",
      "Iteration 774, loss = 0.08663571\n",
      "Iteration 775, loss = 0.08642960\n",
      "Iteration 776, loss = 0.08622318\n",
      "Iteration 777, loss = 0.08601656\n",
      "Iteration 778, loss = 0.08580990\n",
      "Iteration 779, loss = 0.08560323\n",
      "Iteration 780, loss = 0.08539662\n",
      "Iteration 781, loss = 0.08519007\n",
      "Iteration 782, loss = 0.08498364\n",
      "Iteration 783, loss = 0.08477747\n",
      "Iteration 784, loss = 0.08457149\n",
      "Iteration 785, loss = 0.08436573\n",
      "Iteration 786, loss = 0.08416031\n",
      "Iteration 787, loss = 0.08395530\n",
      "Iteration 788, loss = 0.08375064\n",
      "Iteration 789, loss = 0.08354649\n",
      "Iteration 790, loss = 0.08334282\n",
      "Iteration 791, loss = 0.08313963\n",
      "Iteration 792, loss = 0.08293695\n",
      "Iteration 793, loss = 0.08273491\n",
      "Iteration 794, loss = 0.08253348\n",
      "Iteration 795, loss = 0.08233267\n",
      "Iteration 796, loss = 0.08213263\n",
      "Iteration 797, loss = 0.08193332\n",
      "Iteration 798, loss = 0.08173472\n",
      "Iteration 799, loss = 0.08153688\n",
      "Iteration 800, loss = 0.08133982\n",
      "Iteration 801, loss = 0.08114356\n",
      "Iteration 802, loss = 0.08094815\n",
      "Iteration 803, loss = 0.08075359\n",
      "Iteration 804, loss = 0.08055992\n",
      "Iteration 805, loss = 0.08036720\n",
      "Iteration 806, loss = 0.08017544\n",
      "Iteration 807, loss = 0.07998495\n",
      "Iteration 808, loss = 0.07979548\n",
      "Iteration 809, loss = 0.07960690\n",
      "Iteration 810, loss = 0.07941926\n",
      "Iteration 811, loss = 0.07923260\n",
      "Iteration 812, loss = 0.07904690\n",
      "Iteration 813, loss = 0.07886215\n",
      "Iteration 814, loss = 0.07867837\n",
      "Iteration 815, loss = 0.07849561\n",
      "Iteration 816, loss = 0.07831387\n",
      "Iteration 817, loss = 0.07813311\n",
      "Iteration 818, loss = 0.07795337\n",
      "Iteration 819, loss = 0.07777466\n",
      "Iteration 820, loss = 0.07759698\n",
      "Iteration 821, loss = 0.07742033\n",
      "Iteration 822, loss = 0.07724471\n",
      "Iteration 823, loss = 0.07707012\n",
      "Iteration 824, loss = 0.07689663\n",
      "Iteration 825, loss = 0.07672420\n",
      "Iteration 826, loss = 0.07655283\n",
      "Iteration 827, loss = 0.07638250\n",
      "Iteration 828, loss = 0.07621322\n",
      "Iteration 829, loss = 0.07604503\n",
      "Iteration 830, loss = 0.07587791\n",
      "Iteration 831, loss = 0.07571186\n",
      "Iteration 832, loss = 0.07554685\n",
      "Iteration 833, loss = 0.07538291\n",
      "Iteration 834, loss = 0.07522000\n",
      "Iteration 835, loss = 0.07505813\n",
      "Iteration 836, loss = 0.07489732\n",
      "Iteration 837, loss = 0.07473754\n",
      "Iteration 838, loss = 0.07457883\n",
      "Iteration 839, loss = 0.07442116\n",
      "Iteration 840, loss = 0.07426453\n",
      "Iteration 841, loss = 0.07410893\n",
      "Iteration 842, loss = 0.07395434\n",
      "Iteration 843, loss = 0.07380084\n",
      "Iteration 844, loss = 0.07364833\n",
      "Iteration 845, loss = 0.07349681\n",
      "Iteration 846, loss = 0.07334626\n",
      "Iteration 847, loss = 0.07319670\n",
      "Iteration 848, loss = 0.07304818\n",
      "Iteration 849, loss = 0.07290065\n",
      "Iteration 850, loss = 0.07275411\n",
      "Iteration 851, loss = 0.07260854\n",
      "Iteration 852, loss = 0.07246394\n",
      "Iteration 853, loss = 0.07232032\n",
      "Iteration 854, loss = 0.07217768\n",
      "Iteration 855, loss = 0.07203600\n",
      "Iteration 856, loss = 0.07189528\n",
      "Iteration 857, loss = 0.07175556\n",
      "Iteration 858, loss = 0.07161678\n",
      "Iteration 859, loss = 0.07147901\n",
      "Iteration 860, loss = 0.07134217\n",
      "Iteration 861, loss = 0.07120623\n",
      "Iteration 862, loss = 0.07107122\n",
      "Iteration 863, loss = 0.07093712\n",
      "Iteration 864, loss = 0.07080393\n",
      "Iteration 865, loss = 0.07067162\n",
      "Iteration 866, loss = 0.07054022\n",
      "Iteration 867, loss = 0.07040970\n",
      "Iteration 868, loss = 0.07028006\n",
      "Iteration 869, loss = 0.07015130\n",
      "Iteration 870, loss = 0.07002340\n",
      "Iteration 871, loss = 0.06989637\n",
      "Iteration 872, loss = 0.06977019\n",
      "Iteration 873, loss = 0.06964485\n",
      "Iteration 874, loss = 0.06952037\n",
      "Iteration 875, loss = 0.06939671\n",
      "Iteration 876, loss = 0.06927389\n",
      "Iteration 877, loss = 0.06915190\n",
      "Iteration 878, loss = 0.06903072\n",
      "Iteration 879, loss = 0.06891035\n",
      "Iteration 880, loss = 0.06879079\n",
      "Iteration 881, loss = 0.06867203\n",
      "Iteration 882, loss = 0.06855406\n",
      "Iteration 883, loss = 0.06843688\n",
      "Iteration 884, loss = 0.06832048\n",
      "Iteration 885, loss = 0.06820485\n",
      "Iteration 886, loss = 0.06809000\n",
      "Iteration 887, loss = 0.06797591\n",
      "Iteration 888, loss = 0.06786258\n",
      "Iteration 889, loss = 0.06775000\n",
      "Iteration 890, loss = 0.06763817\n",
      "Iteration 891, loss = 0.06752708\n",
      "Iteration 892, loss = 0.06741672\n",
      "Iteration 893, loss = 0.06730709\n",
      "Iteration 894, loss = 0.06719818\n",
      "Iteration 895, loss = 0.06708999\n",
      "Iteration 896, loss = 0.06698251\n",
      "Iteration 897, loss = 0.06687573\n",
      "Iteration 898, loss = 0.06676965\n",
      "Iteration 899, loss = 0.06666426\n",
      "Iteration 900, loss = 0.06655957\n",
      "Iteration 901, loss = 0.06645557\n",
      "Iteration 902, loss = 0.06635224\n",
      "Iteration 903, loss = 0.06624957\n",
      "Iteration 904, loss = 0.06614756\n",
      "Iteration 905, loss = 0.06604620\n",
      "Iteration 906, loss = 0.06594552\n",
      "Iteration 907, loss = 0.06584548\n",
      "Iteration 908, loss = 0.06574608\n",
      "Iteration 909, loss = 0.06564731\n",
      "Iteration 910, loss = 0.06554918\n",
      "Iteration 911, loss = 0.06545167\n",
      "Iteration 912, loss = 0.06535479\n",
      "Iteration 913, loss = 0.06525860\n",
      "Iteration 914, loss = 0.06516295\n",
      "Iteration 915, loss = 0.06506801\n",
      "Iteration 916, loss = 0.06497364\n",
      "Iteration 917, loss = 0.06487982\n",
      "Iteration 918, loss = 0.06478667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.89706643\n",
      "Iteration 3, loss = 1.85360582\n",
      "Iteration 4, loss = 1.81120620\n",
      "Iteration 5, loss = 1.76986631\n",
      "Iteration 6, loss = 1.72940288\n",
      "Iteration 7, loss = 1.68973291\n",
      "Iteration 8, loss = 1.65090503\n",
      "Iteration 9, loss = 1.61282383\n",
      "Iteration 10, loss = 1.57546901\n",
      "Iteration 11, loss = 1.53894153\n",
      "Iteration 12, loss = 1.50323608\n",
      "Iteration 13, loss = 1.46836897\n",
      "Iteration 14, loss = 1.43436061\n",
      "Iteration 15, loss = 1.40126340\n",
      "Iteration 16, loss = 1.36906430\n",
      "Iteration 17, loss = 1.33782019\n",
      "Iteration 18, loss = 1.30758383\n",
      "Iteration 19, loss = 1.27844544\n",
      "Iteration 20, loss = 1.25043352\n",
      "Iteration 21, loss = 1.22364446\n",
      "Iteration 22, loss = 1.19813967\n",
      "Iteration 23, loss = 1.17397847\n",
      "Iteration 24, loss = 1.15122802\n",
      "Iteration 25, loss = 1.12987500\n",
      "Iteration 26, loss = 1.10996813\n",
      "Iteration 27, loss = 1.09150251\n",
      "Iteration 28, loss = 1.07446544\n",
      "Iteration 29, loss = 1.05881366\n",
      "Iteration 30, loss = 1.04449221\n",
      "Iteration 31, loss = 1.03139868\n",
      "Iteration 32, loss = 1.01944946\n",
      "Iteration 33, loss = 1.00854664\n",
      "Iteration 34, loss = 0.99852832\n",
      "Iteration 35, loss = 0.98926911\n",
      "Iteration 36, loss = 0.98064174\n",
      "Iteration 37, loss = 0.97250386\n",
      "Iteration 38, loss = 0.96471685\n",
      "Iteration 39, loss = 0.95714802\n",
      "Iteration 40, loss = 0.94969529\n",
      "Iteration 41, loss = 0.94225679\n",
      "Iteration 42, loss = 0.93476594\n",
      "Iteration 43, loss = 0.92718691\n",
      "Iteration 44, loss = 0.91957167\n",
      "Iteration 45, loss = 0.91191092\n",
      "Iteration 46, loss = 0.90423249\n",
      "Iteration 47, loss = 0.89655111\n",
      "Iteration 48, loss = 0.88894509\n",
      "Iteration 49, loss = 0.88142332\n",
      "Iteration 50, loss = 0.87394037\n",
      "Iteration 51, loss = 0.86658641\n",
      "Iteration 52, loss = 0.85932518\n",
      "Iteration 53, loss = 0.85212408\n",
      "Iteration 54, loss = 0.84496899\n",
      "Iteration 55, loss = 0.83785458\n",
      "Iteration 56, loss = 0.83083209\n",
      "Iteration 57, loss = 0.82396451\n",
      "Iteration 58, loss = 0.81725308\n",
      "Iteration 59, loss = 0.81075373\n",
      "Iteration 60, loss = 0.80448135\n",
      "Iteration 61, loss = 0.79839765\n",
      "Iteration 62, loss = 0.79254260\n",
      "Iteration 63, loss = 0.78695224\n",
      "Iteration 64, loss = 0.78154479\n",
      "Iteration 65, loss = 0.77633695\n",
      "Iteration 66, loss = 0.77133363\n",
      "Iteration 67, loss = 0.76651703\n",
      "Iteration 68, loss = 0.76181350\n",
      "Iteration 69, loss = 0.75722279\n",
      "Iteration 70, loss = 0.75269895\n",
      "Iteration 71, loss = 0.74822069\n",
      "Iteration 72, loss = 0.74377036\n",
      "Iteration 73, loss = 0.73934253\n",
      "Iteration 74, loss = 0.73493063\n",
      "Iteration 75, loss = 0.73053744\n",
      "Iteration 76, loss = 0.72616949\n",
      "Iteration 77, loss = 0.72183605\n",
      "Iteration 78, loss = 0.71753094\n",
      "Iteration 79, loss = 0.71325682\n",
      "Iteration 80, loss = 0.70902241\n",
      "Iteration 81, loss = 0.70482512\n",
      "Iteration 82, loss = 0.70066747\n",
      "Iteration 83, loss = 0.69655636\n",
      "Iteration 84, loss = 0.69249737\n",
      "Iteration 85, loss = 0.68848274\n",
      "Iteration 86, loss = 0.68450920\n",
      "Iteration 87, loss = 0.68057471\n",
      "Iteration 88, loss = 0.67667598\n",
      "Iteration 89, loss = 0.67282282\n",
      "Iteration 90, loss = 0.66901646\n",
      "Iteration 91, loss = 0.66524529\n",
      "Iteration 92, loss = 0.66151104\n",
      "Iteration 93, loss = 0.65781603\n",
      "Iteration 94, loss = 0.65415973\n",
      "Iteration 95, loss = 0.65054179\n",
      "Iteration 96, loss = 0.64696034\n",
      "Iteration 97, loss = 0.64342923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 98, loss = 0.63993994\n",
      "Iteration 99, loss = 0.63648800\n",
      "Iteration 100, loss = 0.63307357\n",
      "Iteration 101, loss = 0.62969974\n",
      "Iteration 102, loss = 0.62636549\n",
      "Iteration 103, loss = 0.62306715\n",
      "Iteration 104, loss = 0.61980375\n",
      "Iteration 105, loss = 0.61656584\n",
      "Iteration 106, loss = 0.61334942\n",
      "Iteration 107, loss = 0.61016280\n",
      "Iteration 108, loss = 0.60701021\n",
      "Iteration 109, loss = 0.60388620\n",
      "Iteration 110, loss = 0.60079880\n",
      "Iteration 111, loss = 0.59774429\n",
      "Iteration 112, loss = 0.59472760\n",
      "Iteration 113, loss = 0.59173429\n",
      "Iteration 114, loss = 0.58877039\n",
      "Iteration 115, loss = 0.58583368\n",
      "Iteration 116, loss = 0.58292597\n",
      "Iteration 117, loss = 0.58005574\n",
      "Iteration 118, loss = 0.57723339\n",
      "Iteration 119, loss = 0.57444097\n",
      "Iteration 120, loss = 0.57167857\n",
      "Iteration 121, loss = 0.56894631\n",
      "Iteration 122, loss = 0.56624173\n",
      "Iteration 123, loss = 0.56356709\n",
      "Iteration 124, loss = 0.56092755\n",
      "Iteration 125, loss = 0.55831687\n",
      "Iteration 126, loss = 0.55574614\n",
      "Iteration 127, loss = 0.55320946\n",
      "Iteration 128, loss = 0.55071274\n",
      "Iteration 129, loss = 0.54823913\n",
      "Iteration 130, loss = 0.54578810\n",
      "Iteration 131, loss = 0.54336859\n",
      "Iteration 132, loss = 0.54096848\n",
      "Iteration 133, loss = 0.53859012\n",
      "Iteration 134, loss = 0.53623723\n",
      "Iteration 135, loss = 0.53390381\n",
      "Iteration 136, loss = 0.53160171\n",
      "Iteration 137, loss = 0.52932996\n",
      "Iteration 138, loss = 0.52707702\n",
      "Iteration 139, loss = 0.52484548\n",
      "Iteration 140, loss = 0.52263734\n",
      "Iteration 141, loss = 0.52046593\n",
      "Iteration 142, loss = 0.51831160\n",
      "Iteration 143, loss = 0.51617501\n",
      "Iteration 144, loss = 0.51407197\n",
      "Iteration 145, loss = 0.51199779\n",
      "Iteration 146, loss = 0.50994975\n",
      "Iteration 147, loss = 0.50792135\n",
      "Iteration 148, loss = 0.50591533\n",
      "Iteration 149, loss = 0.50393267\n",
      "Iteration 150, loss = 0.50197040\n",
      "Iteration 151, loss = 0.50003088\n",
      "Iteration 152, loss = 0.49811598\n",
      "Iteration 153, loss = 0.49621943\n",
      "Iteration 154, loss = 0.49434245\n",
      "Iteration 155, loss = 0.49248889\n",
      "Iteration 156, loss = 0.49065490\n",
      "Iteration 157, loss = 0.48883665\n",
      "Iteration 158, loss = 0.48703426\n",
      "Iteration 159, loss = 0.48524764\n",
      "Iteration 160, loss = 0.48347728\n",
      "Iteration 161, loss = 0.48172291\n",
      "Iteration 162, loss = 0.47998361\n",
      "Iteration 163, loss = 0.47825893\n",
      "Iteration 164, loss = 0.47655267\n",
      "Iteration 165, loss = 0.47486132\n",
      "Iteration 166, loss = 0.47318400\n",
      "Iteration 167, loss = 0.47152018\n",
      "Iteration 168, loss = 0.46986962\n",
      "Iteration 169, loss = 0.46823209\n",
      "Iteration 170, loss = 0.46660734\n",
      "Iteration 171, loss = 0.46499515\n",
      "Iteration 172, loss = 0.46339546\n",
      "Iteration 173, loss = 0.46180773\n",
      "Iteration 174, loss = 0.46023189\n",
      "Iteration 175, loss = 0.45866833\n",
      "Iteration 176, loss = 0.45711670\n",
      "Iteration 177, loss = 0.45557636\n",
      "Iteration 178, loss = 0.45404734\n",
      "Iteration 179, loss = 0.45252924\n",
      "Iteration 180, loss = 0.45102202\n",
      "Iteration 181, loss = 0.44952518\n",
      "Iteration 182, loss = 0.44803868\n",
      "Iteration 183, loss = 0.44656220\n",
      "Iteration 184, loss = 0.44509595\n",
      "Iteration 185, loss = 0.44363969\n",
      "Iteration 186, loss = 0.44219334\n",
      "Iteration 187, loss = 0.44075741\n",
      "Iteration 188, loss = 0.43933139\n",
      "Iteration 189, loss = 0.43791470\n",
      "Iteration 190, loss = 0.43650656\n",
      "Iteration 191, loss = 0.43510737\n",
      "Iteration 192, loss = 0.43371692\n",
      "Iteration 193, loss = 0.43233507\n",
      "Iteration 194, loss = 0.43096172\n",
      "Iteration 195, loss = 0.42959678\n",
      "Iteration 196, loss = 0.42823989\n",
      "Iteration 197, loss = 0.42689078\n",
      "Iteration 198, loss = 0.42554952\n",
      "Iteration 199, loss = 0.42421606\n",
      "Iteration 200, loss = 0.42289020\n",
      "Iteration 201, loss = 0.42157180\n",
      "Iteration 202, loss = 0.42026074\n",
      "Iteration 203, loss = 0.41895721\n",
      "Iteration 204, loss = 0.41766081\n",
      "Iteration 205, loss = 0.41637124\n",
      "Iteration 206, loss = 0.41508843\n",
      "Iteration 207, loss = 0.41381244\n",
      "Iteration 208, loss = 0.41254316\n",
      "Iteration 209, loss = 0.41128018\n",
      "Iteration 210, loss = 0.41002396\n",
      "Iteration 211, loss = 0.40877419\n",
      "Iteration 212, loss = 0.40753128\n",
      "Iteration 213, loss = 0.40629485\n",
      "Iteration 214, loss = 0.40506444\n",
      "Iteration 215, loss = 0.40383982\n",
      "Iteration 216, loss = 0.40262104\n",
      "Iteration 217, loss = 0.40140827\n",
      "Iteration 218, loss = 0.40020134\n",
      "Iteration 219, loss = 0.39900039\n",
      "Iteration 220, loss = 0.39780587\n",
      "Iteration 221, loss = 0.39661722\n",
      "Iteration 222, loss = 0.39543401\n",
      "Iteration 223, loss = 0.39425622\n",
      "Iteration 224, loss = 0.39308390\n",
      "Iteration 225, loss = 0.39191700\n",
      "Iteration 226, loss = 0.39075548\n",
      "Iteration 227, loss = 0.38959908\n",
      "Iteration 228, loss = 0.38844786\n",
      "Iteration 229, loss = 0.38730179\n",
      "Iteration 230, loss = 0.38616096\n",
      "Iteration 231, loss = 0.38502516\n",
      "Iteration 232, loss = 0.38389434\n",
      "Iteration 233, loss = 0.38276845\n",
      "Iteration 234, loss = 0.38164744\n",
      "Iteration 235, loss = 0.38053133\n",
      "Iteration 236, loss = 0.37942006\n",
      "Iteration 237, loss = 0.37831368\n",
      "Iteration 238, loss = 0.37721208\n",
      "Iteration 239, loss = 0.37611536\n",
      "Iteration 240, loss = 0.37502365\n",
      "Iteration 241, loss = 0.37393648\n",
      "Iteration 242, loss = 0.37285377\n",
      "Iteration 243, loss = 0.37177525\n",
      "Iteration 244, loss = 0.37070093\n",
      "Iteration 245, loss = 0.36963087\n",
      "Iteration 246, loss = 0.36856516\n",
      "Iteration 247, loss = 0.36750373\n",
      "Iteration 248, loss = 0.36644649\n",
      "Iteration 249, loss = 0.36539342\n",
      "Iteration 250, loss = 0.36434446\n",
      "Iteration 251, loss = 0.36329962\n",
      "Iteration 252, loss = 0.36225879\n",
      "Iteration 253, loss = 0.36122189\n",
      "Iteration 254, loss = 0.36018889\n",
      "Iteration 255, loss = 0.35915993\n",
      "Iteration 256, loss = 0.35813488\n",
      "Iteration 257, loss = 0.35711351\n",
      "Iteration 258, loss = 0.35609592\n",
      "Iteration 259, loss = 0.35508226\n",
      "Iteration 260, loss = 0.35407233\n",
      "Iteration 261, loss = 0.35306609\n",
      "Iteration 262, loss = 0.35206407\n",
      "Iteration 263, loss = 0.35106631\n",
      "Iteration 264, loss = 0.35007224\n",
      "Iteration 265, loss = 0.34908229\n",
      "Iteration 266, loss = 0.34809635\n",
      "Iteration 267, loss = 0.34711434\n",
      "Iteration 268, loss = 0.34613553\n",
      "Iteration 269, loss = 0.34515990\n",
      "Iteration 270, loss = 0.34418762\n",
      "Iteration 271, loss = 0.34321898\n",
      "Iteration 272, loss = 0.34225402\n",
      "Iteration 273, loss = 0.34129269\n",
      "Iteration 274, loss = 0.34033485\n",
      "Iteration 275, loss = 0.33938042\n",
      "Iteration 276, loss = 0.33842948\n",
      "Iteration 277, loss = 0.33748236\n",
      "Iteration 278, loss = 0.33653858\n",
      "Iteration 279, loss = 0.33559815\n",
      "Iteration 280, loss = 0.33466107\n",
      "Iteration 281, loss = 0.33372723\n",
      "Iteration 282, loss = 0.33279686\n",
      "Iteration 283, loss = 0.33186975\n",
      "Iteration 284, loss = 0.33094611\n",
      "Iteration 285, loss = 0.33002592\n",
      "Iteration 286, loss = 0.32910906\n",
      "Iteration 287, loss = 0.32819553\n",
      "Iteration 288, loss = 0.32728529\n",
      "Iteration 289, loss = 0.32637829\n",
      "Iteration 290, loss = 0.32547450\n",
      "Iteration 291, loss = 0.32457395\n",
      "Iteration 292, loss = 0.32367661\n",
      "Iteration 293, loss = 0.32278271\n",
      "Iteration 294, loss = 0.32189192\n",
      "Iteration 295, loss = 0.32100425\n",
      "Iteration 296, loss = 0.32011971\n",
      "Iteration 297, loss = 0.31923813\n",
      "Iteration 298, loss = 0.31835965\n",
      "Iteration 299, loss = 0.31748450\n",
      "Iteration 300, loss = 0.31661245\n",
      "Iteration 301, loss = 0.31574395\n",
      "Iteration 302, loss = 0.31487845\n",
      "Iteration 303, loss = 0.31401593\n",
      "Iteration 304, loss = 0.31315639\n",
      "Iteration 305, loss = 0.31229982\n",
      "Iteration 306, loss = 0.31144623\n",
      "Iteration 307, loss = 0.31059573\n",
      "Iteration 308, loss = 0.30974816\n",
      "Iteration 309, loss = 0.30890340\n",
      "Iteration 310, loss = 0.30806178\n",
      "Iteration 311, loss = 0.30722308\n",
      "Iteration 312, loss = 0.30638734\n",
      "Iteration 313, loss = 0.30555448\n",
      "Iteration 314, loss = 0.30472454\n",
      "Iteration 315, loss = 0.30389752\n",
      "Iteration 316, loss = 0.30307337\n",
      "Iteration 317, loss = 0.30225208\n",
      "Iteration 318, loss = 0.30143374\n",
      "Iteration 319, loss = 0.30061814\n",
      "Iteration 320, loss = 0.29980560\n",
      "Iteration 321, loss = 0.29899596\n",
      "Iteration 322, loss = 0.29818912\n",
      "Iteration 323, loss = 0.29738507\n",
      "Iteration 324, loss = 0.29658384\n",
      "Iteration 325, loss = 0.29578543\n",
      "Iteration 326, loss = 0.29498981\n",
      "Iteration 327, loss = 0.29419696\n",
      "Iteration 328, loss = 0.29340687\n",
      "Iteration 329, loss = 0.29261961\n",
      "Iteration 330, loss = 0.29183500\n",
      "Iteration 331, loss = 0.29105333\n",
      "Iteration 332, loss = 0.29027438\n",
      "Iteration 333, loss = 0.28949815\n",
      "Iteration 334, loss = 0.28872463\n",
      "Iteration 335, loss = 0.28795389\n",
      "Iteration 336, loss = 0.28718578\n",
      "Iteration 337, loss = 0.28642046\n",
      "Iteration 338, loss = 0.28565793\n",
      "Iteration 339, loss = 0.28489805\n",
      "Iteration 340, loss = 0.28414083\n",
      "Iteration 341, loss = 0.28338629\n",
      "Iteration 342, loss = 0.28263444\n",
      "Iteration 343, loss = 0.28188527\n",
      "Iteration 344, loss = 0.28113881\n",
      "Iteration 345, loss = 0.28039496\n",
      "Iteration 346, loss = 0.27965380\n",
      "Iteration 347, loss = 0.27891536\n",
      "Iteration 348, loss = 0.27817943\n",
      "Iteration 349, loss = 0.27744628\n",
      "Iteration 350, loss = 0.27671576\n",
      "Iteration 351, loss = 0.27598787\n",
      "Iteration 352, loss = 0.27526268\n",
      "Iteration 353, loss = 0.27454012\n",
      "Iteration 354, loss = 0.27382019\n",
      "Iteration 355, loss = 0.27310286\n",
      "Iteration 356, loss = 0.27238813\n",
      "Iteration 357, loss = 0.27167599\n",
      "Iteration 358, loss = 0.27096663\n",
      "Iteration 359, loss = 0.27025949\n",
      "Iteration 360, loss = 0.26955514\n",
      "Iteration 361, loss = 0.26885332\n",
      "Iteration 362, loss = 0.26815405\n",
      "Iteration 363, loss = 0.26745735\n",
      "Iteration 364, loss = 0.26676323\n",
      "Iteration 365, loss = 0.26607168\n",
      "Iteration 366, loss = 0.26538264\n",
      "Iteration 367, loss = 0.26469616\n",
      "Iteration 368, loss = 0.26401221\n",
      "Iteration 369, loss = 0.26333076\n",
      "Iteration 370, loss = 0.26265184\n",
      "Iteration 371, loss = 0.26197540\n",
      "Iteration 372, loss = 0.26130147\n",
      "Iteration 373, loss = 0.26063003\n",
      "Iteration 374, loss = 0.25996109\n",
      "Iteration 375, loss = 0.25929466\n",
      "Iteration 376, loss = 0.25863084\n",
      "Iteration 377, loss = 0.25796949\n",
      "Iteration 378, loss = 0.25731057\n",
      "Iteration 379, loss = 0.25665405\n",
      "Iteration 380, loss = 0.25600007\n",
      "Iteration 381, loss = 0.25534855\n",
      "Iteration 382, loss = 0.25469944\n",
      "Iteration 383, loss = 0.25405274\n",
      "Iteration 384, loss = 0.25340846\n",
      "Iteration 385, loss = 0.25276659\n",
      "Iteration 386, loss = 0.25212713\n",
      "Iteration 387, loss = 0.25149008\n",
      "Iteration 388, loss = 0.25085549\n",
      "Iteration 389, loss = 0.25022331\n",
      "Iteration 390, loss = 0.24959352\n",
      "Iteration 391, loss = 0.24896617\n",
      "Iteration 392, loss = 0.24834165\n",
      "Iteration 393, loss = 0.24771967\n",
      "Iteration 394, loss = 0.24710011\n",
      "Iteration 395, loss = 0.24648295\n",
      "Iteration 396, loss = 0.24586819\n",
      "Iteration 397, loss = 0.24525586\n",
      "Iteration 398, loss = 0.24464595\n",
      "Iteration 399, loss = 0.24403842\n",
      "Iteration 400, loss = 0.24343323\n",
      "Iteration 401, loss = 0.24283038\n",
      "Iteration 402, loss = 0.24222986\n",
      "Iteration 403, loss = 0.24163164\n",
      "Iteration 404, loss = 0.24103490\n",
      "Iteration 405, loss = 0.24043374\n",
      "Iteration 406, loss = 0.23983208\n",
      "Iteration 407, loss = 0.23923086\n",
      "Iteration 408, loss = 0.23863106\n",
      "Iteration 409, loss = 0.23803232\n",
      "Iteration 410, loss = 0.23743381\n",
      "Iteration 411, loss = 0.23683507\n",
      "Iteration 412, loss = 0.23623577\n",
      "Iteration 413, loss = 0.23563559\n",
      "Iteration 414, loss = 0.23503430\n",
      "Iteration 415, loss = 0.23443177\n",
      "Iteration 416, loss = 0.23382791\n",
      "Iteration 417, loss = 0.23322269\n",
      "Iteration 418, loss = 0.23261615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 419, loss = 0.23200822\n",
      "Iteration 420, loss = 0.23139895\n",
      "Iteration 421, loss = 0.23078839\n",
      "Iteration 422, loss = 0.23017650\n",
      "Iteration 423, loss = 0.22956329\n",
      "Iteration 424, loss = 0.22894878\n",
      "Iteration 425, loss = 0.22833298\n",
      "Iteration 426, loss = 0.22771606\n",
      "Iteration 427, loss = 0.22709806\n",
      "Iteration 428, loss = 0.22647921\n",
      "Iteration 429, loss = 0.22585958\n",
      "Iteration 430, loss = 0.22523934\n",
      "Iteration 431, loss = 0.22461856\n",
      "Iteration 432, loss = 0.22399704\n",
      "Iteration 433, loss = 0.22337487\n",
      "Iteration 434, loss = 0.22275186\n",
      "Iteration 435, loss = 0.22212830\n",
      "Iteration 436, loss = 0.22150439\n",
      "Iteration 437, loss = 0.22088025\n",
      "Iteration 438, loss = 0.22025593\n",
      "Iteration 439, loss = 0.21963142\n",
      "Iteration 440, loss = 0.21900692\n",
      "Iteration 441, loss = 0.21838221\n",
      "Iteration 442, loss = 0.21775756\n",
      "Iteration 443, loss = 0.21713306\n",
      "Iteration 444, loss = 0.21650932\n",
      "Iteration 445, loss = 0.21588641\n",
      "Iteration 446, loss = 0.21526365\n",
      "Iteration 447, loss = 0.21464126\n",
      "Iteration 448, loss = 0.21401987\n",
      "Iteration 449, loss = 0.21339943\n",
      "Iteration 450, loss = 0.21277948\n",
      "Iteration 451, loss = 0.21216001\n",
      "Iteration 452, loss = 0.21154147\n",
      "Iteration 453, loss = 0.21092433\n",
      "Iteration 454, loss = 0.21030834\n",
      "Iteration 455, loss = 0.20969358\n",
      "Iteration 456, loss = 0.20908010\n",
      "Iteration 457, loss = 0.20846814\n",
      "Iteration 458, loss = 0.20785774\n",
      "Iteration 459, loss = 0.20724875\n",
      "Iteration 460, loss = 0.20664131\n",
      "Iteration 461, loss = 0.20603526\n",
      "Iteration 462, loss = 0.20543098\n",
      "Iteration 463, loss = 0.20482848\n",
      "Iteration 464, loss = 0.20422768\n",
      "Iteration 465, loss = 0.20362857\n",
      "Iteration 466, loss = 0.20303138\n",
      "Iteration 467, loss = 0.20243612\n",
      "Iteration 468, loss = 0.20184270\n",
      "Iteration 469, loss = 0.20125105\n",
      "Iteration 470, loss = 0.20066123\n",
      "Iteration 471, loss = 0.20007356\n",
      "Iteration 472, loss = 0.19948790\n",
      "Iteration 473, loss = 0.19890418\n",
      "Iteration 474, loss = 0.19832271\n",
      "Iteration 475, loss = 0.19774360\n",
      "Iteration 476, loss = 0.19716707\n",
      "Iteration 477, loss = 0.19659287\n",
      "Iteration 478, loss = 0.19602129\n",
      "Iteration 479, loss = 0.19545187\n",
      "Iteration 480, loss = 0.19488434\n",
      "Iteration 481, loss = 0.19431941\n",
      "Iteration 482, loss = 0.19375729\n",
      "Iteration 483, loss = 0.19319735\n",
      "Iteration 484, loss = 0.19264000\n",
      "Iteration 485, loss = 0.19208541\n",
      "Iteration 486, loss = 0.19153304\n",
      "Iteration 487, loss = 0.19098344\n",
      "Iteration 488, loss = 0.19043635\n",
      "Iteration 489, loss = 0.18989180\n",
      "Iteration 490, loss = 0.18934976\n",
      "Iteration 491, loss = 0.18881014\n",
      "Iteration 492, loss = 0.18827287\n",
      "Iteration 493, loss = 0.18773835\n",
      "Iteration 494, loss = 0.18720628\n",
      "Iteration 495, loss = 0.18667662\n",
      "Iteration 496, loss = 0.18614939\n",
      "Iteration 497, loss = 0.18562454\n",
      "Iteration 498, loss = 0.18510246\n",
      "Iteration 499, loss = 0.18458310\n",
      "Iteration 500, loss = 0.18406605\n",
      "Iteration 501, loss = 0.18355143\n",
      "Iteration 502, loss = 0.18303912\n",
      "Iteration 503, loss = 0.18252929\n",
      "Iteration 504, loss = 0.18202265\n",
      "Iteration 505, loss = 0.18151840\n",
      "Iteration 506, loss = 0.18101692\n",
      "Iteration 507, loss = 0.18051822\n",
      "Iteration 508, loss = 0.18002183\n",
      "Iteration 509, loss = 0.17952761\n",
      "Iteration 510, loss = 0.17903643\n",
      "Iteration 511, loss = 0.17854773\n",
      "Iteration 512, loss = 0.17806140\n",
      "Iteration 513, loss = 0.17757767\n",
      "Iteration 514, loss = 0.17709661\n",
      "Iteration 515, loss = 0.17661804\n",
      "Iteration 516, loss = 0.17614198\n",
      "Iteration 517, loss = 0.17566851\n",
      "Iteration 518, loss = 0.17519767\n",
      "Iteration 519, loss = 0.17472935\n",
      "Iteration 520, loss = 0.17426343\n",
      "Iteration 521, loss = 0.17380005\n",
      "Iteration 522, loss = 0.17333910\n",
      "Iteration 523, loss = 0.17288046\n",
      "Iteration 524, loss = 0.17242411\n",
      "Iteration 525, loss = 0.17197011\n",
      "Iteration 526, loss = 0.17151839\n",
      "Iteration 527, loss = 0.17106902\n",
      "Iteration 528, loss = 0.17062232\n",
      "Iteration 529, loss = 0.17017806\n",
      "Iteration 530, loss = 0.16973604\n",
      "Iteration 531, loss = 0.16929602\n",
      "Iteration 532, loss = 0.16885820\n",
      "Iteration 533, loss = 0.16842266\n",
      "Iteration 534, loss = 0.16798920\n",
      "Iteration 535, loss = 0.16755780\n",
      "Iteration 536, loss = 0.16712853\n",
      "Iteration 537, loss = 0.16670141\n",
      "Iteration 538, loss = 0.16627619\n",
      "Iteration 539, loss = 0.16585381\n",
      "Iteration 540, loss = 0.16543388\n",
      "Iteration 541, loss = 0.16501597\n",
      "Iteration 542, loss = 0.16460012\n",
      "Iteration 543, loss = 0.16418568\n",
      "Iteration 544, loss = 0.16377347\n",
      "Iteration 545, loss = 0.16336257\n",
      "Iteration 546, loss = 0.16295324\n",
      "Iteration 547, loss = 0.16254535\n",
      "Iteration 548, loss = 0.16213823\n",
      "Iteration 549, loss = 0.16173269\n",
      "Iteration 550, loss = 0.16132780\n",
      "Iteration 551, loss = 0.16092343\n",
      "Iteration 552, loss = 0.16051988\n",
      "Iteration 553, loss = 0.16011669\n",
      "Iteration 554, loss = 0.15971347\n",
      "Iteration 555, loss = 0.15931004\n",
      "Iteration 556, loss = 0.15890662\n",
      "Iteration 557, loss = 0.15850310\n",
      "Iteration 558, loss = 0.15809919\n",
      "Iteration 559, loss = 0.15769483\n",
      "Iteration 560, loss = 0.15729003\n",
      "Iteration 561, loss = 0.15688488\n",
      "Iteration 562, loss = 0.15647913\n",
      "Iteration 563, loss = 0.15607227\n",
      "Iteration 564, loss = 0.15566392\n",
      "Iteration 565, loss = 0.15525356\n",
      "Iteration 566, loss = 0.15484225\n",
      "Iteration 567, loss = 0.15442935\n",
      "Iteration 568, loss = 0.15401392\n",
      "Iteration 569, loss = 0.15359713\n",
      "Iteration 570, loss = 0.15317803\n",
      "Iteration 571, loss = 0.15275630\n",
      "Iteration 572, loss = 0.15233189\n",
      "Iteration 573, loss = 0.15190518\n",
      "Iteration 574, loss = 0.15147648\n",
      "Iteration 575, loss = 0.15104626\n",
      "Iteration 576, loss = 0.15061506\n",
      "Iteration 577, loss = 0.15018286\n",
      "Iteration 578, loss = 0.14974926\n",
      "Iteration 579, loss = 0.14931448\n",
      "Iteration 580, loss = 0.14887895\n",
      "Iteration 581, loss = 0.14844294\n",
      "Iteration 582, loss = 0.14800643\n",
      "Iteration 583, loss = 0.14756972\n",
      "Iteration 584, loss = 0.14713300\n",
      "Iteration 585, loss = 0.14669642\n",
      "Iteration 586, loss = 0.14626006\n",
      "Iteration 587, loss = 0.14582400\n",
      "Iteration 588, loss = 0.14538844\n",
      "Iteration 589, loss = 0.14495378\n",
      "Iteration 590, loss = 0.14451988\n",
      "Iteration 591, loss = 0.14408685\n",
      "Iteration 592, loss = 0.14365486\n",
      "Iteration 593, loss = 0.14322407\n",
      "Iteration 594, loss = 0.14279456\n",
      "Iteration 595, loss = 0.14236641\n",
      "Iteration 596, loss = 0.14193971\n",
      "Iteration 597, loss = 0.14151455\n",
      "Iteration 598, loss = 0.14109100\n",
      "Iteration 599, loss = 0.14066919\n",
      "Iteration 600, loss = 0.14024919\n",
      "Iteration 601, loss = 0.13983103\n",
      "Iteration 602, loss = 0.13941475\n",
      "Iteration 603, loss = 0.13900046\n",
      "Iteration 604, loss = 0.13858820\n",
      "Iteration 605, loss = 0.13817801\n",
      "Iteration 606, loss = 0.13776997\n",
      "Iteration 607, loss = 0.13736410\n",
      "Iteration 608, loss = 0.13696047\n",
      "Iteration 609, loss = 0.13655910\n",
      "Iteration 610, loss = 0.13616011\n",
      "Iteration 611, loss = 0.13576348\n",
      "Iteration 612, loss = 0.13536922\n",
      "Iteration 613, loss = 0.13497737\n",
      "Iteration 614, loss = 0.13458796\n",
      "Iteration 615, loss = 0.13420101\n",
      "Iteration 616, loss = 0.13381660\n",
      "Iteration 617, loss = 0.13343471\n",
      "Iteration 618, loss = 0.13305534\n",
      "Iteration 619, loss = 0.13267848\n",
      "Iteration 620, loss = 0.13230414\n",
      "Iteration 621, loss = 0.13193234\n",
      "Iteration 622, loss = 0.13156316\n",
      "Iteration 623, loss = 0.13119659\n",
      "Iteration 624, loss = 0.13083259\n",
      "Iteration 625, loss = 0.13047137\n",
      "Iteration 626, loss = 0.13011277\n",
      "Iteration 627, loss = 0.12975705\n",
      "Iteration 628, loss = 0.12940393\n",
      "Iteration 629, loss = 0.12905342\n",
      "Iteration 630, loss = 0.12870551\n",
      "Iteration 631, loss = 0.12836021\n",
      "Iteration 632, loss = 0.12801751\n",
      "Iteration 633, loss = 0.12767740\n",
      "Iteration 634, loss = 0.12733989\n",
      "Iteration 635, loss = 0.12700497\n",
      "Iteration 636, loss = 0.12667261\n",
      "Iteration 637, loss = 0.12634283\n",
      "Iteration 638, loss = 0.12601560\n",
      "Iteration 639, loss = 0.12569091\n",
      "Iteration 640, loss = 0.12536879\n",
      "Iteration 641, loss = 0.12504932\n",
      "Iteration 642, loss = 0.12473235\n",
      "Iteration 643, loss = 0.12441787\n",
      "Iteration 644, loss = 0.12410588\n",
      "Iteration 645, loss = 0.12379634\n",
      "Iteration 646, loss = 0.12348932\n",
      "Iteration 647, loss = 0.12318482\n",
      "Iteration 648, loss = 0.12288273\n",
      "Iteration 649, loss = 0.12258303\n",
      "Iteration 650, loss = 0.12228570\n",
      "Iteration 651, loss = 0.12199079\n",
      "Iteration 652, loss = 0.12169832\n",
      "Iteration 653, loss = 0.12140824\n",
      "Iteration 654, loss = 0.12112058\n",
      "Iteration 655, loss = 0.12083539\n",
      "Iteration 656, loss = 0.12055246\n",
      "Iteration 657, loss = 0.12027166\n",
      "Iteration 658, loss = 0.11999282\n",
      "Iteration 659, loss = 0.11971645\n",
      "Iteration 660, loss = 0.11944158\n",
      "Iteration 661, loss = 0.11916939\n",
      "Iteration 662, loss = 0.11889853\n",
      "Iteration 663, loss = 0.11862995\n",
      "Iteration 664, loss = 0.11836230\n",
      "Iteration 665, loss = 0.11809659\n",
      "Iteration 666, loss = 0.11783204\n",
      "Iteration 667, loss = 0.11756873\n",
      "Iteration 668, loss = 0.11730641\n",
      "Iteration 669, loss = 0.11704499\n",
      "Iteration 670, loss = 0.11678444\n",
      "Iteration 671, loss = 0.11652449\n",
      "Iteration 672, loss = 0.11626528\n",
      "Iteration 673, loss = 0.11600668\n",
      "Iteration 674, loss = 0.11574866\n",
      "Iteration 675, loss = 0.11549123\n",
      "Iteration 676, loss = 0.11523432\n",
      "Iteration 677, loss = 0.11497802\n",
      "Iteration 678, loss = 0.11472233\n",
      "Iteration 679, loss = 0.11446737\n",
      "Iteration 680, loss = 0.11421285\n",
      "Iteration 681, loss = 0.11395936\n",
      "Iteration 682, loss = 0.11370630\n",
      "Iteration 683, loss = 0.11345439\n",
      "Iteration 684, loss = 0.11320311\n",
      "Iteration 685, loss = 0.11295324\n",
      "Iteration 686, loss = 0.11270467\n",
      "Iteration 687, loss = 0.11245680\n",
      "Iteration 688, loss = 0.11221023\n",
      "Iteration 689, loss = 0.11196478\n",
      "Iteration 690, loss = 0.11172022\n",
      "Iteration 691, loss = 0.11147709\n",
      "Iteration 692, loss = 0.11123530\n",
      "Iteration 693, loss = 0.11099483\n",
      "Iteration 694, loss = 0.11075578\n",
      "Iteration 695, loss = 0.11051767\n",
      "Iteration 696, loss = 0.11028196\n",
      "Iteration 697, loss = 0.11004662\n",
      "Iteration 698, loss = 0.10981258\n",
      "Iteration 699, loss = 0.10958052\n",
      "Iteration 700, loss = 0.10934974\n",
      "Iteration 701, loss = 0.10912011\n",
      "Iteration 702, loss = 0.10889279\n",
      "Iteration 703, loss = 0.10866653\n",
      "Iteration 704, loss = 0.10844102\n",
      "Iteration 705, loss = 0.10821808\n",
      "Iteration 706, loss = 0.10799666\n",
      "Iteration 707, loss = 0.10777630\n",
      "Iteration 708, loss = 0.10755709\n",
      "Iteration 709, loss = 0.10734063\n",
      "Iteration 710, loss = 0.10712561\n",
      "Iteration 711, loss = 0.10691133\n",
      "Iteration 712, loss = 0.10669861\n",
      "Iteration 713, loss = 0.10648868\n",
      "Iteration 714, loss = 0.10627982\n",
      "Iteration 715, loss = 0.10607210\n",
      "Iteration 716, loss = 0.10586660\n",
      "Iteration 717, loss = 0.10566320\n",
      "Iteration 718, loss = 0.10546083\n",
      "Iteration 719, loss = 0.10525992\n",
      "Iteration 720, loss = 0.10506146\n",
      "Iteration 721, loss = 0.10486445\n",
      "Iteration 722, loss = 0.10466864\n",
      "Iteration 723, loss = 0.10447532\n",
      "Iteration 724, loss = 0.10428388\n",
      "Iteration 725, loss = 0.10409393\n",
      "Iteration 726, loss = 0.10390555\n",
      "Iteration 727, loss = 0.10371887\n",
      "Iteration 728, loss = 0.10353384\n",
      "Iteration 729, loss = 0.10335023\n",
      "Iteration 730, loss = 0.10316793\n",
      "Iteration 731, loss = 0.10298715\n",
      "Iteration 732, loss = 0.10280786\n",
      "Iteration 733, loss = 0.10263016\n",
      "Iteration 734, loss = 0.10245354\n",
      "Iteration 735, loss = 0.10227793\n",
      "Iteration 736, loss = 0.10210346\n",
      "Iteration 737, loss = 0.10193170\n",
      "Iteration 738, loss = 0.10175924\n",
      "Iteration 739, loss = 0.10158995\n",
      "Iteration 740, loss = 0.10142155\n",
      "Iteration 741, loss = 0.10125394\n",
      "Iteration 742, loss = 0.10108699\n",
      "Iteration 743, loss = 0.10092064\n",
      "Iteration 744, loss = 0.10075483\n",
      "Iteration 745, loss = 0.10058983\n",
      "Iteration 746, loss = 0.10042642\n",
      "Iteration 747, loss = 0.10026488\n",
      "Iteration 748, loss = 0.10010297\n",
      "Iteration 749, loss = 0.09994297\n",
      "Iteration 750, loss = 0.09978419\n",
      "Iteration 751, loss = 0.09962582\n",
      "Iteration 752, loss = 0.09946781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 753, loss = 0.09931093\n",
      "Iteration 754, loss = 0.09915478\n",
      "Iteration 755, loss = 0.09899957\n",
      "Iteration 756, loss = 0.09884508\n",
      "Iteration 757, loss = 0.09869129\n",
      "Iteration 758, loss = 0.09853803\n",
      "Iteration 759, loss = 0.09838544\n",
      "Iteration 760, loss = 0.09823357\n",
      "Iteration 761, loss = 0.09808237\n",
      "Iteration 762, loss = 0.09793190\n",
      "Iteration 763, loss = 0.09778216\n",
      "Iteration 764, loss = 0.09763314\n",
      "Iteration 765, loss = 0.09748483\n",
      "Iteration 766, loss = 0.09733716\n",
      "Iteration 767, loss = 0.09719029\n",
      "Iteration 768, loss = 0.09704414\n",
      "Iteration 769, loss = 0.09689870\n",
      "Iteration 770, loss = 0.09675401\n",
      "Iteration 771, loss = 0.09660998\n",
      "Iteration 772, loss = 0.09646686\n",
      "Iteration 773, loss = 0.09632448\n",
      "Iteration 774, loss = 0.09618283\n",
      "Iteration 775, loss = 0.09604206\n",
      "Iteration 776, loss = 0.09590210\n",
      "Iteration 777, loss = 0.09576292\n",
      "Iteration 778, loss = 0.09562465\n",
      "Iteration 779, loss = 0.09548724\n",
      "Iteration 780, loss = 0.09535068\n",
      "Iteration 781, loss = 0.09521503\n",
      "Iteration 782, loss = 0.09508030\n",
      "Iteration 783, loss = 0.09494647\n",
      "Iteration 784, loss = 0.09481347\n",
      "Iteration 785, loss = 0.09468148\n",
      "Iteration 786, loss = 0.09455040\n",
      "Iteration 787, loss = 0.09442020\n",
      "Iteration 788, loss = 0.09429092\n",
      "Iteration 789, loss = 0.09416259\n",
      "Iteration 790, loss = 0.09403522\n",
      "Iteration 791, loss = 0.09390886\n",
      "Iteration 792, loss = 0.09378341\n",
      "Iteration 793, loss = 0.09365896\n",
      "Iteration 794, loss = 0.09353550\n",
      "Iteration 795, loss = 0.09341300\n",
      "Iteration 796, loss = 0.09329145\n",
      "Iteration 797, loss = 0.09317087\n",
      "Iteration 798, loss = 0.09305126\n",
      "Iteration 799, loss = 0.09293260\n",
      "Iteration 800, loss = 0.09281497\n",
      "Iteration 801, loss = 0.09269832\n",
      "Iteration 802, loss = 0.09258263\n",
      "Iteration 803, loss = 0.09246794\n",
      "Iteration 804, loss = 0.09235417\n",
      "Iteration 805, loss = 0.09224140\n",
      "Iteration 806, loss = 0.09212954\n",
      "Iteration 807, loss = 0.09201875\n",
      "Iteration 808, loss = 0.09190884\n",
      "Iteration 809, loss = 0.09179995\n",
      "Iteration 810, loss = 0.09169199\n",
      "Iteration 811, loss = 0.09158499\n",
      "Iteration 812, loss = 0.09147893\n",
      "Iteration 813, loss = 0.09137378\n",
      "Iteration 814, loss = 0.09126961\n",
      "Iteration 815, loss = 0.09116639\n",
      "Iteration 816, loss = 0.09106409\n",
      "Iteration 817, loss = 0.09096270\n",
      "Iteration 818, loss = 0.09086221\n",
      "Iteration 819, loss = 0.09076261\n",
      "Iteration 820, loss = 0.09066394\n",
      "Iteration 821, loss = 0.09056617\n",
      "Iteration 822, loss = 0.09046931\n",
      "Iteration 823, loss = 0.09037333\n",
      "Iteration 824, loss = 0.09027821\n",
      "Iteration 825, loss = 0.09018398\n",
      "Iteration 826, loss = 0.09009063\n",
      "Iteration 827, loss = 0.08999812\n",
      "Iteration 828, loss = 0.08990646\n",
      "Iteration 829, loss = 0.08981567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91716398\n",
      "Iteration 2, loss = 1.87313059\n",
      "Iteration 3, loss = 1.81385433\n",
      "Iteration 4, loss = 1.74399279\n",
      "Iteration 5, loss = 1.66770202\n",
      "Iteration 6, loss = 1.58837376\n",
      "Iteration 7, loss = 1.50859797\n",
      "Iteration 8, loss = 1.43062368\n",
      "Iteration 9, loss = 1.35639573\n",
      "Iteration 10, loss = 1.28796397\n",
      "Iteration 11, loss = 1.22729996\n",
      "Iteration 12, loss = 1.17623069\n",
      "Iteration 13, loss = 1.13604701\n",
      "Iteration 14, loss = 1.10700034\n",
      "Iteration 15, loss = 1.08805499\n",
      "Iteration 16, loss = 1.07716720\n",
      "Iteration 17, loss = 1.07140957\n",
      "Iteration 18, loss = 1.06801160\n",
      "Iteration 19, loss = 1.06454328\n",
      "Iteration 20, loss = 1.05937107\n",
      "Iteration 21, loss = 1.05161786\n",
      "Iteration 22, loss = 1.04127466\n",
      "Iteration 23, loss = 1.02887273\n",
      "Iteration 24, loss = 1.01525435\n",
      "Iteration 25, loss = 1.00120496\n",
      "Iteration 26, loss = 0.98740206\n",
      "Iteration 27, loss = 0.97426639\n",
      "Iteration 28, loss = 0.96196205\n",
      "Iteration 29, loss = 0.95032633\n",
      "Iteration 30, loss = 0.93922248\n",
      "Iteration 31, loss = 0.92844711\n",
      "Iteration 32, loss = 0.91772250\n",
      "Iteration 33, loss = 0.90681174\n",
      "Iteration 34, loss = 0.89575511\n",
      "Iteration 35, loss = 0.88448211\n",
      "Iteration 36, loss = 0.87304956\n",
      "Iteration 37, loss = 0.86175182\n",
      "Iteration 38, loss = 0.85095307\n",
      "Iteration 39, loss = 0.84080095\n",
      "Iteration 40, loss = 0.83138924\n",
      "Iteration 41, loss = 0.82256944\n",
      "Iteration 42, loss = 0.81432509\n",
      "Iteration 43, loss = 0.80667385\n",
      "Iteration 44, loss = 0.79948590\n",
      "Iteration 45, loss = 0.79251995\n",
      "Iteration 46, loss = 0.78567401\n",
      "Iteration 47, loss = 0.77895334\n",
      "Iteration 48, loss = 0.77225296\n",
      "Iteration 49, loss = 0.76563137\n",
      "Iteration 50, loss = 0.75914668\n",
      "Iteration 51, loss = 0.75287454\n",
      "Iteration 52, loss = 0.74676296\n",
      "Iteration 53, loss = 0.74079107\n",
      "Iteration 54, loss = 0.73501883\n",
      "Iteration 55, loss = 0.72951325\n",
      "Iteration 56, loss = 0.72424516\n",
      "Iteration 57, loss = 0.71923496\n",
      "Iteration 58, loss = 0.71440690\n",
      "Iteration 59, loss = 0.70967573\n",
      "Iteration 60, loss = 0.70504079\n",
      "Iteration 61, loss = 0.70048445\n",
      "Iteration 62, loss = 0.69603236\n",
      "Iteration 63, loss = 0.69168514\n",
      "Iteration 64, loss = 0.68742046\n",
      "Iteration 65, loss = 0.68323648\n",
      "Iteration 66, loss = 0.67912717\n",
      "Iteration 67, loss = 0.67512017\n",
      "Iteration 68, loss = 0.67124067\n",
      "Iteration 69, loss = 0.66745088\n",
      "Iteration 70, loss = 0.66375420\n",
      "Iteration 71, loss = 0.66014505\n",
      "Iteration 72, loss = 0.65659702\n",
      "Iteration 73, loss = 0.65311019\n",
      "Iteration 74, loss = 0.64968122\n",
      "Iteration 75, loss = 0.64632374\n",
      "Iteration 76, loss = 0.64301924\n",
      "Iteration 77, loss = 0.63976109\n",
      "Iteration 78, loss = 0.63654892\n",
      "Iteration 79, loss = 0.63338785\n",
      "Iteration 80, loss = 0.63027351\n",
      "Iteration 81, loss = 0.62721401\n",
      "Iteration 82, loss = 0.62420645\n",
      "Iteration 83, loss = 0.62124546\n",
      "Iteration 84, loss = 0.61833369\n",
      "Iteration 85, loss = 0.61546508\n",
      "Iteration 86, loss = 0.61264004\n",
      "Iteration 87, loss = 0.60985652\n",
      "Iteration 88, loss = 0.60711409\n",
      "Iteration 89, loss = 0.60441033\n",
      "Iteration 90, loss = 0.60174454\n",
      "Iteration 91, loss = 0.59911498\n",
      "Iteration 92, loss = 0.59652071\n",
      "Iteration 93, loss = 0.59396089\n",
      "Iteration 94, loss = 0.59143475\n",
      "Iteration 95, loss = 0.58894160\n",
      "Iteration 96, loss = 0.58648080\n",
      "Iteration 97, loss = 0.58405217\n",
      "Iteration 98, loss = 0.58165439\n",
      "Iteration 99, loss = 0.57928838\n",
      "Iteration 100, loss = 0.57695247\n",
      "Iteration 101, loss = 0.57464589\n",
      "Iteration 102, loss = 0.57236761\n",
      "Iteration 103, loss = 0.57011795\n",
      "Iteration 104, loss = 0.56789664\n",
      "Iteration 105, loss = 0.56570333\n",
      "Iteration 106, loss = 0.56353729\n",
      "Iteration 107, loss = 0.56139798\n",
      "Iteration 108, loss = 0.55928495\n",
      "Iteration 109, loss = 0.55719771\n",
      "Iteration 110, loss = 0.55513577\n",
      "Iteration 111, loss = 0.55309868\n",
      "Iteration 112, loss = 0.55108599\n",
      "Iteration 113, loss = 0.54909732\n",
      "Iteration 114, loss = 0.54713237\n",
      "Iteration 115, loss = 0.54519049\n",
      "Iteration 116, loss = 0.54327125\n",
      "Iteration 117, loss = 0.54137422\n",
      "Iteration 118, loss = 0.53949898\n",
      "Iteration 119, loss = 0.53764683\n",
      "Iteration 120, loss = 0.53581640\n",
      "Iteration 121, loss = 0.53400670\n",
      "Iteration 122, loss = 0.53221735\n",
      "Iteration 123, loss = 0.53044893\n",
      "Iteration 124, loss = 0.52870028\n",
      "Iteration 125, loss = 0.52697096\n",
      "Iteration 126, loss = 0.52526066\n",
      "Iteration 127, loss = 0.52356907\n",
      "Iteration 128, loss = 0.52189589\n",
      "Iteration 129, loss = 0.52024089\n",
      "Iteration 130, loss = 0.51860369\n",
      "Iteration 131, loss = 0.51698394\n",
      "Iteration 132, loss = 0.51538139\n",
      "Iteration 133, loss = 0.51379572\n",
      "Iteration 134, loss = 0.51222665\n",
      "Iteration 135, loss = 0.51067466\n",
      "Iteration 136, loss = 0.50913905\n",
      "Iteration 137, loss = 0.50761928\n",
      "Iteration 138, loss = 0.50611568\n",
      "Iteration 139, loss = 0.50462757\n",
      "Iteration 140, loss = 0.50315495\n",
      "Iteration 141, loss = 0.50169708\n",
      "Iteration 142, loss = 0.50025369\n",
      "Iteration 143, loss = 0.49882510\n",
      "Iteration 144, loss = 0.49741041\n",
      "Iteration 145, loss = 0.49600947\n",
      "Iteration 146, loss = 0.49462209\n",
      "Iteration 147, loss = 0.49324822\n",
      "Iteration 148, loss = 0.49188789\n",
      "Iteration 149, loss = 0.49054052\n",
      "Iteration 150, loss = 0.48920589\n",
      "Iteration 151, loss = 0.48788412\n",
      "Iteration 152, loss = 0.48657444\n",
      "Iteration 153, loss = 0.48527693\n",
      "Iteration 154, loss = 0.48399154\n",
      "Iteration 155, loss = 0.48271791\n",
      "Iteration 156, loss = 0.48145577\n",
      "Iteration 157, loss = 0.48020493\n",
      "Iteration 158, loss = 0.47896522\n",
      "Iteration 159, loss = 0.47773644\n",
      "Iteration 160, loss = 0.47651881\n",
      "Iteration 161, loss = 0.47531179\n",
      "Iteration 162, loss = 0.47411526\n",
      "Iteration 163, loss = 0.47292899\n",
      "Iteration 164, loss = 0.47175275\n",
      "Iteration 165, loss = 0.47058640\n",
      "Iteration 166, loss = 0.46942989\n",
      "Iteration 167, loss = 0.46828285\n",
      "Iteration 168, loss = 0.46714550\n",
      "Iteration 169, loss = 0.46601711\n",
      "Iteration 170, loss = 0.46489793\n",
      "Iteration 171, loss = 0.46378778\n",
      "Iteration 172, loss = 0.46268640\n",
      "Iteration 173, loss = 0.46159375\n",
      "Iteration 174, loss = 0.46050962\n",
      "Iteration 175, loss = 0.45943387\n",
      "Iteration 176, loss = 0.45836644\n",
      "Iteration 177, loss = 0.45730711\n",
      "Iteration 178, loss = 0.45625578\n",
      "Iteration 179, loss = 0.45521234\n",
      "Iteration 180, loss = 0.45417669\n",
      "Iteration 181, loss = 0.45314865\n",
      "Iteration 182, loss = 0.45212810\n",
      "Iteration 183, loss = 0.45111495\n",
      "Iteration 184, loss = 0.45010911\n",
      "Iteration 185, loss = 0.44911063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 186, loss = 0.44811925\n",
      "Iteration 187, loss = 0.44713594\n",
      "Iteration 188, loss = 0.44615958\n",
      "Iteration 189, loss = 0.44519002\n",
      "Iteration 190, loss = 0.44422720\n",
      "Iteration 191, loss = 0.44327107\n",
      "Iteration 192, loss = 0.44232145\n",
      "Iteration 193, loss = 0.44137903\n",
      "Iteration 194, loss = 0.44044272\n",
      "Iteration 195, loss = 0.43951286\n",
      "Iteration 196, loss = 0.43858922\n",
      "Iteration 197, loss = 0.43767169\n",
      "Iteration 198, loss = 0.43676019\n",
      "Iteration 199, loss = 0.43585463\n",
      "Iteration 200, loss = 0.43495493\n",
      "Iteration 201, loss = 0.43406108\n",
      "Iteration 202, loss = 0.43317287\n",
      "Iteration 203, loss = 0.43229030\n",
      "Iteration 204, loss = 0.43141331\n",
      "Iteration 205, loss = 0.43054174\n",
      "Iteration 206, loss = 0.42967555\n",
      "Iteration 207, loss = 0.42881473\n",
      "Iteration 208, loss = 0.42795909\n",
      "Iteration 209, loss = 0.42710862\n",
      "Iteration 210, loss = 0.42626332\n",
      "Iteration 211, loss = 0.42542307\n",
      "Iteration 212, loss = 0.42458791\n",
      "Iteration 213, loss = 0.42375803\n",
      "Iteration 214, loss = 0.42293292\n",
      "Iteration 215, loss = 0.42211256\n",
      "Iteration 216, loss = 0.42129689\n",
      "Iteration 217, loss = 0.42048588\n",
      "Iteration 218, loss = 0.41967948\n",
      "Iteration 219, loss = 0.41887780\n",
      "Iteration 220, loss = 0.41808068\n",
      "Iteration 221, loss = 0.41728786\n",
      "Iteration 222, loss = 0.41649964\n",
      "Iteration 223, loss = 0.41571560\n",
      "Iteration 224, loss = 0.41493591\n",
      "Iteration 225, loss = 0.41416059\n",
      "Iteration 226, loss = 0.41338929\n",
      "Iteration 227, loss = 0.41262218\n",
      "Iteration 228, loss = 0.41185922\n",
      "Iteration 229, loss = 0.41110030\n",
      "Iteration 230, loss = 0.41034553\n",
      "Iteration 231, loss = 0.40959486\n",
      "Iteration 232, loss = 0.40884810\n",
      "Iteration 233, loss = 0.40810528\n",
      "Iteration 234, loss = 0.40736633\n",
      "Iteration 235, loss = 0.40663118\n",
      "Iteration 236, loss = 0.40589969\n",
      "Iteration 237, loss = 0.40517206\n",
      "Iteration 238, loss = 0.40444830\n",
      "Iteration 239, loss = 0.40372815\n",
      "Iteration 240, loss = 0.40301163\n",
      "Iteration 241, loss = 0.40229874\n",
      "Iteration 242, loss = 0.40158962\n",
      "Iteration 243, loss = 0.40088397\n",
      "Iteration 244, loss = 0.40018175\n",
      "Iteration 245, loss = 0.39948292\n",
      "Iteration 246, loss = 0.39878743\n",
      "Iteration 247, loss = 0.39809524\n",
      "Iteration 248, loss = 0.39740632\n",
      "Iteration 249, loss = 0.39672074\n",
      "Iteration 250, loss = 0.39603846\n",
      "Iteration 251, loss = 0.39535935\n",
      "Iteration 252, loss = 0.39468334\n",
      "Iteration 253, loss = 0.39401042\n",
      "Iteration 254, loss = 0.39334057\n",
      "Iteration 255, loss = 0.39267378\n",
      "Iteration 256, loss = 0.39200997\n",
      "Iteration 257, loss = 0.39134911\n",
      "Iteration 258, loss = 0.39069116\n",
      "Iteration 259, loss = 0.39003608\n",
      "Iteration 260, loss = 0.38938385\n",
      "Iteration 261, loss = 0.38873444\n",
      "Iteration 262, loss = 0.38808781\n",
      "Iteration 263, loss = 0.38744393\n",
      "Iteration 264, loss = 0.38680277\n",
      "Iteration 265, loss = 0.38616434\n",
      "Iteration 266, loss = 0.38552859\n",
      "Iteration 267, loss = 0.38489547\n",
      "Iteration 268, loss = 0.38426497\n",
      "Iteration 269, loss = 0.38363706\n",
      "Iteration 270, loss = 0.38301173\n",
      "Iteration 271, loss = 0.38238893\n",
      "Iteration 272, loss = 0.38176864\n",
      "Iteration 273, loss = 0.38115083\n",
      "Iteration 274, loss = 0.38053570\n",
      "Iteration 275, loss = 0.37992308\n",
      "Iteration 276, loss = 0.37931288\n",
      "Iteration 277, loss = 0.37870509\n",
      "Iteration 278, loss = 0.37809966\n",
      "Iteration 279, loss = 0.37749659\n",
      "Iteration 280, loss = 0.37689584\n",
      "Iteration 281, loss = 0.37629739\n",
      "Iteration 282, loss = 0.37570123\n",
      "Iteration 283, loss = 0.37510732\n",
      "Iteration 284, loss = 0.37451585\n",
      "Iteration 285, loss = 0.37392718\n",
      "Iteration 286, loss = 0.37334065\n",
      "Iteration 287, loss = 0.37275625\n",
      "Iteration 288, loss = 0.37217397\n",
      "Iteration 289, loss = 0.37159383\n",
      "Iteration 290, loss = 0.37101580\n",
      "Iteration 291, loss = 0.37043991\n",
      "Iteration 292, loss = 0.36986613\n",
      "Iteration 293, loss = 0.36929443\n",
      "Iteration 294, loss = 0.36872501\n",
      "Iteration 295, loss = 0.36815755\n",
      "Iteration 296, loss = 0.36759206\n",
      "Iteration 297, loss = 0.36702853\n",
      "Iteration 298, loss = 0.36646711\n",
      "Iteration 299, loss = 0.36590772\n",
      "Iteration 300, loss = 0.36535024\n",
      "Iteration 301, loss = 0.36479469\n",
      "Iteration 302, loss = 0.36424107\n",
      "Iteration 303, loss = 0.36368942\n",
      "Iteration 304, loss = 0.36313962\n",
      "Iteration 305, loss = 0.36259209\n",
      "Iteration 306, loss = 0.36204651\n",
      "Iteration 307, loss = 0.36150272\n",
      "Iteration 308, loss = 0.36096078\n",
      "Iteration 309, loss = 0.36042065\n",
      "Iteration 310, loss = 0.35988233\n",
      "Iteration 311, loss = 0.35934586\n",
      "Iteration 312, loss = 0.35881114\n",
      "Iteration 313, loss = 0.35827866\n",
      "Iteration 314, loss = 0.35774813\n",
      "Iteration 315, loss = 0.35721939\n",
      "Iteration 316, loss = 0.35669243\n",
      "Iteration 317, loss = 0.35616728\n",
      "Iteration 318, loss = 0.35564381\n",
      "Iteration 319, loss = 0.35512211\n",
      "Iteration 320, loss = 0.35460215\n",
      "Iteration 321, loss = 0.35408389\n",
      "Iteration 322, loss = 0.35356730\n",
      "Iteration 323, loss = 0.35305241\n",
      "Iteration 324, loss = 0.35253918\n",
      "Iteration 325, loss = 0.35202762\n",
      "Iteration 326, loss = 0.35151787\n",
      "Iteration 327, loss = 0.35100994\n",
      "Iteration 328, loss = 0.35050356\n",
      "Iteration 329, loss = 0.34999875\n",
      "Iteration 330, loss = 0.34949560\n",
      "Iteration 331, loss = 0.34899409\n",
      "Iteration 332, loss = 0.34849433\n",
      "Iteration 333, loss = 0.34799615\n",
      "Iteration 334, loss = 0.34749952\n",
      "Iteration 335, loss = 0.34700443\n",
      "Iteration 336, loss = 0.34651089\n",
      "Iteration 337, loss = 0.34601887\n",
      "Iteration 338, loss = 0.34552836\n",
      "Iteration 339, loss = 0.34503935\n",
      "Iteration 340, loss = 0.34455182\n",
      "Iteration 341, loss = 0.34406584\n",
      "Iteration 342, loss = 0.34358134\n",
      "Iteration 343, loss = 0.34309831\n",
      "Iteration 344, loss = 0.34261705\n",
      "Iteration 345, loss = 0.34213719\n",
      "Iteration 346, loss = 0.34165883\n",
      "Iteration 347, loss = 0.34118186\n",
      "Iteration 348, loss = 0.34070634\n",
      "Iteration 349, loss = 0.34023241\n",
      "Iteration 350, loss = 0.33975987\n",
      "Iteration 351, loss = 0.33928875\n",
      "Iteration 352, loss = 0.33881901\n",
      "Iteration 353, loss = 0.33835066\n",
      "Iteration 354, loss = 0.33788369\n",
      "Iteration 355, loss = 0.33741807\n",
      "Iteration 356, loss = 0.33695387\n",
      "Iteration 357, loss = 0.33649116\n",
      "Iteration 358, loss = 0.33602982\n",
      "Iteration 359, loss = 0.33556987\n",
      "Iteration 360, loss = 0.33511134\n",
      "Iteration 361, loss = 0.33465411\n",
      "Iteration 362, loss = 0.33419820\n",
      "Iteration 363, loss = 0.33374369\n",
      "Iteration 364, loss = 0.33329051\n",
      "Iteration 365, loss = 0.33283864\n",
      "Iteration 366, loss = 0.33238805\n",
      "Iteration 367, loss = 0.33193874\n",
      "Iteration 368, loss = 0.33149067\n",
      "Iteration 369, loss = 0.33104387\n",
      "Iteration 370, loss = 0.33059830\n",
      "Iteration 371, loss = 0.33015396\n",
      "Iteration 372, loss = 0.32971114\n",
      "Iteration 373, loss = 0.32926947\n",
      "Iteration 374, loss = 0.32882910\n",
      "Iteration 375, loss = 0.32839005\n",
      "Iteration 376, loss = 0.32795215\n",
      "Iteration 377, loss = 0.32751556\n",
      "Iteration 378, loss = 0.32708021\n",
      "Iteration 379, loss = 0.32664607\n",
      "Iteration 380, loss = 0.32621308\n",
      "Iteration 381, loss = 0.32578127\n",
      "Iteration 382, loss = 0.32535069\n",
      "Iteration 383, loss = 0.32492121\n",
      "Iteration 384, loss = 0.32449299\n",
      "Iteration 385, loss = 0.32406584\n",
      "Iteration 386, loss = 0.32363988\n",
      "Iteration 387, loss = 0.32321507\n",
      "Iteration 388, loss = 0.32279136\n",
      "Iteration 389, loss = 0.32236879\n",
      "Iteration 390, loss = 0.32194735\n",
      "Iteration 391, loss = 0.32152698\n",
      "Iteration 392, loss = 0.32110781\n",
      "Iteration 393, loss = 0.32068965\n",
      "Iteration 394, loss = 0.32027263\n",
      "Iteration 395, loss = 0.31985667\n",
      "Iteration 396, loss = 0.31944183\n",
      "Iteration 397, loss = 0.31902806\n",
      "Iteration 398, loss = 0.31861536\n",
      "Iteration 399, loss = 0.31820374\n",
      "Iteration 400, loss = 0.31779316\n",
      "Iteration 401, loss = 0.31738370\n",
      "Iteration 402, loss = 0.31697523\n",
      "Iteration 403, loss = 0.31656787\n",
      "Iteration 404, loss = 0.31616150\n",
      "Iteration 405, loss = 0.31575621\n",
      "Iteration 406, loss = 0.31535190\n",
      "Iteration 407, loss = 0.31494872\n",
      "Iteration 408, loss = 0.31454648\n",
      "Iteration 409, loss = 0.31414531\n",
      "Iteration 410, loss = 0.31374518\n",
      "Iteration 411, loss = 0.31334620\n",
      "Iteration 412, loss = 0.31294816\n",
      "Iteration 413, loss = 0.31255123\n",
      "Iteration 414, loss = 0.31215525\n",
      "Iteration 415, loss = 0.31176027\n",
      "Iteration 416, loss = 0.31136649\n",
      "Iteration 417, loss = 0.31097369\n",
      "Iteration 418, loss = 0.31058189\n",
      "Iteration 419, loss = 0.31019103\n",
      "Iteration 420, loss = 0.30980121\n",
      "Iteration 421, loss = 0.30941235\n",
      "Iteration 422, loss = 0.30902449\n",
      "Iteration 423, loss = 0.30863772\n",
      "Iteration 424, loss = 0.30825195\n",
      "Iteration 425, loss = 0.30786717\n",
      "Iteration 426, loss = 0.30748331\n",
      "Iteration 427, loss = 0.30710046\n",
      "Iteration 428, loss = 0.30671852\n",
      "Iteration 429, loss = 0.30633756\n",
      "Iteration 430, loss = 0.30595760\n",
      "Iteration 431, loss = 0.30557861\n",
      "Iteration 432, loss = 0.30520071\n",
      "Iteration 433, loss = 0.30482372\n",
      "Iteration 434, loss = 0.30444765\n",
      "Iteration 435, loss = 0.30407260\n",
      "Iteration 436, loss = 0.30369840\n",
      "Iteration 437, loss = 0.30332516\n",
      "Iteration 438, loss = 0.30295283\n",
      "Iteration 439, loss = 0.30258140\n",
      "Iteration 440, loss = 0.30221088\n",
      "Iteration 441, loss = 0.30184132\n",
      "Iteration 442, loss = 0.30147277\n",
      "Iteration 443, loss = 0.30110514\n",
      "Iteration 444, loss = 0.30073844\n",
      "Iteration 445, loss = 0.30037255\n",
      "Iteration 446, loss = 0.30000764\n",
      "Iteration 447, loss = 0.29964358\n",
      "Iteration 448, loss = 0.29928039\n",
      "Iteration 449, loss = 0.29891812\n",
      "Iteration 450, loss = 0.29855668\n",
      "Iteration 451, loss = 0.29819613\n",
      "Iteration 452, loss = 0.29783645\n",
      "Iteration 453, loss = 0.29747765\n",
      "Iteration 454, loss = 0.29711968\n",
      "Iteration 455, loss = 0.29676270\n",
      "Iteration 456, loss = 0.29640655\n",
      "Iteration 457, loss = 0.29605128\n",
      "Iteration 458, loss = 0.29569685\n",
      "Iteration 459, loss = 0.29534326\n",
      "Iteration 460, loss = 0.29499054\n",
      "Iteration 461, loss = 0.29463870\n",
      "Iteration 462, loss = 0.29428775\n",
      "Iteration 463, loss = 0.29393754\n",
      "Iteration 464, loss = 0.29358827\n",
      "Iteration 465, loss = 0.29323976\n",
      "Iteration 466, loss = 0.29289211\n",
      "Iteration 467, loss = 0.29254529\n",
      "Iteration 468, loss = 0.29219930\n",
      "Iteration 469, loss = 0.29185412\n",
      "Iteration 470, loss = 0.29150974\n",
      "Iteration 471, loss = 0.29116617\n",
      "Iteration 472, loss = 0.29082342\n",
      "Iteration 473, loss = 0.29048149\n",
      "Iteration 474, loss = 0.29014039\n",
      "Iteration 475, loss = 0.28980008\n",
      "Iteration 476, loss = 0.28946056\n",
      "Iteration 477, loss = 0.28912189\n",
      "Iteration 478, loss = 0.28878397\n",
      "Iteration 479, loss = 0.28844701\n",
      "Iteration 480, loss = 0.28811084\n",
      "Iteration 481, loss = 0.28777554\n",
      "Iteration 482, loss = 0.28744104\n",
      "Iteration 483, loss = 0.28710732\n",
      "Iteration 484, loss = 0.28677440\n",
      "Iteration 485, loss = 0.28644225\n",
      "Iteration 486, loss = 0.28611088\n",
      "Iteration 487, loss = 0.28578032\n",
      "Iteration 488, loss = 0.28545051\n",
      "Iteration 489, loss = 0.28512147\n",
      "Iteration 490, loss = 0.28479320\n",
      "Iteration 491, loss = 0.28446569\n",
      "Iteration 492, loss = 0.28413898\n",
      "Iteration 493, loss = 0.28381297\n",
      "Iteration 494, loss = 0.28348774\n",
      "Iteration 495, loss = 0.28316327\n",
      "Iteration 496, loss = 0.28283953\n",
      "Iteration 497, loss = 0.28251654\n",
      "Iteration 498, loss = 0.28219432\n",
      "Iteration 499, loss = 0.28187282\n",
      "Iteration 500, loss = 0.28155207\n",
      "Iteration 501, loss = 0.28123205\n",
      "Iteration 502, loss = 0.28091279\n",
      "Iteration 503, loss = 0.28059422\n",
      "Iteration 504, loss = 0.28027640\n",
      "Iteration 505, loss = 0.27995933\n",
      "Iteration 506, loss = 0.27964297\n",
      "Iteration 507, loss = 0.27932735\n",
      "Iteration 508, loss = 0.27901246\n",
      "Iteration 509, loss = 0.27869828\n",
      "Iteration 510, loss = 0.27838482\n",
      "Iteration 511, loss = 0.27807209\n",
      "Iteration 512, loss = 0.27776013\n",
      "Iteration 513, loss = 0.27744891\n",
      "Iteration 514, loss = 0.27713840\n",
      "Iteration 515, loss = 0.27682866\n",
      "Iteration 516, loss = 0.27651963\n",
      "Iteration 517, loss = 0.27621130\n",
      "Iteration 518, loss = 0.27590368\n",
      "Iteration 519, loss = 0.27559677\n",
      "Iteration 520, loss = 0.27529057\n",
      "Iteration 521, loss = 0.27498507\n",
      "Iteration 522, loss = 0.27468029\n",
      "Iteration 523, loss = 0.27437618\n",
      "Iteration 524, loss = 0.27407278\n",
      "Iteration 525, loss = 0.27377008\n",
      "Iteration 526, loss = 0.27346806\n",
      "Iteration 527, loss = 0.27316673\n",
      "Iteration 528, loss = 0.27286607\n",
      "Iteration 529, loss = 0.27256611\n",
      "Iteration 530, loss = 0.27226682\n",
      "Iteration 531, loss = 0.27196821\n",
      "Iteration 532, loss = 0.27167028\n",
      "Iteration 533, loss = 0.27137301\n",
      "Iteration 534, loss = 0.27107642\n",
      "Iteration 535, loss = 0.27078052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 536, loss = 0.27048525\n",
      "Iteration 537, loss = 0.27019066\n",
      "Iteration 538, loss = 0.26989674\n",
      "Iteration 539, loss = 0.26960347\n",
      "Iteration 540, loss = 0.26931086\n",
      "Iteration 541, loss = 0.26901891\n",
      "Iteration 542, loss = 0.26872761\n",
      "Iteration 543, loss = 0.26843697\n",
      "Iteration 544, loss = 0.26814701\n",
      "Iteration 545, loss = 0.26785765\n",
      "Iteration 546, loss = 0.26756896\n",
      "Iteration 547, loss = 0.26728092\n",
      "Iteration 548, loss = 0.26699352\n",
      "Iteration 549, loss = 0.26670677\n",
      "Iteration 550, loss = 0.26642067\n",
      "Iteration 551, loss = 0.26613520\n",
      "Iteration 552, loss = 0.26585037\n",
      "Iteration 553, loss = 0.26556619\n",
      "Iteration 554, loss = 0.26528269\n",
      "Iteration 555, loss = 0.26499979\n",
      "Iteration 556, loss = 0.26471750\n",
      "Iteration 557, loss = 0.26443585\n",
      "Iteration 558, loss = 0.26415485\n",
      "Iteration 559, loss = 0.26387448\n",
      "Iteration 560, loss = 0.26359474\n",
      "Iteration 561, loss = 0.26331563\n",
      "Iteration 562, loss = 0.26303713\n",
      "Iteration 563, loss = 0.26275925\n",
      "Iteration 564, loss = 0.26248198\n",
      "Iteration 565, loss = 0.26220535\n",
      "Iteration 566, loss = 0.26192931\n",
      "Iteration 567, loss = 0.26165389\n",
      "Iteration 568, loss = 0.26137910\n",
      "Iteration 569, loss = 0.26110490\n",
      "Iteration 570, loss = 0.26083131\n",
      "Iteration 571, loss = 0.26055834\n",
      "Iteration 572, loss = 0.26028599\n",
      "Iteration 573, loss = 0.26001422\n",
      "Iteration 574, loss = 0.25974306\n",
      "Iteration 575, loss = 0.25947252\n",
      "Iteration 576, loss = 0.25920258\n",
      "Iteration 577, loss = 0.25893324\n",
      "Iteration 578, loss = 0.25866449\n",
      "Iteration 579, loss = 0.25839634\n",
      "Iteration 580, loss = 0.25812878\n",
      "Iteration 581, loss = 0.25786181\n",
      "Iteration 582, loss = 0.25759543\n",
      "Iteration 583, loss = 0.25732969\n",
      "Iteration 584, loss = 0.25706457\n",
      "Iteration 585, loss = 0.25680003\n",
      "Iteration 586, loss = 0.25653610\n",
      "Iteration 587, loss = 0.25627274\n",
      "Iteration 588, loss = 0.25600996\n",
      "Iteration 589, loss = 0.25574778\n",
      "Iteration 590, loss = 0.25548616\n",
      "Iteration 591, loss = 0.25522513\n",
      "Iteration 592, loss = 0.25496467\n",
      "Iteration 593, loss = 0.25470480\n",
      "Iteration 594, loss = 0.25444549\n",
      "Iteration 595, loss = 0.25418676\n",
      "Iteration 596, loss = 0.25392861\n",
      "Iteration 597, loss = 0.25367101\n",
      "Iteration 598, loss = 0.25341399\n",
      "Iteration 599, loss = 0.25315753\n",
      "Iteration 600, loss = 0.25290163\n",
      "Iteration 601, loss = 0.25264629\n",
      "Iteration 602, loss = 0.25239153\n",
      "Iteration 603, loss = 0.25213738\n",
      "Iteration 604, loss = 0.25188379\n",
      "Iteration 605, loss = 0.25163078\n",
      "Iteration 606, loss = 0.25137837\n",
      "Iteration 607, loss = 0.25112653\n",
      "Iteration 608, loss = 0.25087525\n",
      "Iteration 609, loss = 0.25062451\n",
      "Iteration 610, loss = 0.25037433\n",
      "Iteration 611, loss = 0.25012472\n",
      "Iteration 612, loss = 0.24987565\n",
      "Iteration 613, loss = 0.24962712\n",
      "Iteration 614, loss = 0.24937914\n",
      "Iteration 615, loss = 0.24913171\n",
      "Iteration 616, loss = 0.24888482\n",
      "Iteration 617, loss = 0.24863848\n",
      "Iteration 618, loss = 0.24839268\n",
      "Iteration 619, loss = 0.24814741\n",
      "Iteration 620, loss = 0.24790273\n",
      "Iteration 621, loss = 0.24765855\n",
      "Iteration 622, loss = 0.24741491\n",
      "Iteration 623, loss = 0.24717180\n",
      "Iteration 624, loss = 0.24692922\n",
      "Iteration 625, loss = 0.24668719\n",
      "Iteration 626, loss = 0.24644571\n",
      "Iteration 627, loss = 0.24620484\n",
      "Iteration 628, loss = 0.24596448\n",
      "Iteration 629, loss = 0.24572465\n",
      "Iteration 630, loss = 0.24548535\n",
      "Iteration 631, loss = 0.24524657\n",
      "Iteration 632, loss = 0.24500832\n",
      "Iteration 633, loss = 0.24477059\n",
      "Iteration 634, loss = 0.24453338\n",
      "Iteration 635, loss = 0.24429669\n",
      "Iteration 636, loss = 0.24406050\n",
      "Iteration 637, loss = 0.24382484\n",
      "Iteration 638, loss = 0.24358968\n",
      "Iteration 639, loss = 0.24335505\n",
      "Iteration 640, loss = 0.24312092\n",
      "Iteration 641, loss = 0.24288735\n",
      "Iteration 642, loss = 0.24265427\n",
      "Iteration 643, loss = 0.24242170\n",
      "Iteration 644, loss = 0.24218963\n",
      "Iteration 645, loss = 0.24195808\n",
      "Iteration 646, loss = 0.24172701\n",
      "Iteration 647, loss = 0.24149646\n",
      "Iteration 648, loss = 0.24126640\n",
      "Iteration 649, loss = 0.24103684\n",
      "Iteration 650, loss = 0.24080777\n",
      "Iteration 651, loss = 0.24057920\n",
      "Iteration 652, loss = 0.24035112\n",
      "Iteration 653, loss = 0.24012354\n",
      "Iteration 654, loss = 0.23989644\n",
      "Iteration 655, loss = 0.23966984\n",
      "Iteration 656, loss = 0.23944372\n",
      "Iteration 657, loss = 0.23921809\n",
      "Iteration 658, loss = 0.23899294\n",
      "Iteration 659, loss = 0.23876828\n",
      "Iteration 660, loss = 0.23854411\n",
      "Iteration 661, loss = 0.23832041\n",
      "Iteration 662, loss = 0.23809720\n",
      "Iteration 663, loss = 0.23787446\n",
      "Iteration 664, loss = 0.23765221\n",
      "Iteration 665, loss = 0.23743042\n",
      "Iteration 666, loss = 0.23720912\n",
      "Iteration 667, loss = 0.23698828\n",
      "Iteration 668, loss = 0.23676792\n",
      "Iteration 669, loss = 0.23654804\n",
      "Iteration 670, loss = 0.23632862\n",
      "Iteration 671, loss = 0.23610968\n",
      "Iteration 672, loss = 0.23589120\n",
      "Iteration 673, loss = 0.23567319\n",
      "Iteration 674, loss = 0.23545565\n",
      "Iteration 675, loss = 0.23523857\n",
      "Iteration 676, loss = 0.23502196\n",
      "Iteration 677, loss = 0.23480584\n",
      "Iteration 678, loss = 0.23459018\n",
      "Iteration 679, loss = 0.23437499\n",
      "Iteration 680, loss = 0.23416027\n",
      "Iteration 681, loss = 0.23394601\n",
      "Iteration 682, loss = 0.23373220\n",
      "Iteration 683, loss = 0.23351888\n",
      "Iteration 684, loss = 0.23330599\n",
      "Iteration 685, loss = 0.23309355\n",
      "Iteration 686, loss = 0.23288158\n",
      "Iteration 687, loss = 0.23267005\n",
      "Iteration 688, loss = 0.23245897\n",
      "Iteration 689, loss = 0.23224835\n",
      "Iteration 690, loss = 0.23203818\n",
      "Iteration 691, loss = 0.23182847\n",
      "Iteration 692, loss = 0.23161919\n",
      "Iteration 693, loss = 0.23141037\n",
      "Iteration 694, loss = 0.23120199\n",
      "Iteration 695, loss = 0.23099404\n",
      "Iteration 696, loss = 0.23078655\n",
      "Iteration 697, loss = 0.23057948\n",
      "Iteration 698, loss = 0.23037287\n",
      "Iteration 699, loss = 0.23016670\n",
      "Iteration 700, loss = 0.22996097\n",
      "Iteration 701, loss = 0.22975570\n",
      "Iteration 702, loss = 0.22955084\n",
      "Iteration 703, loss = 0.22934642\n",
      "Iteration 704, loss = 0.22914244\n",
      "Iteration 705, loss = 0.22893889\n",
      "Iteration 706, loss = 0.22873576\n",
      "Iteration 707, loss = 0.22853308\n",
      "Iteration 708, loss = 0.22833081\n",
      "Iteration 709, loss = 0.22812898\n",
      "Iteration 710, loss = 0.22792757\n",
      "Iteration 711, loss = 0.22772658\n",
      "Iteration 712, loss = 0.22752603\n",
      "Iteration 713, loss = 0.22732589\n",
      "Iteration 714, loss = 0.22712617\n",
      "Iteration 715, loss = 0.22692688\n",
      "Iteration 716, loss = 0.22672801\n",
      "Iteration 717, loss = 0.22652955\n",
      "Iteration 718, loss = 0.22633153\n",
      "Iteration 719, loss = 0.22613392\n",
      "Iteration 720, loss = 0.22593674\n",
      "Iteration 721, loss = 0.22573997\n",
      "Iteration 722, loss = 0.22554361\n",
      "Iteration 723, loss = 0.22534767\n",
      "Iteration 724, loss = 0.22515213\n",
      "Iteration 725, loss = 0.22495702\n",
      "Iteration 726, loss = 0.22476230\n",
      "Iteration 727, loss = 0.22456801\n",
      "Iteration 728, loss = 0.22437411\n",
      "Iteration 729, loss = 0.22418063\n",
      "Iteration 730, loss = 0.22398754\n",
      "Iteration 731, loss = 0.22379487\n",
      "Iteration 732, loss = 0.22360260\n",
      "Iteration 733, loss = 0.22341073\n",
      "Iteration 734, loss = 0.22321926\n",
      "Iteration 735, loss = 0.22302819\n",
      "Iteration 736, loss = 0.22283753\n",
      "Iteration 737, loss = 0.22264726\n",
      "Iteration 738, loss = 0.22245739\n",
      "Iteration 739, loss = 0.22226792\n",
      "Iteration 740, loss = 0.22207884\n",
      "Iteration 741, loss = 0.22189016\n",
      "Iteration 742, loss = 0.22170187\n",
      "Iteration 743, loss = 0.22151398\n",
      "Iteration 744, loss = 0.22132647\n",
      "Iteration 745, loss = 0.22113936\n",
      "Iteration 746, loss = 0.22095264\n",
      "Iteration 747, loss = 0.22076630\n",
      "Iteration 748, loss = 0.22058036\n",
      "Iteration 749, loss = 0.22039480\n",
      "Iteration 750, loss = 0.22020963\n",
      "Iteration 751, loss = 0.22002484\n",
      "Iteration 752, loss = 0.21984044\n",
      "Iteration 753, loss = 0.21965642\n",
      "Iteration 754, loss = 0.21947278\n",
      "Iteration 755, loss = 0.21928953\n",
      "Iteration 756, loss = 0.21910665\n",
      "Iteration 757, loss = 0.21892416\n",
      "Iteration 758, loss = 0.21874204\n",
      "Iteration 759, loss = 0.21856030\n",
      "Iteration 760, loss = 0.21837893\n",
      "Iteration 761, loss = 0.21819795\n",
      "Iteration 762, loss = 0.21801733\n",
      "Iteration 763, loss = 0.21783710\n",
      "Iteration 764, loss = 0.21765723\n",
      "Iteration 765, loss = 0.21747774\n",
      "Iteration 766, loss = 0.21729862\n",
      "Iteration 767, loss = 0.21711987\n",
      "Iteration 768, loss = 0.21694148\n",
      "Iteration 769, loss = 0.21676347\n",
      "Iteration 770, loss = 0.21658582\n",
      "Iteration 771, loss = 0.21640855\n",
      "Iteration 772, loss = 0.21623163\n",
      "Iteration 773, loss = 0.21605508\n",
      "Iteration 774, loss = 0.21587890\n",
      "Iteration 775, loss = 0.21570308\n",
      "Iteration 776, loss = 0.21552762\n",
      "Iteration 777, loss = 0.21535252\n",
      "Iteration 778, loss = 0.21517779\n",
      "Iteration 779, loss = 0.21500341\n",
      "Iteration 780, loss = 0.21482939\n",
      "Iteration 781, loss = 0.21465573\n",
      "Iteration 782, loss = 0.21448242\n",
      "Iteration 783, loss = 0.21430948\n",
      "Iteration 784, loss = 0.21413688\n",
      "Iteration 785, loss = 0.21396465\n",
      "Iteration 786, loss = 0.21379276\n",
      "Iteration 787, loss = 0.21362123\n",
      "Iteration 788, loss = 0.21345005\n",
      "Iteration 789, loss = 0.21327922\n",
      "Iteration 790, loss = 0.21310874\n",
      "Iteration 791, loss = 0.21293861\n",
      "Iteration 792, loss = 0.21276883\n",
      "Iteration 793, loss = 0.21259939\n",
      "Iteration 794, loss = 0.21243031\n",
      "Iteration 795, loss = 0.21226156\n",
      "Iteration 796, loss = 0.21209317\n",
      "Iteration 797, loss = 0.21192511\n",
      "Iteration 798, loss = 0.21175740\n",
      "Iteration 799, loss = 0.21159004\n",
      "Iteration 800, loss = 0.21142301\n",
      "Iteration 801, loss = 0.21125633\n",
      "Iteration 802, loss = 0.21108998\n",
      "Iteration 803, loss = 0.21092397\n",
      "Iteration 804, loss = 0.21075830\n",
      "Iteration 805, loss = 0.21059297\n",
      "Iteration 806, loss = 0.21042798\n",
      "Iteration 807, loss = 0.21026332\n",
      "Iteration 808, loss = 0.21009900\n",
      "Iteration 809, loss = 0.20993501\n",
      "Iteration 810, loss = 0.20977135\n",
      "Iteration 811, loss = 0.20960803\n",
      "Iteration 812, loss = 0.20944504\n",
      "Iteration 813, loss = 0.20928238\n",
      "Iteration 814, loss = 0.20912005\n",
      "Iteration 815, loss = 0.20895805\n",
      "Iteration 816, loss = 0.20879638\n",
      "Iteration 817, loss = 0.20863503\n",
      "Iteration 818, loss = 0.20847401\n",
      "Iteration 819, loss = 0.20831332\n",
      "Iteration 820, loss = 0.20815295\n",
      "Iteration 821, loss = 0.20799291\n",
      "Iteration 822, loss = 0.20783319\n",
      "Iteration 823, loss = 0.20767379\n",
      "Iteration 824, loss = 0.20751472\n",
      "Iteration 825, loss = 0.20735597\n",
      "Iteration 826, loss = 0.20719754\n",
      "Iteration 827, loss = 0.20703942\n",
      "Iteration 828, loss = 0.20688163\n",
      "Iteration 829, loss = 0.20672416\n",
      "Iteration 830, loss = 0.20656700\n",
      "Iteration 831, loss = 0.20641016\n",
      "Iteration 832, loss = 0.20625363\n",
      "Iteration 833, loss = 0.20609742\n",
      "Iteration 834, loss = 0.20594153\n",
      "Iteration 835, loss = 0.20578594\n",
      "Iteration 836, loss = 0.20563067\n",
      "Iteration 837, loss = 0.20547572\n",
      "Iteration 838, loss = 0.20532107\n",
      "Iteration 839, loss = 0.20516674\n",
      "Iteration 840, loss = 0.20501271\n",
      "Iteration 841, loss = 0.20485900\n",
      "Iteration 842, loss = 0.20470559\n",
      "Iteration 843, loss = 0.20455249\n",
      "Iteration 844, loss = 0.20439969\n",
      "Iteration 845, loss = 0.20424721\n",
      "Iteration 846, loss = 0.20409502\n",
      "Iteration 847, loss = 0.20394314\n",
      "Iteration 848, loss = 0.20379157\n",
      "Iteration 849, loss = 0.20364030\n",
      "Iteration 850, loss = 0.20348933\n",
      "Iteration 851, loss = 0.20333866\n",
      "Iteration 852, loss = 0.20318831\n",
      "Iteration 853, loss = 0.20303824\n",
      "Iteration 854, loss = 0.20288847\n",
      "Iteration 855, loss = 0.20273901\n",
      "Iteration 856, loss = 0.20258985\n",
      "Iteration 857, loss = 0.20244097\n",
      "Iteration 858, loss = 0.20229241\n",
      "Iteration 859, loss = 0.20214412\n",
      "Iteration 860, loss = 0.20199615\n",
      "Iteration 861, loss = 0.20184846\n",
      "Iteration 862, loss = 0.20170106\n",
      "Iteration 863, loss = 0.20155396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 864, loss = 0.20140715\n",
      "Iteration 865, loss = 0.20126064\n",
      "Iteration 866, loss = 0.20111441\n",
      "Iteration 867, loss = 0.20096847\n",
      "Iteration 868, loss = 0.20082282\n",
      "Iteration 869, loss = 0.20067746\n",
      "Iteration 870, loss = 0.20053240\n",
      "Iteration 871, loss = 0.20038761\n",
      "Iteration 872, loss = 0.20024311\n",
      "Iteration 873, loss = 0.20009891\n",
      "Iteration 874, loss = 0.19995498\n",
      "Iteration 875, loss = 0.19981134\n",
      "Iteration 876, loss = 0.19966798\n",
      "Iteration 877, loss = 0.19952491\n",
      "Iteration 878, loss = 0.19938211\n",
      "Iteration 879, loss = 0.19923960\n",
      "Iteration 880, loss = 0.19909738\n",
      "Iteration 881, loss = 0.19895545\n",
      "Iteration 882, loss = 0.19881377\n",
      "Iteration 883, loss = 0.19867238\n",
      "Iteration 884, loss = 0.19853127\n",
      "Iteration 885, loss = 0.19839044\n",
      "Iteration 886, loss = 0.19824989\n",
      "Iteration 887, loss = 0.19810961\n",
      "Iteration 888, loss = 0.19796961\n",
      "Iteration 889, loss = 0.19782988\n",
      "Iteration 890, loss = 0.19769042\n",
      "Iteration 891, loss = 0.19755124\n",
      "Iteration 892, loss = 0.19741234\n",
      "Iteration 893, loss = 0.19727370\n",
      "Iteration 894, loss = 0.19713533\n",
      "Iteration 895, loss = 0.19699724\n",
      "Iteration 896, loss = 0.19685942\n",
      "Iteration 897, loss = 0.19672186\n",
      "Iteration 898, loss = 0.19658461\n",
      "Iteration 899, loss = 0.19644757\n",
      "Iteration 900, loss = 0.19631083\n",
      "Iteration 901, loss = 0.19617435\n",
      "Iteration 902, loss = 0.19603813\n",
      "Iteration 903, loss = 0.19590219\n",
      "Iteration 904, loss = 0.19576651\n",
      "Iteration 905, loss = 0.19563109\n",
      "Iteration 906, loss = 0.19549593\n",
      "Iteration 907, loss = 0.19536104\n",
      "Iteration 908, loss = 0.19522641\n",
      "Iteration 909, loss = 0.19509204\n",
      "Iteration 910, loss = 0.19495794\n",
      "Iteration 911, loss = 0.19482409\n",
      "Iteration 912, loss = 0.19469052\n",
      "Iteration 913, loss = 0.19455720\n",
      "Iteration 914, loss = 0.19442414\n",
      "Iteration 915, loss = 0.19429135\n",
      "Iteration 916, loss = 0.19415881\n",
      "Iteration 917, loss = 0.19402652\n",
      "Iteration 918, loss = 0.19389450\n",
      "Iteration 919, loss = 0.19376276\n",
      "Iteration 920, loss = 0.19363122\n",
      "Iteration 921, loss = 0.19349997\n",
      "Iteration 922, loss = 0.19336896\n",
      "Iteration 923, loss = 0.19323821\n",
      "Iteration 924, loss = 0.19310772\n",
      "Iteration 925, loss = 0.19297747\n",
      "Iteration 926, loss = 0.19284748\n",
      "Iteration 927, loss = 0.19271778\n",
      "Iteration 928, loss = 0.19258826\n",
      "Iteration 929, loss = 0.19245903\n",
      "Iteration 930, loss = 0.19233005\n",
      "Iteration 931, loss = 0.19220131\n",
      "Iteration 932, loss = 0.19207282\n",
      "Iteration 933, loss = 0.19194457\n",
      "Iteration 934, loss = 0.19181657\n",
      "Iteration 935, loss = 0.19168882\n",
      "Iteration 936, loss = 0.19156132\n",
      "Iteration 937, loss = 0.19143406\n",
      "Iteration 938, loss = 0.19130705\n",
      "Iteration 939, loss = 0.19118028\n",
      "Iteration 940, loss = 0.19105379\n",
      "Iteration 941, loss = 0.19092750\n",
      "Iteration 942, loss = 0.19080146\n",
      "Iteration 943, loss = 0.19067567\n",
      "Iteration 944, loss = 0.19055013\n",
      "Iteration 945, loss = 0.19042482\n",
      "Iteration 946, loss = 0.19029977\n",
      "Iteration 947, loss = 0.19017497\n",
      "Iteration 948, loss = 0.19005040\n",
      "Iteration 949, loss = 0.18992605\n",
      "Iteration 950, loss = 0.18980198\n",
      "Iteration 951, loss = 0.18967810\n",
      "Iteration 952, loss = 0.18955448\n",
      "Iteration 953, loss = 0.18943109\n",
      "Iteration 954, loss = 0.18930795\n",
      "Iteration 955, loss = 0.18918502\n",
      "Iteration 956, loss = 0.18906234\n",
      "Iteration 957, loss = 0.18893989\n",
      "Iteration 958, loss = 0.18881767\n",
      "Iteration 959, loss = 0.18869570\n",
      "Iteration 960, loss = 0.18857393\n",
      "Iteration 961, loss = 0.18845242\n",
      "Iteration 962, loss = 0.18833114\n",
      "Iteration 963, loss = 0.18821007\n",
      "Iteration 964, loss = 0.18808924\n",
      "Iteration 965, loss = 0.18796864\n",
      "Iteration 966, loss = 0.18784827\n",
      "Iteration 967, loss = 0.18772811\n",
      "Iteration 968, loss = 0.18760821\n",
      "Iteration 969, loss = 0.18748851\n",
      "Iteration 970, loss = 0.18736905\n",
      "Iteration 971, loss = 0.18724980\n",
      "Iteration 972, loss = 0.18713078\n",
      "Iteration 973, loss = 0.18701200\n",
      "Iteration 974, loss = 0.18689344\n",
      "Iteration 975, loss = 0.18677511\n",
      "Iteration 976, loss = 0.18665699\n",
      "Iteration 977, loss = 0.18653911\n",
      "Iteration 978, loss = 0.18642143\n",
      "Iteration 979, loss = 0.18630400\n",
      "Iteration 980, loss = 0.18618676\n",
      "Iteration 981, loss = 0.18606976\n",
      "Iteration 982, loss = 0.18595297\n",
      "Iteration 983, loss = 0.18583640\n",
      "Iteration 984, loss = 0.18572005\n",
      "Iteration 985, loss = 0.18560393\n",
      "Iteration 986, loss = 0.18548802\n",
      "Iteration 987, loss = 0.18537233\n",
      "Iteration 988, loss = 0.18525684\n",
      "Iteration 989, loss = 0.18514159\n",
      "Iteration 990, loss = 0.18502654\n",
      "Iteration 991, loss = 0.18491171\n",
      "Iteration 992, loss = 0.18479709\n",
      "Iteration 993, loss = 0.18468268\n",
      "Iteration 994, loss = 0.18456851\n",
      "Iteration 995, loss = 0.18445452\n",
      "Iteration 996, loss = 0.18434076\n",
      "Iteration 997, loss = 0.18422720\n",
      "Iteration 998, loss = 0.18411387\n",
      "Iteration 999, loss = 0.18400073\n",
      "Iteration 1000, loss = 0.18388783\n",
      "Iteration 1, loss = 1.92564666\n",
      "Iteration 2, loss = 1.88107456\n",
      "Iteration 3, loss = 1.82101339\n",
      "Iteration 4, loss = 1.75008524\n",
      "Iteration 5, loss = 1.67244843\n",
      "Iteration 6, loss = 1.59154726\n",
      "Iteration 7, loss = 1.51009324\n",
      "Iteration 8, loss = 1.43041917\n",
      "Iteration 9, loss = 1.35466016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 10, loss = 1.28493478\n",
      "Iteration 11, loss = 1.22333732\n",
      "Iteration 12, loss = 1.17183741\n",
      "Iteration 13, loss = 1.13174506\n",
      "Iteration 14, loss = 1.10329544\n",
      "Iteration 15, loss = 1.08522187\n",
      "Iteration 16, loss = 1.07529163\n",
      "Iteration 17, loss = 1.07042661\n",
      "Iteration 18, loss = 1.06765570\n",
      "Iteration 19, loss = 1.06447737\n",
      "Iteration 20, loss = 1.05927821\n",
      "Iteration 21, loss = 1.05131848\n",
      "Iteration 22, loss = 1.04067059\n",
      "Iteration 23, loss = 1.02800606\n",
      "Iteration 24, loss = 1.01417583\n",
      "Iteration 25, loss = 1.00004941\n",
      "Iteration 26, loss = 0.98628830\n",
      "Iteration 27, loss = 0.97326051\n",
      "Iteration 28, loss = 0.96110353\n",
      "Iteration 29, loss = 0.94973248\n",
      "Iteration 30, loss = 0.93885771\n",
      "Iteration 31, loss = 0.92826082\n",
      "Iteration 32, loss = 0.91771249\n",
      "Iteration 33, loss = 0.90687638\n",
      "Iteration 34, loss = 0.89580853\n",
      "Iteration 35, loss = 0.88452833\n",
      "Iteration 36, loss = 0.87301887\n",
      "Iteration 37, loss = 0.86152290\n",
      "Iteration 38, loss = 0.85043450\n",
      "Iteration 39, loss = 0.83995684\n",
      "Iteration 40, loss = 0.83012889\n",
      "Iteration 41, loss = 0.82095886\n",
      "Iteration 42, loss = 0.81251023\n",
      "Iteration 43, loss = 0.80479117\n",
      "Iteration 44, loss = 0.79777150\n",
      "Iteration 45, loss = 0.79114850\n",
      "Iteration 46, loss = 0.78466579\n",
      "Iteration 47, loss = 0.77825801\n",
      "Iteration 48, loss = 0.77186891\n",
      "Iteration 49, loss = 0.76553437\n",
      "Iteration 50, loss = 0.75924040\n",
      "Iteration 51, loss = 0.75311652\n",
      "Iteration 52, loss = 0.74719689\n",
      "Iteration 53, loss = 0.74142523\n",
      "Iteration 54, loss = 0.73588054\n",
      "Iteration 55, loss = 0.73056587\n",
      "Iteration 56, loss = 0.72546616\n",
      "Iteration 57, loss = 0.72058323\n",
      "Iteration 58, loss = 0.71586660\n",
      "Iteration 59, loss = 0.71129223\n",
      "Iteration 60, loss = 0.70679519\n",
      "Iteration 61, loss = 0.70235897\n",
      "Iteration 62, loss = 0.69799263\n",
      "Iteration 63, loss = 0.69367378\n",
      "Iteration 64, loss = 0.68944718\n",
      "Iteration 65, loss = 0.68532898\n",
      "Iteration 66, loss = 0.68130008\n",
      "Iteration 67, loss = 0.67737786\n",
      "Iteration 68, loss = 0.67355587\n",
      "Iteration 69, loss = 0.66981374\n",
      "Iteration 70, loss = 0.66615345\n",
      "Iteration 71, loss = 0.66257912\n",
      "Iteration 72, loss = 0.65906681\n",
      "Iteration 73, loss = 0.65560746\n",
      "Iteration 74, loss = 0.65220146\n",
      "Iteration 75, loss = 0.64884982\n",
      "Iteration 76, loss = 0.64554988\n",
      "Iteration 77, loss = 0.64229668\n",
      "Iteration 78, loss = 0.63909021\n",
      "Iteration 79, loss = 0.63593173\n",
      "Iteration 80, loss = 0.63281949\n",
      "Iteration 81, loss = 0.62975035\n",
      "Iteration 82, loss = 0.62672693\n",
      "Iteration 83, loss = 0.62374596\n",
      "Iteration 84, loss = 0.62080493\n",
      "Iteration 85, loss = 0.61790274\n",
      "Iteration 86, loss = 0.61504010\n",
      "Iteration 87, loss = 0.61222005\n",
      "Iteration 88, loss = 0.60944115\n",
      "Iteration 89, loss = 0.60670233\n",
      "Iteration 90, loss = 0.60399903\n",
      "Iteration 91, loss = 0.60133043\n",
      "Iteration 92, loss = 0.59869574\n",
      "Iteration 93, loss = 0.59609426\n",
      "Iteration 94, loss = 0.59352534\n",
      "Iteration 95, loss = 0.59098837\n",
      "Iteration 96, loss = 0.58848225\n",
      "Iteration 97, loss = 0.58600720\n",
      "Iteration 98, loss = 0.58356520\n",
      "Iteration 99, loss = 0.58115540\n",
      "Iteration 100, loss = 0.57877433\n",
      "Iteration 101, loss = 0.57642358\n",
      "Iteration 102, loss = 0.57410368\n",
      "Iteration 103, loss = 0.57181213\n",
      "Iteration 104, loss = 0.56954844\n",
      "Iteration 105, loss = 0.56731220\n",
      "Iteration 106, loss = 0.56510280\n",
      "Iteration 107, loss = 0.56292046\n",
      "Iteration 108, loss = 0.56076461\n",
      "Iteration 109, loss = 0.55863530\n",
      "Iteration 110, loss = 0.55653135\n",
      "Iteration 111, loss = 0.55445241\n",
      "Iteration 112, loss = 0.55239804\n",
      "Iteration 113, loss = 0.55036780\n",
      "Iteration 114, loss = 0.54836129\n",
      "Iteration 115, loss = 0.54637808\n",
      "Iteration 116, loss = 0.54441777\n",
      "Iteration 117, loss = 0.54247995\n",
      "Iteration 118, loss = 0.54056423\n",
      "Iteration 119, loss = 0.53867019\n",
      "Iteration 120, loss = 0.53679746\n",
      "Iteration 121, loss = 0.53494565\n",
      "Iteration 122, loss = 0.53311440\n",
      "Iteration 123, loss = 0.53130339\n",
      "Iteration 124, loss = 0.52951227\n",
      "Iteration 125, loss = 0.52774063\n",
      "Iteration 126, loss = 0.52598812\n",
      "Iteration 127, loss = 0.52425443\n",
      "Iteration 128, loss = 0.52253925\n",
      "Iteration 129, loss = 0.52084333\n",
      "Iteration 130, loss = 0.51916451\n",
      "Iteration 131, loss = 0.51750313\n",
      "Iteration 132, loss = 0.51585943\n",
      "Iteration 133, loss = 0.51423308\n",
      "Iteration 134, loss = 0.51262330\n",
      "Iteration 135, loss = 0.51103003\n",
      "Iteration 136, loss = 0.50945303\n",
      "Iteration 137, loss = 0.50789224\n",
      "Iteration 138, loss = 0.50634683\n",
      "Iteration 139, loss = 0.50481705\n",
      "Iteration 140, loss = 0.50330241\n",
      "Iteration 141, loss = 0.50180270\n",
      "Iteration 142, loss = 0.50031766\n",
      "Iteration 143, loss = 0.49884712\n",
      "Iteration 144, loss = 0.49739079\n",
      "Iteration 145, loss = 0.49594842\n",
      "Iteration 146, loss = 0.49451979\n",
      "Iteration 147, loss = 0.49310466\n",
      "Iteration 148, loss = 0.49170282\n",
      "Iteration 149, loss = 0.49031416\n",
      "Iteration 150, loss = 0.48893877\n",
      "Iteration 151, loss = 0.48757609\n",
      "Iteration 152, loss = 0.48622580\n",
      "Iteration 153, loss = 0.48488810\n",
      "Iteration 154, loss = 0.48356259\n",
      "Iteration 155, loss = 0.48224903\n",
      "Iteration 156, loss = 0.48094728\n",
      "Iteration 157, loss = 0.47965731\n",
      "Iteration 158, loss = 0.47837852\n",
      "Iteration 159, loss = 0.47711083\n",
      "Iteration 160, loss = 0.47585427\n",
      "Iteration 161, loss = 0.47460905\n",
      "Iteration 162, loss = 0.47337501\n",
      "Iteration 163, loss = 0.47215115\n",
      "Iteration 164, loss = 0.47093721\n",
      "Iteration 165, loss = 0.46973381\n",
      "Iteration 166, loss = 0.46854041\n",
      "Iteration 167, loss = 0.46735711\n",
      "Iteration 168, loss = 0.46618365\n",
      "Iteration 169, loss = 0.46501976\n",
      "Iteration 170, loss = 0.46386527\n",
      "Iteration 171, loss = 0.46272001\n",
      "Iteration 172, loss = 0.46158384\n",
      "Iteration 173, loss = 0.46045663\n",
      "Iteration 174, loss = 0.45933824\n",
      "Iteration 175, loss = 0.45822854\n",
      "Iteration 176, loss = 0.45712735\n",
      "Iteration 177, loss = 0.45603461\n",
      "Iteration 178, loss = 0.45495015\n",
      "Iteration 179, loss = 0.45387383\n",
      "Iteration 180, loss = 0.45280568\n",
      "Iteration 181, loss = 0.45174534\n",
      "Iteration 182, loss = 0.45069287\n",
      "Iteration 183, loss = 0.44964804\n",
      "Iteration 184, loss = 0.44861122\n",
      "Iteration 185, loss = 0.44758206\n",
      "Iteration 186, loss = 0.44656048\n",
      "Iteration 187, loss = 0.44554612\n",
      "Iteration 188, loss = 0.44453934\n",
      "Iteration 189, loss = 0.44353946\n",
      "Iteration 190, loss = 0.44254660\n",
      "Iteration 191, loss = 0.44156069\n",
      "Iteration 192, loss = 0.44058149\n",
      "Iteration 193, loss = 0.43960908\n",
      "Iteration 194, loss = 0.43864319\n",
      "Iteration 195, loss = 0.43768382\n",
      "Iteration 196, loss = 0.43673074\n",
      "Iteration 197, loss = 0.43578391\n",
      "Iteration 198, loss = 0.43484339\n",
      "Iteration 199, loss = 0.43390875\n",
      "Iteration 200, loss = 0.43298018\n",
      "Iteration 201, loss = 0.43205758\n",
      "Iteration 202, loss = 0.43114073\n",
      "Iteration 203, loss = 0.43022963\n",
      "Iteration 204, loss = 0.42932418\n",
      "Iteration 205, loss = 0.42842432\n",
      "Iteration 206, loss = 0.42752995\n",
      "Iteration 207, loss = 0.42664102\n",
      "Iteration 208, loss = 0.42575753\n",
      "Iteration 209, loss = 0.42487921\n",
      "Iteration 210, loss = 0.42400615\n",
      "Iteration 211, loss = 0.42313823\n",
      "Iteration 212, loss = 0.42227539\n",
      "Iteration 213, loss = 0.42141756\n",
      "Iteration 214, loss = 0.42056467\n",
      "Iteration 215, loss = 0.41971665\n",
      "Iteration 216, loss = 0.41887344\n",
      "Iteration 217, loss = 0.41803497\n",
      "Iteration 218, loss = 0.41720121\n",
      "Iteration 219, loss = 0.41637232\n",
      "Iteration 220, loss = 0.41554808\n",
      "Iteration 221, loss = 0.41472837\n",
      "Iteration 222, loss = 0.41391314\n",
      "Iteration 223, loss = 0.41310235\n",
      "Iteration 224, loss = 0.41229592\n",
      "Iteration 225, loss = 0.41149380\n",
      "Iteration 226, loss = 0.41069622\n",
      "Iteration 227, loss = 0.40990289\n",
      "Iteration 228, loss = 0.40911371\n",
      "Iteration 229, loss = 0.40832864\n",
      "Iteration 230, loss = 0.40754766\n",
      "Iteration 231, loss = 0.40677065\n",
      "Iteration 232, loss = 0.40599762\n",
      "Iteration 233, loss = 0.40522915\n",
      "Iteration 234, loss = 0.40446490\n",
      "Iteration 235, loss = 0.40370476\n",
      "Iteration 236, loss = 0.40294818\n",
      "Iteration 237, loss = 0.40219515\n",
      "Iteration 238, loss = 0.40144571\n",
      "Iteration 239, loss = 0.40069974\n",
      "Iteration 240, loss = 0.39995731\n",
      "Iteration 241, loss = 0.39921844\n",
      "Iteration 242, loss = 0.39848337\n",
      "Iteration 243, loss = 0.39775178\n",
      "Iteration 244, loss = 0.39702449\n",
      "Iteration 245, loss = 0.39630052\n",
      "Iteration 246, loss = 0.39557992\n",
      "Iteration 247, loss = 0.39486263\n",
      "Iteration 248, loss = 0.39414866\n",
      "Iteration 249, loss = 0.39343796\n",
      "Iteration 250, loss = 0.39273049\n",
      "Iteration 251, loss = 0.39202626\n",
      "Iteration 252, loss = 0.39132517\n",
      "Iteration 253, loss = 0.39062726\n",
      "Iteration 254, loss = 0.38993245\n",
      "Iteration 255, loss = 0.38924072\n",
      "Iteration 256, loss = 0.38855206\n",
      "Iteration 257, loss = 0.38786643\n",
      "Iteration 258, loss = 0.38718379\n",
      "Iteration 259, loss = 0.38650413\n",
      "Iteration 260, loss = 0.38582740\n",
      "Iteration 261, loss = 0.38515358\n",
      "Iteration 262, loss = 0.38448266\n",
      "Iteration 263, loss = 0.38381464\n",
      "Iteration 264, loss = 0.38314947\n",
      "Iteration 265, loss = 0.38248725\n",
      "Iteration 266, loss = 0.38182767\n",
      "Iteration 267, loss = 0.38117082\n",
      "Iteration 268, loss = 0.38051667\n",
      "Iteration 269, loss = 0.37986527\n",
      "Iteration 270, loss = 0.37921652\n",
      "Iteration 271, loss = 0.37857044\n",
      "Iteration 272, loss = 0.37792696\n",
      "Iteration 273, loss = 0.37728602\n",
      "Iteration 274, loss = 0.37664774\n",
      "Iteration 275, loss = 0.37601190\n",
      "Iteration 276, loss = 0.37537860\n",
      "Iteration 277, loss = 0.37474782\n",
      "Iteration 278, loss = 0.37411945\n",
      "Iteration 279, loss = 0.37349367\n",
      "Iteration 280, loss = 0.37287021\n",
      "Iteration 281, loss = 0.37224922\n",
      "Iteration 282, loss = 0.37163063\n",
      "Iteration 283, loss = 0.37101437\n",
      "Iteration 284, loss = 0.37040060\n",
      "Iteration 285, loss = 0.36978901\n",
      "Iteration 286, loss = 0.36917978\n",
      "Iteration 287, loss = 0.36857291\n",
      "Iteration 288, loss = 0.36796813\n",
      "Iteration 289, loss = 0.36736575\n",
      "Iteration 290, loss = 0.36676553\n",
      "Iteration 291, loss = 0.36616747\n",
      "Iteration 292, loss = 0.36557180\n",
      "Iteration 293, loss = 0.36497803\n",
      "Iteration 294, loss = 0.36438676\n",
      "Iteration 295, loss = 0.36379768\n",
      "Iteration 296, loss = 0.36321078\n",
      "Iteration 297, loss = 0.36262599\n",
      "Iteration 298, loss = 0.36204329\n",
      "Iteration 299, loss = 0.36146275\n",
      "Iteration 300, loss = 0.36088416\n",
      "Iteration 301, loss = 0.36030774\n",
      "Iteration 302, loss = 0.35973352\n",
      "Iteration 303, loss = 0.35916148\n",
      "Iteration 304, loss = 0.35859127\n",
      "Iteration 305, loss = 0.35802325\n",
      "Iteration 306, loss = 0.35745735\n",
      "Iteration 307, loss = 0.35689337\n",
      "Iteration 308, loss = 0.35633159\n",
      "Iteration 309, loss = 0.35577161\n",
      "Iteration 310, loss = 0.35521352\n",
      "Iteration 311, loss = 0.35465734\n",
      "Iteration 312, loss = 0.35410334\n",
      "Iteration 313, loss = 0.35355086\n",
      "Iteration 314, loss = 0.35300033\n",
      "Iteration 315, loss = 0.35245164\n",
      "Iteration 316, loss = 0.35190509\n",
      "Iteration 317, loss = 0.35136035\n",
      "Iteration 318, loss = 0.35081758\n",
      "Iteration 319, loss = 0.35027645\n",
      "Iteration 320, loss = 0.34973751\n",
      "Iteration 321, loss = 0.34920065\n",
      "Iteration 322, loss = 0.34866550\n",
      "Iteration 323, loss = 0.34813214\n",
      "Iteration 324, loss = 0.34760069\n",
      "Iteration 325, loss = 0.34707111\n",
      "Iteration 326, loss = 0.34654326\n",
      "Iteration 327, loss = 0.34601731\n",
      "Iteration 328, loss = 0.34549306\n",
      "Iteration 329, loss = 0.34497056\n",
      "Iteration 330, loss = 0.34444963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 331, loss = 0.34393037\n",
      "Iteration 332, loss = 0.34341288\n",
      "Iteration 333, loss = 0.34289709\n",
      "Iteration 334, loss = 0.34238293\n",
      "Iteration 335, loss = 0.34187036\n",
      "Iteration 336, loss = 0.34135938\n",
      "Iteration 337, loss = 0.34084999\n",
      "Iteration 338, loss = 0.34034212\n",
      "Iteration 339, loss = 0.33983593\n",
      "Iteration 340, loss = 0.33933115\n",
      "Iteration 341, loss = 0.33882798\n",
      "Iteration 342, loss = 0.33832650\n",
      "Iteration 343, loss = 0.33782657\n",
      "Iteration 344, loss = 0.33732813\n",
      "Iteration 345, loss = 0.33683116\n",
      "Iteration 346, loss = 0.33633572\n",
      "Iteration 347, loss = 0.33584193\n",
      "Iteration 348, loss = 0.33534962\n",
      "Iteration 349, loss = 0.33485879\n",
      "Iteration 350, loss = 0.33436939\n",
      "Iteration 351, loss = 0.33388144\n",
      "Iteration 352, loss = 0.33339490\n",
      "Iteration 353, loss = 0.33290977\n",
      "Iteration 354, loss = 0.33242608\n",
      "Iteration 355, loss = 0.33194384\n",
      "Iteration 356, loss = 0.33146302\n",
      "Iteration 357, loss = 0.33098354\n",
      "Iteration 358, loss = 0.33050548\n",
      "Iteration 359, loss = 0.33002883\n",
      "Iteration 360, loss = 0.32955369\n",
      "Iteration 361, loss = 0.32907991\n",
      "Iteration 362, loss = 0.32860749\n",
      "Iteration 363, loss = 0.32813640\n",
      "Iteration 364, loss = 0.32766664\n",
      "Iteration 365, loss = 0.32719820\n",
      "Iteration 366, loss = 0.32673108\n",
      "Iteration 367, loss = 0.32626526\n",
      "Iteration 368, loss = 0.32580080\n",
      "Iteration 369, loss = 0.32533772\n",
      "Iteration 370, loss = 0.32487595\n",
      "Iteration 371, loss = 0.32441546\n",
      "Iteration 372, loss = 0.32395633\n",
      "Iteration 373, loss = 0.32349855\n",
      "Iteration 374, loss = 0.32304207\n",
      "Iteration 375, loss = 0.32258681\n",
      "Iteration 376, loss = 0.32213282\n",
      "Iteration 377, loss = 0.32168012\n",
      "Iteration 378, loss = 0.32122881\n",
      "Iteration 379, loss = 0.32077876\n",
      "Iteration 380, loss = 0.32032991\n",
      "Iteration 381, loss = 0.31988230\n",
      "Iteration 382, loss = 0.31943590\n",
      "Iteration 383, loss = 0.31899070\n",
      "Iteration 384, loss = 0.31854672\n",
      "Iteration 385, loss = 0.31810408\n",
      "Iteration 386, loss = 0.31766263\n",
      "Iteration 387, loss = 0.31722236\n",
      "Iteration 388, loss = 0.31678327\n",
      "Iteration 389, loss = 0.31634543\n",
      "Iteration 390, loss = 0.31590883\n",
      "Iteration 391, loss = 0.31547340\n",
      "Iteration 392, loss = 0.31503913\n",
      "Iteration 393, loss = 0.31460620\n",
      "Iteration 394, loss = 0.31417442\n",
      "Iteration 395, loss = 0.31374379\n",
      "Iteration 396, loss = 0.31331430\n",
      "Iteration 397, loss = 0.31288593\n",
      "Iteration 398, loss = 0.31245869\n",
      "Iteration 399, loss = 0.31203256\n",
      "Iteration 400, loss = 0.31160755\n",
      "Iteration 401, loss = 0.31118363\n",
      "Iteration 402, loss = 0.31076082\n",
      "Iteration 403, loss = 0.31033919\n",
      "Iteration 404, loss = 0.30991867\n",
      "Iteration 405, loss = 0.30949930\n",
      "Iteration 406, loss = 0.30908103\n",
      "Iteration 407, loss = 0.30866385\n",
      "Iteration 408, loss = 0.30824773\n",
      "Iteration 409, loss = 0.30783269\n",
      "Iteration 410, loss = 0.30741871\n",
      "Iteration 411, loss = 0.30700578\n",
      "Iteration 412, loss = 0.30659390\n",
      "Iteration 413, loss = 0.30618307\n",
      "Iteration 414, loss = 0.30577328\n",
      "Iteration 415, loss = 0.30536452\n",
      "Iteration 416, loss = 0.30495680\n",
      "Iteration 417, loss = 0.30455010\n",
      "Iteration 418, loss = 0.30414441\n",
      "Iteration 419, loss = 0.30373975\n",
      "Iteration 420, loss = 0.30333610\n",
      "Iteration 421, loss = 0.30293345\n",
      "Iteration 422, loss = 0.30253182\n",
      "Iteration 423, loss = 0.30213120\n",
      "Iteration 424, loss = 0.30173161\n",
      "Iteration 425, loss = 0.30133301\n",
      "Iteration 426, loss = 0.30093542\n",
      "Iteration 427, loss = 0.30053881\n",
      "Iteration 428, loss = 0.30014319\n",
      "Iteration 429, loss = 0.29974855\n",
      "Iteration 430, loss = 0.29935487\n",
      "Iteration 431, loss = 0.29896217\n",
      "Iteration 432, loss = 0.29857043\n",
      "Iteration 433, loss = 0.29817965\n",
      "Iteration 434, loss = 0.29778984\n",
      "Iteration 435, loss = 0.29740099\n",
      "Iteration 436, loss = 0.29701311\n",
      "Iteration 437, loss = 0.29662624\n",
      "Iteration 438, loss = 0.29624032\n",
      "Iteration 439, loss = 0.29585536\n",
      "Iteration 440, loss = 0.29547134\n",
      "Iteration 441, loss = 0.29508827\n",
      "Iteration 442, loss = 0.29470613\n",
      "Iteration 443, loss = 0.29432492\n",
      "Iteration 444, loss = 0.29394463\n",
      "Iteration 445, loss = 0.29356528\n",
      "Iteration 446, loss = 0.29318691\n",
      "Iteration 447, loss = 0.29280947\n",
      "Iteration 448, loss = 0.29243297\n",
      "Iteration 449, loss = 0.29205738\n",
      "Iteration 450, loss = 0.29168271\n",
      "Iteration 451, loss = 0.29130895\n",
      "Iteration 452, loss = 0.29093608\n",
      "Iteration 453, loss = 0.29056412\n",
      "Iteration 454, loss = 0.29019307\n",
      "Iteration 455, loss = 0.28982290\n",
      "Iteration 456, loss = 0.28945363\n",
      "Iteration 457, loss = 0.28908524\n",
      "Iteration 458, loss = 0.28871773\n",
      "Iteration 459, loss = 0.28835111\n",
      "Iteration 460, loss = 0.28798538\n",
      "Iteration 461, loss = 0.28762051\n",
      "Iteration 462, loss = 0.28725652\n",
      "Iteration 463, loss = 0.28689369\n",
      "Iteration 464, loss = 0.28653162\n",
      "Iteration 465, loss = 0.28617036\n",
      "Iteration 466, loss = 0.28581006\n",
      "Iteration 467, loss = 0.28545041\n",
      "Iteration 468, loss = 0.28509169\n",
      "Iteration 469, loss = 0.28473379\n",
      "Iteration 470, loss = 0.28437687\n",
      "Iteration 471, loss = 0.28402082\n",
      "Iteration 472, loss = 0.28366561\n",
      "Iteration 473, loss = 0.28331123\n",
      "Iteration 474, loss = 0.28295768\n",
      "Iteration 475, loss = 0.28260495\n",
      "Iteration 476, loss = 0.28225305\n",
      "Iteration 477, loss = 0.28190210\n",
      "Iteration 478, loss = 0.28155188\n",
      "Iteration 479, loss = 0.28120247\n",
      "Iteration 480, loss = 0.28085395\n",
      "Iteration 481, loss = 0.28050623\n",
      "Iteration 482, loss = 0.28015931\n",
      "Iteration 483, loss = 0.27981323\n",
      "Iteration 484, loss = 0.27946794\n",
      "Iteration 485, loss = 0.27912356\n",
      "Iteration 486, loss = 0.27877986\n",
      "Iteration 487, loss = 0.27843704\n",
      "Iteration 488, loss = 0.27809502\n",
      "Iteration 489, loss = 0.27775389\n",
      "Iteration 490, loss = 0.27741365\n",
      "Iteration 491, loss = 0.27707414\n",
      "Iteration 492, loss = 0.27673551\n",
      "Iteration 493, loss = 0.27639762\n",
      "Iteration 494, loss = 0.27606053\n",
      "Iteration 495, loss = 0.27572429\n",
      "Iteration 496, loss = 0.27538878\n",
      "Iteration 497, loss = 0.27505416\n",
      "Iteration 498, loss = 0.27472026\n",
      "Iteration 499, loss = 0.27438715\n",
      "Iteration 500, loss = 0.27405482\n",
      "Iteration 501, loss = 0.27372338\n",
      "Iteration 502, loss = 0.27339270\n",
      "Iteration 503, loss = 0.27306282\n",
      "Iteration 504, loss = 0.27273368\n",
      "Iteration 505, loss = 0.27240538\n",
      "Iteration 506, loss = 0.27207779\n",
      "Iteration 507, loss = 0.27175100\n",
      "Iteration 508, loss = 0.27142506\n",
      "Iteration 509, loss = 0.27109994\n",
      "Iteration 510, loss = 0.27077555\n",
      "Iteration 511, loss = 0.27045198\n",
      "Iteration 512, loss = 0.27012916\n",
      "Iteration 513, loss = 0.26980723\n",
      "Iteration 514, loss = 0.26948607\n",
      "Iteration 515, loss = 0.26916571\n",
      "Iteration 516, loss = 0.26884610\n",
      "Iteration 517, loss = 0.26852728\n",
      "Iteration 518, loss = 0.26820916\n",
      "Iteration 519, loss = 0.26789187\n",
      "Iteration 520, loss = 0.26757522\n",
      "Iteration 521, loss = 0.26725936\n",
      "Iteration 522, loss = 0.26694421\n",
      "Iteration 523, loss = 0.26662984\n",
      "Iteration 524, loss = 0.26631615\n",
      "Iteration 525, loss = 0.26600322\n",
      "Iteration 526, loss = 0.26569099\n",
      "Iteration 527, loss = 0.26537954\n",
      "Iteration 528, loss = 0.26506895\n",
      "Iteration 529, loss = 0.26475913\n",
      "Iteration 530, loss = 0.26444999\n",
      "Iteration 531, loss = 0.26414161\n",
      "Iteration 532, loss = 0.26383391\n",
      "Iteration 533, loss = 0.26352694\n",
      "Iteration 534, loss = 0.26322068\n",
      "Iteration 535, loss = 0.26291511\n",
      "Iteration 536, loss = 0.26261029\n",
      "Iteration 537, loss = 0.26230613\n",
      "Iteration 538, loss = 0.26200269\n",
      "Iteration 539, loss = 0.26169992\n",
      "Iteration 540, loss = 0.26139793\n",
      "Iteration 541, loss = 0.26109652\n",
      "Iteration 542, loss = 0.26079589\n",
      "Iteration 543, loss = 0.26049589\n",
      "Iteration 544, loss = 0.26019662\n",
      "Iteration 545, loss = 0.25989801\n",
      "Iteration 546, loss = 0.25960008\n",
      "Iteration 547, loss = 0.25930287\n",
      "Iteration 548, loss = 0.25900628\n",
      "Iteration 549, loss = 0.25871045\n",
      "Iteration 550, loss = 0.25841521\n",
      "Iteration 551, loss = 0.25812066\n",
      "Iteration 552, loss = 0.25782683\n",
      "Iteration 553, loss = 0.25753362\n",
      "Iteration 554, loss = 0.25724114\n",
      "Iteration 555, loss = 0.25694936\n",
      "Iteration 556, loss = 0.25665831\n",
      "Iteration 557, loss = 0.25636787\n",
      "Iteration 558, loss = 0.25607812\n",
      "Iteration 559, loss = 0.25578905\n",
      "Iteration 560, loss = 0.25550060\n",
      "Iteration 561, loss = 0.25521282\n",
      "Iteration 562, loss = 0.25492569\n",
      "Iteration 563, loss = 0.25463924\n",
      "Iteration 564, loss = 0.25435341\n",
      "Iteration 565, loss = 0.25406824\n",
      "Iteration 566, loss = 0.25378377\n",
      "Iteration 567, loss = 0.25349992\n",
      "Iteration 568, loss = 0.25321673\n",
      "Iteration 569, loss = 0.25293417\n",
      "Iteration 570, loss = 0.25265226\n",
      "Iteration 571, loss = 0.25237101\n",
      "Iteration 572, loss = 0.25209038\n",
      "Iteration 573, loss = 0.25181041\n",
      "Iteration 574, loss = 0.25153104\n",
      "Iteration 575, loss = 0.25125235\n",
      "Iteration 576, loss = 0.25097428\n",
      "Iteration 577, loss = 0.25069683\n",
      "Iteration 578, loss = 0.25042003\n",
      "Iteration 579, loss = 0.25014383\n",
      "Iteration 580, loss = 0.24986827\n",
      "Iteration 581, loss = 0.24959338\n",
      "Iteration 582, loss = 0.24931916\n",
      "Iteration 583, loss = 0.24904557\n",
      "Iteration 584, loss = 0.24877258\n",
      "Iteration 585, loss = 0.24850018\n",
      "Iteration 586, loss = 0.24822844\n",
      "Iteration 587, loss = 0.24795726\n",
      "Iteration 588, loss = 0.24768669\n",
      "Iteration 589, loss = 0.24741682\n",
      "Iteration 590, loss = 0.24714748\n",
      "Iteration 591, loss = 0.24687880\n",
      "Iteration 592, loss = 0.24661069\n",
      "Iteration 593, loss = 0.24634318\n",
      "Iteration 594, loss = 0.24607632\n",
      "Iteration 595, loss = 0.24581002\n",
      "Iteration 596, loss = 0.24554432\n",
      "Iteration 597, loss = 0.24527925\n",
      "Iteration 598, loss = 0.24501474\n",
      "Iteration 599, loss = 0.24475086\n",
      "Iteration 600, loss = 0.24448754\n",
      "Iteration 601, loss = 0.24422484\n",
      "Iteration 602, loss = 0.24396271\n",
      "Iteration 603, loss = 0.24370117\n",
      "Iteration 604, loss = 0.24344022\n",
      "Iteration 605, loss = 0.24317987\n",
      "Iteration 606, loss = 0.24292007\n",
      "Iteration 607, loss = 0.24266091\n",
      "Iteration 608, loss = 0.24240226\n",
      "Iteration 609, loss = 0.24214424\n",
      "Iteration 610, loss = 0.24188678\n",
      "Iteration 611, loss = 0.24162990\n",
      "Iteration 612, loss = 0.24137359\n",
      "Iteration 613, loss = 0.24111786\n",
      "Iteration 614, loss = 0.24086269\n",
      "Iteration 615, loss = 0.24060811\n",
      "Iteration 616, loss = 0.24035408\n",
      "Iteration 617, loss = 0.24010063\n",
      "Iteration 618, loss = 0.23984775\n",
      "Iteration 619, loss = 0.23959542\n",
      "Iteration 620, loss = 0.23934365\n",
      "Iteration 621, loss = 0.23909248\n",
      "Iteration 622, loss = 0.23884183\n",
      "Iteration 623, loss = 0.23859175\n",
      "Iteration 624, loss = 0.23834225\n",
      "Iteration 625, loss = 0.23809326\n",
      "Iteration 626, loss = 0.23784488\n",
      "Iteration 627, loss = 0.23759702\n",
      "Iteration 628, loss = 0.23734972\n",
      "Iteration 629, loss = 0.23710294\n",
      "Iteration 630, loss = 0.23685678\n",
      "Iteration 631, loss = 0.23661110\n",
      "Iteration 632, loss = 0.23636601\n",
      "Iteration 633, loss = 0.23612144\n",
      "Iteration 634, loss = 0.23587743\n",
      "Iteration 635, loss = 0.23563393\n",
      "Iteration 636, loss = 0.23539102\n",
      "Iteration 637, loss = 0.23514862\n",
      "Iteration 638, loss = 0.23490676\n",
      "Iteration 639, loss = 0.23466546\n",
      "Iteration 640, loss = 0.23442466\n",
      "Iteration 641, loss = 0.23418443\n",
      "Iteration 642, loss = 0.23394470\n",
      "Iteration 643, loss = 0.23370552\n",
      "Iteration 644, loss = 0.23346689\n",
      "Iteration 645, loss = 0.23322876\n",
      "Iteration 646, loss = 0.23299116\n",
      "Iteration 647, loss = 0.23275410\n",
      "Iteration 648, loss = 0.23251757\n",
      "Iteration 649, loss = 0.23228154\n",
      "Iteration 650, loss = 0.23204605\n",
      "Iteration 651, loss = 0.23181108\n",
      "Iteration 652, loss = 0.23157661\n",
      "Iteration 653, loss = 0.23134269\n",
      "Iteration 654, loss = 0.23110927\n",
      "Iteration 655, loss = 0.23087638\n",
      "Iteration 656, loss = 0.23064399\n",
      "Iteration 657, loss = 0.23041212\n",
      "Iteration 658, loss = 0.23018076\n",
      "Iteration 659, loss = 0.22994991\n",
      "Iteration 660, loss = 0.22971958\n",
      "Iteration 661, loss = 0.22948974\n",
      "Iteration 662, loss = 0.22926043\n",
      "Iteration 663, loss = 0.22903162\n",
      "Iteration 664, loss = 0.22880333\n",
      "Iteration 665, loss = 0.22857552\n",
      "Iteration 666, loss = 0.22834827\n",
      "Iteration 667, loss = 0.22812147\n",
      "Iteration 668, loss = 0.22789521\n",
      "Iteration 669, loss = 0.22766943\n",
      "Iteration 670, loss = 0.22744415\n",
      "Iteration 671, loss = 0.22721936\n",
      "Iteration 672, loss = 0.22699510\n",
      "Iteration 673, loss = 0.22677130\n",
      "Iteration 674, loss = 0.22654801\n",
      "Iteration 675, loss = 0.22632524\n",
      "Iteration 676, loss = 0.22610294\n",
      "Iteration 677, loss = 0.22588113\n",
      "Iteration 678, loss = 0.22565983\n",
      "Iteration 679, loss = 0.22543901\n",
      "Iteration 680, loss = 0.22521866\n",
      "Iteration 681, loss = 0.22499880\n",
      "Iteration 682, loss = 0.22477944\n",
      "Iteration 683, loss = 0.22456055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 684, loss = 0.22434214\n",
      "Iteration 685, loss = 0.22412420\n",
      "Iteration 686, loss = 0.22390676\n",
      "Iteration 687, loss = 0.22368978\n",
      "Iteration 688, loss = 0.22347329\n",
      "Iteration 689, loss = 0.22325727\n",
      "Iteration 690, loss = 0.22304170\n",
      "Iteration 691, loss = 0.22282663\n",
      "Iteration 692, loss = 0.22261204\n",
      "Iteration 693, loss = 0.22239788\n",
      "Iteration 694, loss = 0.22218421\n",
      "Iteration 695, loss = 0.22197103\n",
      "Iteration 696, loss = 0.22175830\n",
      "Iteration 697, loss = 0.22154606\n",
      "Iteration 698, loss = 0.22133426\n",
      "Iteration 699, loss = 0.22112295\n",
      "Iteration 700, loss = 0.22091207\n",
      "Iteration 701, loss = 0.22070167\n",
      "Iteration 702, loss = 0.22049173\n",
      "Iteration 703, loss = 0.22028224\n",
      "Iteration 704, loss = 0.22007322\n",
      "Iteration 705, loss = 0.21986464\n",
      "Iteration 706, loss = 0.21965654\n",
      "Iteration 707, loss = 0.21944886\n",
      "Iteration 708, loss = 0.21924165\n",
      "Iteration 709, loss = 0.21903491\n",
      "Iteration 710, loss = 0.21882860\n",
      "Iteration 711, loss = 0.21862274\n",
      "Iteration 712, loss = 0.21841732\n",
      "Iteration 713, loss = 0.21821237\n",
      "Iteration 714, loss = 0.21800784\n",
      "Iteration 715, loss = 0.21780376\n",
      "Iteration 716, loss = 0.21760014\n",
      "Iteration 717, loss = 0.21739695\n",
      "Iteration 718, loss = 0.21719420\n",
      "Iteration 719, loss = 0.21699190\n",
      "Iteration 720, loss = 0.21679003\n",
      "Iteration 721, loss = 0.21658860\n",
      "Iteration 722, loss = 0.21638760\n",
      "Iteration 723, loss = 0.21618705\n",
      "Iteration 724, loss = 0.21598693\n",
      "Iteration 725, loss = 0.21578723\n",
      "Iteration 726, loss = 0.21558799\n",
      "Iteration 727, loss = 0.21538915\n",
      "Iteration 728, loss = 0.21519076\n",
      "Iteration 729, loss = 0.21499279\n",
      "Iteration 730, loss = 0.21479527\n",
      "Iteration 731, loss = 0.21459815\n",
      "Iteration 732, loss = 0.21440146\n",
      "Iteration 733, loss = 0.21420521\n",
      "Iteration 734, loss = 0.21400937\n",
      "Iteration 735, loss = 0.21381395\n",
      "Iteration 736, loss = 0.21361897\n",
      "Iteration 737, loss = 0.21342440\n",
      "Iteration 738, loss = 0.21323025\n",
      "Iteration 739, loss = 0.21303651\n",
      "Iteration 740, loss = 0.21284322\n",
      "Iteration 741, loss = 0.21265030\n",
      "Iteration 742, loss = 0.21245782\n",
      "Iteration 743, loss = 0.21226576\n",
      "Iteration 744, loss = 0.21207411\n",
      "Iteration 745, loss = 0.21188286\n",
      "Iteration 746, loss = 0.21169203\n",
      "Iteration 747, loss = 0.21150162\n",
      "Iteration 748, loss = 0.21131160\n",
      "Iteration 749, loss = 0.21112199\n",
      "Iteration 750, loss = 0.21093281\n",
      "Iteration 751, loss = 0.21074402\n",
      "Iteration 752, loss = 0.21055564\n",
      "Iteration 753, loss = 0.21036765\n",
      "Iteration 754, loss = 0.21018009\n",
      "Iteration 755, loss = 0.20999292\n",
      "Iteration 756, loss = 0.20980614\n",
      "Iteration 757, loss = 0.20961978\n",
      "Iteration 758, loss = 0.20943382\n",
      "Iteration 759, loss = 0.20924825\n",
      "Iteration 760, loss = 0.20906307\n",
      "Iteration 761, loss = 0.20887830\n",
      "Iteration 762, loss = 0.20869392\n",
      "Iteration 763, loss = 0.20850994\n",
      "Iteration 764, loss = 0.20832634\n",
      "Iteration 765, loss = 0.20814316\n",
      "Iteration 766, loss = 0.20796034\n",
      "Iteration 767, loss = 0.20777792\n",
      "Iteration 768, loss = 0.20759590\n",
      "Iteration 769, loss = 0.20741426\n",
      "Iteration 770, loss = 0.20723301\n",
      "Iteration 771, loss = 0.20705214\n",
      "Iteration 772, loss = 0.20687168\n",
      "Iteration 773, loss = 0.20669158\n",
      "Iteration 774, loss = 0.20651186\n",
      "Iteration 775, loss = 0.20633253\n",
      "Iteration 776, loss = 0.20615360\n",
      "Iteration 777, loss = 0.20597502\n",
      "Iteration 778, loss = 0.20579683\n",
      "Iteration 779, loss = 0.20561904\n",
      "Iteration 780, loss = 0.20544161\n",
      "Iteration 781, loss = 0.20526456\n",
      "Iteration 782, loss = 0.20508787\n",
      "Iteration 783, loss = 0.20491159\n",
      "Iteration 784, loss = 0.20473565\n",
      "Iteration 785, loss = 0.20456010\n",
      "Iteration 786, loss = 0.20438493\n",
      "Iteration 787, loss = 0.20421012\n",
      "Iteration 788, loss = 0.20403568\n",
      "Iteration 789, loss = 0.20386160\n",
      "Iteration 790, loss = 0.20368792\n",
      "Iteration 791, loss = 0.20351458\n",
      "Iteration 792, loss = 0.20334161\n",
      "Iteration 793, loss = 0.20316902\n",
      "Iteration 794, loss = 0.20299678\n",
      "Iteration 795, loss = 0.20282491\n",
      "Iteration 796, loss = 0.20265340\n",
      "Iteration 797, loss = 0.20248228\n",
      "Iteration 798, loss = 0.20231148\n",
      "Iteration 799, loss = 0.20214106\n",
      "Iteration 800, loss = 0.20197101\n",
      "Iteration 801, loss = 0.20180129\n",
      "Iteration 802, loss = 0.20163195\n",
      "Iteration 803, loss = 0.20146295\n",
      "Iteration 804, loss = 0.20129434\n",
      "Iteration 805, loss = 0.20112605\n",
      "Iteration 806, loss = 0.20095812\n",
      "Iteration 807, loss = 0.20079056\n",
      "Iteration 808, loss = 0.20062334\n",
      "Iteration 809, loss = 0.20045648\n",
      "Iteration 810, loss = 0.20028996\n",
      "Iteration 811, loss = 0.20012380\n",
      "Iteration 812, loss = 0.19995798\n",
      "Iteration 813, loss = 0.19979251\n",
      "Iteration 814, loss = 0.19962739\n",
      "Iteration 815, loss = 0.19946262\n",
      "Iteration 816, loss = 0.19929819\n",
      "Iteration 817, loss = 0.19913411\n",
      "Iteration 818, loss = 0.19897037\n",
      "Iteration 819, loss = 0.19880698\n",
      "Iteration 820, loss = 0.19864393\n",
      "Iteration 821, loss = 0.19848122\n",
      "Iteration 822, loss = 0.19831887\n",
      "Iteration 823, loss = 0.19815684\n",
      "Iteration 824, loss = 0.19799515\n",
      "Iteration 825, loss = 0.19783381\n",
      "Iteration 826, loss = 0.19767280\n",
      "Iteration 827, loss = 0.19751212\n",
      "Iteration 828, loss = 0.19735178\n",
      "Iteration 829, loss = 0.19719179\n",
      "Iteration 830, loss = 0.19703211\n",
      "Iteration 831, loss = 0.19687277\n",
      "Iteration 832, loss = 0.19671377\n",
      "Iteration 833, loss = 0.19655510\n",
      "Iteration 834, loss = 0.19639676\n",
      "Iteration 835, loss = 0.19623874\n",
      "Iteration 836, loss = 0.19608106\n",
      "Iteration 837, loss = 0.19592370\n",
      "Iteration 838, loss = 0.19576667\n",
      "Iteration 839, loss = 0.19560998\n",
      "Iteration 840, loss = 0.19545359\n",
      "Iteration 841, loss = 0.19529753\n",
      "Iteration 842, loss = 0.19514181\n",
      "Iteration 843, loss = 0.19498640\n",
      "Iteration 844, loss = 0.19483132\n",
      "Iteration 845, loss = 0.19467656\n",
      "Iteration 846, loss = 0.19452212\n",
      "Iteration 847, loss = 0.19436799\n",
      "Iteration 848, loss = 0.19421419\n",
      "Iteration 849, loss = 0.19406070\n",
      "Iteration 850, loss = 0.19390754\n",
      "Iteration 851, loss = 0.19375469\n",
      "Iteration 852, loss = 0.19360215\n",
      "Iteration 853, loss = 0.19344994\n",
      "Iteration 854, loss = 0.19329803\n",
      "Iteration 855, loss = 0.19314644\n",
      "Iteration 856, loss = 0.19299516\n",
      "Iteration 857, loss = 0.19284420\n",
      "Iteration 858, loss = 0.19269354\n",
      "Iteration 859, loss = 0.19254319\n",
      "Iteration 860, loss = 0.19239317\n",
      "Iteration 861, loss = 0.19224343\n",
      "Iteration 862, loss = 0.19209401\n",
      "Iteration 863, loss = 0.19194491\n",
      "Iteration 864, loss = 0.19179610\n",
      "Iteration 865, loss = 0.19164760\n",
      "Iteration 866, loss = 0.19149940\n",
      "Iteration 867, loss = 0.19135152\n",
      "Iteration 868, loss = 0.19120393\n",
      "Iteration 869, loss = 0.19105664\n",
      "Iteration 870, loss = 0.19090967\n",
      "Iteration 871, loss = 0.19076299\n",
      "Iteration 872, loss = 0.19061660\n",
      "Iteration 873, loss = 0.19047052\n",
      "Iteration 874, loss = 0.19032476\n",
      "Iteration 875, loss = 0.19017926\n",
      "Iteration 876, loss = 0.19003408\n",
      "Iteration 877, loss = 0.18988919\n",
      "Iteration 878, loss = 0.18974460\n",
      "Iteration 879, loss = 0.18960030\n",
      "Iteration 880, loss = 0.18945632\n",
      "Iteration 881, loss = 0.18931262\n",
      "Iteration 882, loss = 0.18916921\n",
      "Iteration 883, loss = 0.18902610\n",
      "Iteration 884, loss = 0.18888329\n",
      "Iteration 885, loss = 0.18874076\n",
      "Iteration 886, loss = 0.18859852\n",
      "Iteration 887, loss = 0.18845659\n",
      "Iteration 888, loss = 0.18831493\n",
      "Iteration 889, loss = 0.18817356\n",
      "Iteration 890, loss = 0.18803248\n",
      "Iteration 891, loss = 0.18789169\n",
      "Iteration 892, loss = 0.18775118\n",
      "Iteration 893, loss = 0.18761096\n",
      "Iteration 894, loss = 0.18747103\n",
      "Iteration 895, loss = 0.18733138\n",
      "Iteration 896, loss = 0.18719200\n",
      "Iteration 897, loss = 0.18705292\n",
      "Iteration 898, loss = 0.18691412\n",
      "Iteration 899, loss = 0.18677559\n",
      "Iteration 900, loss = 0.18663735\n",
      "Iteration 901, loss = 0.18649939\n",
      "Iteration 902, loss = 0.18636170\n",
      "Iteration 903, loss = 0.18622430\n",
      "Iteration 904, loss = 0.18608717\n",
      "Iteration 905, loss = 0.18595031\n",
      "Iteration 906, loss = 0.18581375\n",
      "Iteration 907, loss = 0.18567744\n",
      "Iteration 908, loss = 0.18554142\n",
      "Iteration 909, loss = 0.18540567\n",
      "Iteration 910, loss = 0.18527019\n",
      "Iteration 911, loss = 0.18513500\n",
      "Iteration 912, loss = 0.18500005\n",
      "Iteration 913, loss = 0.18486541\n",
      "Iteration 914, loss = 0.18473101\n",
      "Iteration 915, loss = 0.18459690\n",
      "Iteration 916, loss = 0.18446305\n",
      "Iteration 917, loss = 0.18432947\n",
      "Iteration 918, loss = 0.18419616\n",
      "Iteration 919, loss = 0.18406312\n",
      "Iteration 920, loss = 0.18393034\n",
      "Iteration 921, loss = 0.18379782\n",
      "Iteration 922, loss = 0.18366558\n",
      "Iteration 923, loss = 0.18353360\n",
      "Iteration 924, loss = 0.18340188\n",
      "Iteration 925, loss = 0.18327044\n",
      "Iteration 926, loss = 0.18313926\n",
      "Iteration 927, loss = 0.18300834\n",
      "Iteration 928, loss = 0.18287768\n",
      "Iteration 929, loss = 0.18274729\n",
      "Iteration 930, loss = 0.18261716\n",
      "Iteration 931, loss = 0.18248728\n",
      "Iteration 932, loss = 0.18235768\n",
      "Iteration 933, loss = 0.18222831\n",
      "Iteration 934, loss = 0.18209922\n",
      "Iteration 935, loss = 0.18197038\n",
      "Iteration 936, loss = 0.18184179\n",
      "Iteration 937, loss = 0.18171346\n",
      "Iteration 938, loss = 0.18158539\n",
      "Iteration 939, loss = 0.18145757\n",
      "Iteration 940, loss = 0.18133001\n",
      "Iteration 941, loss = 0.18120270\n",
      "Iteration 942, loss = 0.18107564\n",
      "Iteration 943, loss = 0.18094883\n",
      "Iteration 944, loss = 0.18082228\n",
      "Iteration 945, loss = 0.18069597\n",
      "Iteration 946, loss = 0.18056991\n",
      "Iteration 947, loss = 0.18044412\n",
      "Iteration 948, loss = 0.18031855\n",
      "Iteration 949, loss = 0.18019325\n",
      "Iteration 950, loss = 0.18006820\n",
      "Iteration 951, loss = 0.17994338\n",
      "Iteration 952, loss = 0.17981881\n",
      "Iteration 953, loss = 0.17969449\n",
      "Iteration 954, loss = 0.17957042\n",
      "Iteration 955, loss = 0.17944659\n",
      "Iteration 956, loss = 0.17932301\n",
      "Iteration 957, loss = 0.17919966\n",
      "Iteration 958, loss = 0.17907656\n",
      "Iteration 959, loss = 0.17895371\n",
      "Iteration 960, loss = 0.17883109\n",
      "Iteration 961, loss = 0.17870872\n",
      "Iteration 962, loss = 0.17858658\n",
      "Iteration 963, loss = 0.17846469\n",
      "Iteration 964, loss = 0.17834303\n",
      "Iteration 965, loss = 0.17822162\n",
      "Iteration 966, loss = 0.17810044\n",
      "Iteration 967, loss = 0.17797949\n",
      "Iteration 968, loss = 0.17785880\n",
      "Iteration 969, loss = 0.17773833\n",
      "Iteration 970, loss = 0.17761810\n",
      "Iteration 971, loss = 0.17749810\n",
      "Iteration 972, loss = 0.17737834\n",
      "Iteration 973, loss = 0.17725881\n",
      "Iteration 974, loss = 0.17713952\n",
      "Iteration 975, loss = 0.17702046\n",
      "Iteration 976, loss = 0.17690163\n",
      "Iteration 977, loss = 0.17678304\n",
      "Iteration 978, loss = 0.17666466\n",
      "Iteration 979, loss = 0.17654653\n",
      "Iteration 980, loss = 0.17642863\n",
      "Iteration 981, loss = 0.17631095\n",
      "Iteration 982, loss = 0.17619351\n",
      "Iteration 983, loss = 0.17607629\n",
      "Iteration 984, loss = 0.17595930\n",
      "Iteration 985, loss = 0.17584254\n",
      "Iteration 986, loss = 0.17572600\n",
      "Iteration 987, loss = 0.17560969\n",
      "Iteration 988, loss = 0.17549362\n",
      "Iteration 989, loss = 0.17537775\n",
      "Iteration 990, loss = 0.17526212\n",
      "Iteration 991, loss = 0.17514672\n",
      "Iteration 992, loss = 0.17503153\n",
      "Iteration 993, loss = 0.17491658\n",
      "Iteration 994, loss = 0.17480184\n",
      "Iteration 995, loss = 0.17468732\n",
      "Iteration 996, loss = 0.17457302\n",
      "Iteration 997, loss = 0.17445896\n",
      "Iteration 998, loss = 0.17434509\n",
      "Iteration 999, loss = 0.17423146\n",
      "Iteration 1000, loss = 0.17411805\n",
      "Iteration 1, loss = 1.90632821\n",
      "Iteration 2, loss = 1.86307472\n",
      "Iteration 3, loss = 1.80477302\n",
      "Iteration 4, loss = 1.73594792\n",
      "Iteration 5, loss = 1.66064792\n",
      "Iteration 6, loss = 1.58219730\n",
      "Iteration 7, loss = 1.50315280\n",
      "Iteration 8, loss = 1.42572581\n",
      "Iteration 9, loss = 1.35195416\n",
      "Iteration 10, loss = 1.28373555\n",
      "Iteration 11, loss = 1.22308546\n",
      "Iteration 12, loss = 1.17184819\n",
      "Iteration 13, loss = 1.13132138\n",
      "Iteration 14, loss = 1.10178134\n",
      "Iteration 15, loss = 1.08216894\n",
      "Iteration 16, loss = 1.07059221\n",
      "Iteration 17, loss = 1.06418638\n",
      "Iteration 18, loss = 1.06014108\n",
      "Iteration 19, loss = 1.05606350\n",
      "Iteration 20, loss = 1.05040104\n",
      "Iteration 21, loss = 1.04226978\n",
      "Iteration 22, loss = 1.03161972\n",
      "Iteration 23, loss = 1.01894451\n",
      "Iteration 24, loss = 1.00506922\n",
      "Iteration 25, loss = 0.99069161\n",
      "Iteration 26, loss = 0.97650337\n",
      "Iteration 27, loss = 0.96292748\n",
      "Iteration 28, loss = 0.95017062\n",
      "Iteration 29, loss = 0.93817777\n",
      "Iteration 30, loss = 0.92673713\n",
      "Iteration 31, loss = 0.91564634\n",
      "Iteration 32, loss = 0.90466522\n",
      "Iteration 33, loss = 0.89343732\n",
      "Iteration 34, loss = 0.88203946\n",
      "Iteration 35, loss = 0.87049809\n",
      "Iteration 36, loss = 0.85877471\n",
      "Iteration 37, loss = 0.84717928\n",
      "Iteration 38, loss = 0.83590328\n",
      "Iteration 39, loss = 0.82521303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 40, loss = 0.81523123\n",
      "Iteration 41, loss = 0.80597913\n",
      "Iteration 42, loss = 0.79748853\n",
      "Iteration 43, loss = 0.78970429\n",
      "Iteration 44, loss = 0.78259287\n",
      "Iteration 45, loss = 0.77586166\n",
      "Iteration 46, loss = 0.76935714\n",
      "Iteration 47, loss = 0.76294884\n",
      "Iteration 48, loss = 0.75658439\n",
      "Iteration 49, loss = 0.75031111\n",
      "Iteration 50, loss = 0.74413790\n",
      "Iteration 51, loss = 0.73813293\n",
      "Iteration 52, loss = 0.73225967\n",
      "Iteration 53, loss = 0.72657126\n",
      "Iteration 54, loss = 0.72113499\n",
      "Iteration 55, loss = 0.71591916\n",
      "Iteration 56, loss = 0.71090241\n",
      "Iteration 57, loss = 0.70606740\n",
      "Iteration 58, loss = 0.70133515\n",
      "Iteration 59, loss = 0.69670474\n",
      "Iteration 60, loss = 0.69215917\n",
      "Iteration 61, loss = 0.68767828\n",
      "Iteration 62, loss = 0.68328853\n",
      "Iteration 63, loss = 0.67899035\n",
      "Iteration 64, loss = 0.67480037\n",
      "Iteration 65, loss = 0.67073151\n",
      "Iteration 66, loss = 0.66676918\n",
      "Iteration 67, loss = 0.66289264\n",
      "Iteration 68, loss = 0.65913628\n",
      "Iteration 69, loss = 0.65546013\n",
      "Iteration 70, loss = 0.65184591\n",
      "Iteration 71, loss = 0.64830357\n",
      "Iteration 72, loss = 0.64482276\n",
      "Iteration 73, loss = 0.64138948\n",
      "Iteration 74, loss = 0.63800031\n",
      "Iteration 75, loss = 0.63465265\n",
      "Iteration 76, loss = 0.63134920\n",
      "Iteration 77, loss = 0.62809007\n",
      "Iteration 78, loss = 0.62487539\n",
      "Iteration 79, loss = 0.62170574\n",
      "Iteration 80, loss = 0.61858603\n",
      "Iteration 81, loss = 0.61551494\n",
      "Iteration 82, loss = 0.61248987\n",
      "Iteration 83, loss = 0.60950888\n",
      "Iteration 84, loss = 0.60657587\n",
      "Iteration 85, loss = 0.60368833\n",
      "Iteration 86, loss = 0.60084281\n",
      "Iteration 87, loss = 0.59803621\n",
      "Iteration 88, loss = 0.59526739\n",
      "Iteration 89, loss = 0.59253532\n",
      "Iteration 90, loss = 0.58983900\n",
      "Iteration 91, loss = 0.58717750\n",
      "Iteration 92, loss = 0.58454997\n",
      "Iteration 93, loss = 0.58195683\n",
      "Iteration 94, loss = 0.57939727\n",
      "Iteration 95, loss = 0.57687009\n",
      "Iteration 96, loss = 0.57437629\n",
      "Iteration 97, loss = 0.57191567\n",
      "Iteration 98, loss = 0.56948601\n",
      "Iteration 99, loss = 0.56708680\n",
      "Iteration 100, loss = 0.56471747\n",
      "Iteration 101, loss = 0.56237731\n",
      "Iteration 102, loss = 0.56006511\n",
      "Iteration 103, loss = 0.55778117\n",
      "Iteration 104, loss = 0.55552621\n",
      "Iteration 105, loss = 0.55329878\n",
      "Iteration 106, loss = 0.55109822\n",
      "Iteration 107, loss = 0.54892409\n",
      "Iteration 108, loss = 0.54677593\n",
      "Iteration 109, loss = 0.54465330\n",
      "Iteration 110, loss = 0.54255624\n",
      "Iteration 111, loss = 0.54048454\n",
      "Iteration 112, loss = 0.53843707\n",
      "Iteration 113, loss = 0.53641337\n",
      "Iteration 114, loss = 0.53441303\n",
      "Iteration 115, loss = 0.53243565\n",
      "Iteration 116, loss = 0.53048080\n",
      "Iteration 117, loss = 0.52854807\n",
      "Iteration 118, loss = 0.52663707\n",
      "Iteration 119, loss = 0.52474739\n",
      "Iteration 120, loss = 0.52287883\n",
      "Iteration 121, loss = 0.52103094\n",
      "Iteration 122, loss = 0.51920321\n",
      "Iteration 123, loss = 0.51739530\n",
      "Iteration 124, loss = 0.51560712\n",
      "Iteration 125, loss = 0.51383799\n",
      "Iteration 126, loss = 0.51208776\n",
      "Iteration 127, loss = 0.51035602\n",
      "Iteration 128, loss = 0.50864257\n",
      "Iteration 129, loss = 0.50694688\n",
      "Iteration 130, loss = 0.50526917\n",
      "Iteration 131, loss = 0.50360960\n",
      "Iteration 132, loss = 0.50196699\n",
      "Iteration 133, loss = 0.50034108\n",
      "Iteration 134, loss = 0.49873159\n",
      "Iteration 135, loss = 0.49713828\n",
      "Iteration 136, loss = 0.49556087\n",
      "Iteration 137, loss = 0.49399910\n",
      "Iteration 138, loss = 0.49245271\n",
      "Iteration 139, loss = 0.49092144\n",
      "Iteration 140, loss = 0.48940503\n",
      "Iteration 141, loss = 0.48790324\n",
      "Iteration 142, loss = 0.48641581\n",
      "Iteration 143, loss = 0.48494249\n",
      "Iteration 144, loss = 0.48348304\n",
      "Iteration 145, loss = 0.48203721\n",
      "Iteration 146, loss = 0.48060518\n",
      "Iteration 147, loss = 0.47918662\n",
      "Iteration 148, loss = 0.47778098\n",
      "Iteration 149, loss = 0.47638816\n",
      "Iteration 150, loss = 0.47500794\n",
      "Iteration 151, loss = 0.47363996\n",
      "Iteration 152, loss = 0.47228430\n",
      "Iteration 153, loss = 0.47094041\n",
      "Iteration 154, loss = 0.46960835\n",
      "Iteration 155, loss = 0.46828804\n",
      "Iteration 156, loss = 0.46697947\n",
      "Iteration 157, loss = 0.46568206\n",
      "Iteration 158, loss = 0.46439565\n",
      "Iteration 159, loss = 0.46312012\n",
      "Iteration 160, loss = 0.46185518\n",
      "Iteration 161, loss = 0.46060069\n",
      "Iteration 162, loss = 0.45935650\n",
      "Iteration 163, loss = 0.45812260\n",
      "Iteration 164, loss = 0.45689864\n",
      "Iteration 165, loss = 0.45568449\n",
      "Iteration 166, loss = 0.45447999\n",
      "Iteration 167, loss = 0.45328497\n",
      "Iteration 168, loss = 0.45209930\n",
      "Iteration 169, loss = 0.45092283\n",
      "Iteration 170, loss = 0.44975540\n",
      "Iteration 171, loss = 0.44859688\n",
      "Iteration 172, loss = 0.44744714\n",
      "Iteration 173, loss = 0.44630610\n",
      "Iteration 174, loss = 0.44517365\n",
      "Iteration 175, loss = 0.44404955\n",
      "Iteration 176, loss = 0.44293367\n",
      "Iteration 177, loss = 0.44182588\n",
      "Iteration 178, loss = 0.44072605\n",
      "Iteration 179, loss = 0.43963406\n",
      "Iteration 180, loss = 0.43854980\n",
      "Iteration 181, loss = 0.43747313\n",
      "Iteration 182, loss = 0.43640394\n",
      "Iteration 183, loss = 0.43534212\n",
      "Iteration 184, loss = 0.43428755\n",
      "Iteration 185, loss = 0.43324012\n",
      "Iteration 186, loss = 0.43219973\n",
      "Iteration 187, loss = 0.43116627\n",
      "Iteration 188, loss = 0.43013978\n",
      "Iteration 189, loss = 0.42912088\n",
      "Iteration 190, loss = 0.42810866\n",
      "Iteration 191, loss = 0.42710335\n",
      "Iteration 192, loss = 0.42610436\n",
      "Iteration 193, loss = 0.42511163\n",
      "Iteration 194, loss = 0.42412508\n",
      "Iteration 195, loss = 0.42314465\n",
      "Iteration 196, loss = 0.42217027\n",
      "Iteration 197, loss = 0.42120286\n",
      "Iteration 198, loss = 0.42024182\n",
      "Iteration 199, loss = 0.41928673\n",
      "Iteration 200, loss = 0.41833748\n",
      "Iteration 201, loss = 0.41739401\n",
      "Iteration 202, loss = 0.41645623\n",
      "Iteration 203, loss = 0.41552408\n",
      "Iteration 204, loss = 0.41459749\n",
      "Iteration 205, loss = 0.41367638\n",
      "Iteration 206, loss = 0.41276069\n",
      "Iteration 207, loss = 0.41185038\n",
      "Iteration 208, loss = 0.41094535\n",
      "Iteration 209, loss = 0.41004554\n",
      "Iteration 210, loss = 0.40915087\n",
      "Iteration 211, loss = 0.40826128\n",
      "Iteration 212, loss = 0.40737670\n",
      "Iteration 213, loss = 0.40649724\n",
      "Iteration 214, loss = 0.40562263\n",
      "Iteration 215, loss = 0.40475281\n",
      "Iteration 216, loss = 0.40388783\n",
      "Iteration 217, loss = 0.40302787\n",
      "Iteration 218, loss = 0.40217264\n",
      "Iteration 219, loss = 0.40132206\n",
      "Iteration 220, loss = 0.40047641\n",
      "Iteration 221, loss = 0.39963508\n",
      "Iteration 222, loss = 0.39879811\n",
      "Iteration 223, loss = 0.39796581\n",
      "Iteration 224, loss = 0.39713795\n",
      "Iteration 225, loss = 0.39631476\n",
      "Iteration 226, loss = 0.39549593\n",
      "Iteration 227, loss = 0.39468148\n",
      "Iteration 228, loss = 0.39387122\n",
      "Iteration 229, loss = 0.39306501\n",
      "Iteration 230, loss = 0.39226300\n",
      "Iteration 231, loss = 0.39146511\n",
      "Iteration 232, loss = 0.39067102\n",
      "Iteration 233, loss = 0.38988101\n",
      "Iteration 234, loss = 0.38909519\n",
      "Iteration 235, loss = 0.38831315\n",
      "Iteration 236, loss = 0.38753497\n",
      "Iteration 237, loss = 0.38676047\n",
      "Iteration 238, loss = 0.38598978\n",
      "Iteration 239, loss = 0.38522271\n",
      "Iteration 240, loss = 0.38445924\n",
      "Iteration 241, loss = 0.38369938\n",
      "Iteration 242, loss = 0.38294314\n",
      "Iteration 243, loss = 0.38219027\n",
      "Iteration 244, loss = 0.38144089\n",
      "Iteration 245, loss = 0.38069507\n",
      "Iteration 246, loss = 0.37995245\n",
      "Iteration 247, loss = 0.37921324\n",
      "Iteration 248, loss = 0.37847735\n",
      "Iteration 249, loss = 0.37774478\n",
      "Iteration 250, loss = 0.37701543\n",
      "Iteration 251, loss = 0.37628926\n",
      "Iteration 252, loss = 0.37556626\n",
      "Iteration 253, loss = 0.37484641\n",
      "Iteration 254, loss = 0.37412965\n",
      "Iteration 255, loss = 0.37341617\n",
      "Iteration 256, loss = 0.37270593\n",
      "Iteration 257, loss = 0.37199870\n",
      "Iteration 258, loss = 0.37129447\n",
      "Iteration 259, loss = 0.37059320\n",
      "Iteration 260, loss = 0.36989488\n",
      "Iteration 261, loss = 0.36919946\n",
      "Iteration 262, loss = 0.36850690\n",
      "Iteration 263, loss = 0.36781720\n",
      "Iteration 264, loss = 0.36713032\n",
      "Iteration 265, loss = 0.36644622\n",
      "Iteration 266, loss = 0.36576488\n",
      "Iteration 267, loss = 0.36508627\n",
      "Iteration 268, loss = 0.36441042\n",
      "Iteration 269, loss = 0.36373729\n",
      "Iteration 270, loss = 0.36306682\n",
      "Iteration 271, loss = 0.36239898\n",
      "Iteration 272, loss = 0.36173374\n",
      "Iteration 273, loss = 0.36107109\n",
      "Iteration 274, loss = 0.36041098\n",
      "Iteration 275, loss = 0.35975343\n",
      "Iteration 276, loss = 0.35909842\n",
      "Iteration 277, loss = 0.35844590\n",
      "Iteration 278, loss = 0.35779584\n",
      "Iteration 279, loss = 0.35714825\n",
      "Iteration 280, loss = 0.35650305\n",
      "Iteration 281, loss = 0.35586024\n",
      "Iteration 282, loss = 0.35521983\n",
      "Iteration 283, loss = 0.35458175\n",
      "Iteration 284, loss = 0.35394601\n",
      "Iteration 285, loss = 0.35331257\n",
      "Iteration 286, loss = 0.35268143\n",
      "Iteration 287, loss = 0.35205257\n",
      "Iteration 288, loss = 0.35142594\n",
      "Iteration 289, loss = 0.35080165\n",
      "Iteration 290, loss = 0.35018007\n",
      "Iteration 291, loss = 0.34956092\n",
      "Iteration 292, loss = 0.34894403\n",
      "Iteration 293, loss = 0.34832946\n",
      "Iteration 294, loss = 0.34771725\n",
      "Iteration 295, loss = 0.34710723\n",
      "Iteration 296, loss = 0.34649932\n",
      "Iteration 297, loss = 0.34589357\n",
      "Iteration 298, loss = 0.34528993\n",
      "Iteration 299, loss = 0.34468837\n",
      "Iteration 300, loss = 0.34408891\n",
      "Iteration 301, loss = 0.34349162\n",
      "Iteration 302, loss = 0.34289652\n",
      "Iteration 303, loss = 0.34230347\n",
      "Iteration 304, loss = 0.34171243\n",
      "Iteration 305, loss = 0.34112337\n",
      "Iteration 306, loss = 0.34053654\n",
      "Iteration 307, loss = 0.33995176\n",
      "Iteration 308, loss = 0.33936896\n",
      "Iteration 309, loss = 0.33878807\n",
      "Iteration 310, loss = 0.33820911\n",
      "Iteration 311, loss = 0.33763205\n",
      "Iteration 312, loss = 0.33705691\n",
      "Iteration 313, loss = 0.33648361\n",
      "Iteration 314, loss = 0.33591218\n",
      "Iteration 315, loss = 0.33534258\n",
      "Iteration 316, loss = 0.33477481\n",
      "Iteration 317, loss = 0.33420962\n",
      "Iteration 318, loss = 0.33364628\n",
      "Iteration 319, loss = 0.33308468\n",
      "Iteration 320, loss = 0.33252484\n",
      "Iteration 321, loss = 0.33196681\n",
      "Iteration 322, loss = 0.33141072\n",
      "Iteration 323, loss = 0.33085668\n",
      "Iteration 324, loss = 0.33030436\n",
      "Iteration 325, loss = 0.32975380\n",
      "Iteration 326, loss = 0.32920494\n",
      "Iteration 327, loss = 0.32865780\n",
      "Iteration 328, loss = 0.32811237\n",
      "Iteration 329, loss = 0.32756937\n",
      "Iteration 330, loss = 0.32702804\n",
      "Iteration 331, loss = 0.32648852\n",
      "Iteration 332, loss = 0.32595058\n",
      "Iteration 333, loss = 0.32541425\n",
      "Iteration 334, loss = 0.32487960\n",
      "Iteration 335, loss = 0.32434679\n",
      "Iteration 336, loss = 0.32381559\n",
      "Iteration 337, loss = 0.32328598\n",
      "Iteration 338, loss = 0.32275806\n",
      "Iteration 339, loss = 0.32223160\n",
      "Iteration 340, loss = 0.32170680\n",
      "Iteration 341, loss = 0.32118353\n",
      "Iteration 342, loss = 0.32066183\n",
      "Iteration 343, loss = 0.32014170\n",
      "Iteration 344, loss = 0.31962346\n",
      "Iteration 345, loss = 0.31910674\n",
      "Iteration 346, loss = 0.31859164\n",
      "Iteration 347, loss = 0.31807809\n",
      "Iteration 348, loss = 0.31756599\n",
      "Iteration 349, loss = 0.31705542\n",
      "Iteration 350, loss = 0.31654636\n",
      "Iteration 351, loss = 0.31603883\n",
      "Iteration 352, loss = 0.31553285\n",
      "Iteration 353, loss = 0.31502842\n",
      "Iteration 354, loss = 0.31452545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 355, loss = 0.31402394\n",
      "Iteration 356, loss = 0.31352393\n",
      "Iteration 357, loss = 0.31302537\n",
      "Iteration 358, loss = 0.31252822\n",
      "Iteration 359, loss = 0.31203258\n",
      "Iteration 360, loss = 0.31153829\n",
      "Iteration 361, loss = 0.31104548\n",
      "Iteration 362, loss = 0.31055402\n",
      "Iteration 363, loss = 0.31006394\n",
      "Iteration 364, loss = 0.30957523\n",
      "Iteration 365, loss = 0.30908791\n",
      "Iteration 366, loss = 0.30860193\n",
      "Iteration 367, loss = 0.30811733\n",
      "Iteration 368, loss = 0.30763408\n",
      "Iteration 369, loss = 0.30715214\n",
      "Iteration 370, loss = 0.30667154\n",
      "Iteration 371, loss = 0.30619231\n",
      "Iteration 372, loss = 0.30571453\n",
      "Iteration 373, loss = 0.30523808\n",
      "Iteration 374, loss = 0.30476299\n",
      "Iteration 375, loss = 0.30428917\n",
      "Iteration 376, loss = 0.30381665\n",
      "Iteration 377, loss = 0.30334541\n",
      "Iteration 378, loss = 0.30287549\n",
      "Iteration 379, loss = 0.30240681\n",
      "Iteration 380, loss = 0.30193946\n",
      "Iteration 381, loss = 0.30147340\n",
      "Iteration 382, loss = 0.30100856\n",
      "Iteration 383, loss = 0.30054498\n",
      "Iteration 384, loss = 0.30008265\n",
      "Iteration 385, loss = 0.29962161\n",
      "Iteration 386, loss = 0.29916177\n",
      "Iteration 387, loss = 0.29870314\n",
      "Iteration 388, loss = 0.29824576\n",
      "Iteration 389, loss = 0.29778963\n",
      "Iteration 390, loss = 0.29733472\n",
      "Iteration 391, loss = 0.29688102\n",
      "Iteration 392, loss = 0.29642872\n",
      "Iteration 393, loss = 0.29597760\n",
      "Iteration 394, loss = 0.29552769\n",
      "Iteration 395, loss = 0.29507898\n",
      "Iteration 396, loss = 0.29463148\n",
      "Iteration 397, loss = 0.29418514\n",
      "Iteration 398, loss = 0.29373998\n",
      "Iteration 399, loss = 0.29329603\n",
      "Iteration 400, loss = 0.29285339\n",
      "Iteration 401, loss = 0.29241186\n",
      "Iteration 402, loss = 0.29197151\n",
      "Iteration 403, loss = 0.29153236\n",
      "Iteration 404, loss = 0.29109432\n",
      "Iteration 405, loss = 0.29065744\n",
      "Iteration 406, loss = 0.29022176\n",
      "Iteration 407, loss = 0.28978729\n",
      "Iteration 408, loss = 0.28935397\n",
      "Iteration 409, loss = 0.28892177\n",
      "Iteration 410, loss = 0.28849082\n",
      "Iteration 411, loss = 0.28806102\n",
      "Iteration 412, loss = 0.28763237\n",
      "Iteration 413, loss = 0.28720481\n",
      "Iteration 414, loss = 0.28677837\n",
      "Iteration 415, loss = 0.28635303\n",
      "Iteration 416, loss = 0.28592882\n",
      "Iteration 417, loss = 0.28550566\n",
      "Iteration 418, loss = 0.28508361\n",
      "Iteration 419, loss = 0.28466264\n",
      "Iteration 420, loss = 0.28424277\n",
      "Iteration 421, loss = 0.28382395\n",
      "Iteration 422, loss = 0.28340621\n",
      "Iteration 423, loss = 0.28298960\n",
      "Iteration 424, loss = 0.28257409\n",
      "Iteration 425, loss = 0.28215968\n",
      "Iteration 426, loss = 0.28174628\n",
      "Iteration 427, loss = 0.28133396\n",
      "Iteration 428, loss = 0.28092268\n",
      "Iteration 429, loss = 0.28051245\n",
      "Iteration 430, loss = 0.28010326\n",
      "Iteration 431, loss = 0.27969510\n",
      "Iteration 432, loss = 0.27928796\n",
      "Iteration 433, loss = 0.27888185\n",
      "Iteration 434, loss = 0.27847681\n",
      "Iteration 435, loss = 0.27807277\n",
      "Iteration 436, loss = 0.27766976\n",
      "Iteration 437, loss = 0.27726777\n",
      "Iteration 438, loss = 0.27686680\n",
      "Iteration 439, loss = 0.27646696\n",
      "Iteration 440, loss = 0.27606800\n",
      "Iteration 441, loss = 0.27566997\n",
      "Iteration 442, loss = 0.27527303\n",
      "Iteration 443, loss = 0.27487708\n",
      "Iteration 444, loss = 0.27448212\n",
      "Iteration 445, loss = 0.27408813\n",
      "Iteration 446, loss = 0.27369512\n",
      "Iteration 447, loss = 0.27330313\n",
      "Iteration 448, loss = 0.27291218\n",
      "Iteration 449, loss = 0.27252231\n",
      "Iteration 450, loss = 0.27213336\n",
      "Iteration 451, loss = 0.27174541\n",
      "Iteration 452, loss = 0.27135851\n",
      "Iteration 453, loss = 0.27097247\n",
      "Iteration 454, loss = 0.27058742\n",
      "Iteration 455, loss = 0.27020333\n",
      "Iteration 456, loss = 0.26982024\n",
      "Iteration 457, loss = 0.26943805\n",
      "Iteration 458, loss = 0.26905683\n",
      "Iteration 459, loss = 0.26867657\n",
      "Iteration 460, loss = 0.26829725\n",
      "Iteration 461, loss = 0.26791890\n",
      "Iteration 462, loss = 0.26754144\n",
      "Iteration 463, loss = 0.26716492\n",
      "Iteration 464, loss = 0.26678936\n",
      "Iteration 465, loss = 0.26641469\n",
      "Iteration 466, loss = 0.26604095\n",
      "Iteration 467, loss = 0.26566815\n",
      "Iteration 468, loss = 0.26529625\n",
      "Iteration 469, loss = 0.26492525\n",
      "Iteration 470, loss = 0.26455515\n",
      "Iteration 471, loss = 0.26418604\n",
      "Iteration 472, loss = 0.26381774\n",
      "Iteration 473, loss = 0.26345037\n",
      "Iteration 474, loss = 0.26308395\n",
      "Iteration 475, loss = 0.26271837\n",
      "Iteration 476, loss = 0.26235370\n",
      "Iteration 477, loss = 0.26198996\n",
      "Iteration 478, loss = 0.26162710\n",
      "Iteration 479, loss = 0.26126511\n",
      "Iteration 480, loss = 0.26090402\n",
      "Iteration 481, loss = 0.26054384\n",
      "Iteration 482, loss = 0.26018445\n",
      "Iteration 483, loss = 0.25982602\n",
      "Iteration 484, loss = 0.25946847\n",
      "Iteration 485, loss = 0.25911175\n",
      "Iteration 486, loss = 0.25875591\n",
      "Iteration 487, loss = 0.25840096\n",
      "Iteration 488, loss = 0.25804685\n",
      "Iteration 489, loss = 0.25769359\n",
      "Iteration 490, loss = 0.25734123\n",
      "Iteration 491, loss = 0.25698966\n",
      "Iteration 492, loss = 0.25663899\n",
      "Iteration 493, loss = 0.25628914\n",
      "Iteration 494, loss = 0.25594017\n",
      "Iteration 495, loss = 0.25559200\n",
      "Iteration 496, loss = 0.25524470\n",
      "Iteration 497, loss = 0.25489824\n",
      "Iteration 498, loss = 0.25455262\n",
      "Iteration 499, loss = 0.25420781\n",
      "Iteration 500, loss = 0.25386383\n",
      "Iteration 501, loss = 0.25352072\n",
      "Iteration 502, loss = 0.25317842\n",
      "Iteration 503, loss = 0.25283691\n",
      "Iteration 504, loss = 0.25249628\n",
      "Iteration 505, loss = 0.25215640\n",
      "Iteration 506, loss = 0.25181741\n",
      "Iteration 507, loss = 0.25147920\n",
      "Iteration 508, loss = 0.25114179\n",
      "Iteration 509, loss = 0.25080521\n",
      "Iteration 510, loss = 0.25046941\n",
      "Iteration 511, loss = 0.25013447\n",
      "Iteration 512, loss = 0.24980029\n",
      "Iteration 513, loss = 0.24946691\n",
      "Iteration 514, loss = 0.24913437\n",
      "Iteration 515, loss = 0.24880258\n",
      "Iteration 516, loss = 0.24847163\n",
      "Iteration 517, loss = 0.24814144\n",
      "Iteration 518, loss = 0.24781203\n",
      "Iteration 519, loss = 0.24748346\n",
      "Iteration 520, loss = 0.24715564\n",
      "Iteration 521, loss = 0.24682866\n",
      "Iteration 522, loss = 0.24650242\n",
      "Iteration 523, loss = 0.24617696\n",
      "Iteration 524, loss = 0.24585231\n",
      "Iteration 525, loss = 0.24552842\n",
      "Iteration 526, loss = 0.24520532\n",
      "Iteration 527, loss = 0.24488299\n",
      "Iteration 528, loss = 0.24456144\n",
      "Iteration 529, loss = 0.24424065\n",
      "Iteration 530, loss = 0.24392063\n",
      "Iteration 531, loss = 0.24360138\n",
      "Iteration 532, loss = 0.24328286\n",
      "Iteration 533, loss = 0.24296516\n",
      "Iteration 534, loss = 0.24264816\n",
      "Iteration 535, loss = 0.24233193\n",
      "Iteration 536, loss = 0.24201647\n",
      "Iteration 537, loss = 0.24170175\n",
      "Iteration 538, loss = 0.24138780\n",
      "Iteration 539, loss = 0.24107455\n",
      "Iteration 540, loss = 0.24076210\n",
      "Iteration 541, loss = 0.24045035\n",
      "Iteration 542, loss = 0.24013937\n",
      "Iteration 543, loss = 0.23982911\n",
      "Iteration 544, loss = 0.23951959\n",
      "Iteration 545, loss = 0.23921082\n",
      "Iteration 546, loss = 0.23890277\n",
      "Iteration 547, loss = 0.23859545\n",
      "Iteration 548, loss = 0.23828886\n",
      "Iteration 549, loss = 0.23798301\n",
      "Iteration 550, loss = 0.23767786\n",
      "Iteration 551, loss = 0.23737346\n",
      "Iteration 552, loss = 0.23706977\n",
      "Iteration 553, loss = 0.23676679\n",
      "Iteration 554, loss = 0.23646456\n",
      "Iteration 555, loss = 0.23616300\n",
      "Iteration 556, loss = 0.23586220\n",
      "Iteration 557, loss = 0.23556206\n",
      "Iteration 558, loss = 0.23526277\n",
      "Iteration 559, loss = 0.23496402\n",
      "Iteration 560, loss = 0.23466605\n",
      "Iteration 561, loss = 0.23436887\n",
      "Iteration 562, loss = 0.23407238\n",
      "Iteration 563, loss = 0.23377659\n",
      "Iteration 564, loss = 0.23348152\n",
      "Iteration 565, loss = 0.23318712\n",
      "Iteration 566, loss = 0.23289346\n",
      "Iteration 567, loss = 0.23260045\n",
      "Iteration 568, loss = 0.23230819\n",
      "Iteration 569, loss = 0.23201656\n",
      "Iteration 570, loss = 0.23172568\n",
      "Iteration 571, loss = 0.23143545\n",
      "Iteration 572, loss = 0.23114590\n",
      "Iteration 573, loss = 0.23085705\n",
      "Iteration 574, loss = 0.23056887\n",
      "Iteration 575, loss = 0.23028141\n",
      "Iteration 576, loss = 0.22999460\n",
      "Iteration 577, loss = 0.22970850\n",
      "Iteration 578, loss = 0.22942305\n",
      "Iteration 579, loss = 0.22913834\n",
      "Iteration 580, loss = 0.22885432\n",
      "Iteration 581, loss = 0.22857097\n",
      "Iteration 582, loss = 0.22828833\n",
      "Iteration 583, loss = 0.22800633\n",
      "Iteration 584, loss = 0.22772502\n",
      "Iteration 585, loss = 0.22744434\n",
      "Iteration 586, loss = 0.22716438\n",
      "Iteration 587, loss = 0.22688503\n",
      "Iteration 588, loss = 0.22660637\n",
      "Iteration 589, loss = 0.22632834\n",
      "Iteration 590, loss = 0.22605102\n",
      "Iteration 591, loss = 0.22577435\n",
      "Iteration 592, loss = 0.22549839\n",
      "Iteration 593, loss = 0.22522311\n",
      "Iteration 594, loss = 0.22494848\n",
      "Iteration 595, loss = 0.22467450\n",
      "Iteration 596, loss = 0.22440116\n",
      "Iteration 597, loss = 0.22412848\n",
      "Iteration 598, loss = 0.22385642\n",
      "Iteration 599, loss = 0.22358503\n",
      "Iteration 600, loss = 0.22331424\n",
      "Iteration 601, loss = 0.22304412\n",
      "Iteration 602, loss = 0.22277462\n",
      "Iteration 603, loss = 0.22250576\n",
      "Iteration 604, loss = 0.22223751\n",
      "Iteration 605, loss = 0.22196991\n",
      "Iteration 606, loss = 0.22170292\n",
      "Iteration 607, loss = 0.22143657\n",
      "Iteration 608, loss = 0.22117083\n",
      "Iteration 609, loss = 0.22090573\n",
      "Iteration 610, loss = 0.22064124\n",
      "Iteration 611, loss = 0.22037735\n",
      "Iteration 612, loss = 0.22011411\n",
      "Iteration 613, loss = 0.21985150\n",
      "Iteration 614, loss = 0.21958959\n",
      "Iteration 615, loss = 0.21932826\n",
      "Iteration 616, loss = 0.21906757\n",
      "Iteration 617, loss = 0.21880749\n",
      "Iteration 618, loss = 0.21854800\n",
      "Iteration 619, loss = 0.21828912\n",
      "Iteration 620, loss = 0.21803086\n",
      "Iteration 621, loss = 0.21777320\n",
      "Iteration 622, loss = 0.21751613\n",
      "Iteration 623, loss = 0.21725967\n",
      "Iteration 624, loss = 0.21700379\n",
      "Iteration 625, loss = 0.21674854\n",
      "Iteration 626, loss = 0.21649384\n",
      "Iteration 627, loss = 0.21623976\n",
      "Iteration 628, loss = 0.21598626\n",
      "Iteration 629, loss = 0.21573336\n",
      "Iteration 630, loss = 0.21548103\n",
      "Iteration 631, loss = 0.21522930\n",
      "Iteration 632, loss = 0.21497814\n",
      "Iteration 633, loss = 0.21472758\n",
      "Iteration 634, loss = 0.21447760\n",
      "Iteration 635, loss = 0.21422819\n",
      "Iteration 636, loss = 0.21397937\n",
      "Iteration 637, loss = 0.21373110\n",
      "Iteration 638, loss = 0.21348344\n",
      "Iteration 639, loss = 0.21323633\n",
      "Iteration 640, loss = 0.21298980\n",
      "Iteration 641, loss = 0.21274382\n",
      "Iteration 642, loss = 0.21249845\n",
      "Iteration 643, loss = 0.21225361\n",
      "Iteration 644, loss = 0.21200934\n",
      "Iteration 645, loss = 0.21176565\n",
      "Iteration 646, loss = 0.21152251\n",
      "Iteration 647, loss = 0.21127994\n",
      "Iteration 648, loss = 0.21103791\n",
      "Iteration 649, loss = 0.21079647\n",
      "Iteration 650, loss = 0.21055555\n",
      "Iteration 651, loss = 0.21031523\n",
      "Iteration 652, loss = 0.21007543\n",
      "Iteration 653, loss = 0.20983618\n",
      "Iteration 654, loss = 0.20959748\n",
      "Iteration 655, loss = 0.20935936\n",
      "Iteration 656, loss = 0.20912175\n",
      "Iteration 657, loss = 0.20888473\n",
      "Iteration 658, loss = 0.20864822\n",
      "Iteration 659, loss = 0.20841225\n",
      "Iteration 660, loss = 0.20817684\n",
      "Iteration 661, loss = 0.20794196\n",
      "Iteration 662, loss = 0.20770763\n",
      "Iteration 663, loss = 0.20747383\n",
      "Iteration 664, loss = 0.20724058\n",
      "Iteration 665, loss = 0.20700785\n",
      "Iteration 666, loss = 0.20677567\n",
      "Iteration 667, loss = 0.20654400\n",
      "Iteration 668, loss = 0.20631288\n",
      "Iteration 669, loss = 0.20608227\n",
      "Iteration 670, loss = 0.20585221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 671, loss = 0.20562265\n",
      "Iteration 672, loss = 0.20539366\n",
      "Iteration 673, loss = 0.20516514\n",
      "Iteration 674, loss = 0.20493719\n",
      "Iteration 675, loss = 0.20470973\n",
      "Iteration 676, loss = 0.20448279\n",
      "Iteration 677, loss = 0.20425640\n",
      "Iteration 678, loss = 0.20403049\n",
      "Iteration 679, loss = 0.20380511\n",
      "Iteration 680, loss = 0.20358027\n",
      "Iteration 681, loss = 0.20335595\n",
      "Iteration 682, loss = 0.20313212\n",
      "Iteration 683, loss = 0.20290881\n",
      "Iteration 684, loss = 0.20268600\n",
      "Iteration 685, loss = 0.20246372\n",
      "Iteration 686, loss = 0.20224192\n",
      "Iteration 687, loss = 0.20202064\n",
      "Iteration 688, loss = 0.20179986\n",
      "Iteration 689, loss = 0.20157959\n",
      "Iteration 690, loss = 0.20135981\n",
      "Iteration 691, loss = 0.20114053\n",
      "Iteration 692, loss = 0.20092176\n",
      "Iteration 693, loss = 0.20070348\n",
      "Iteration 694, loss = 0.20048570\n",
      "Iteration 695, loss = 0.20026840\n",
      "Iteration 696, loss = 0.20005163\n",
      "Iteration 697, loss = 0.19983531\n",
      "Iteration 698, loss = 0.19961950\n",
      "Iteration 699, loss = 0.19940417\n",
      "Iteration 700, loss = 0.19918935\n",
      "Iteration 701, loss = 0.19897499\n",
      "Iteration 702, loss = 0.19876113\n",
      "Iteration 703, loss = 0.19854774\n",
      "Iteration 704, loss = 0.19833487\n",
      "Iteration 705, loss = 0.19812243\n",
      "Iteration 706, loss = 0.19791051\n",
      "Iteration 707, loss = 0.19769905\n",
      "Iteration 708, loss = 0.19748807\n",
      "Iteration 709, loss = 0.19727759\n",
      "Iteration 710, loss = 0.19706755\n",
      "Iteration 711, loss = 0.19685799\n",
      "Iteration 712, loss = 0.19664892\n",
      "Iteration 713, loss = 0.19644031\n",
      "Iteration 714, loss = 0.19623217\n",
      "Iteration 715, loss = 0.19602451\n",
      "Iteration 716, loss = 0.19581731\n",
      "Iteration 717, loss = 0.19561059\n",
      "Iteration 718, loss = 0.19540431\n",
      "Iteration 719, loss = 0.19519852\n",
      "Iteration 720, loss = 0.19499317\n",
      "Iteration 721, loss = 0.19478831\n",
      "Iteration 722, loss = 0.19458388\n",
      "Iteration 723, loss = 0.19437993\n",
      "Iteration 724, loss = 0.19417642\n",
      "Iteration 725, loss = 0.19397340\n",
      "Iteration 726, loss = 0.19377080\n",
      "Iteration 727, loss = 0.19356868\n",
      "Iteration 728, loss = 0.19336700\n",
      "Iteration 729, loss = 0.19316577\n",
      "Iteration 730, loss = 0.19296501\n",
      "Iteration 731, loss = 0.19276468\n",
      "Iteration 732, loss = 0.19256481\n",
      "Iteration 733, loss = 0.19236538\n",
      "Iteration 734, loss = 0.19216641\n",
      "Iteration 735, loss = 0.19196787\n",
      "Iteration 736, loss = 0.19176979\n",
      "Iteration 737, loss = 0.19157213\n",
      "Iteration 738, loss = 0.19137494\n",
      "Iteration 739, loss = 0.19117819\n",
      "Iteration 740, loss = 0.19098189\n",
      "Iteration 741, loss = 0.19078601\n",
      "Iteration 742, loss = 0.19059059\n",
      "Iteration 743, loss = 0.19039558\n",
      "Iteration 744, loss = 0.19020103\n",
      "Iteration 745, loss = 0.19000689\n",
      "Iteration 746, loss = 0.18981320\n",
      "Iteration 747, loss = 0.18961995\n",
      "Iteration 748, loss = 0.18942711\n",
      "Iteration 749, loss = 0.18923472\n",
      "Iteration 750, loss = 0.18904274\n",
      "Iteration 751, loss = 0.18885119\n",
      "Iteration 752, loss = 0.18866007\n",
      "Iteration 753, loss = 0.18846938\n",
      "Iteration 754, loss = 0.18827910\n",
      "Iteration 755, loss = 0.18808928\n",
      "Iteration 756, loss = 0.18789985\n",
      "Iteration 757, loss = 0.18771084\n",
      "Iteration 758, loss = 0.18752227\n",
      "Iteration 759, loss = 0.18733411\n",
      "Iteration 760, loss = 0.18714638\n",
      "Iteration 761, loss = 0.18695904\n",
      "Iteration 762, loss = 0.18677212\n",
      "Iteration 763, loss = 0.18658562\n",
      "Iteration 764, loss = 0.18639955\n",
      "Iteration 765, loss = 0.18621387\n",
      "Iteration 766, loss = 0.18602861\n",
      "Iteration 767, loss = 0.18584375\n",
      "Iteration 768, loss = 0.18565932\n",
      "Iteration 769, loss = 0.18547527\n",
      "Iteration 770, loss = 0.18529165\n",
      "Iteration 771, loss = 0.18510841\n",
      "Iteration 772, loss = 0.18492559\n",
      "Iteration 773, loss = 0.18474318\n",
      "Iteration 774, loss = 0.18456117\n",
      "Iteration 775, loss = 0.18437957\n",
      "Iteration 776, loss = 0.18419835\n",
      "Iteration 777, loss = 0.18401755\n",
      "Iteration 778, loss = 0.18383714\n",
      "Iteration 779, loss = 0.18365715\n",
      "Iteration 780, loss = 0.18347754\n",
      "Iteration 781, loss = 0.18329835\n",
      "Iteration 782, loss = 0.18311953\n",
      "Iteration 783, loss = 0.18294113\n",
      "Iteration 784, loss = 0.18276310\n",
      "Iteration 785, loss = 0.18258547\n",
      "Iteration 786, loss = 0.18240824\n",
      "Iteration 787, loss = 0.18223138\n",
      "Iteration 788, loss = 0.18205492\n",
      "Iteration 789, loss = 0.18187883\n",
      "Iteration 790, loss = 0.18170315\n",
      "Iteration 791, loss = 0.18152784\n",
      "Iteration 792, loss = 0.18135292\n",
      "Iteration 793, loss = 0.18117838\n",
      "Iteration 794, loss = 0.18100422\n",
      "Iteration 795, loss = 0.18083046\n",
      "Iteration 796, loss = 0.18065705\n",
      "Iteration 797, loss = 0.18048403\n",
      "Iteration 798, loss = 0.18031139\n",
      "Iteration 799, loss = 0.18013913\n",
      "Iteration 800, loss = 0.17996724\n",
      "Iteration 801, loss = 0.17979573\n",
      "Iteration 802, loss = 0.17962460\n",
      "Iteration 803, loss = 0.17945383\n",
      "Iteration 804, loss = 0.17928344\n",
      "Iteration 805, loss = 0.17911341\n",
      "Iteration 806, loss = 0.17894377\n",
      "Iteration 807, loss = 0.17877448\n",
      "Iteration 808, loss = 0.17860556\n",
      "Iteration 809, loss = 0.17843703\n",
      "Iteration 810, loss = 0.17826884\n",
      "Iteration 811, loss = 0.17810102\n",
      "Iteration 812, loss = 0.17793356\n",
      "Iteration 813, loss = 0.17776648\n",
      "Iteration 814, loss = 0.17759974\n",
      "Iteration 815, loss = 0.17743338\n",
      "Iteration 816, loss = 0.17726737\n",
      "Iteration 817, loss = 0.17710172\n",
      "Iteration 818, loss = 0.17693644\n",
      "Iteration 819, loss = 0.17677150\n",
      "Iteration 820, loss = 0.17660693\n",
      "Iteration 821, loss = 0.17644271\n",
      "Iteration 822, loss = 0.17627885\n",
      "Iteration 823, loss = 0.17611534\n",
      "Iteration 824, loss = 0.17595218\n",
      "Iteration 825, loss = 0.17578939\n",
      "Iteration 826, loss = 0.17562692\n",
      "Iteration 827, loss = 0.17546483\n",
      "Iteration 828, loss = 0.17530307\n",
      "Iteration 829, loss = 0.17514166\n",
      "Iteration 830, loss = 0.17498061\n",
      "Iteration 831, loss = 0.17481989\n",
      "Iteration 832, loss = 0.17465953\n",
      "Iteration 833, loss = 0.17449951\n",
      "Iteration 834, loss = 0.17433983\n",
      "Iteration 835, loss = 0.17418050\n",
      "Iteration 836, loss = 0.17402150\n",
      "Iteration 837, loss = 0.17386286\n",
      "Iteration 838, loss = 0.17370454\n",
      "Iteration 839, loss = 0.17354658\n",
      "Iteration 840, loss = 0.17338895\n",
      "Iteration 841, loss = 0.17323165\n",
      "Iteration 842, loss = 0.17307470\n",
      "Iteration 843, loss = 0.17291807\n",
      "Iteration 844, loss = 0.17276179\n",
      "Iteration 845, loss = 0.17260583\n",
      "Iteration 846, loss = 0.17245021\n",
      "Iteration 847, loss = 0.17229493\n",
      "Iteration 848, loss = 0.17213997\n",
      "Iteration 849, loss = 0.17198536\n",
      "Iteration 850, loss = 0.17183105\n",
      "Iteration 851, loss = 0.17167710\n",
      "Iteration 852, loss = 0.17152346\n",
      "Iteration 853, loss = 0.17137016\n",
      "Iteration 854, loss = 0.17121719\n",
      "Iteration 855, loss = 0.17106455\n",
      "Iteration 856, loss = 0.17091222\n",
      "Iteration 857, loss = 0.17076023\n",
      "Iteration 858, loss = 0.17060855\n",
      "Iteration 859, loss = 0.17045721\n",
      "Iteration 860, loss = 0.17030617\n",
      "Iteration 861, loss = 0.17015547\n",
      "Iteration 862, loss = 0.17000508\n",
      "Iteration 863, loss = 0.16985501\n",
      "Iteration 864, loss = 0.16970526\n",
      "Iteration 865, loss = 0.16955582\n",
      "Iteration 866, loss = 0.16940671\n",
      "Iteration 867, loss = 0.16925790\n",
      "Iteration 868, loss = 0.16910942\n",
      "Iteration 869, loss = 0.16896125\n",
      "Iteration 870, loss = 0.16881339\n",
      "Iteration 871, loss = 0.16866584\n",
      "Iteration 872, loss = 0.16851861\n",
      "Iteration 873, loss = 0.16837168\n",
      "Iteration 874, loss = 0.16822507\n",
      "Iteration 875, loss = 0.16807875\n",
      "Iteration 876, loss = 0.16793276\n",
      "Iteration 877, loss = 0.16778708\n",
      "Iteration 878, loss = 0.16764169\n",
      "Iteration 879, loss = 0.16749663\n",
      "Iteration 880, loss = 0.16735185\n",
      "Iteration 881, loss = 0.16720740\n",
      "Iteration 882, loss = 0.16706324\n",
      "Iteration 883, loss = 0.16691938\n",
      "Iteration 884, loss = 0.16677583\n",
      "Iteration 885, loss = 0.16663258\n",
      "Iteration 886, loss = 0.16648962\n",
      "Iteration 887, loss = 0.16634698\n",
      "Iteration 888, loss = 0.16620462\n",
      "Iteration 889, loss = 0.16606256\n",
      "Iteration 890, loss = 0.16592081\n",
      "Iteration 891, loss = 0.16577935\n",
      "Iteration 892, loss = 0.16563819\n",
      "Iteration 893, loss = 0.16549732\n",
      "Iteration 894, loss = 0.16535674\n",
      "Iteration 895, loss = 0.16521646\n",
      "Iteration 896, loss = 0.16507647\n",
      "Iteration 897, loss = 0.16493678\n",
      "Iteration 898, loss = 0.16479738\n",
      "Iteration 899, loss = 0.16465826\n",
      "Iteration 900, loss = 0.16451944\n",
      "Iteration 901, loss = 0.16438090\n",
      "Iteration 902, loss = 0.16424266\n",
      "Iteration 903, loss = 0.16410470\n",
      "Iteration 904, loss = 0.16396704\n",
      "Iteration 905, loss = 0.16382965\n",
      "Iteration 906, loss = 0.16369256\n",
      "Iteration 907, loss = 0.16355575\n",
      "Iteration 908, loss = 0.16341923\n",
      "Iteration 909, loss = 0.16328299\n",
      "Iteration 910, loss = 0.16314703\n",
      "Iteration 911, loss = 0.16301135\n",
      "Iteration 912, loss = 0.16287596\n",
      "Iteration 913, loss = 0.16274085\n",
      "Iteration 914, loss = 0.16260602\n",
      "Iteration 915, loss = 0.16247146\n",
      "Iteration 916, loss = 0.16233718\n",
      "Iteration 917, loss = 0.16220319\n",
      "Iteration 918, loss = 0.16206946\n",
      "Iteration 919, loss = 0.16193606\n",
      "Iteration 920, loss = 0.16180288\n",
      "Iteration 921, loss = 0.16166999\n",
      "Iteration 922, loss = 0.16153737\n",
      "Iteration 923, loss = 0.16140504\n",
      "Iteration 924, loss = 0.16127296\n",
      "Iteration 925, loss = 0.16114117\n",
      "Iteration 926, loss = 0.16100965\n",
      "Iteration 927, loss = 0.16087840\n",
      "Iteration 928, loss = 0.16074741\n",
      "Iteration 929, loss = 0.16061671\n",
      "Iteration 930, loss = 0.16048625\n",
      "Iteration 931, loss = 0.16035608\n",
      "Iteration 932, loss = 0.16022617\n",
      "Iteration 933, loss = 0.16009653\n",
      "Iteration 934, loss = 0.15996715\n",
      "Iteration 935, loss = 0.15983804\n",
      "Iteration 936, loss = 0.15970919\n",
      "Iteration 937, loss = 0.15958060\n",
      "Iteration 938, loss = 0.15945228\n",
      "Iteration 939, loss = 0.15932421\n",
      "Iteration 940, loss = 0.15919642\n",
      "Iteration 941, loss = 0.15906889\n",
      "Iteration 942, loss = 0.15894160\n",
      "Iteration 943, loss = 0.15881458\n",
      "Iteration 944, loss = 0.15868782\n",
      "Iteration 945, loss = 0.15856132\n",
      "Iteration 946, loss = 0.15843508\n",
      "Iteration 947, loss = 0.15830909\n",
      "Iteration 948, loss = 0.15818337\n",
      "Iteration 949, loss = 0.15805788\n",
      "Iteration 950, loss = 0.15793266\n",
      "Iteration 951, loss = 0.15780770\n",
      "Iteration 952, loss = 0.15768299\n",
      "Iteration 953, loss = 0.15755853\n",
      "Iteration 954, loss = 0.15743431\n",
      "Iteration 955, loss = 0.15731037\n",
      "Iteration 956, loss = 0.15718665\n",
      "Iteration 957, loss = 0.15706319\n",
      "Iteration 958, loss = 0.15694000\n",
      "Iteration 959, loss = 0.15681704\n",
      "Iteration 960, loss = 0.15669433\n",
      "Iteration 961, loss = 0.15657187\n",
      "Iteration 962, loss = 0.15644967\n",
      "Iteration 963, loss = 0.15632768\n",
      "Iteration 964, loss = 0.15620596\n",
      "Iteration 965, loss = 0.15608451\n",
      "Iteration 966, loss = 0.15596327\n",
      "Iteration 967, loss = 0.15584232\n",
      "Iteration 968, loss = 0.15572160\n",
      "Iteration 969, loss = 0.15560112\n",
      "Iteration 970, loss = 0.15548090\n",
      "Iteration 971, loss = 0.15536090\n",
      "Iteration 972, loss = 0.15524115\n",
      "Iteration 973, loss = 0.15512164\n",
      "Iteration 974, loss = 0.15500238\n",
      "Iteration 975, loss = 0.15488335\n",
      "Iteration 976, loss = 0.15476455\n",
      "Iteration 977, loss = 0.15464602\n",
      "Iteration 978, loss = 0.15452770\n",
      "Iteration 979, loss = 0.15440963\n",
      "Iteration 980, loss = 0.15429178\n",
      "Iteration 981, loss = 0.15417419\n",
      "Iteration 982, loss = 0.15405681\n",
      "Iteration 983, loss = 0.15393968\n",
      "Iteration 984, loss = 0.15382279\n",
      "Iteration 985, loss = 0.15370612\n",
      "Iteration 986, loss = 0.15358969\n",
      "Iteration 987, loss = 0.15347348\n",
      "Iteration 988, loss = 0.15335751\n",
      "Iteration 989, loss = 0.15324178\n",
      "Iteration 990, loss = 0.15312626\n",
      "Iteration 991, loss = 0.15301099\n",
      "Iteration 992, loss = 0.15289594\n",
      "Iteration 993, loss = 0.15278112\n",
      "Iteration 994, loss = 0.15266652\n",
      "Iteration 995, loss = 0.15255215\n",
      "Iteration 996, loss = 0.15243801\n",
      "Iteration 997, loss = 0.15232410\n",
      "Iteration 998, loss = 0.15221040\n",
      "Iteration 999, loss = 0.15209695\n",
      "Iteration 1000, loss = 0.15198371\n",
      "Iteration 1, loss = 1.92980133\n",
      "Iteration 2, loss = 1.88518426\n",
      "Iteration 3, loss = 1.82506927\n",
      "Iteration 4, loss = 1.75414080\n",
      "Iteration 5, loss = 1.67658784\n",
      "Iteration 6, loss = 1.59584692\n",
      "Iteration 7, loss = 1.51468181\n",
      "Iteration 8, loss = 1.43528533\n",
      "Iteration 9, loss = 1.35978758\n",
      "Iteration 10, loss = 1.29026894\n",
      "Iteration 11, loss = 1.22876565\n",
      "Iteration 12, loss = 1.17721979\n",
      "Iteration 13, loss = 1.13688210\n",
      "Iteration 14, loss = 1.10798197\n",
      "Iteration 15, loss = 1.08931936\n",
      "Iteration 16, loss = 1.07876043\n",
      "Iteration 17, loss = 1.07318294\n",
      "Iteration 18, loss = 1.06973996\n",
      "Iteration 19, loss = 1.06597052\n",
      "Iteration 20, loss = 1.06032669\n",
      "Iteration 21, loss = 1.05198506\n",
      "Iteration 22, loss = 1.04096885\n",
      "Iteration 23, loss = 1.02791836\n",
      "Iteration 24, loss = 1.01365941\n",
      "Iteration 25, loss = 0.99901043\n",
      "Iteration 26, loss = 0.98463310\n",
      "Iteration 27, loss = 0.97097987\n",
      "Iteration 28, loss = 0.95819480\n",
      "Iteration 29, loss = 0.94621547\n",
      "Iteration 30, loss = 0.93479107\n",
      "Iteration 31, loss = 0.92367936\n",
      "Iteration 32, loss = 0.91264274\n",
      "Iteration 33, loss = 0.90138626\n",
      "Iteration 34, loss = 0.88991004\n",
      "Iteration 35, loss = 0.87825485\n",
      "Iteration 36, loss = 0.86640113\n",
      "Iteration 37, loss = 0.85462406\n",
      "Iteration 38, loss = 0.84322924\n",
      "Iteration 39, loss = 0.83245640"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 40, loss = 0.82236096\n",
      "Iteration 41, loss = 0.81305514\n",
      "Iteration 42, loss = 0.80458821\n",
      "Iteration 43, loss = 0.79688681\n",
      "Iteration 44, loss = 0.78983377\n",
      "Iteration 45, loss = 0.78306477\n",
      "Iteration 46, loss = 0.77648842\n",
      "Iteration 47, loss = 0.77000034\n",
      "Iteration 48, loss = 0.76360724\n",
      "Iteration 49, loss = 0.75735149\n",
      "Iteration 50, loss = 0.75120680\n",
      "Iteration 51, loss = 0.74519189\n",
      "Iteration 52, loss = 0.73929031\n",
      "Iteration 53, loss = 0.73355469\n",
      "Iteration 54, loss = 0.72803309\n",
      "Iteration 55, loss = 0.72279694\n",
      "Iteration 56, loss = 0.71784722\n",
      "Iteration 57, loss = 0.71308605\n",
      "Iteration 58, loss = 0.70846113\n",
      "Iteration 59, loss = 0.70391450\n",
      "Iteration 60, loss = 0.69943041\n",
      "Iteration 61, loss = 0.69502819\n",
      "Iteration 62, loss = 0.69069976\n",
      "Iteration 63, loss = 0.68644417\n",
      "Iteration 64, loss = 0.68226744\n",
      "Iteration 65, loss = 0.67818157\n",
      "Iteration 66, loss = 0.67417368\n",
      "Iteration 67, loss = 0.67027437\n",
      "Iteration 68, loss = 0.66647544\n",
      "Iteration 69, loss = 0.66274658\n",
      "Iteration 70, loss = 0.65907555\n",
      "Iteration 71, loss = 0.65547952\n",
      "Iteration 72, loss = 0.65194234\n",
      "Iteration 73, loss = 0.64845404\n",
      "Iteration 74, loss = 0.64500881\n",
      "Iteration 75, loss = 0.64160970\n",
      "Iteration 76, loss = 0.63825666\n",
      "Iteration 77, loss = 0.63494790\n",
      "Iteration 78, loss = 0.63168214\n",
      "Iteration 79, loss = 0.62846159\n",
      "Iteration 80, loss = 0.62528923\n",
      "Iteration 81, loss = 0.62216696\n",
      "Iteration 82, loss = 0.61909229\n",
      "Iteration 83, loss = 0.61606725\n",
      "Iteration 84, loss = 0.61309091\n",
      "Iteration 85, loss = 0.61015281\n",
      "Iteration 86, loss = 0.60725234\n",
      "Iteration 87, loss = 0.60438852\n",
      "Iteration 88, loss = 0.60156043\n",
      "Iteration 89, loss = 0.59876720\n",
      "Iteration 90, loss = 0.59600777\n",
      "Iteration 91, loss = 0.59328306\n",
      "Iteration 92, loss = 0.59059557\n",
      "Iteration 93, loss = 0.58794890\n",
      "Iteration 94, loss = 0.58533653\n",
      "Iteration 95, loss = 0.58275686\n",
      "Iteration 96, loss = 0.58020926\n",
      "Iteration 97, loss = 0.57769325\n",
      "Iteration 98, loss = 0.57520836\n",
      "Iteration 99, loss = 0.57275411\n",
      "Iteration 100, loss = 0.57033003\n",
      "Iteration 101, loss = 0.56793565\n",
      "Iteration 102, loss = 0.56557050\n",
      "Iteration 103, loss = 0.56323448\n",
      "Iteration 104, loss = 0.56092681\n",
      "Iteration 105, loss = 0.55864681\n",
      "Iteration 106, loss = 0.55639400\n",
      "Iteration 107, loss = 0.55416795\n",
      "Iteration 108, loss = 0.55196744\n",
      "Iteration 109, loss = 0.54979275\n",
      "Iteration 110, loss = 0.54764344\n",
      "Iteration 111, loss = 0.54551914\n",
      "Iteration 112, loss = 0.54341939\n",
      "Iteration 113, loss = 0.54134389\n",
      "Iteration 114, loss = 0.53929257\n",
      "Iteration 115, loss = 0.53726458\n",
      "Iteration 116, loss = 0.53525951\n",
      "Iteration 117, loss = 0.53327695\n",
      "Iteration 118, loss = 0.53131651\n",
      "Iteration 119, loss = 0.52937781\n",
      "Iteration 120, loss = 0.52746045\n",
      "Iteration 121, loss = 0.52556428\n",
      "Iteration 122, loss = 0.52368880\n",
      "Iteration 123, loss = 0.52183356\n",
      "Iteration 124, loss = 0.51999819\n",
      "Iteration 125, loss = 0.51818234\n",
      "Iteration 126, loss = 0.51638565\n",
      "Iteration 127, loss = 0.51460780\n",
      "Iteration 128, loss = 0.51284843\n",
      "Iteration 129, loss = 0.51110723\n",
      "Iteration 130, loss = 0.50938471\n",
      "Iteration 131, loss = 0.50767998\n",
      "Iteration 132, loss = 0.50599249\n",
      "Iteration 133, loss = 0.50432215\n",
      "Iteration 134, loss = 0.50266852\n",
      "Iteration 135, loss = 0.50103127\n",
      "Iteration 136, loss = 0.49941012\n",
      "Iteration 137, loss = 0.49780481\n",
      "Iteration 138, loss = 0.49621506\n",
      "Iteration 139, loss = 0.49464060\n",
      "Iteration 140, loss = 0.49308118\n",
      "Iteration 141, loss = 0.49153654\n",
      "Iteration 142, loss = 0.49000643\n",
      "Iteration 143, loss = 0.48849059\n",
      "Iteration 144, loss = 0.48698878\n",
      "Iteration 145, loss = 0.48550076\n",
      "Iteration 146, loss = 0.48402629\n",
      "Iteration 147, loss = 0.48256581\n",
      "Iteration 148, loss = 0.48111970\n",
      "Iteration 149, loss = 0.47968657\n",
      "Iteration 150, loss = 0.47826621\n",
      "Iteration 151, loss = 0.47685845\n",
      "Iteration 152, loss = 0.47546307\n",
      "Iteration 153, loss = 0.47407989\n",
      "Iteration 154, loss = 0.47270916\n",
      "Iteration 155, loss = 0.47135005\n",
      "Iteration 156, loss = 0.47000225\n",
      "Iteration 157, loss = 0.46866616\n",
      "Iteration 158, loss = 0.46734169\n",
      "Iteration 159, loss = 0.46602850\n",
      "Iteration 160, loss = 0.46472621\n",
      "Iteration 161, loss = 0.46343466\n",
      "Iteration 162, loss = 0.46215368\n",
      "Iteration 163, loss = 0.46088311\n",
      "Iteration 164, loss = 0.45962294\n",
      "Iteration 165, loss = 0.45837295\n",
      "Iteration 166, loss = 0.45713287\n",
      "Iteration 167, loss = 0.45590264\n",
      "Iteration 168, loss = 0.45468223\n",
      "Iteration 169, loss = 0.45347120\n",
      "Iteration 170, loss = 0.45226951\n",
      "Iteration 171, loss = 0.45107726\n",
      "Iteration 172, loss = 0.44989381\n",
      "Iteration 173, loss = 0.44871951\n",
      "Iteration 174, loss = 0.44755395\n",
      "Iteration 175, loss = 0.44639706\n",
      "Iteration 176, loss = 0.44524872\n",
      "Iteration 177, loss = 0.44410879\n",
      "Iteration 178, loss = 0.44297717\n",
      "Iteration 179, loss = 0.44185376\n",
      "Iteration 180, loss = 0.44073856\n",
      "Iteration 181, loss = 0.43963183\n",
      "Iteration 182, loss = 0.43853296\n",
      "Iteration 183, loss = 0.43744182\n",
      "Iteration 184, loss = 0.43635829\n",
      "Iteration 185, loss = 0.43528228\n",
      "Iteration 186, loss = 0.43421365\n",
      "Iteration 187, loss = 0.43315231\n",
      "Iteration 188, loss = 0.43209814\n",
      "Iteration 189, loss = 0.43105104\n",
      "Iteration 190, loss = 0.43001091\n",
      "Iteration 191, loss = 0.42897763\n",
      "Iteration 192, loss = 0.42795119\n",
      "Iteration 193, loss = 0.42693157\n",
      "Iteration 194, loss = 0.42591943\n",
      "Iteration 195, loss = 0.42491467\n",
      "Iteration 196, loss = 0.42391626\n",
      "Iteration 197, loss = 0.42292397\n",
      "Iteration 198, loss = 0.42193782\n",
      "Iteration 199, loss = 0.42095781\n",
      "Iteration 200, loss = 0.41998374\n",
      "Iteration 201, loss = 0.41901584\n",
      "Iteration 202, loss = 0.41805376\n",
      "Iteration 203, loss = 0.41709785\n",
      "Iteration 204, loss = 0.41614792\n",
      "Iteration 205, loss = 0.41520384\n",
      "Iteration 206, loss = 0.41426546\n",
      "Iteration 207, loss = 0.41333258\n",
      "Iteration 208, loss = 0.41240516\n",
      "Iteration 209, loss = 0.41148316\n",
      "Iteration 210, loss = 0.41056666\n",
      "Iteration 211, loss = 0.40965528\n",
      "Iteration 212, loss = 0.40874931\n",
      "Iteration 213, loss = 0.40784839\n",
      "Iteration 214, loss = 0.40695254\n",
      "Iteration 215, loss = 0.40606170\n",
      "Iteration 216, loss = 0.40517591\n",
      "Iteration 217, loss = 0.40429500\n",
      "Iteration 218, loss = 0.40341892\n",
      "Iteration 219, loss = 0.40254762\n",
      "Iteration 220, loss = 0.40168102\n",
      "Iteration 221, loss = 0.40081914\n",
      "Iteration 222, loss = 0.39996179\n",
      "Iteration 223, loss = 0.39910917\n",
      "Iteration 224, loss = 0.39826136\n",
      "Iteration 225, loss = 0.39741800\n",
      "Iteration 226, loss = 0.39657902\n",
      "Iteration 227, loss = 0.39574445\n",
      "Iteration 228, loss = 0.39491420\n",
      "Iteration 229, loss = 0.39408819\n",
      "Iteration 230, loss = 0.39326646\n",
      "Iteration 231, loss = 0.39244909\n",
      "Iteration 232, loss = 0.39163587\n",
      "Iteration 233, loss = 0.39082666\n",
      "Iteration 234, loss = 0.39002158\n",
      "Iteration 235, loss = 0.38922040\n",
      "Iteration 236, loss = 0.38842321\n",
      "Iteration 237, loss = 0.38762982\n",
      "Iteration 238, loss = 0.38684034\n",
      "Iteration 239, loss = 0.38605459\n",
      "Iteration 240, loss = 0.38527264\n",
      "Iteration 241, loss = 0.38449434\n",
      "Iteration 242, loss = 0.38371974\n",
      "Iteration 243, loss = 0.38294873\n",
      "Iteration 244, loss = 0.38218130\n",
      "Iteration 245, loss = 0.38141747\n",
      "Iteration 246, loss = 0.38065721\n",
      "Iteration 247, loss = 0.37990064\n",
      "Iteration 248, loss = 0.37914740\n",
      "Iteration 249, loss = 0.37839750\n",
      "Iteration 250, loss = 0.37765106\n",
      "Iteration 251, loss = 0.37690790\n",
      "Iteration 252, loss = 0.37616844\n",
      "Iteration 253, loss = 0.37543215\n",
      "Iteration 254, loss = 0.37469937\n",
      "Iteration 255, loss = 0.37396960\n",
      "Iteration 256, loss = 0.37324308\n",
      "Iteration 257, loss = 0.37251978\n",
      "Iteration 258, loss = 0.37179952\n",
      "Iteration 259, loss = 0.37108248\n",
      "Iteration 260, loss = 0.37036841\n",
      "Iteration 261, loss = 0.36965748\n",
      "Iteration 262, loss = 0.36894944\n",
      "Iteration 263, loss = 0.36824453\n",
      "Iteration 264, loss = 0.36754240\n",
      "Iteration 265, loss = 0.36684331\n",
      "Iteration 266, loss = 0.36614699\n",
      "Iteration 267, loss = 0.36545364\n",
      "Iteration 268, loss = 0.36476299\n",
      "Iteration 269, loss = 0.36407523\n",
      "Iteration 270, loss = 0.36339021\n",
      "Iteration 271, loss = 0.36270794\n",
      "Iteration 272, loss = 0.36202843\n",
      "Iteration 273, loss = 0.36135156\n",
      "Iteration 274, loss = 0.36067736\n",
      "Iteration 275, loss = 0.36000592\n",
      "Iteration 276, loss = 0.35933695\n",
      "Iteration 277, loss = 0.35867064\n",
      "Iteration 278, loss = 0.35800699\n",
      "Iteration 279, loss = 0.35734583\n",
      "Iteration 280, loss = 0.35668719\n",
      "Iteration 281, loss = 0.35603107\n",
      "Iteration 282, loss = 0.35537749\n",
      "Iteration 283, loss = 0.35472633\n",
      "Iteration 284, loss = 0.35407764\n",
      "Iteration 285, loss = 0.35343147\n",
      "Iteration 286, loss = 0.35278816\n",
      "Iteration 287, loss = 0.35214755\n",
      "Iteration 288, loss = 0.35150933\n",
      "Iteration 289, loss = 0.35087374\n",
      "Iteration 290, loss = 0.35024062\n",
      "Iteration 291, loss = 0.34960985\n",
      "Iteration 292, loss = 0.34898146\n",
      "Iteration 293, loss = 0.34835537\n",
      "Iteration 294, loss = 0.34773155\n",
      "Iteration 295, loss = 0.34711001\n",
      "Iteration 296, loss = 0.34649074\n",
      "Iteration 297, loss = 0.34587400\n",
      "Iteration 298, loss = 0.34525950\n",
      "Iteration 299, loss = 0.34464718\n",
      "Iteration 300, loss = 0.34403706\n",
      "Iteration 301, loss = 0.34342920\n",
      "Iteration 302, loss = 0.34282369\n",
      "Iteration 303, loss = 0.34222030\n",
      "Iteration 304, loss = 0.34161902\n",
      "Iteration 305, loss = 0.34101986\n",
      "Iteration 306, loss = 0.34042276\n",
      "Iteration 307, loss = 0.33982774\n",
      "Iteration 308, loss = 0.33923474\n",
      "Iteration 309, loss = 0.33864378\n",
      "Iteration 310, loss = 0.33805482\n",
      "Iteration 311, loss = 0.33746791\n",
      "Iteration 312, loss = 0.33688306\n",
      "Iteration 313, loss = 0.33630049\n",
      "Iteration 314, loss = 0.33571987\n",
      "Iteration 315, loss = 0.33514121\n",
      "Iteration 316, loss = 0.33456465\n",
      "Iteration 317, loss = 0.33399009\n",
      "Iteration 318, loss = 0.33341772\n",
      "Iteration 319, loss = 0.33284733\n",
      "Iteration 320, loss = 0.33227882\n",
      "Iteration 321, loss = 0.33171221\n",
      "Iteration 322, loss = 0.33114766\n",
      "Iteration 323, loss = 0.33058514\n",
      "Iteration 324, loss = 0.33002461\n",
      "Iteration 325, loss = 0.32946612\n",
      "Iteration 326, loss = 0.32890951\n",
      "Iteration 327, loss = 0.32835472\n",
      "Iteration 328, loss = 0.32780173\n",
      "Iteration 329, loss = 0.32725052\n",
      "Iteration 330, loss = 0.32670109\n",
      "Iteration 331, loss = 0.32615340\n",
      "Iteration 332, loss = 0.32560745\n",
      "Iteration 333, loss = 0.32506322\n",
      "Iteration 334, loss = 0.32452071\n",
      "Iteration 335, loss = 0.32397989\n",
      "Iteration 336, loss = 0.32344075\n",
      "Iteration 337, loss = 0.32290333\n",
      "Iteration 338, loss = 0.32236780\n",
      "Iteration 339, loss = 0.32183393\n",
      "Iteration 340, loss = 0.32130172\n",
      "Iteration 341, loss = 0.32077112\n",
      "Iteration 342, loss = 0.32024217\n",
      "Iteration 343, loss = 0.31971482\n",
      "Iteration 344, loss = 0.31918907\n",
      "Iteration 345, loss = 0.31866491\n",
      "Iteration 346, loss = 0.31814243\n",
      "Iteration 347, loss = 0.31762164\n",
      "Iteration 348, loss = 0.31710241\n",
      "Iteration 349, loss = 0.31658475\n",
      "Iteration 350, loss = 0.31606864\n",
      "Iteration 351, loss = 0.31555405\n",
      "Iteration 352, loss = 0.31504100\n",
      "Iteration 353, loss = 0.31452946\n",
      "Iteration 354, loss = 0.31401945\n",
      "Iteration 355, loss = 0.31351091\n",
      "Iteration 356, loss = 0.31300386\n",
      "Iteration 357, loss = 0.31249830\n",
      "Iteration 358, loss = 0.31199421\n",
      "Iteration 359, loss = 0.31149171\n",
      "Iteration 360, loss = 0.31099069\n",
      "Iteration 361, loss = 0.31049107\n",
      "Iteration 362, loss = 0.30999287\n",
      "Iteration 363, loss = 0.30949607\n",
      "Iteration 364, loss = 0.30900078\n",
      "Iteration 365, loss = 0.30850696\n",
      "Iteration 366, loss = 0.30801460\n",
      "Iteration 367, loss = 0.30752369\n",
      "Iteration 368, loss = 0.30703417\n",
      "Iteration 369, loss = 0.30654604\n",
      "Iteration 370, loss = 0.30605928\n",
      "Iteration 371, loss = 0.30557400\n",
      "Iteration 372, loss = 0.30509022\n",
      "Iteration 373, loss = 0.30460775\n",
      "Iteration 374, loss = 0.30412665\n",
      "Iteration 375, loss = 0.30364692\n",
      "Iteration 376, loss = 0.30316867\n",
      "Iteration 377, loss = 0.30269178\n",
      "Iteration 378, loss = 0.30221622\n",
      "Iteration 379, loss = 0.30174203\n",
      "Iteration 380, loss = 0.30126912\n",
      "Iteration 381, loss = 0.30079757\n",
      "Iteration 382, loss = 0.30032731\n",
      "Iteration 383, loss = 0.29985834\n",
      "Iteration 384, loss = 0.29939084\n",
      "Iteration 385, loss = 0.29892463\n",
      "Iteration 386, loss = 0.29845976\n",
      "Iteration 387, loss = 0.29799615\n",
      "Iteration 388, loss = 0.29753380\n",
      "Iteration 389, loss = 0.29707277\n",
      "Iteration 390, loss = 0.29661296\n",
      "Iteration 391, loss = 0.29615455\n",
      "Iteration 392, loss = 0.29569739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 393, loss = 0.29524149\n",
      "Iteration 394, loss = 0.29478686\n",
      "Iteration 395, loss = 0.29433344\n",
      "Iteration 396, loss = 0.29388176\n",
      "Iteration 397, loss = 0.29343088\n",
      "Iteration 398, loss = 0.29298127\n",
      "Iteration 399, loss = 0.29253317\n",
      "Iteration 400, loss = 0.29208625\n",
      "Iteration 401, loss = 0.29164052\n",
      "Iteration 402, loss = 0.29119598\n",
      "Iteration 403, loss = 0.29075273\n",
      "Iteration 404, loss = 0.29031058\n",
      "Iteration 405, loss = 0.28986974\n",
      "Iteration 406, loss = 0.28942993\n",
      "Iteration 407, loss = 0.28899143\n",
      "Iteration 408, loss = 0.28855403\n",
      "Iteration 409, loss = 0.28811779\n",
      "Iteration 410, loss = 0.28768280\n",
      "Iteration 411, loss = 0.28724887\n",
      "Iteration 412, loss = 0.28681611\n",
      "Iteration 413, loss = 0.28638454\n",
      "Iteration 414, loss = 0.28595406\n",
      "Iteration 415, loss = 0.28552478\n",
      "Iteration 416, loss = 0.28509652\n",
      "Iteration 417, loss = 0.28466954\n",
      "Iteration 418, loss = 0.28424353\n",
      "Iteration 419, loss = 0.28381875\n",
      "Iteration 420, loss = 0.28339506\n",
      "Iteration 421, loss = 0.28297245\n",
      "Iteration 422, loss = 0.28255106\n",
      "Iteration 423, loss = 0.28213062\n",
      "Iteration 424, loss = 0.28171147\n",
      "Iteration 425, loss = 0.28129327\n",
      "Iteration 426, loss = 0.28087615\n",
      "Iteration 427, loss = 0.28046019\n",
      "Iteration 428, loss = 0.28004525\n",
      "Iteration 429, loss = 0.27963142\n",
      "Iteration 430, loss = 0.27921863\n",
      "Iteration 431, loss = 0.27880696\n",
      "Iteration 432, loss = 0.27839628\n",
      "Iteration 433, loss = 0.27798678\n",
      "Iteration 434, loss = 0.27757823\n",
      "Iteration 435, loss = 0.27717086\n",
      "Iteration 436, loss = 0.27676444\n",
      "Iteration 437, loss = 0.27635926\n",
      "Iteration 438, loss = 0.27595519\n",
      "Iteration 439, loss = 0.27555212\n",
      "Iteration 440, loss = 0.27515020\n",
      "Iteration 441, loss = 0.27474921\n",
      "Iteration 442, loss = 0.27434934\n",
      "Iteration 443, loss = 0.27395041\n",
      "Iteration 444, loss = 0.27355263\n",
      "Iteration 445, loss = 0.27315575\n",
      "Iteration 446, loss = 0.27275995\n",
      "Iteration 447, loss = 0.27236512\n",
      "Iteration 448, loss = 0.27197136\n",
      "Iteration 449, loss = 0.27157853\n",
      "Iteration 450, loss = 0.27118679\n",
      "Iteration 451, loss = 0.27079595\n",
      "Iteration 452, loss = 0.27040622\n",
      "Iteration 453, loss = 0.27001741\n",
      "Iteration 454, loss = 0.26962952\n",
      "Iteration 455, loss = 0.26924273\n",
      "Iteration 456, loss = 0.26885682\n",
      "Iteration 457, loss = 0.26847199\n",
      "Iteration 458, loss = 0.26808806\n",
      "Iteration 459, loss = 0.26770518\n",
      "Iteration 460, loss = 0.26732321\n",
      "Iteration 461, loss = 0.26694224\n",
      "Iteration 462, loss = 0.26656227\n",
      "Iteration 463, loss = 0.26618319\n",
      "Iteration 464, loss = 0.26580512\n",
      "Iteration 465, loss = 0.26542800\n",
      "Iteration 466, loss = 0.26505181\n",
      "Iteration 467, loss = 0.26467660\n",
      "Iteration 468, loss = 0.26430237\n",
      "Iteration 469, loss = 0.26392905\n",
      "Iteration 470, loss = 0.26355669\n",
      "Iteration 471, loss = 0.26318522\n",
      "Iteration 472, loss = 0.26281473\n",
      "Iteration 473, loss = 0.26244513\n",
      "Iteration 474, loss = 0.26207650\n",
      "Iteration 475, loss = 0.26170874\n",
      "Iteration 476, loss = 0.26134199\n",
      "Iteration 477, loss = 0.26097605\n",
      "Iteration 478, loss = 0.26061102\n",
      "Iteration 479, loss = 0.26024696\n",
      "Iteration 480, loss = 0.25988380\n",
      "Iteration 481, loss = 0.25952147\n",
      "Iteration 482, loss = 0.25916016\n",
      "Iteration 483, loss = 0.25879965\n",
      "Iteration 484, loss = 0.25844006\n",
      "Iteration 485, loss = 0.25808132\n",
      "Iteration 486, loss = 0.25772359\n",
      "Iteration 487, loss = 0.25736659\n",
      "Iteration 488, loss = 0.25701058\n",
      "Iteration 489, loss = 0.25665541\n",
      "Iteration 490, loss = 0.25630111\n",
      "Iteration 491, loss = 0.25594768\n",
      "Iteration 492, loss = 0.25559517\n",
      "Iteration 493, loss = 0.25524351\n",
      "Iteration 494, loss = 0.25489272\n",
      "Iteration 495, loss = 0.25454277\n",
      "Iteration 496, loss = 0.25419370\n",
      "Iteration 497, loss = 0.25384551\n",
      "Iteration 498, loss = 0.25349815\n",
      "Iteration 499, loss = 0.25315166\n",
      "Iteration 500, loss = 0.25280596\n",
      "Iteration 501, loss = 0.25246123\n",
      "Iteration 502, loss = 0.25211723\n",
      "Iteration 503, loss = 0.25177412\n",
      "Iteration 504, loss = 0.25143180\n",
      "Iteration 505, loss = 0.25109042\n",
      "Iteration 506, loss = 0.25074978\n",
      "Iteration 507, loss = 0.25041006\n",
      "Iteration 508, loss = 0.25007108\n",
      "Iteration 509, loss = 0.24973294\n",
      "Iteration 510, loss = 0.24939568\n",
      "Iteration 511, loss = 0.24905918\n",
      "Iteration 512, loss = 0.24872356\n",
      "Iteration 513, loss = 0.24838872\n",
      "Iteration 514, loss = 0.24805475\n",
      "Iteration 515, loss = 0.24772151\n",
      "Iteration 516, loss = 0.24738911\n",
      "Iteration 517, loss = 0.24705752\n",
      "Iteration 518, loss = 0.24672679\n",
      "Iteration 519, loss = 0.24639677\n",
      "Iteration 520, loss = 0.24606764\n",
      "Iteration 521, loss = 0.24573924\n",
      "Iteration 522, loss = 0.24541165\n",
      "Iteration 523, loss = 0.24508489\n",
      "Iteration 524, loss = 0.24475887\n",
      "Iteration 525, loss = 0.24443369\n",
      "Iteration 526, loss = 0.24410923\n",
      "Iteration 527, loss = 0.24378569\n",
      "Iteration 528, loss = 0.24346278\n",
      "Iteration 529, loss = 0.24314072\n",
      "Iteration 530, loss = 0.24281941\n",
      "Iteration 531, loss = 0.24249896\n",
      "Iteration 532, loss = 0.24217915\n",
      "Iteration 533, loss = 0.24186026\n",
      "Iteration 534, loss = 0.24154205\n",
      "Iteration 535, loss = 0.24122455\n",
      "Iteration 536, loss = 0.24090804\n",
      "Iteration 537, loss = 0.24059220\n",
      "Iteration 538, loss = 0.24027720\n",
      "Iteration 539, loss = 0.23996284\n",
      "Iteration 540, loss = 0.23964941\n",
      "Iteration 541, loss = 0.23933656\n",
      "Iteration 542, loss = 0.23902460\n",
      "Iteration 543, loss = 0.23871331\n",
      "Iteration 544, loss = 0.23840280\n",
      "Iteration 545, loss = 0.23809307\n",
      "Iteration 546, loss = 0.23778404\n",
      "Iteration 547, loss = 0.23747577\n",
      "Iteration 548, loss = 0.23716820\n",
      "Iteration 549, loss = 0.23686140\n",
      "Iteration 550, loss = 0.23655532\n",
      "Iteration 551, loss = 0.23625004\n",
      "Iteration 552, loss = 0.23594542\n",
      "Iteration 553, loss = 0.23564157\n",
      "Iteration 554, loss = 0.23533840\n",
      "Iteration 555, loss = 0.23503606\n",
      "Iteration 556, loss = 0.23473443\n",
      "Iteration 557, loss = 0.23443358\n",
      "Iteration 558, loss = 0.23413340\n",
      "Iteration 559, loss = 0.23383392\n",
      "Iteration 560, loss = 0.23353524\n",
      "Iteration 561, loss = 0.23323722\n",
      "Iteration 562, loss = 0.23293993\n",
      "Iteration 563, loss = 0.23264342\n",
      "Iteration 564, loss = 0.23234765\n",
      "Iteration 565, loss = 0.23205254\n",
      "Iteration 566, loss = 0.23175820\n",
      "Iteration 567, loss = 0.23146453\n",
      "Iteration 568, loss = 0.23117158\n",
      "Iteration 569, loss = 0.23087932\n",
      "Iteration 570, loss = 0.23058782\n",
      "Iteration 571, loss = 0.23029697\n",
      "Iteration 572, loss = 0.23000683\n",
      "Iteration 573, loss = 0.22971738\n",
      "Iteration 574, loss = 0.22942861\n",
      "Iteration 575, loss = 0.22914058\n",
      "Iteration 576, loss = 0.22885318\n",
      "Iteration 577, loss = 0.22856655\n",
      "Iteration 578, loss = 0.22828055\n",
      "Iteration 579, loss = 0.22799532\n",
      "Iteration 580, loss = 0.22771071\n",
      "Iteration 581, loss = 0.22742678\n",
      "Iteration 582, loss = 0.22714354\n",
      "Iteration 583, loss = 0.22686097\n",
      "Iteration 584, loss = 0.22657905\n",
      "Iteration 585, loss = 0.22629785\n",
      "Iteration 586, loss = 0.22601731\n",
      "Iteration 587, loss = 0.22573746\n",
      "Iteration 588, loss = 0.22545828\n",
      "Iteration 589, loss = 0.22517973\n",
      "Iteration 590, loss = 0.22490196\n",
      "Iteration 591, loss = 0.22462475\n",
      "Iteration 592, loss = 0.22434829\n",
      "Iteration 593, loss = 0.22407242\n",
      "Iteration 594, loss = 0.22379724\n",
      "Iteration 595, loss = 0.22352267\n",
      "Iteration 596, loss = 0.22324880\n",
      "Iteration 597, loss = 0.22297552\n",
      "Iteration 598, loss = 0.22270293\n",
      "Iteration 599, loss = 0.22243097\n",
      "Iteration 600, loss = 0.22215964\n",
      "Iteration 601, loss = 0.22188894\n",
      "Iteration 602, loss = 0.22161892\n",
      "Iteration 603, loss = 0.22134948\n",
      "Iteration 604, loss = 0.22108074\n",
      "Iteration 605, loss = 0.22081258\n",
      "Iteration 606, loss = 0.22054503\n",
      "Iteration 607, loss = 0.22027815\n",
      "Iteration 608, loss = 0.22001195\n",
      "Iteration 609, loss = 0.21974631\n",
      "Iteration 610, loss = 0.21948131\n",
      "Iteration 611, loss = 0.21921697\n",
      "Iteration 612, loss = 0.21895325\n",
      "Iteration 613, loss = 0.21869016\n",
      "Iteration 614, loss = 0.21842769\n",
      "Iteration 615, loss = 0.21816585\n",
      "Iteration 616, loss = 0.21790460\n",
      "Iteration 617, loss = 0.21764398\n",
      "Iteration 618, loss = 0.21738396\n",
      "Iteration 619, loss = 0.21712455\n",
      "Iteration 620, loss = 0.21686574\n",
      "Iteration 621, loss = 0.21660754\n",
      "Iteration 622, loss = 0.21634994\n",
      "Iteration 623, loss = 0.21609294\n",
      "Iteration 624, loss = 0.21583653\n",
      "Iteration 625, loss = 0.21558073\n",
      "Iteration 626, loss = 0.21532551\n",
      "Iteration 627, loss = 0.21507089\n",
      "Iteration 628, loss = 0.21481686\n",
      "Iteration 629, loss = 0.21456342\n",
      "Iteration 630, loss = 0.21431058\n",
      "Iteration 631, loss = 0.21405831\n",
      "Iteration 632, loss = 0.21380665\n",
      "Iteration 633, loss = 0.21355555\n",
      "Iteration 634, loss = 0.21330505\n",
      "Iteration 635, loss = 0.21305511\n",
      "Iteration 636, loss = 0.21280576\n",
      "Iteration 637, loss = 0.21255700\n",
      "Iteration 638, loss = 0.21230883\n",
      "Iteration 639, loss = 0.21206124\n",
      "Iteration 640, loss = 0.21181420\n",
      "Iteration 641, loss = 0.21156775\n",
      "Iteration 642, loss = 0.21132186\n",
      "Iteration 643, loss = 0.21107654\n",
      "Iteration 644, loss = 0.21083178\n",
      "Iteration 645, loss = 0.21058759\n",
      "Iteration 646, loss = 0.21034396\n",
      "Iteration 647, loss = 0.21010090\n",
      "Iteration 648, loss = 0.20985841\n",
      "Iteration 649, loss = 0.20961648\n",
      "Iteration 650, loss = 0.20937510\n",
      "Iteration 651, loss = 0.20913428\n",
      "Iteration 652, loss = 0.20889401\n",
      "Iteration 653, loss = 0.20865430\n",
      "Iteration 654, loss = 0.20841513\n",
      "Iteration 655, loss = 0.20817651\n",
      "Iteration 656, loss = 0.20793844\n",
      "Iteration 657, loss = 0.20770091\n",
      "Iteration 658, loss = 0.20746392\n",
      "Iteration 659, loss = 0.20722748\n",
      "Iteration 660, loss = 0.20699159\n",
      "Iteration 661, loss = 0.20675624\n",
      "Iteration 662, loss = 0.20652142\n",
      "Iteration 663, loss = 0.20628714\n",
      "Iteration 664, loss = 0.20605340\n",
      "Iteration 665, loss = 0.20582019\n",
      "Iteration 666, loss = 0.20558751\n",
      "Iteration 667, loss = 0.20535536\n",
      "Iteration 668, loss = 0.20512375\n",
      "Iteration 669, loss = 0.20489265\n",
      "Iteration 670, loss = 0.20466208\n",
      "Iteration 671, loss = 0.20443204\n",
      "Iteration 672, loss = 0.20420253\n",
      "Iteration 673, loss = 0.20397354\n",
      "Iteration 674, loss = 0.20374507\n",
      "Iteration 675, loss = 0.20351711\n",
      "Iteration 676, loss = 0.20328967\n",
      "Iteration 677, loss = 0.20306275\n",
      "Iteration 678, loss = 0.20283635\n",
      "Iteration 679, loss = 0.20261045\n",
      "Iteration 680, loss = 0.20238507\n",
      "Iteration 681, loss = 0.20216020\n",
      "Iteration 682, loss = 0.20193583\n",
      "Iteration 683, loss = 0.20171198\n",
      "Iteration 684, loss = 0.20148863\n",
      "Iteration 685, loss = 0.20126578\n",
      "Iteration 686, loss = 0.20104344\n",
      "Iteration 687, loss = 0.20082160\n",
      "Iteration 688, loss = 0.20060026\n",
      "Iteration 689, loss = 0.20037942\n",
      "Iteration 690, loss = 0.20015907\n",
      "Iteration 691, loss = 0.19993923\n",
      "Iteration 692, loss = 0.19971988\n",
      "Iteration 693, loss = 0.19950102\n",
      "Iteration 694, loss = 0.19928265\n",
      "Iteration 695, loss = 0.19906478\n",
      "Iteration 696, loss = 0.19884740\n",
      "Iteration 697, loss = 0.19863050\n",
      "Iteration 698, loss = 0.19841410\n",
      "Iteration 699, loss = 0.19819817\n",
      "Iteration 700, loss = 0.19798274\n",
      "Iteration 701, loss = 0.19776779\n",
      "Iteration 702, loss = 0.19755332\n",
      "Iteration 703, loss = 0.19733933\n",
      "Iteration 704, loss = 0.19712582\n",
      "Iteration 705, loss = 0.19691279\n",
      "Iteration 706, loss = 0.19670024\n",
      "Iteration 707, loss = 0.19648816\n",
      "Iteration 708, loss = 0.19627656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 709, loss = 0.19606543\n",
      "Iteration 710, loss = 0.19585477\n",
      "Iteration 711, loss = 0.19564458\n",
      "Iteration 712, loss = 0.19543486\n",
      "Iteration 713, loss = 0.19522561\n",
      "Iteration 714, loss = 0.19501682\n",
      "Iteration 715, loss = 0.19480850\n",
      "Iteration 716, loss = 0.19460065\n",
      "Iteration 717, loss = 0.19439325\n",
      "Iteration 718, loss = 0.19418632\n",
      "Iteration 719, loss = 0.19397985\n",
      "Iteration 720, loss = 0.19377383\n",
      "Iteration 721, loss = 0.19356827\n",
      "Iteration 722, loss = 0.19336317\n",
      "Iteration 723, loss = 0.19315853\n",
      "Iteration 724, loss = 0.19295433\n",
      "Iteration 725, loss = 0.19275060\n",
      "Iteration 726, loss = 0.19254731\n",
      "Iteration 727, loss = 0.19234447\n",
      "Iteration 728, loss = 0.19214208\n",
      "Iteration 729, loss = 0.19194014\n",
      "Iteration 730, loss = 0.19173864\n",
      "Iteration 731, loss = 0.19153759\n",
      "Iteration 732, loss = 0.19133699\n",
      "Iteration 733, loss = 0.19113682\n",
      "Iteration 734, loss = 0.19093712\n",
      "Iteration 735, loss = 0.19073786\n",
      "Iteration 736, loss = 0.19053905\n",
      "Iteration 737, loss = 0.19034068\n",
      "Iteration 738, loss = 0.19014275\n",
      "Iteration 739, loss = 0.18994525\n",
      "Iteration 740, loss = 0.18974819\n",
      "Iteration 741, loss = 0.18955156\n",
      "Iteration 742, loss = 0.18935537\n",
      "Iteration 743, loss = 0.18915961\n",
      "Iteration 744, loss = 0.18896428\n",
      "Iteration 745, loss = 0.18876937\n",
      "Iteration 746, loss = 0.18857490\n",
      "Iteration 747, loss = 0.18838085\n",
      "Iteration 748, loss = 0.18818722\n",
      "Iteration 749, loss = 0.18799402\n",
      "Iteration 750, loss = 0.18780125\n",
      "Iteration 751, loss = 0.18760889\n",
      "Iteration 752, loss = 0.18741696\n",
      "Iteration 753, loss = 0.18722545\n",
      "Iteration 754, loss = 0.18703435\n",
      "Iteration 755, loss = 0.18684367\n",
      "Iteration 756, loss = 0.18665341\n",
      "Iteration 757, loss = 0.18646356\n",
      "Iteration 758, loss = 0.18627413\n",
      "Iteration 759, loss = 0.18608510\n",
      "Iteration 760, loss = 0.18589649\n",
      "Iteration 761, loss = 0.18570829\n",
      "Iteration 762, loss = 0.18552050\n",
      "Iteration 763, loss = 0.18533312\n",
      "Iteration 764, loss = 0.18514615\n",
      "Iteration 765, loss = 0.18495960\n",
      "Iteration 766, loss = 0.18477345\n",
      "Iteration 767, loss = 0.18458770\n",
      "Iteration 768, loss = 0.18440235\n",
      "Iteration 769, loss = 0.18421741\n",
      "Iteration 770, loss = 0.18403286\n",
      "Iteration 771, loss = 0.18384873\n",
      "Iteration 772, loss = 0.18366498\n",
      "Iteration 773, loss = 0.18348163\n",
      "Iteration 774, loss = 0.18329869\n",
      "Iteration 775, loss = 0.18311614\n",
      "Iteration 776, loss = 0.18293398\n",
      "Iteration 777, loss = 0.18275223\n",
      "Iteration 778, loss = 0.18257089\n",
      "Iteration 779, loss = 0.18238993\n",
      "Iteration 780, loss = 0.18220937\n",
      "Iteration 781, loss = 0.18202920\n",
      "Iteration 782, loss = 0.18184942\n",
      "Iteration 783, loss = 0.18167004\n",
      "Iteration 784, loss = 0.18149103\n",
      "Iteration 785, loss = 0.18131242\n",
      "Iteration 786, loss = 0.18113419\n",
      "Iteration 787, loss = 0.18095635\n",
      "Iteration 788, loss = 0.18077888\n",
      "Iteration 789, loss = 0.18060180\n",
      "Iteration 790, loss = 0.18042510\n",
      "Iteration 791, loss = 0.18024877\n",
      "Iteration 792, loss = 0.18007283\n",
      "Iteration 793, loss = 0.17989726\n",
      "Iteration 794, loss = 0.17972207\n",
      "Iteration 795, loss = 0.17954726\n",
      "Iteration 796, loss = 0.17937281\n",
      "Iteration 797, loss = 0.17919874\n",
      "Iteration 798, loss = 0.17902504\n",
      "Iteration 799, loss = 0.17885172\n",
      "Iteration 800, loss = 0.17867876\n",
      "Iteration 801, loss = 0.17850618\n",
      "Iteration 802, loss = 0.17833396\n",
      "Iteration 803, loss = 0.17816211\n",
      "Iteration 804, loss = 0.17799062\n",
      "Iteration 805, loss = 0.17781950\n",
      "Iteration 806, loss = 0.17764874\n",
      "Iteration 807, loss = 0.17747835\n",
      "Iteration 808, loss = 0.17730837\n",
      "Iteration 809, loss = 0.17713867\n",
      "Iteration 810, loss = 0.17696938\n",
      "Iteration 811, loss = 0.17680045\n",
      "Iteration 812, loss = 0.17663187\n",
      "Iteration 813, loss = 0.17646364\n",
      "Iteration 814, loss = 0.17629578\n",
      "Iteration 815, loss = 0.17612826\n",
      "Iteration 816, loss = 0.17596111\n",
      "Iteration 817, loss = 0.17579431\n",
      "Iteration 818, loss = 0.17562787\n",
      "Iteration 819, loss = 0.17546177\n",
      "Iteration 820, loss = 0.17529603\n",
      "Iteration 821, loss = 0.17513063\n",
      "Iteration 822, loss = 0.17496558\n",
      "Iteration 823, loss = 0.17480090\n",
      "Iteration 824, loss = 0.17463654\n",
      "Iteration 825, loss = 0.17447255\n",
      "Iteration 826, loss = 0.17430889\n",
      "Iteration 827, loss = 0.17414557\n",
      "Iteration 828, loss = 0.17398260\n",
      "Iteration 829, loss = 0.17382000\n",
      "Iteration 830, loss = 0.17365770\n",
      "Iteration 831, loss = 0.17349576\n",
      "Iteration 832, loss = 0.17333415\n",
      "Iteration 833, loss = 0.17317289\n",
      "Iteration 834, loss = 0.17301198\n",
      "Iteration 835, loss = 0.17285139\n",
      "Iteration 836, loss = 0.17269114\n",
      "Iteration 837, loss = 0.17253122\n",
      "Iteration 838, loss = 0.17237164\n",
      "Iteration 839, loss = 0.17221239\n",
      "Iteration 840, loss = 0.17205349\n",
      "Iteration 841, loss = 0.17189491\n",
      "Iteration 842, loss = 0.17173666\n",
      "Iteration 843, loss = 0.17157874\n",
      "Iteration 844, loss = 0.17142114\n",
      "Iteration 845, loss = 0.17126391\n",
      "Iteration 846, loss = 0.17110695\n",
      "Iteration 847, loss = 0.17095034\n",
      "Iteration 848, loss = 0.17079407\n",
      "Iteration 849, loss = 0.17063812\n",
      "Iteration 850, loss = 0.17048250\n",
      "Iteration 851, loss = 0.17032718\n",
      "Iteration 852, loss = 0.17017220\n",
      "Iteration 853, loss = 0.17001753\n",
      "Iteration 854, loss = 0.16986319\n",
      "Iteration 855, loss = 0.16970917\n",
      "Iteration 856, loss = 0.16955547\n",
      "Iteration 857, loss = 0.16940208\n",
      "Iteration 858, loss = 0.16924902\n",
      "Iteration 859, loss = 0.16909627\n",
      "Iteration 860, loss = 0.16894384\n",
      "Iteration 861, loss = 0.16879172\n",
      "Iteration 862, loss = 0.16863991\n",
      "Iteration 863, loss = 0.16848843\n",
      "Iteration 864, loss = 0.16833725\n",
      "Iteration 865, loss = 0.16818638\n",
      "Iteration 866, loss = 0.16803582\n",
      "Iteration 867, loss = 0.16788559\n",
      "Iteration 868, loss = 0.16773564\n",
      "Iteration 869, loss = 0.16758601\n",
      "Iteration 870, loss = 0.16743669\n",
      "Iteration 871, loss = 0.16728769\n",
      "Iteration 872, loss = 0.16713900\n",
      "Iteration 873, loss = 0.16699060\n",
      "Iteration 874, loss = 0.16684251\n",
      "Iteration 875, loss = 0.16669475\n",
      "Iteration 876, loss = 0.16654727\n",
      "Iteration 877, loss = 0.16640009\n",
      "Iteration 878, loss = 0.16625323\n",
      "Iteration 879, loss = 0.16610667\n",
      "Iteration 880, loss = 0.16596040\n",
      "Iteration 881, loss = 0.16581442\n",
      "Iteration 882, loss = 0.16566875\n",
      "Iteration 883, loss = 0.16552339\n",
      "Iteration 884, loss = 0.16537831\n",
      "Iteration 885, loss = 0.16523352\n",
      "Iteration 886, loss = 0.16508904\n",
      "Iteration 887, loss = 0.16494486\n",
      "Iteration 888, loss = 0.16480096\n",
      "Iteration 889, loss = 0.16465735\n",
      "Iteration 890, loss = 0.16451406\n",
      "Iteration 891, loss = 0.16437102\n",
      "Iteration 892, loss = 0.16422829\n",
      "Iteration 893, loss = 0.16408586\n",
      "Iteration 894, loss = 0.16394371\n",
      "Iteration 895, loss = 0.16380185\n",
      "Iteration 896, loss = 0.16366027\n",
      "Iteration 897, loss = 0.16351899\n",
      "Iteration 898, loss = 0.16337798\n",
      "Iteration 899, loss = 0.16323726\n",
      "Iteration 900, loss = 0.16309684\n",
      "Iteration 901, loss = 0.16295668\n",
      "Iteration 902, loss = 0.16281681\n",
      "Iteration 903, loss = 0.16267724\n",
      "Iteration 904, loss = 0.16253794\n",
      "Iteration 905, loss = 0.16239892\n",
      "Iteration 906, loss = 0.16226017\n",
      "Iteration 907, loss = 0.16212172\n",
      "Iteration 908, loss = 0.16198354\n",
      "Iteration 909, loss = 0.16184563\n",
      "Iteration 910, loss = 0.16170800\n",
      "Iteration 911, loss = 0.16157065\n",
      "Iteration 912, loss = 0.16143357\n",
      "Iteration 913, loss = 0.16129679\n",
      "Iteration 914, loss = 0.16116025\n",
      "Iteration 915, loss = 0.16102399\n",
      "Iteration 916, loss = 0.16088803\n",
      "Iteration 917, loss = 0.16075232\n",
      "Iteration 918, loss = 0.16061688\n",
      "Iteration 919, loss = 0.16048172\n",
      "Iteration 920, loss = 0.16034682\n",
      "Iteration 921, loss = 0.16021219\n",
      "Iteration 922, loss = 0.16007784\n",
      "Iteration 923, loss = 0.15994376\n",
      "Iteration 924, loss = 0.15980993\n",
      "Iteration 925, loss = 0.15967638\n",
      "Iteration 926, loss = 0.15954309\n",
      "Iteration 927, loss = 0.15941006\n",
      "Iteration 928, loss = 0.15927731\n",
      "Iteration 929, loss = 0.15914482\n",
      "Iteration 930, loss = 0.15901258\n",
      "Iteration 931, loss = 0.15888062\n",
      "Iteration 932, loss = 0.15874891\n",
      "Iteration 933, loss = 0.15861745\n",
      "Iteration 934, loss = 0.15848629\n",
      "Iteration 935, loss = 0.15835535\n",
      "Iteration 936, loss = 0.15822468\n",
      "Iteration 937, loss = 0.15809428\n",
      "Iteration 938, loss = 0.15796413\n",
      "Iteration 939, loss = 0.15783423\n",
      "Iteration 940, loss = 0.15770461\n",
      "Iteration 941, loss = 0.15757521\n",
      "Iteration 942, loss = 0.15744610\n",
      "Iteration 943, loss = 0.15731722\n",
      "Iteration 944, loss = 0.15718860\n",
      "Iteration 945, loss = 0.15706024\n",
      "Iteration 946, loss = 0.15693212\n",
      "Iteration 947, loss = 0.15680426\n",
      "Iteration 948, loss = 0.15667667\n",
      "Iteration 949, loss = 0.15654930\n",
      "Iteration 950, loss = 0.15642218\n",
      "Iteration 951, loss = 0.15629534\n",
      "Iteration 952, loss = 0.15616871\n",
      "Iteration 953, loss = 0.15604237\n",
      "Iteration 954, loss = 0.15591625\n",
      "Iteration 955, loss = 0.15579037\n",
      "Iteration 956, loss = 0.15566475\n",
      "Iteration 957, loss = 0.15553937\n",
      "Iteration 958, loss = 0.15541424\n",
      "Iteration 959, loss = 0.15528936\n",
      "Iteration 960, loss = 0.15516471\n",
      "Iteration 961, loss = 0.15504031\n",
      "Iteration 962, loss = 0.15491615\n",
      "Iteration 963, loss = 0.15479223\n",
      "Iteration 964, loss = 0.15466856\n",
      "Iteration 965, loss = 0.15454512\n",
      "Iteration 966, loss = 0.15442193\n",
      "Iteration 967, loss = 0.15429896\n",
      "Iteration 968, loss = 0.15417624\n",
      "Iteration 969, loss = 0.15405377\n",
      "Iteration 970, loss = 0.15393152\n",
      "Iteration 971, loss = 0.15380952\n",
      "Iteration 972, loss = 0.15368774\n",
      "Iteration 973, loss = 0.15356621\n",
      "Iteration 974, loss = 0.15344492\n",
      "Iteration 975, loss = 0.15332384\n",
      "Iteration 976, loss = 0.15320302\n",
      "Iteration 977, loss = 0.15308241\n",
      "Iteration 978, loss = 0.15296205\n",
      "Iteration 979, loss = 0.15284192\n",
      "Iteration 980, loss = 0.15272201\n",
      "Iteration 981, loss = 0.15260235\n",
      "Iteration 982, loss = 0.15248290\n",
      "Iteration 983, loss = 0.15236368\n",
      "Iteration 984, loss = 0.15224471\n",
      "Iteration 985, loss = 0.15212594\n",
      "Iteration 986, loss = 0.15200743\n",
      "Iteration 987, loss = 0.15188912\n",
      "Iteration 988, loss = 0.15177105\n",
      "Iteration 989, loss = 0.15165320\n",
      "Iteration 990, loss = 0.15153559\n",
      "Iteration 991, loss = 0.15141818\n",
      "Iteration 992, loss = 0.15130101\n",
      "Iteration 993, loss = 0.15118407\n",
      "Iteration 994, loss = 0.15106734\n",
      "Iteration 995, loss = 0.15095085\n",
      "Iteration 996, loss = 0.15083455\n",
      "Iteration 997, loss = 0.15071851\n",
      "Iteration 998, loss = 0.15060267\n",
      "Iteration 999, loss = 0.15048704\n",
      "Iteration 1000, loss = 0.15037167\n",
      "Iteration 1, loss = 1.94163030\n",
      "Iteration 2, loss = 1.89615571\n",
      "Iteration 3, loss = 1.83491840\n",
      "Iteration 4, loss = 1.76269787\n",
      "Iteration 5, loss = 1.68374064\n",
      "Iteration 6, loss = 1.60151846\n",
      "Iteration 7, loss = 1.51868529\n",
      "Iteration 8, loss = 1.43754231\n",
      "Iteration 9, loss = 1.36019293\n",
      "Iteration 10, loss = 1.28884933\n",
      "Iteration 11, loss = 1.22563743\n",
      "Iteration 12, loss = 1.17265472\n",
      "Iteration 13, loss = 1.13133588\n",
      "Iteration 14, loss = 1.10190141\n",
      "Iteration 15, loss = 1.08323529\n",
      "Iteration 16, loss = 1.07296091\n",
      "Iteration 17, loss = 1.06786778\n",
      "Iteration 18, loss = 1.06490439\n",
      "Iteration 19, loss = 1.06156363\n",
      "Iteration 20, loss = 1.05607886\n",
      "Iteration 21, loss = 1.04776072\n",
      "Iteration 22, loss = 1.03668055\n",
      "Iteration 23, loss = 1.02355922\n",
      "Iteration 24, loss = 1.00930333\n",
      "Iteration 25, loss = 0.99479521\n",
      "Iteration 26, loss = 0.98069992\n",
      "Iteration 27, loss = 0.96745197\n",
      "Iteration 28, loss = 0.95512370\n",
      "Iteration 29, loss = 0.94352135\n",
      "Iteration 30, loss = 0.93244964\n",
      "Iteration 31, loss = 0.92164569\n",
      "Iteration 32, loss = 0.91085105\n",
      "Iteration 33, loss = 0.89976796\n",
      "Iteration 34, loss = 0.88853597\n",
      "Iteration 35, loss = 0.87710073\n",
      "Iteration 36, loss = 0.86548524\n",
      "Iteration 37, loss = 0.85402874\n",
      "Iteration 38, loss = 0.84303360\n",
      "Iteration 39, loss = 0.83266484\n",
      "Iteration 40, loss = 0.82297049\n",
      "Iteration 41, loss = 0.81397686\n",
      "Iteration 42, loss = 0.80570171\n",
      "Iteration 43, loss = 0.79812150\n",
      "Iteration 44, loss = 0.79104001\n",
      "Iteration 45, loss = 0.78413796\n",
      "Iteration 46, loss = 0.77736463\n",
      "Iteration 47, loss = 0.77069267\n",
      "Iteration 48, loss = 0.76403061\n",
      "Iteration 49, loss = 0.75741310\n",
      "Iteration 50, loss = 0.75091852\n",
      "Iteration 51, loss = 0.74456333\n",
      "Iteration 52, loss = 0.73833875\n",
      "Iteration 53, loss = 0.73230485\n",
      "Iteration 54, loss = 0.72655178\n",
      "Iteration 55, loss = 0.72102883\n",
      "Iteration 56, loss = 0.71573112\n",
      "Iteration 57, loss = 0.71058470\n",
      "Iteration 58, loss = 0.70556162\n",
      "Iteration 59, loss = 0.70068148\n",
      "Iteration 60, loss = 0.69593954\n",
      "Iteration 61, loss = 0.69129259\n",
      "Iteration 62, loss = 0.68676462\n",
      "Iteration 63, loss = 0.68237026\n",
      "Iteration 64, loss = 0.67809260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 65, loss = 0.67389940\n",
      "Iteration 66, loss = 0.66978567\n",
      "Iteration 67, loss = 0.66576086\n",
      "Iteration 68, loss = 0.66183934\n",
      "Iteration 69, loss = 0.65803048\n",
      "Iteration 70, loss = 0.65431298\n",
      "Iteration 71, loss = 0.65068158\n",
      "Iteration 72, loss = 0.64711803\n",
      "Iteration 73, loss = 0.64362480\n",
      "Iteration 74, loss = 0.64020002\n",
      "Iteration 75, loss = 0.63684732\n",
      "Iteration 76, loss = 0.63354541\n",
      "Iteration 77, loss = 0.63029195\n",
      "Iteration 78, loss = 0.62708300\n",
      "Iteration 79, loss = 0.62392033\n",
      "Iteration 80, loss = 0.62080603\n",
      "Iteration 81, loss = 0.61773708\n",
      "Iteration 82, loss = 0.61471217\n",
      "Iteration 83, loss = 0.61173627\n",
      "Iteration 84, loss = 0.60881572\n",
      "Iteration 85, loss = 0.60594529\n",
      "Iteration 86, loss = 0.60312722\n",
      "Iteration 87, loss = 0.60035312\n",
      "Iteration 88, loss = 0.59762250\n",
      "Iteration 89, loss = 0.59493122\n",
      "Iteration 90, loss = 0.59227550\n",
      "Iteration 91, loss = 0.58965434\n",
      "Iteration 92, loss = 0.58706904\n",
      "Iteration 93, loss = 0.58451610\n",
      "Iteration 94, loss = 0.58199618\n",
      "Iteration 95, loss = 0.57951014\n",
      "Iteration 96, loss = 0.57705561\n",
      "Iteration 97, loss = 0.57463309\n",
      "Iteration 98, loss = 0.57224524\n",
      "Iteration 99, loss = 0.56988892\n",
      "Iteration 100, loss = 0.56756299\n",
      "Iteration 101, loss = 0.56526748\n",
      "Iteration 102, loss = 0.56300204\n",
      "Iteration 103, loss = 0.56076538\n",
      "Iteration 104, loss = 0.55855733\n",
      "Iteration 105, loss = 0.55637709\n",
      "Iteration 106, loss = 0.55422396\n",
      "Iteration 107, loss = 0.55209836\n",
      "Iteration 108, loss = 0.54999957\n",
      "Iteration 109, loss = 0.54792744\n",
      "Iteration 110, loss = 0.54588101\n",
      "Iteration 111, loss = 0.54385998\n",
      "Iteration 112, loss = 0.54186367\n",
      "Iteration 113, loss = 0.53989175\n",
      "Iteration 114, loss = 0.53794388\n",
      "Iteration 115, loss = 0.53601941\n",
      "Iteration 116, loss = 0.53411792\n",
      "Iteration 117, loss = 0.53223899\n",
      "Iteration 118, loss = 0.53038220\n",
      "Iteration 119, loss = 0.52854713\n",
      "Iteration 120, loss = 0.52673337\n",
      "Iteration 121, loss = 0.52494053\n",
      "Iteration 122, loss = 0.52316824\n",
      "Iteration 123, loss = 0.52141691\n",
      "Iteration 124, loss = 0.51968582\n",
      "Iteration 125, loss = 0.51797416\n",
      "Iteration 126, loss = 0.51628160\n",
      "Iteration 127, loss = 0.51460781\n",
      "Iteration 128, loss = 0.51295250\n",
      "Iteration 129, loss = 0.51131537\n",
      "Iteration 130, loss = 0.50969600\n",
      "Iteration 131, loss = 0.50809407\n",
      "Iteration 132, loss = 0.50650926\n",
      "Iteration 133, loss = 0.50494127\n",
      "Iteration 134, loss = 0.50339025\n",
      "Iteration 135, loss = 0.50185568\n",
      "Iteration 136, loss = 0.50033689\n",
      "Iteration 137, loss = 0.49883374\n",
      "Iteration 138, loss = 0.49734587\n",
      "Iteration 139, loss = 0.49587303\n",
      "Iteration 140, loss = 0.49441494\n",
      "Iteration 141, loss = 0.49297134\n",
      "Iteration 142, loss = 0.49154195\n",
      "Iteration 143, loss = 0.49012651\n",
      "Iteration 144, loss = 0.48872484\n",
      "Iteration 145, loss = 0.48733668\n",
      "Iteration 146, loss = 0.48596173\n",
      "Iteration 147, loss = 0.48459975\n",
      "Iteration 148, loss = 0.48325049\n",
      "Iteration 149, loss = 0.48191375\n",
      "Iteration 150, loss = 0.48058930\n",
      "Iteration 151, loss = 0.47927691\n",
      "Iteration 152, loss = 0.47797657\n",
      "Iteration 153, loss = 0.47668798\n",
      "Iteration 154, loss = 0.47541204\n",
      "Iteration 155, loss = 0.47414807\n",
      "Iteration 156, loss = 0.47289525\n",
      "Iteration 157, loss = 0.47165339\n",
      "Iteration 158, loss = 0.47042232\n",
      "Iteration 159, loss = 0.46920185\n",
      "Iteration 160, loss = 0.46799196\n",
      "Iteration 161, loss = 0.46679282\n",
      "Iteration 162, loss = 0.46560427\n",
      "Iteration 163, loss = 0.46442570\n",
      "Iteration 164, loss = 0.46325680\n",
      "Iteration 165, loss = 0.46209746\n",
      "Iteration 166, loss = 0.46094764\n",
      "Iteration 167, loss = 0.45980744\n",
      "Iteration 168, loss = 0.45867643\n",
      "Iteration 169, loss = 0.45755510\n",
      "Iteration 170, loss = 0.45644322\n",
      "Iteration 171, loss = 0.45534003\n",
      "Iteration 172, loss = 0.45424557\n",
      "Iteration 173, loss = 0.45315969\n",
      "Iteration 174, loss = 0.45208231\n",
      "Iteration 175, loss = 0.45101319\n",
      "Iteration 176, loss = 0.44995279\n",
      "Iteration 177, loss = 0.44890097\n",
      "Iteration 178, loss = 0.44785731\n",
      "Iteration 179, loss = 0.44682145\n",
      "Iteration 180, loss = 0.44579336\n",
      "Iteration 181, loss = 0.44477290\n",
      "Iteration 182, loss = 0.44376007\n",
      "Iteration 183, loss = 0.44275451\n",
      "Iteration 184, loss = 0.44175629\n",
      "Iteration 185, loss = 0.44076545\n",
      "Iteration 186, loss = 0.43978169\n",
      "Iteration 187, loss = 0.43880491\n",
      "Iteration 188, loss = 0.43783500\n",
      "Iteration 189, loss = 0.43687184\n",
      "Iteration 190, loss = 0.43591533\n",
      "Iteration 191, loss = 0.43496538\n",
      "Iteration 192, loss = 0.43402188\n",
      "Iteration 193, loss = 0.43308474\n",
      "Iteration 194, loss = 0.43215387\n",
      "Iteration 195, loss = 0.43122918\n",
      "Iteration 196, loss = 0.43031062\n",
      "Iteration 197, loss = 0.42939804\n",
      "Iteration 198, loss = 0.42849145\n",
      "Iteration 199, loss = 0.42759065\n",
      "Iteration 200, loss = 0.42669565\n",
      "Iteration 201, loss = 0.42580630\n",
      "Iteration 202, loss = 0.42492262\n",
      "Iteration 203, loss = 0.42404441\n",
      "Iteration 204, loss = 0.42317230\n",
      "Iteration 205, loss = 0.42230545\n",
      "Iteration 206, loss = 0.42144382\n",
      "Iteration 207, loss = 0.42058783\n",
      "Iteration 208, loss = 0.41973664\n",
      "Iteration 209, loss = 0.41889063\n",
      "Iteration 210, loss = 0.41804987\n",
      "Iteration 211, loss = 0.41721443\n",
      "Iteration 212, loss = 0.41638387\n",
      "Iteration 213, loss = 0.41555815\n",
      "Iteration 214, loss = 0.41473740\n",
      "Iteration 215, loss = 0.41392122\n",
      "Iteration 216, loss = 0.41310986\n",
      "Iteration 217, loss = 0.41230313\n",
      "Iteration 218, loss = 0.41150138\n",
      "Iteration 219, loss = 0.41070441\n",
      "Iteration 220, loss = 0.40991201\n",
      "Iteration 221, loss = 0.40912408\n",
      "Iteration 222, loss = 0.40834053\n",
      "Iteration 223, loss = 0.40756122\n",
      "Iteration 224, loss = 0.40678618\n",
      "Iteration 225, loss = 0.40601531\n",
      "Iteration 226, loss = 0.40524862\n",
      "Iteration 227, loss = 0.40448610\n",
      "Iteration 228, loss = 0.40372775\n",
      "Iteration 229, loss = 0.40297337\n",
      "Iteration 230, loss = 0.40222294\n",
      "Iteration 231, loss = 0.40147646\n",
      "Iteration 232, loss = 0.40073370\n",
      "Iteration 233, loss = 0.39999479\n",
      "Iteration 234, loss = 0.39925957\n",
      "Iteration 235, loss = 0.39852804\n",
      "Iteration 236, loss = 0.39780018\n",
      "Iteration 237, loss = 0.39707588\n",
      "Iteration 238, loss = 0.39635515\n",
      "Iteration 239, loss = 0.39563793\n",
      "Iteration 240, loss = 0.39492417\n",
      "Iteration 241, loss = 0.39421387\n",
      "Iteration 242, loss = 0.39350691\n",
      "Iteration 243, loss = 0.39280333\n",
      "Iteration 244, loss = 0.39210305\n",
      "Iteration 245, loss = 0.39140603\n",
      "Iteration 246, loss = 0.39071249\n",
      "Iteration 247, loss = 0.39002200\n",
      "Iteration 248, loss = 0.38933491\n",
      "Iteration 249, loss = 0.38865103\n",
      "Iteration 250, loss = 0.38797033\n",
      "Iteration 251, loss = 0.38729273\n",
      "Iteration 252, loss = 0.38661824\n",
      "Iteration 253, loss = 0.38594668\n",
      "Iteration 254, loss = 0.38527812\n",
      "Iteration 255, loss = 0.38461257\n",
      "Iteration 256, loss = 0.38394997\n",
      "Iteration 257, loss = 0.38329021\n",
      "Iteration 258, loss = 0.38263336\n",
      "Iteration 259, loss = 0.38197929\n",
      "Iteration 260, loss = 0.38132798\n",
      "Iteration 261, loss = 0.38067949\n",
      "Iteration 262, loss = 0.38003364\n",
      "Iteration 263, loss = 0.37939061\n",
      "Iteration 264, loss = 0.37875015\n",
      "Iteration 265, loss = 0.37811236\n",
      "Iteration 266, loss = 0.37747717\n",
      "Iteration 267, loss = 0.37684461\n",
      "Iteration 268, loss = 0.37621458\n",
      "Iteration 269, loss = 0.37558719\n",
      "Iteration 270, loss = 0.37496230\n",
      "Iteration 271, loss = 0.37433990\n",
      "Iteration 272, loss = 0.37371996\n",
      "Iteration 273, loss = 0.37310246\n",
      "Iteration 274, loss = 0.37248740\n",
      "Iteration 275, loss = 0.37187469\n",
      "Iteration 276, loss = 0.37126440\n",
      "Iteration 277, loss = 0.37065644\n",
      "Iteration 278, loss = 0.37005085\n",
      "Iteration 279, loss = 0.36944757\n",
      "Iteration 280, loss = 0.36884654\n",
      "Iteration 281, loss = 0.36824785\n",
      "Iteration 282, loss = 0.36765200\n",
      "Iteration 283, loss = 0.36705841\n",
      "Iteration 284, loss = 0.36646722\n",
      "Iteration 285, loss = 0.36587835\n",
      "Iteration 286, loss = 0.36529174\n",
      "Iteration 287, loss = 0.36470754\n",
      "Iteration 288, loss = 0.36412577\n",
      "Iteration 289, loss = 0.36354620\n",
      "Iteration 290, loss = 0.36296886\n",
      "Iteration 291, loss = 0.36239361\n",
      "Iteration 292, loss = 0.36182052\n",
      "Iteration 293, loss = 0.36124957\n",
      "Iteration 294, loss = 0.36068067\n",
      "Iteration 295, loss = 0.36011416\n",
      "Iteration 296, loss = 0.35954955\n",
      "Iteration 297, loss = 0.35898685\n",
      "Iteration 298, loss = 0.35842610\n",
      "Iteration 299, loss = 0.35786733\n",
      "Iteration 300, loss = 0.35731059\n",
      "Iteration 301, loss = 0.35675588\n",
      "Iteration 302, loss = 0.35620303\n",
      "Iteration 303, loss = 0.35565224\n",
      "Iteration 304, loss = 0.35510321\n",
      "Iteration 305, loss = 0.35455611\n",
      "Iteration 306, loss = 0.35401084\n",
      "Iteration 307, loss = 0.35346749\n",
      "Iteration 308, loss = 0.35292593\n",
      "Iteration 309, loss = 0.35238621\n",
      "Iteration 310, loss = 0.35184830\n",
      "Iteration 311, loss = 0.35131218\n",
      "Iteration 312, loss = 0.35077790\n",
      "Iteration 313, loss = 0.35024534\n",
      "Iteration 314, loss = 0.34971453\n",
      "Iteration 315, loss = 0.34918547\n",
      "Iteration 316, loss = 0.34865828\n",
      "Iteration 317, loss = 0.34813306\n",
      "Iteration 318, loss = 0.34760958\n",
      "Iteration 319, loss = 0.34708787\n",
      "Iteration 320, loss = 0.34656814\n",
      "Iteration 321, loss = 0.34605004\n",
      "Iteration 322, loss = 0.34553366\n",
      "Iteration 323, loss = 0.34501913\n",
      "Iteration 324, loss = 0.34450633\n",
      "Iteration 325, loss = 0.34399549\n",
      "Iteration 326, loss = 0.34348629\n",
      "Iteration 327, loss = 0.34297872\n",
      "Iteration 328, loss = 0.34247298\n",
      "Iteration 329, loss = 0.34196888\n",
      "Iteration 330, loss = 0.34146633\n",
      "Iteration 331, loss = 0.34096542\n",
      "Iteration 332, loss = 0.34046610\n",
      "Iteration 333, loss = 0.33996832\n",
      "Iteration 334, loss = 0.33947210\n",
      "Iteration 335, loss = 0.33897763\n",
      "Iteration 336, loss = 0.33848469\n",
      "Iteration 337, loss = 0.33799335\n",
      "Iteration 338, loss = 0.33750371\n",
      "Iteration 339, loss = 0.33701555\n",
      "Iteration 340, loss = 0.33652887\n",
      "Iteration 341, loss = 0.33604375\n",
      "Iteration 342, loss = 0.33556007\n",
      "Iteration 343, loss = 0.33507786\n",
      "Iteration 344, loss = 0.33459726\n",
      "Iteration 345, loss = 0.33411820\n",
      "Iteration 346, loss = 0.33364056\n",
      "Iteration 347, loss = 0.33316436\n",
      "Iteration 348, loss = 0.33268959\n",
      "Iteration 349, loss = 0.33221637\n",
      "Iteration 350, loss = 0.33174451\n",
      "Iteration 351, loss = 0.33127407\n",
      "Iteration 352, loss = 0.33080502\n",
      "Iteration 353, loss = 0.33033734\n",
      "Iteration 354, loss = 0.32987101\n",
      "Iteration 355, loss = 0.32940603\n",
      "Iteration 356, loss = 0.32894239\n",
      "Iteration 357, loss = 0.32848010\n",
      "Iteration 358, loss = 0.32801944\n",
      "Iteration 359, loss = 0.32755994\n",
      "Iteration 360, loss = 0.32710188\n",
      "Iteration 361, loss = 0.32664512\n",
      "Iteration 362, loss = 0.32618967\n",
      "Iteration 363, loss = 0.32573550\n",
      "Iteration 364, loss = 0.32528267\n",
      "Iteration 365, loss = 0.32483107\n",
      "Iteration 366, loss = 0.32438076\n",
      "Iteration 367, loss = 0.32393171\n",
      "Iteration 368, loss = 0.32348410\n",
      "Iteration 369, loss = 0.32303773\n",
      "Iteration 370, loss = 0.32259261\n",
      "Iteration 371, loss = 0.32214872\n",
      "Iteration 372, loss = 0.32170607\n",
      "Iteration 373, loss = 0.32126480\n",
      "Iteration 374, loss = 0.32082488\n",
      "Iteration 375, loss = 0.32038606\n",
      "Iteration 376, loss = 0.31994854\n",
      "Iteration 377, loss = 0.31951217\n",
      "Iteration 378, loss = 0.31907703\n",
      "Iteration 379, loss = 0.31864307\n",
      "Iteration 380, loss = 0.31821031\n",
      "Iteration 381, loss = 0.31777870\n",
      "Iteration 382, loss = 0.31734825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 383, loss = 0.31691899\n",
      "Iteration 384, loss = 0.31649086\n",
      "Iteration 385, loss = 0.31606392\n",
      "Iteration 386, loss = 0.31563813\n",
      "Iteration 387, loss = 0.31521344\n",
      "Iteration 388, loss = 0.31478990\n",
      "Iteration 389, loss = 0.31436754\n",
      "Iteration 390, loss = 0.31394625\n",
      "Iteration 391, loss = 0.31352608\n",
      "Iteration 392, loss = 0.31310717\n",
      "Iteration 393, loss = 0.31268936\n",
      "Iteration 394, loss = 0.31227270\n",
      "Iteration 395, loss = 0.31185721\n",
      "Iteration 396, loss = 0.31144294\n",
      "Iteration 397, loss = 0.31102976\n",
      "Iteration 398, loss = 0.31061779\n",
      "Iteration 399, loss = 0.31020675\n",
      "Iteration 400, loss = 0.30979690\n",
      "Iteration 401, loss = 0.30938809\n",
      "Iteration 402, loss = 0.30898033\n",
      "Iteration 403, loss = 0.30857371\n",
      "Iteration 404, loss = 0.30816806\n",
      "Iteration 405, loss = 0.30776357\n",
      "Iteration 406, loss = 0.30736014\n",
      "Iteration 407, loss = 0.30695779\n",
      "Iteration 408, loss = 0.30655657\n",
      "Iteration 409, loss = 0.30615631\n",
      "Iteration 410, loss = 0.30575712\n",
      "Iteration 411, loss = 0.30535894\n",
      "Iteration 412, loss = 0.30496182\n",
      "Iteration 413, loss = 0.30456568\n",
      "Iteration 414, loss = 0.30417063\n",
      "Iteration 415, loss = 0.30377650\n",
      "Iteration 416, loss = 0.30338344\n",
      "Iteration 417, loss = 0.30299135\n",
      "Iteration 418, loss = 0.30260026\n",
      "Iteration 419, loss = 0.30221019\n",
      "Iteration 420, loss = 0.30182106\n",
      "Iteration 421, loss = 0.30143298\n",
      "Iteration 422, loss = 0.30104583\n",
      "Iteration 423, loss = 0.30065970\n",
      "Iteration 424, loss = 0.30027459\n",
      "Iteration 425, loss = 0.29989054\n",
      "Iteration 426, loss = 0.29950748\n",
      "Iteration 427, loss = 0.29912547\n",
      "Iteration 428, loss = 0.29874439\n",
      "Iteration 429, loss = 0.29836432\n",
      "Iteration 430, loss = 0.29798515\n",
      "Iteration 431, loss = 0.29760700\n",
      "Iteration 432, loss = 0.29722975\n",
      "Iteration 433, loss = 0.29685345\n",
      "Iteration 434, loss = 0.29647810\n",
      "Iteration 435, loss = 0.29610369\n",
      "Iteration 436, loss = 0.29573021\n",
      "Iteration 437, loss = 0.29535765\n",
      "Iteration 438, loss = 0.29498604\n",
      "Iteration 439, loss = 0.29461532\n",
      "Iteration 440, loss = 0.29424556\n",
      "Iteration 441, loss = 0.29387666\n",
      "Iteration 442, loss = 0.29350871\n",
      "Iteration 443, loss = 0.29314162\n",
      "Iteration 444, loss = 0.29277551\n",
      "Iteration 445, loss = 0.29241025\n",
      "Iteration 446, loss = 0.29204590\n",
      "Iteration 447, loss = 0.29168242\n",
      "Iteration 448, loss = 0.29131985\n",
      "Iteration 449, loss = 0.29095818\n",
      "Iteration 450, loss = 0.29059736\n",
      "Iteration 451, loss = 0.29023745\n",
      "Iteration 452, loss = 0.28987840\n",
      "Iteration 453, loss = 0.28952023\n",
      "Iteration 454, loss = 0.28916294\n",
      "Iteration 455, loss = 0.28880650\n",
      "Iteration 456, loss = 0.28845096\n",
      "Iteration 457, loss = 0.28809626\n",
      "Iteration 458, loss = 0.28774243\n",
      "Iteration 459, loss = 0.28738947\n",
      "Iteration 460, loss = 0.28703734\n",
      "Iteration 461, loss = 0.28668610\n",
      "Iteration 462, loss = 0.28633566\n",
      "Iteration 463, loss = 0.28598609\n",
      "Iteration 464, loss = 0.28563738\n",
      "Iteration 465, loss = 0.28528949\n",
      "Iteration 466, loss = 0.28494243\n",
      "Iteration 467, loss = 0.28459623\n",
      "Iteration 468, loss = 0.28425084\n",
      "Iteration 469, loss = 0.28390630\n",
      "Iteration 470, loss = 0.28356257\n",
      "Iteration 471, loss = 0.28321966\n",
      "Iteration 472, loss = 0.28287762\n",
      "Iteration 473, loss = 0.28253636\n",
      "Iteration 474, loss = 0.28219590\n",
      "Iteration 475, loss = 0.28185631\n",
      "Iteration 476, loss = 0.28151747\n",
      "Iteration 477, loss = 0.28117949\n",
      "Iteration 478, loss = 0.28084232\n",
      "Iteration 479, loss = 0.28050595\n",
      "Iteration 480, loss = 0.28017039\n",
      "Iteration 481, loss = 0.27983565\n",
      "Iteration 482, loss = 0.27950170\n",
      "Iteration 483, loss = 0.27916854\n",
      "Iteration 484, loss = 0.27883625\n",
      "Iteration 485, loss = 0.27850465\n",
      "Iteration 486, loss = 0.27817393\n",
      "Iteration 487, loss = 0.27784396\n",
      "Iteration 488, loss = 0.27751478\n",
      "Iteration 489, loss = 0.27718635\n",
      "Iteration 490, loss = 0.27685872\n",
      "Iteration 491, loss = 0.27653188\n",
      "Iteration 492, loss = 0.27620580\n",
      "Iteration 493, loss = 0.27588052\n",
      "Iteration 494, loss = 0.27555599\n",
      "Iteration 495, loss = 0.27523224\n",
      "Iteration 496, loss = 0.27490928\n",
      "Iteration 497, loss = 0.27458706\n",
      "Iteration 498, loss = 0.27426560\n",
      "Iteration 499, loss = 0.27394490\n",
      "Iteration 500, loss = 0.27362499\n",
      "Iteration 501, loss = 0.27330580\n",
      "Iteration 502, loss = 0.27298736\n",
      "Iteration 503, loss = 0.27266968\n",
      "Iteration 504, loss = 0.27235276\n",
      "Iteration 505, loss = 0.27203660\n",
      "Iteration 506, loss = 0.27172116\n",
      "Iteration 507, loss = 0.27140652\n",
      "Iteration 508, loss = 0.27109255\n",
      "Iteration 509, loss = 0.27077935\n",
      "Iteration 510, loss = 0.27046692\n",
      "Iteration 511, loss = 0.27015519\n",
      "Iteration 512, loss = 0.26984419\n",
      "Iteration 513, loss = 0.26953392\n",
      "Iteration 514, loss = 0.26922437\n",
      "Iteration 515, loss = 0.26891558\n",
      "Iteration 516, loss = 0.26860750\n",
      "Iteration 517, loss = 0.26830021\n",
      "Iteration 518, loss = 0.26799371\n",
      "Iteration 519, loss = 0.26768790\n",
      "Iteration 520, loss = 0.26738290\n",
      "Iteration 521, loss = 0.26707863\n",
      "Iteration 522, loss = 0.26677505\n",
      "Iteration 523, loss = 0.26647220\n",
      "Iteration 524, loss = 0.26617005\n",
      "Iteration 525, loss = 0.26586861\n",
      "Iteration 526, loss = 0.26556791\n",
      "Iteration 527, loss = 0.26526790\n",
      "Iteration 528, loss = 0.26496862\n",
      "Iteration 529, loss = 0.26466999\n",
      "Iteration 530, loss = 0.26437208\n",
      "Iteration 531, loss = 0.26407490\n",
      "Iteration 532, loss = 0.26377837\n",
      "Iteration 533, loss = 0.26348254\n",
      "Iteration 534, loss = 0.26318740\n",
      "Iteration 535, loss = 0.26289299\n",
      "Iteration 536, loss = 0.26259925\n",
      "Iteration 537, loss = 0.26230623\n",
      "Iteration 538, loss = 0.26201394\n",
      "Iteration 539, loss = 0.26172236\n",
      "Iteration 540, loss = 0.26143148\n",
      "Iteration 541, loss = 0.26114133\n",
      "Iteration 542, loss = 0.26085187\n",
      "Iteration 543, loss = 0.26056309\n",
      "Iteration 544, loss = 0.26027497\n",
      "Iteration 545, loss = 0.25998753\n",
      "Iteration 546, loss = 0.25970077\n",
      "Iteration 547, loss = 0.25941467\n",
      "Iteration 548, loss = 0.25912923\n",
      "Iteration 549, loss = 0.25884447\n",
      "Iteration 550, loss = 0.25856036\n",
      "Iteration 551, loss = 0.25827691\n",
      "Iteration 552, loss = 0.25799411\n",
      "Iteration 553, loss = 0.25771196\n",
      "Iteration 554, loss = 0.25743050\n",
      "Iteration 555, loss = 0.25714965\n",
      "Iteration 556, loss = 0.25686946\n",
      "Iteration 557, loss = 0.25658991\n",
      "Iteration 558, loss = 0.25631103\n",
      "Iteration 559, loss = 0.25603278\n",
      "Iteration 560, loss = 0.25575529\n",
      "Iteration 561, loss = 0.25547848\n",
      "Iteration 562, loss = 0.25520232\n",
      "Iteration 563, loss = 0.25492681\n",
      "Iteration 564, loss = 0.25465193\n",
      "Iteration 565, loss = 0.25437768\n",
      "Iteration 566, loss = 0.25410410\n",
      "Iteration 567, loss = 0.25383111\n",
      "Iteration 568, loss = 0.25355876\n",
      "Iteration 569, loss = 0.25328705\n",
      "Iteration 570, loss = 0.25301597\n",
      "Iteration 571, loss = 0.25274553\n",
      "Iteration 572, loss = 0.25247566\n",
      "Iteration 573, loss = 0.25220643\n",
      "Iteration 574, loss = 0.25193783\n",
      "Iteration 575, loss = 0.25166984\n",
      "Iteration 576, loss = 0.25140246\n",
      "Iteration 577, loss = 0.25113569\n",
      "Iteration 578, loss = 0.25086956\n",
      "Iteration 579, loss = 0.25060400\n",
      "Iteration 580, loss = 0.25033908\n",
      "Iteration 581, loss = 0.25007473\n",
      "Iteration 582, loss = 0.24981102\n",
      "Iteration 583, loss = 0.24954789\n",
      "Iteration 584, loss = 0.24928536\n",
      "Iteration 585, loss = 0.24902343\n",
      "Iteration 586, loss = 0.24876213\n",
      "Iteration 587, loss = 0.24850141\n",
      "Iteration 588, loss = 0.24824128\n",
      "Iteration 589, loss = 0.24798176\n",
      "Iteration 590, loss = 0.24772283\n",
      "Iteration 591, loss = 0.24746448\n",
      "Iteration 592, loss = 0.24720672\n",
      "Iteration 593, loss = 0.24694955\n",
      "Iteration 594, loss = 0.24669295\n",
      "Iteration 595, loss = 0.24643696\n",
      "Iteration 596, loss = 0.24618154\n",
      "Iteration 597, loss = 0.24592671\n",
      "Iteration 598, loss = 0.24567249\n",
      "Iteration 599, loss = 0.24541886\n",
      "Iteration 600, loss = 0.24516579\n",
      "Iteration 601, loss = 0.24491330\n",
      "Iteration 602, loss = 0.24466138\n",
      "Iteration 603, loss = 0.24441006\n",
      "Iteration 604, loss = 0.24415928\n",
      "Iteration 605, loss = 0.24390908\n",
      "Iteration 606, loss = 0.24365944\n",
      "Iteration 607, loss = 0.24341040\n",
      "Iteration 608, loss = 0.24316188\n",
      "Iteration 609, loss = 0.24291394\n",
      "Iteration 610, loss = 0.24266656\n",
      "Iteration 611, loss = 0.24241975\n",
      "Iteration 612, loss = 0.24217354\n",
      "Iteration 613, loss = 0.24192787\n",
      "Iteration 614, loss = 0.24168278\n",
      "Iteration 615, loss = 0.24143826\n",
      "Iteration 616, loss = 0.24119432\n",
      "Iteration 617, loss = 0.24095089\n",
      "Iteration 618, loss = 0.24070804\n",
      "Iteration 619, loss = 0.24046572\n",
      "Iteration 620, loss = 0.24022398\n",
      "Iteration 621, loss = 0.23998276\n",
      "Iteration 622, loss = 0.23974209\n",
      "Iteration 623, loss = 0.23950196\n",
      "Iteration 624, loss = 0.23926236\n",
      "Iteration 625, loss = 0.23902335\n",
      "Iteration 626, loss = 0.23878481\n",
      "Iteration 627, loss = 0.23854684\n",
      "Iteration 628, loss = 0.23830942\n",
      "Iteration 629, loss = 0.23807251\n",
      "Iteration 630, loss = 0.23783614\n",
      "Iteration 631, loss = 0.23760031\n",
      "Iteration 632, loss = 0.23736499\n",
      "Iteration 633, loss = 0.23713023\n",
      "Iteration 634, loss = 0.23689596\n",
      "Iteration 635, loss = 0.23666223\n",
      "Iteration 636, loss = 0.23642903\n",
      "Iteration 637, loss = 0.23619637\n",
      "Iteration 638, loss = 0.23596421\n",
      "Iteration 639, loss = 0.23573257\n",
      "Iteration 640, loss = 0.23550144\n",
      "Iteration 641, loss = 0.23527085\n",
      "Iteration 642, loss = 0.23504076\n",
      "Iteration 643, loss = 0.23481120\n",
      "Iteration 644, loss = 0.23458213\n",
      "Iteration 645, loss = 0.23435358\n",
      "Iteration 646, loss = 0.23412557\n",
      "Iteration 647, loss = 0.23389802\n",
      "Iteration 648, loss = 0.23367101\n",
      "Iteration 649, loss = 0.23344451\n",
      "Iteration 650, loss = 0.23321851\n",
      "Iteration 651, loss = 0.23299300\n",
      "Iteration 652, loss = 0.23276800\n",
      "Iteration 653, loss = 0.23254350\n",
      "Iteration 654, loss = 0.23231951\n",
      "Iteration 655, loss = 0.23209603\n",
      "Iteration 656, loss = 0.23187304\n",
      "Iteration 657, loss = 0.23165055\n",
      "Iteration 658, loss = 0.23142855\n",
      "Iteration 659, loss = 0.23120706\n",
      "Iteration 660, loss = 0.23098606\n",
      "Iteration 661, loss = 0.23076554\n",
      "Iteration 662, loss = 0.23054550\n",
      "Iteration 663, loss = 0.23032599\n",
      "Iteration 664, loss = 0.23010693\n",
      "Iteration 665, loss = 0.22988837\n",
      "Iteration 666, loss = 0.22967030\n",
      "Iteration 667, loss = 0.22945271\n",
      "Iteration 668, loss = 0.22923561\n",
      "Iteration 669, loss = 0.22901898\n",
      "Iteration 670, loss = 0.22880284\n",
      "Iteration 671, loss = 0.22858718\n",
      "Iteration 672, loss = 0.22837200\n",
      "Iteration 673, loss = 0.22815730\n",
      "Iteration 674, loss = 0.22794306\n",
      "Iteration 675, loss = 0.22772932\n",
      "Iteration 676, loss = 0.22751603\n",
      "Iteration 677, loss = 0.22730322\n",
      "Iteration 678, loss = 0.22709088\n",
      "Iteration 679, loss = 0.22687902\n",
      "Iteration 680, loss = 0.22666762\n",
      "Iteration 681, loss = 0.22645669\n",
      "Iteration 682, loss = 0.22624622\n",
      "Iteration 683, loss = 0.22603621\n",
      "Iteration 684, loss = 0.22582668\n",
      "Iteration 685, loss = 0.22561760\n",
      "Iteration 686, loss = 0.22540900\n",
      "Iteration 687, loss = 0.22520083\n",
      "Iteration 688, loss = 0.22499315\n",
      "Iteration 689, loss = 0.22478590\n",
      "Iteration 690, loss = 0.22457912\n",
      "Iteration 691, loss = 0.22437280\n",
      "Iteration 692, loss = 0.22416693\n",
      "Iteration 693, loss = 0.22396150\n",
      "Iteration 694, loss = 0.22375654\n",
      "Iteration 695, loss = 0.22355202\n",
      "Iteration 696, loss = 0.22334796\n",
      "Iteration 697, loss = 0.22314433\n",
      "Iteration 698, loss = 0.22294117\n",
      "Iteration 699, loss = 0.22273844\n",
      "Iteration 700, loss = 0.22253619\n",
      "Iteration 701, loss = 0.22233437\n",
      "Iteration 702, loss = 0.22213300\n",
      "Iteration 703, loss = 0.22193206\n",
      "Iteration 704, loss = 0.22173158\n",
      "Iteration 705, loss = 0.22153152\n",
      "Iteration 706, loss = 0.22133192\n",
      "Iteration 707, loss = 0.22113273\n",
      "Iteration 708, loss = 0.22093401\n",
      "Iteration 709, loss = 0.22073570\n",
      "Iteration 710, loss = 0.22053783\n",
      "Iteration 711, loss = 0.22034040\n",
      "Iteration 712, loss = 0.22014340\n",
      "Iteration 713, loss = 0.21994684\n",
      "Iteration 714, loss = 0.21975069\n",
      "Iteration 715, loss = 0.21955497\n",
      "Iteration 716, loss = 0.21935969\n",
      "Iteration 717, loss = 0.21916483\n",
      "Iteration 718, loss = 0.21897040\n",
      "Iteration 719, loss = 0.21877638\n",
      "Iteration 720, loss = 0.21858280\n",
      "Iteration 721, loss = 0.21838964\n",
      "Iteration 722, loss = 0.21819690\n",
      "Iteration 723, loss = 0.21800456\n",
      "Iteration 724, loss = 0.21781268\n",
      "Iteration 725, loss = 0.21762118\n",
      "Iteration 726, loss = 0.21743010\n",
      "Iteration 727, loss = 0.21723946\n",
      "Iteration 728, loss = 0.21704922\n",
      "Iteration 729, loss = 0.21685939\n",
      "Iteration 730, loss = 0.21666997\n",
      "Iteration 731, loss = 0.21648098\n",
      "Iteration 732, loss = 0.21629238\n",
      "Iteration 733, loss = 0.21610420\n",
      "Iteration 734, loss = 0.21591643\n",
      "Iteration 735, loss = 0.21572907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 736, loss = 0.21554210\n",
      "Iteration 737, loss = 0.21535555\n",
      "Iteration 738, loss = 0.21516939\n",
      "Iteration 739, loss = 0.21498365\n",
      "Iteration 740, loss = 0.21479830\n",
      "Iteration 741, loss = 0.21461335\n",
      "Iteration 742, loss = 0.21442881\n",
      "Iteration 743, loss = 0.21424467\n",
      "Iteration 744, loss = 0.21406092\n",
      "Iteration 745, loss = 0.21387756\n",
      "Iteration 746, loss = 0.21369461\n",
      "Iteration 747, loss = 0.21351205\n",
      "Iteration 748, loss = 0.21332988\n",
      "Iteration 749, loss = 0.21314810\n",
      "Iteration 750, loss = 0.21296674\n",
      "Iteration 751, loss = 0.21278574\n",
      "Iteration 752, loss = 0.21260513\n",
      "Iteration 753, loss = 0.21242493\n",
      "Iteration 754, loss = 0.21224510\n",
      "Iteration 755, loss = 0.21206567\n",
      "Iteration 756, loss = 0.21188661\n",
      "Iteration 757, loss = 0.21170795\n",
      "Iteration 758, loss = 0.21152967\n",
      "Iteration 759, loss = 0.21135177\n",
      "Iteration 760, loss = 0.21117425\n",
      "Iteration 761, loss = 0.21099713\n",
      "Iteration 762, loss = 0.21082037\n",
      "Iteration 763, loss = 0.21064399\n",
      "Iteration 764, loss = 0.21046799\n",
      "Iteration 765, loss = 0.21029238\n",
      "Iteration 766, loss = 0.21011713\n",
      "Iteration 767, loss = 0.20994227\n",
      "Iteration 768, loss = 0.20976778\n",
      "Iteration 769, loss = 0.20959365\n",
      "Iteration 770, loss = 0.20941990\n",
      "Iteration 771, loss = 0.20924654\n",
      "Iteration 772, loss = 0.20907353\n",
      "Iteration 773, loss = 0.20890090\n",
      "Iteration 774, loss = 0.20872862\n",
      "Iteration 775, loss = 0.20855673\n",
      "Iteration 776, loss = 0.20838519\n",
      "Iteration 777, loss = 0.20821402\n",
      "Iteration 778, loss = 0.20804323\n",
      "Iteration 779, loss = 0.20787279\n",
      "Iteration 780, loss = 0.20770271\n",
      "Iteration 781, loss = 0.20753301\n",
      "Iteration 782, loss = 0.20736366\n",
      "Iteration 783, loss = 0.20719466\n",
      "Iteration 784, loss = 0.20702603\n",
      "Iteration 785, loss = 0.20685777\n",
      "Iteration 786, loss = 0.20668986\n",
      "Iteration 787, loss = 0.20652229\n",
      "Iteration 788, loss = 0.20635510\n",
      "Iteration 789, loss = 0.20618825\n",
      "Iteration 790, loss = 0.20602175\n",
      "Iteration 791, loss = 0.20585563\n",
      "Iteration 792, loss = 0.20568985\n",
      "Iteration 793, loss = 0.20552441\n",
      "Iteration 794, loss = 0.20535934\n",
      "Iteration 795, loss = 0.20519462\n",
      "Iteration 796, loss = 0.20503024\n",
      "Iteration 797, loss = 0.20486620\n",
      "Iteration 798, loss = 0.20470252\n",
      "Iteration 799, loss = 0.20453919\n",
      "Iteration 800, loss = 0.20437619\n",
      "Iteration 801, loss = 0.20421356\n",
      "Iteration 802, loss = 0.20405124\n",
      "Iteration 803, loss = 0.20388928\n",
      "Iteration 804, loss = 0.20372767\n",
      "Iteration 805, loss = 0.20356639\n",
      "Iteration 806, loss = 0.20340545\n",
      "Iteration 807, loss = 0.20324484\n",
      "Iteration 808, loss = 0.20308460\n",
      "Iteration 809, loss = 0.20292466\n",
      "Iteration 810, loss = 0.20276509\n",
      "Iteration 811, loss = 0.20260584\n",
      "Iteration 812, loss = 0.20244691\n",
      "Iteration 813, loss = 0.20228834\n",
      "Iteration 814, loss = 0.20213010\n",
      "Iteration 815, loss = 0.20197217\n",
      "Iteration 816, loss = 0.20181459\n",
      "Iteration 817, loss = 0.20165734\n",
      "Iteration 818, loss = 0.20150041\n",
      "Iteration 819, loss = 0.20134381\n",
      "Iteration 820, loss = 0.20118756\n",
      "Iteration 821, loss = 0.20103161\n",
      "Iteration 822, loss = 0.20087601\n",
      "Iteration 823, loss = 0.20072071\n",
      "Iteration 824, loss = 0.20056575\n",
      "Iteration 825, loss = 0.20041112\n",
      "Iteration 826, loss = 0.20025680\n",
      "Iteration 827, loss = 0.20010281\n",
      "Iteration 828, loss = 0.19994914\n",
      "Iteration 829, loss = 0.19979579\n",
      "Iteration 830, loss = 0.19964275\n",
      "Iteration 831, loss = 0.19949006\n",
      "Iteration 832, loss = 0.19933766\n",
      "Iteration 833, loss = 0.19918558\n",
      "Iteration 834, loss = 0.19903383\n",
      "Iteration 835, loss = 0.19888239\n",
      "Iteration 836, loss = 0.19873126\n",
      "Iteration 837, loss = 0.19858045\n",
      "Iteration 838, loss = 0.19842994\n",
      "Iteration 839, loss = 0.19827976\n",
      "Iteration 840, loss = 0.19812989\n",
      "Iteration 841, loss = 0.19798033\n",
      "Iteration 842, loss = 0.19783107\n",
      "Iteration 843, loss = 0.19768214\n",
      "Iteration 844, loss = 0.19753350\n",
      "Iteration 845, loss = 0.19738517\n",
      "Iteration 846, loss = 0.19723716\n",
      "Iteration 847, loss = 0.19708944\n",
      "Iteration 848, loss = 0.19694204\n",
      "Iteration 849, loss = 0.19679493\n",
      "Iteration 850, loss = 0.19664814\n",
      "Iteration 851, loss = 0.19650164\n",
      "Iteration 852, loss = 0.19635545\n",
      "Iteration 853, loss = 0.19620957\n",
      "Iteration 854, loss = 0.19606398\n",
      "Iteration 855, loss = 0.19591869\n",
      "Iteration 856, loss = 0.19577371\n",
      "Iteration 857, loss = 0.19562901\n",
      "Iteration 858, loss = 0.19548463\n",
      "Iteration 859, loss = 0.19534053\n",
      "Iteration 860, loss = 0.19519673\n",
      "Iteration 861, loss = 0.19505324\n",
      "Iteration 862, loss = 0.19491003\n",
      "Iteration 863, loss = 0.19476712\n",
      "Iteration 864, loss = 0.19462450\n",
      "Iteration 865, loss = 0.19448218\n",
      "Iteration 866, loss = 0.19434014\n",
      "Iteration 867, loss = 0.19419841\n",
      "Iteration 868, loss = 0.19405695\n",
      "Iteration 869, loss = 0.19391580\n",
      "Iteration 870, loss = 0.19377492\n",
      "Iteration 871, loss = 0.19363434\n",
      "Iteration 872, loss = 0.19349405\n",
      "Iteration 873, loss = 0.19335404\n",
      "Iteration 874, loss = 0.19321432\n",
      "Iteration 875, loss = 0.19307488\n",
      "Iteration 876, loss = 0.19293573\n",
      "Iteration 877, loss = 0.19279686\n",
      "Iteration 878, loss = 0.19265828\n",
      "Iteration 879, loss = 0.19251999\n",
      "Iteration 880, loss = 0.19238196\n",
      "Iteration 881, loss = 0.19224423\n",
      "Iteration 882, loss = 0.19210677\n",
      "Iteration 883, loss = 0.19196959\n",
      "Iteration 884, loss = 0.19183270\n",
      "Iteration 885, loss = 0.19169608\n",
      "Iteration 886, loss = 0.19155974\n",
      "Iteration 887, loss = 0.19142368\n",
      "Iteration 888, loss = 0.19128789\n",
      "Iteration 889, loss = 0.19115239\n",
      "Iteration 890, loss = 0.19101714\n",
      "Iteration 891, loss = 0.19088219\n",
      "Iteration 892, loss = 0.19074750\n",
      "Iteration 893, loss = 0.19061307\n",
      "Iteration 894, loss = 0.19047895\n",
      "Iteration 895, loss = 0.19034507\n",
      "Iteration 896, loss = 0.19021147\n",
      "Iteration 897, loss = 0.19007813\n",
      "Iteration 898, loss = 0.18994508\n",
      "Iteration 899, loss = 0.18981228\n",
      "Iteration 900, loss = 0.18967976\n",
      "Iteration 901, loss = 0.18954751\n",
      "Iteration 902, loss = 0.18941551\n",
      "Iteration 903, loss = 0.18928381\n",
      "Iteration 904, loss = 0.18915234\n",
      "Iteration 905, loss = 0.18902116\n",
      "Iteration 906, loss = 0.18889022\n",
      "Iteration 907, loss = 0.18875955\n",
      "Iteration 908, loss = 0.18862917\n",
      "Iteration 909, loss = 0.18849902\n",
      "Iteration 910, loss = 0.18836914\n",
      "Iteration 911, loss = 0.18823953\n",
      "Iteration 912, loss = 0.18811018\n",
      "Iteration 913, loss = 0.18798108\n",
      "Iteration 914, loss = 0.18785224\n",
      "Iteration 915, loss = 0.18772366\n",
      "Iteration 916, loss = 0.18759534\n",
      "Iteration 917, loss = 0.18746728\n",
      "Iteration 918, loss = 0.18733946\n",
      "Iteration 919, loss = 0.18721192\n",
      "Iteration 920, loss = 0.18708462\n",
      "Iteration 921, loss = 0.18695758\n",
      "Iteration 922, loss = 0.18683079\n",
      "Iteration 923, loss = 0.18670426\n",
      "Iteration 924, loss = 0.18657798\n",
      "Iteration 925, loss = 0.18645195\n",
      "Iteration 926, loss = 0.18632618\n",
      "Iteration 927, loss = 0.18620065\n",
      "Iteration 928, loss = 0.18607537\n",
      "Iteration 929, loss = 0.18595035\n",
      "Iteration 930, loss = 0.18582557\n",
      "Iteration 931, loss = 0.18570104\n",
      "Iteration 932, loss = 0.18557677\n",
      "Iteration 933, loss = 0.18545272\n",
      "Iteration 934, loss = 0.18532895\n",
      "Iteration 935, loss = 0.18520540\n",
      "Iteration 936, loss = 0.18508211\n",
      "Iteration 937, loss = 0.18495905\n",
      "Iteration 938, loss = 0.18483626\n",
      "Iteration 939, loss = 0.18471369\n",
      "Iteration 940, loss = 0.18459138\n",
      "Iteration 941, loss = 0.18446930\n",
      "Iteration 942, loss = 0.18434748\n",
      "Iteration 943, loss = 0.18422588\n",
      "Iteration 944, loss = 0.18410453\n",
      "Iteration 945, loss = 0.18398341\n",
      "Iteration 946, loss = 0.18386255\n",
      "Iteration 947, loss = 0.18374191\n",
      "Iteration 948, loss = 0.18362154\n",
      "Iteration 949, loss = 0.18350137\n",
      "Iteration 950, loss = 0.18338146\n",
      "Iteration 951, loss = 0.18326177\n",
      "Iteration 952, loss = 0.18314233\n",
      "Iteration 953, loss = 0.18302312\n",
      "Iteration 954, loss = 0.18290416\n",
      "Iteration 955, loss = 0.18278541\n",
      "Iteration 956, loss = 0.18266691\n",
      "Iteration 957, loss = 0.18254864\n",
      "Iteration 958, loss = 0.18243060\n",
      "Iteration 959, loss = 0.18231279\n",
      "Iteration 960, loss = 0.18219522\n",
      "Iteration 961, loss = 0.18207787\n",
      "Iteration 962, loss = 0.18196076\n",
      "Iteration 963, loss = 0.18184387\n",
      "Iteration 964, loss = 0.18172722\n",
      "Iteration 965, loss = 0.18161078\n",
      "Iteration 966, loss = 0.18149461\n",
      "Iteration 967, loss = 0.18137862\n",
      "Iteration 968, loss = 0.18126287\n",
      "Iteration 969, loss = 0.18114735\n",
      "Iteration 970, loss = 0.18103206\n",
      "Iteration 971, loss = 0.18091702\n",
      "Iteration 972, loss = 0.18080217\n",
      "Iteration 973, loss = 0.18068756\n",
      "Iteration 974, loss = 0.18057317\n",
      "Iteration 975, loss = 0.18045901\n",
      "Iteration 976, loss = 0.18034509\n",
      "Iteration 977, loss = 0.18023137\n",
      "Iteration 978, loss = 0.18011787\n",
      "Iteration 979, loss = 0.18000462\n",
      "Iteration 980, loss = 0.17989156\n",
      "Iteration 981, loss = 0.17977872\n",
      "Iteration 982, loss = 0.17966613\n",
      "Iteration 983, loss = 0.17955372\n",
      "Iteration 984, loss = 0.17944156\n",
      "Iteration 985, loss = 0.17932960\n",
      "Iteration 986, loss = 0.17921786\n",
      "Iteration 987, loss = 0.17910633\n",
      "Iteration 988, loss = 0.17899505\n",
      "Iteration 989, loss = 0.17888394\n",
      "Iteration 990, loss = 0.17877308\n",
      "Iteration 991, loss = 0.17866243\n",
      "Iteration 992, loss = 0.17855199\n",
      "Iteration 993, loss = 0.17844178\n",
      "Iteration 994, loss = 0.17833177\n",
      "Iteration 995, loss = 0.17822198\n",
      "Iteration 996, loss = 0.17811238\n",
      "Iteration 997, loss = 0.17800301\n",
      "Iteration 998, loss = 0.17789384\n",
      "Iteration 999, loss = 0.17778489\n",
      "Iteration 1000, loss = 0.17767615\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 6.15821746\n",
      "Iteration 3, loss = 1.40627723\n",
      "Iteration 4, loss = 1.06166550\n",
      "Iteration 5, loss = 1.56767691\n",
      "Iteration 6, loss = 0.75594181\n",
      "Iteration 7, loss = 0.91846541\n",
      "Iteration 8, loss = 0.73890705\n",
      "Iteration 9, loss = 0.54608341\n",
      "Iteration 10, loss = 0.50458009\n",
      "Iteration 11, loss = 0.49168241\n",
      "Iteration 12, loss = 0.44375178\n",
      "Iteration 13, loss = 0.38933385\n",
      "Iteration 14, loss = 0.37698816\n",
      "Iteration 15, loss = 0.36896043\n",
      "Iteration 16, loss = 0.32981335\n",
      "Iteration 17, loss = 0.31047318\n",
      "Iteration 18, loss = 0.30934248\n",
      "Iteration 19, loss = 0.26863843\n",
      "Iteration 20, loss = 0.25669295\n",
      "Iteration 21, loss = 0.24491851\n",
      "Iteration 22, loss = 0.21465917\n",
      "Iteration 23, loss = 0.21974319\n",
      "Iteration 24, loss = 0.19065326\n",
      "Iteration 25, loss = 0.18943989\n",
      "Iteration 26, loss = 0.17377333\n",
      "Iteration 27, loss = 0.16478256\n",
      "Iteration 28, loss = 0.16005211\n",
      "Iteration 29, loss = 0.14854019\n",
      "Iteration 30, loss = 0.14920247\n",
      "Iteration 31, loss = 0.13665447\n",
      "Iteration 32, loss = 0.14064984\n",
      "Iteration 33, loss = 0.12921778\n",
      "Iteration 34, loss = 0.13290695\n",
      "Iteration 35, loss = 0.12367661\n",
      "Iteration 36, loss = 0.12751251\n",
      "Iteration 37, loss = 0.11958079\n",
      "Iteration 38, loss = 0.12254848\n",
      "Iteration 39, loss = 0.11654566\n",
      "Iteration 40, loss = 0.11921043\n",
      "Iteration 41, loss = 0.11391183\n",
      "Iteration 42, loss = 0.11601652\n",
      "Iteration 43, loss = 0.11186149\n",
      "Iteration 44, loss = 0.11379045\n",
      "Iteration 45, loss = 0.11006256\n",
      "Iteration 46, loss = 0.11146611\n",
      "Iteration 47, loss = 0.10852553\n",
      "Iteration 48, loss = 0.10968153\n",
      "Iteration 49, loss = 0.10734939\n",
      "Iteration 50, loss = 0.10780924\n",
      "Iteration 51, loss = 0.10622906\n",
      "Iteration 52, loss = 0.10616828\n",
      "Iteration 53, loss = 0.10540428\n",
      "Iteration 54, loss = 0.10470810\n",
      "Iteration 55, loss = 0.10455611\n",
      "Iteration 56, loss = 0.10339459\n",
      "Iteration 57, loss = 0.10362683\n",
      "Iteration 58, loss = 0.10241331\n",
      "Iteration 59, loss = 0.10266429\n",
      "Iteration 60, loss = 0.10167735\n",
      "Iteration 61, loss = 0.10154854\n",
      "Iteration 62, loss = 0.10104500\n",
      "Iteration 63, loss = 0.10048976\n",
      "Iteration 64, loss = 0.10038609\n",
      "Iteration 65, loss = 0.09963153\n",
      "Iteration 66, loss = 0.09960589\n",
      "Iteration 67, loss = 0.09898019\n",
      "Iteration 68, loss = 0.09869535\n",
      "Iteration 69, loss = 0.09840154\n",
      "Iteration 70, loss = 0.09786020\n",
      "Iteration 71, loss = 0.09772382\n",
      "Iteration 72, loss = 0.09721268\n",
      "Iteration 73, loss = 0.09695416\n",
      "Iteration 74, loss = 0.09666244\n",
      "Iteration 75, loss = 0.09622566\n",
      "Iteration 76, loss = 0.09604602\n",
      "Iteration 77, loss = 0.09564558\n",
      "Iteration 78, loss = 0.09534565\n",
      "Iteration 79, loss = 0.09511107\n",
      "Iteration 80, loss = 0.09472272\n",
      "Iteration 81, loss = 0.09449129\n",
      "Iteration 82, loss = 0.09420181\n",
      "Iteration 83, loss = 0.09387021\n",
      "Iteration 84, loss = 0.09364926\n",
      "Iteration 85, loss = 0.09334272\n",
      "Iteration 86, loss = 0.09305873\n",
      "Iteration 87, loss = 0.09283001\n",
      "Iteration 88, loss = 0.09253159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 0.09227594\n",
      "Iteration 90, loss = 0.09204210\n",
      "Iteration 91, loss = 0.09176033\n",
      "Iteration 92, loss = 0.09151996\n",
      "Iteration 93, loss = 0.09128760\n",
      "Iteration 94, loss = 0.09102264\n",
      "Iteration 95, loss = 0.09079092\n",
      "Iteration 96, loss = 0.09056454\n",
      "Iteration 97, loss = 0.09031440\n",
      "Iteration 98, loss = 0.09008823\n",
      "Iteration 99, loss = 0.08986965\n",
      "Iteration 100, loss = 0.08963283\n",
      "Iteration 101, loss = 0.08941088\n",
      "Iteration 102, loss = 0.08919996\n",
      "Iteration 103, loss = 0.08897606\n",
      "Iteration 104, loss = 0.08875841\n",
      "Iteration 105, loss = 0.08855378\n",
      "Iteration 106, loss = 0.08834266\n",
      "Iteration 107, loss = 0.08813074\n",
      "Iteration 108, loss = 0.08793028\n",
      "Iteration 109, loss = 0.08773060\n",
      "Iteration 110, loss = 0.08752683\n",
      "Iteration 111, loss = 0.08732928\n",
      "Iteration 112, loss = 0.08713775\n",
      "Iteration 113, loss = 0.08694403\n",
      "Iteration 114, loss = 0.08675072\n",
      "Iteration 115, loss = 0.08656363\n",
      "Iteration 116, loss = 0.08637907\n",
      "Iteration 117, loss = 0.08619317\n",
      "Iteration 118, loss = 0.08600955\n",
      "Iteration 119, loss = 0.08583068\n",
      "Iteration 120, loss = 0.08565354\n",
      "Iteration 121, loss = 0.08547623\n",
      "Iteration 122, loss = 0.08530114\n",
      "Iteration 123, loss = 0.08512965\n",
      "Iteration 124, loss = 0.08495987\n",
      "Iteration 125, loss = 0.08479052\n",
      "Iteration 126, loss = 0.08462284\n",
      "Iteration 127, loss = 0.08445799\n",
      "Iteration 128, loss = 0.08429520\n",
      "Iteration 129, loss = 0.08413339\n",
      "Iteration 130, loss = 0.08397274\n",
      "Iteration 131, loss = 0.08381414\n",
      "Iteration 132, loss = 0.08365771\n",
      "Iteration 133, loss = 0.08350279\n",
      "Iteration 134, loss = 0.08334894\n",
      "Iteration 135, loss = 0.08319643\n",
      "Iteration 136, loss = 0.08304571\n",
      "Iteration 137, loss = 0.08289679\n",
      "Iteration 138, loss = 0.08274932\n",
      "Iteration 139, loss = 0.08260305\n",
      "Iteration 140, loss = 0.08245800\n",
      "Iteration 141, loss = 0.08231440\n",
      "Iteration 142, loss = 0.08217234\n",
      "Iteration 143, loss = 0.08203176\n",
      "Iteration 144, loss = 0.08189249\n",
      "Iteration 145, loss = 0.08175441\n",
      "Iteration 146, loss = 0.08161759\n",
      "Iteration 147, loss = 0.08148209\n",
      "Iteration 148, loss = 0.08134791\n",
      "Iteration 149, loss = 0.08121505\n",
      "Iteration 150, loss = 0.08108346\n",
      "Iteration 151, loss = 0.08095309\n",
      "Iteration 152, loss = 0.08082388\n",
      "Iteration 153, loss = 0.08069583\n",
      "Iteration 154, loss = 0.08056891\n",
      "Iteration 155, loss = 0.08044312\n",
      "Iteration 156, loss = 0.08031847\n",
      "Iteration 157, loss = 0.08019494\n",
      "Iteration 158, loss = 0.08007257\n",
      "Iteration 159, loss = 0.07995132\n",
      "Iteration 160, loss = 0.07983116\n",
      "Iteration 161, loss = 0.07971207\n",
      "Iteration 162, loss = 0.07959404\n",
      "Iteration 163, loss = 0.07947706\n",
      "Iteration 164, loss = 0.07936112\n",
      "Iteration 165, loss = 0.07924620\n",
      "Iteration 166, loss = 0.07913233\n",
      "Iteration 167, loss = 0.07901950\n",
      "Iteration 168, loss = 0.07890773\n",
      "Iteration 169, loss = 0.07879710\n",
      "Iteration 170, loss = 0.07868779\n",
      "Iteration 171, loss = 0.07858021\n",
      "Iteration 172, loss = 0.07847534\n",
      "Iteration 173, loss = 0.07837532\n",
      "Iteration 174, loss = 0.07828595\n",
      "Iteration 175, loss = 0.07821948\n",
      "Iteration 176, loss = 0.07821428\n",
      "Iteration 177, loss = 0.07834295\n",
      "Iteration 178, loss = 0.07889509\n",
      "Iteration 179, loss = 0.08022992\n",
      "Iteration 180, loss = 0.08472145\n",
      "Iteration 181, loss = 0.09119532\n",
      "Iteration 182, loss = 0.11398409\n",
      "Iteration 183, loss = 0.10327969\n",
      "Iteration 184, loss = 0.10326214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 6.21312686\n",
      "Iteration 3, loss = 1.41030512\n",
      "Iteration 4, loss = 1.05862602\n",
      "Iteration 5, loss = 1.50349088\n",
      "Iteration 6, loss = 0.79629236\n",
      "Iteration 7, loss = 0.83441146\n",
      "Iteration 8, loss = 0.71521704\n",
      "Iteration 9, loss = 0.54729226\n",
      "Iteration 10, loss = 0.54415640\n",
      "Iteration 11, loss = 0.53723144\n",
      "Iteration 12, loss = 0.45684335\n",
      "Iteration 13, loss = 0.39914816\n",
      "Iteration 14, loss = 0.41767712\n",
      "Iteration 15, loss = 0.43284026\n",
      "Iteration 16, loss = 0.39757949\n",
      "Iteration 17, loss = 0.33763392\n",
      "Iteration 18, loss = 0.33360980\n",
      "Iteration 19, loss = 0.33249371\n",
      "Iteration 20, loss = 0.28359387\n",
      "Iteration 21, loss = 0.27051087\n",
      "Iteration 22, loss = 0.26883924\n",
      "Iteration 23, loss = 0.22658514\n",
      "Iteration 24, loss = 0.21969320\n",
      "Iteration 25, loss = 0.21736293\n",
      "Iteration 26, loss = 0.18007119\n",
      "Iteration 27, loss = 0.18971542\n",
      "Iteration 28, loss = 0.16543727\n",
      "Iteration 29, loss = 0.15759390\n",
      "Iteration 30, loss = 0.15845847\n",
      "Iteration 31, loss = 0.13772123\n",
      "Iteration 32, loss = 0.14712208\n",
      "Iteration 33, loss = 0.12934715\n",
      "Iteration 34, loss = 0.13301391\n",
      "Iteration 35, loss = 0.12607856\n",
      "Iteration 36, loss = 0.12105201\n",
      "Iteration 37, loss = 0.12370841\n",
      "Iteration 38, loss = 0.11327336\n",
      "Iteration 39, loss = 0.11939428\n",
      "Iteration 40, loss = 0.11004288\n",
      "Iteration 41, loss = 0.11345381\n",
      "Iteration 42, loss = 0.10922393\n",
      "Iteration 43, loss = 0.10805039\n",
      "Iteration 44, loss = 0.10812376\n",
      "Iteration 45, loss = 0.10454556\n",
      "Iteration 46, loss = 0.10664447\n",
      "Iteration 47, loss = 0.10215515\n",
      "Iteration 48, loss = 0.10478270\n",
      "Iteration 49, loss = 0.10087399\n",
      "Iteration 50, loss = 0.10255416\n",
      "Iteration 51, loss = 0.10019185\n",
      "Iteration 52, loss = 0.10059872\n",
      "Iteration 53, loss = 0.09939701\n",
      "Iteration 54, loss = 0.09902258\n",
      "Iteration 55, loss = 0.09872490\n",
      "Iteration 56, loss = 0.09764924\n",
      "Iteration 57, loss = 0.09790310\n",
      "Iteration 58, loss = 0.09666634\n",
      "Iteration 59, loss = 0.09705219\n",
      "Iteration 60, loss = 0.09576570\n",
      "Iteration 61, loss = 0.09624186\n",
      "Iteration 62, loss = 0.09504931\n",
      "Iteration 63, loss = 0.09541915\n",
      "Iteration 64, loss = 0.09437046\n",
      "Iteration 65, loss = 0.09468716\n",
      "Iteration 66, loss = 0.09375783\n",
      "Iteration 67, loss = 0.09396294\n",
      "Iteration 68, loss = 0.09316867\n",
      "Iteration 69, loss = 0.09331373\n",
      "Iteration 70, loss = 0.09261232\n",
      "Iteration 71, loss = 0.09267506\n",
      "Iteration 72, loss = 0.09207217\n",
      "Iteration 73, loss = 0.09208554\n",
      "Iteration 74, loss = 0.09156087\n",
      "Iteration 75, loss = 0.09150707\n",
      "Iteration 76, loss = 0.09106194\n",
      "Iteration 77, loss = 0.09095893\n",
      "Iteration 78, loss = 0.09058977\n",
      "Iteration 79, loss = 0.09042621\n",
      "Iteration 80, loss = 0.09012604\n",
      "Iteration 81, loss = 0.08991339\n",
      "Iteration 82, loss = 0.08967895\n",
      "Iteration 83, loss = 0.08942345\n",
      "Iteration 84, loss = 0.08923668\n",
      "Iteration 85, loss = 0.08895283\n",
      "Iteration 86, loss = 0.08879698\n",
      "Iteration 87, loss = 0.08850806\n",
      "Iteration 88, loss = 0.08836031\n",
      "Iteration 89, loss = 0.08808483\n",
      "Iteration 90, loss = 0.08792326\n",
      "Iteration 91, loss = 0.08767957\n",
      "Iteration 92, loss = 0.08749373\n",
      "Iteration 93, loss = 0.08728701\n",
      "Iteration 94, loss = 0.08707633\n",
      "Iteration 95, loss = 0.08689857\n",
      "Iteration 96, loss = 0.08667571\n",
      "Iteration 97, loss = 0.08650966\n",
      "Iteration 98, loss = 0.08629384\n",
      "Iteration 99, loss = 0.08612228\n",
      "Iteration 100, loss = 0.08592653\n",
      "Iteration 101, loss = 0.08574091\n",
      "Iteration 102, loss = 0.08556603\n",
      "Iteration 103, loss = 0.08537207\n",
      "Iteration 104, loss = 0.08520682\n",
      "Iteration 105, loss = 0.08501815\n",
      "Iteration 106, loss = 0.08484966\n",
      "Iteration 107, loss = 0.08467526\n",
      "Iteration 108, loss = 0.08449955\n",
      "Iteration 109, loss = 0.08433695\n",
      "Iteration 110, loss = 0.08416118\n",
      "Iteration 111, loss = 0.08400055\n",
      "Iteration 112, loss = 0.08383400\n",
      "Iteration 113, loss = 0.08366970\n",
      "Iteration 114, loss = 0.08351263\n",
      "Iteration 115, loss = 0.08334852\n",
      "Iteration 116, loss = 0.08319404\n",
      "Iteration 117, loss = 0.08303673\n",
      "Iteration 118, loss = 0.08288054\n",
      "Iteration 119, loss = 0.08273011\n",
      "Iteration 120, loss = 0.08257563\n",
      "Iteration 121, loss = 0.08242675\n",
      "Iteration 122, loss = 0.08227832\n",
      "Iteration 123, loss = 0.08212927\n",
      "Iteration 124, loss = 0.08198518\n",
      "Iteration 125, loss = 0.08183955\n",
      "Iteration 126, loss = 0.08169616\n",
      "Iteration 127, loss = 0.08155542\n",
      "Iteration 128, loss = 0.08141370\n",
      "Iteration 129, loss = 0.08127513\n",
      "Iteration 130, loss = 0.08113762\n",
      "Iteration 131, loss = 0.08100025\n",
      "Iteration 132, loss = 0.08086584\n",
      "Iteration 133, loss = 0.08073179\n",
      "Iteration 134, loss = 0.08059870\n",
      "Iteration 135, loss = 0.08046790\n",
      "Iteration 136, loss = 0.08033739\n",
      "Iteration 137, loss = 0.08020821\n",
      "Iteration 138, loss = 0.08008085\n",
      "Iteration 139, loss = 0.07995393\n",
      "Iteration 140, loss = 0.07982840\n",
      "Iteration 141, loss = 0.07970444\n",
      "Iteration 142, loss = 0.07958102\n",
      "Iteration 143, loss = 0.07945894\n",
      "Iteration 144, loss = 0.07933823\n",
      "Iteration 145, loss = 0.07921818\n",
      "Iteration 146, loss = 0.07909936\n",
      "Iteration 147, loss = 0.07898183\n",
      "Iteration 148, loss = 0.07886504\n",
      "Iteration 149, loss = 0.07874936\n",
      "Iteration 150, loss = 0.07863489\n",
      "Iteration 151, loss = 0.07852126\n",
      "Iteration 152, loss = 0.07840862\n",
      "Iteration 153, loss = 0.07829714\n",
      "Iteration 154, loss = 0.07818653\n",
      "Iteration 155, loss = 0.07807682\n",
      "Iteration 156, loss = 0.07796820\n",
      "Iteration 157, loss = 0.07786051\n",
      "Iteration 158, loss = 0.07775366\n",
      "Iteration 159, loss = 0.07764781\n",
      "Iteration 160, loss = 0.07754292\n",
      "Iteration 161, loss = 0.07743886\n",
      "Iteration 162, loss = 0.07733570\n",
      "Iteration 163, loss = 0.07723349\n",
      "Iteration 164, loss = 0.07713214\n",
      "Iteration 165, loss = 0.07703164\n",
      "Iteration 166, loss = 0.07693204\n",
      "Iteration 167, loss = 0.07683331\n",
      "Iteration 168, loss = 0.07673541\n",
      "Iteration 169, loss = 0.07663832\n",
      "Iteration 170, loss = 0.07654209\n",
      "Iteration 171, loss = 0.07644669\n",
      "Iteration 172, loss = 0.07635207\n",
      "Iteration 173, loss = 0.07625826\n",
      "Iteration 174, loss = 0.07616526\n",
      "Iteration 175, loss = 0.07607305\n",
      "Iteration 176, loss = 0.07598161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 6.14206204\n",
      "Iteration 3, loss = 1.40124252\n",
      "Iteration 4, loss = 1.05406316\n",
      "Iteration 5, loss = 1.57627794\n",
      "Iteration 6, loss = 0.74734847\n",
      "Iteration 7, loss = 0.90759428\n",
      "Iteration 8, loss = 0.72326031\n",
      "Iteration 9, loss = 0.53870897\n",
      "Iteration 10, loss = 0.49234008\n",
      "Iteration 11, loss = 0.47858155\n",
      "Iteration 12, loss = 0.44495377\n",
      "Iteration 13, loss = 0.39209010\n",
      "Iteration 14, loss = 0.36257410\n",
      "Iteration 15, loss = 0.36412975\n",
      "Iteration 16, loss = 0.34903967\n",
      "Iteration 17, loss = 0.30868608\n",
      "Iteration 18, loss = 0.29232460\n",
      "Iteration 19, loss = 0.28861827\n",
      "Iteration 20, loss = 0.24986895\n",
      "Iteration 21, loss = 0.23316082\n",
      "Iteration 22, loss = 0.22785907\n",
      "Iteration 23, loss = 0.19584233\n",
      "Iteration 24, loss = 0.18882006\n",
      "Iteration 25, loss = 0.17979777\n",
      "Iteration 26, loss = 0.15681988\n",
      "Iteration 27, loss = 0.15881222\n",
      "Iteration 28, loss = 0.14244631\n",
      "Iteration 29, loss = 0.13373847\n",
      "Iteration 30, loss = 0.13323249\n",
      "Iteration 31, loss = 0.11823703\n",
      "Iteration 32, loss = 0.11967806\n",
      "Iteration 33, loss = 0.11270401\n",
      "Iteration 34, loss = 0.10603605\n",
      "Iteration 35, loss = 0.10779405\n",
      "Iteration 36, loss = 0.09937082\n",
      "Iteration 37, loss = 0.09887537\n",
      "Iteration 38, loss = 0.09508903\n",
      "Iteration 39, loss = 0.09040438\n",
      "Iteration 40, loss = 0.09365668\n",
      "Iteration 41, loss = 0.08805858\n",
      "Iteration 42, loss = 0.08866182\n",
      "Iteration 43, loss = 0.08887348\n",
      "Iteration 44, loss = 0.08496373\n",
      "Iteration 45, loss = 0.08720330\n",
      "Iteration 46, loss = 0.08459575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.08402277\n",
      "Iteration 48, loss = 0.08479594\n",
      "Iteration 49, loss = 0.08223028\n",
      "Iteration 50, loss = 0.08318994\n",
      "Iteration 51, loss = 0.08212083\n",
      "Iteration 52, loss = 0.08115750\n",
      "Iteration 53, loss = 0.08181281\n",
      "Iteration 54, loss = 0.08019774\n",
      "Iteration 55, loss = 0.08044925\n",
      "Iteration 56, loss = 0.07998860\n",
      "Iteration 57, loss = 0.07909060\n",
      "Iteration 58, loss = 0.07945697\n",
      "Iteration 59, loss = 0.07841573\n",
      "Iteration 60, loss = 0.07837007\n",
      "Iteration 61, loss = 0.07807513\n",
      "Iteration 62, loss = 0.07737332\n",
      "Iteration 63, loss = 0.07750149\n",
      "Iteration 64, loss = 0.07677569\n",
      "Iteration 65, loss = 0.07663804\n",
      "Iteration 66, loss = 0.07635412\n",
      "Iteration 67, loss = 0.07583557\n",
      "Iteration 68, loss = 0.07580820\n",
      "Iteration 69, loss = 0.07525972\n",
      "Iteration 70, loss = 0.07510584\n",
      "Iteration 71, loss = 0.07480178\n",
      "Iteration 72, loss = 0.07442413\n",
      "Iteration 73, loss = 0.07429880\n",
      "Iteration 74, loss = 0.07386599\n",
      "Iteration 75, loss = 0.07371134\n",
      "Iteration 76, loss = 0.07339718\n",
      "Iteration 77, loss = 0.07311838\n",
      "Iteration 78, loss = 0.07293092\n",
      "Iteration 79, loss = 0.07258777\n",
      "Iteration 80, loss = 0.07242613\n",
      "Iteration 81, loss = 0.07212260\n",
      "Iteration 82, loss = 0.07190712\n",
      "Iteration 83, loss = 0.07168247\n",
      "Iteration 84, loss = 0.07141351\n",
      "Iteration 85, loss = 0.07123420\n",
      "Iteration 86, loss = 0.07096159\n",
      "Iteration 87, loss = 0.07077639\n",
      "Iteration 88, loss = 0.07053912\n",
      "Iteration 89, loss = 0.07032577\n",
      "Iteration 90, loss = 0.07012696\n",
      "Iteration 91, loss = 0.06989648\n",
      "Iteration 92, loss = 0.06971587\n",
      "Iteration 93, loss = 0.06949030\n",
      "Iteration 94, loss = 0.06930770\n",
      "Iteration 95, loss = 0.06910125\n",
      "Iteration 96, loss = 0.06890890\n",
      "Iteration 97, loss = 0.06872223\n",
      "Iteration 98, loss = 0.06852399\n",
      "Iteration 99, loss = 0.06834918\n",
      "Iteration 100, loss = 0.06815362\n",
      "Iteration 101, loss = 0.06798190\n",
      "Iteration 102, loss = 0.06779551\n",
      "Iteration 103, loss = 0.06762229\n",
      "Iteration 104, loss = 0.06744649\n",
      "Iteration 105, loss = 0.06727189\n",
      "Iteration 106, loss = 0.06710473\n",
      "Iteration 107, loss = 0.06693136\n",
      "Iteration 108, loss = 0.06676960\n",
      "Iteration 109, loss = 0.06660014\n",
      "Iteration 110, loss = 0.06644107\n",
      "Iteration 111, loss = 0.06627711\n",
      "Iteration 112, loss = 0.06611955\n",
      "Iteration 113, loss = 0.06596125\n",
      "Iteration 114, loss = 0.06580518\n",
      "Iteration 115, loss = 0.06565186\n",
      "Iteration 116, loss = 0.06549790\n",
      "Iteration 117, loss = 0.06534857\n",
      "Iteration 118, loss = 0.06519739\n",
      "Iteration 119, loss = 0.06505115\n",
      "Iteration 120, loss = 0.06490323\n",
      "Iteration 121, loss = 0.06475954\n",
      "Iteration 122, loss = 0.06461504\n",
      "Iteration 123, loss = 0.06447362\n",
      "Iteration 124, loss = 0.06433246\n",
      "Iteration 125, loss = 0.06419328\n",
      "Iteration 126, loss = 0.06405523\n",
      "Iteration 127, loss = 0.06391832\n",
      "Iteration 128, loss = 0.06378312\n",
      "Iteration 129, loss = 0.06364854\n",
      "Iteration 130, loss = 0.06351596\n",
      "Iteration 131, loss = 0.06338375\n",
      "Iteration 132, loss = 0.06325358\n",
      "Iteration 133, loss = 0.06312378\n",
      "Iteration 134, loss = 0.06299588\n",
      "Iteration 135, loss = 0.06286840\n",
      "Iteration 136, loss = 0.06274265\n",
      "Iteration 137, loss = 0.06261747\n",
      "Iteration 138, loss = 0.06249384\n",
      "Iteration 139, loss = 0.06237088\n",
      "Iteration 140, loss = 0.06224928\n",
      "Iteration 141, loss = 0.06212847\n",
      "Iteration 142, loss = 0.06200887\n",
      "Iteration 143, loss = 0.06189013\n",
      "Iteration 144, loss = 0.06177249\n",
      "Iteration 145, loss = 0.06165575\n",
      "Iteration 146, loss = 0.06154003\n",
      "Iteration 147, loss = 0.06142522\n",
      "Iteration 148, loss = 0.06131138\n",
      "Iteration 149, loss = 0.06119846\n",
      "Iteration 150, loss = 0.06108644\n",
      "Iteration 151, loss = 0.06097535\n",
      "Iteration 152, loss = 0.06086514\n",
      "Iteration 153, loss = 0.06075582\n",
      "Iteration 154, loss = 0.06064736\n",
      "Iteration 155, loss = 0.06053978\n",
      "Iteration 156, loss = 0.06043304\n",
      "Iteration 157, loss = 0.06032725\n",
      "Iteration 158, loss = 0.06022234\n",
      "Iteration 159, loss = 0.06011828\n",
      "Iteration 160, loss = 0.06001503\n",
      "Iteration 161, loss = 0.05991261\n",
      "Iteration 162, loss = 0.05981098\n",
      "Iteration 163, loss = 0.05971016\n",
      "Iteration 164, loss = 0.05961012\n",
      "Iteration 165, loss = 0.05951094\n",
      "Iteration 166, loss = 0.05941258\n",
      "Iteration 167, loss = 0.05931496\n",
      "Iteration 168, loss = 0.05921811\n",
      "Iteration 169, loss = 0.05912201\n",
      "Iteration 170, loss = 0.05902665\n",
      "Iteration 171, loss = 0.05893202\n",
      "Iteration 172, loss = 0.05883812\n",
      "Iteration 173, loss = 0.05874493\n",
      "Iteration 174, loss = 0.05865246\n",
      "Iteration 175, loss = 0.05856068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 6.17958358\n",
      "Iteration 3, loss = 1.41501411\n",
      "Iteration 4, loss = 1.04441986\n",
      "Iteration 5, loss = 1.45322877\n",
      "Iteration 6, loss = 0.81971655\n",
      "Iteration 7, loss = 0.81595909\n",
      "Iteration 8, loss = 0.71429571\n",
      "Iteration 9, loss = 0.55740610\n",
      "Iteration 10, loss = 0.54711220\n",
      "Iteration 11, loss = 0.52875947\n",
      "Iteration 12, loss = 0.44928749\n",
      "Iteration 13, loss = 0.40077234\n",
      "Iteration 14, loss = 0.41779514\n",
      "Iteration 15, loss = 0.42089622\n",
      "Iteration 16, loss = 0.36937648\n",
      "Iteration 17, loss = 0.31478123\n",
      "Iteration 18, loss = 0.31918790\n",
      "Iteration 19, loss = 0.30733517\n",
      "Iteration 20, loss = 0.26626756\n",
      "Iteration 21, loss = 0.24751933\n",
      "Iteration 22, loss = 0.22641799\n",
      "Iteration 23, loss = 0.19470955\n",
      "Iteration 24, loss = 0.19472849\n",
      "Iteration 25, loss = 0.17482384\n",
      "Iteration 26, loss = 0.16221585\n",
      "Iteration 27, loss = 0.15921342\n",
      "Iteration 28, loss = 0.13776692\n",
      "Iteration 29, loss = 0.13781932\n",
      "Iteration 30, loss = 0.12265678\n",
      "Iteration 31, loss = 0.11887187\n",
      "Iteration 32, loss = 0.11450087\n",
      "Iteration 33, loss = 0.10527406\n",
      "Iteration 34, loss = 0.10718495\n",
      "Iteration 35, loss = 0.09738716\n",
      "Iteration 36, loss = 0.09921071\n",
      "Iteration 37, loss = 0.09314963\n",
      "Iteration 38, loss = 0.09200094\n",
      "Iteration 39, loss = 0.09011596\n",
      "Iteration 40, loss = 0.08662569\n",
      "Iteration 41, loss = 0.08730829\n",
      "Iteration 42, loss = 0.08292956\n",
      "Iteration 43, loss = 0.08427015\n",
      "Iteration 44, loss = 0.08068535\n",
      "Iteration 45, loss = 0.08111634\n",
      "Iteration 46, loss = 0.07923829\n",
      "Iteration 47, loss = 0.07836218\n",
      "Iteration 48, loss = 0.07783095\n",
      "Iteration 49, loss = 0.07627119\n",
      "Iteration 50, loss = 0.07643366\n",
      "Iteration 51, loss = 0.07459103\n",
      "Iteration 52, loss = 0.07501956\n",
      "Iteration 53, loss = 0.07331197\n",
      "Iteration 54, loss = 0.07358464\n",
      "Iteration 55, loss = 0.07224483\n",
      "Iteration 56, loss = 0.07228119\n",
      "Iteration 57, loss = 0.07125055\n",
      "Iteration 58, loss = 0.07108333\n",
      "Iteration 59, loss = 0.07034807\n",
      "Iteration 60, loss = 0.07000317\n",
      "Iteration 61, loss = 0.06945098\n",
      "Iteration 62, loss = 0.06903843\n",
      "Iteration 63, loss = 0.06860464\n",
      "Iteration 64, loss = 0.06814410\n",
      "Iteration 65, loss = 0.06777754\n",
      "Iteration 66, loss = 0.06732993\n",
      "Iteration 67, loss = 0.06699056\n",
      "Iteration 68, loss = 0.06655763\n",
      "Iteration 69, loss = 0.06623013\n",
      "Iteration 70, loss = 0.06583575\n",
      "Iteration 71, loss = 0.06550417\n",
      "Iteration 72, loss = 0.06514258\n",
      "Iteration 73, loss = 0.06480349\n",
      "Iteration 74, loss = 0.06448241\n",
      "Iteration 75, loss = 0.06413355\n",
      "Iteration 76, loss = 0.06384447\n",
      "Iteration 77, loss = 0.06348683\n",
      "Iteration 78, loss = 0.06322798\n",
      "Iteration 79, loss = 0.06286912\n",
      "Iteration 80, loss = 0.06263000\n",
      "Iteration 81, loss = 0.06227525\n",
      "Iteration 82, loss = 0.06204535\n",
      "Iteration 83, loss = 0.06170641\n",
      "Iteration 84, loss = 0.06147533\n",
      "Iteration 85, loss = 0.06116005\n",
      "Iteration 86, loss = 0.06091772\n",
      "Iteration 87, loss = 0.06063232\n",
      "Iteration 88, loss = 0.06037568\n",
      "Iteration 89, loss = 0.06011985\n",
      "Iteration 90, loss = 0.05985190\n",
      "Iteration 91, loss = 0.05961760\n",
      "Iteration 92, loss = 0.05934776\n",
      "Iteration 93, loss = 0.05912272\n",
      "Iteration 94, loss = 0.05886338\n",
      "Iteration 95, loss = 0.05863630\n",
      "Iteration 96, loss = 0.05839535\n",
      "Iteration 97, loss = 0.05816135\n",
      "Iteration 98, loss = 0.05793827\n",
      "Iteration 99, loss = 0.05770189\n",
      "Iteration 100, loss = 0.05748832\n",
      "Iteration 101, loss = 0.05725914\n",
      "Iteration 102, loss = 0.05704582\n",
      "Iteration 103, loss = 0.05683013\n",
      "Iteration 104, loss = 0.05661439\n",
      "Iteration 105, loss = 0.05640999\n",
      "Iteration 106, loss = 0.05619725\n",
      "Iteration 107, loss = 0.05599670\n",
      "Iteration 108, loss = 0.05579337\n",
      "Iteration 109, loss = 0.05559248\n",
      "Iteration 110, loss = 0.05539857\n",
      "Iteration 111, loss = 0.05520057\n",
      "Iteration 112, loss = 0.05501063\n",
      "Iteration 113, loss = 0.05482035\n",
      "Iteration 114, loss = 0.05463143\n",
      "Iteration 115, loss = 0.05444832\n",
      "Iteration 116, loss = 0.05426339\n",
      "Iteration 117, loss = 0.05408319\n",
      "Iteration 118, loss = 0.05390519\n",
      "Iteration 119, loss = 0.05372727\n",
      "Iteration 120, loss = 0.05355413\n",
      "Iteration 121, loss = 0.05338137\n",
      "Iteration 122, loss = 0.05321057\n",
      "Iteration 123, loss = 0.05304328\n",
      "Iteration 124, loss = 0.05287623\n",
      "Iteration 125, loss = 0.05271196\n",
      "Iteration 126, loss = 0.05255008\n",
      "Iteration 127, loss = 0.05238888\n",
      "Iteration 128, loss = 0.05223053\n",
      "Iteration 129, loss = 0.05207394\n",
      "Iteration 130, loss = 0.05191843\n",
      "Iteration 131, loss = 0.05176553\n",
      "Iteration 132, loss = 0.05161412\n",
      "Iteration 133, loss = 0.05146403\n",
      "Iteration 134, loss = 0.05131627\n",
      "Iteration 135, loss = 0.05116997\n",
      "Iteration 136, loss = 0.05102513\n",
      "Iteration 137, loss = 0.05088246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 138, loss = 0.05074122\n",
      "Iteration 139, loss = 0.05060135\n",
      "Iteration 140, loss = 0.05046338\n",
      "Iteration 141, loss = 0.05032687\n",
      "Iteration 142, loss = 0.05019166\n",
      "Iteration 143, loss = 0.05005817\n",
      "Iteration 144, loss = 0.04992616\n",
      "Iteration 145, loss = 0.04979539\n",
      "Iteration 146, loss = 0.04966618\n",
      "Iteration 147, loss = 0.04953844\n",
      "Iteration 148, loss = 0.04941192\n",
      "Iteration 149, loss = 0.04928680\n",
      "Iteration 150, loss = 0.04916312\n",
      "Iteration 151, loss = 0.04904067\n",
      "Iteration 152, loss = 0.04891948\n",
      "Iteration 153, loss = 0.04879965\n",
      "Iteration 154, loss = 0.04868107\n",
      "Iteration 155, loss = 0.04856366\n",
      "Iteration 156, loss = 0.04844751\n",
      "Iteration 157, loss = 0.04833260\n",
      "Iteration 158, loss = 0.04821885\n",
      "Iteration 159, loss = 0.04810623\n",
      "Iteration 160, loss = 0.04799480\n",
      "Iteration 161, loss = 0.04788451\n",
      "Iteration 162, loss = 0.04777532\n",
      "Iteration 163, loss = 0.04766723\n",
      "Iteration 164, loss = 0.04756029\n",
      "Iteration 165, loss = 0.04745442\n",
      "Iteration 166, loss = 0.04734958\n",
      "Iteration 167, loss = 0.04724577\n",
      "Iteration 168, loss = 0.04714300\n",
      "Iteration 169, loss = 0.04704124\n",
      "Iteration 170, loss = 0.04694045\n",
      "Iteration 171, loss = 0.04684064\n",
      "Iteration 172, loss = 0.04674181\n",
      "Iteration 173, loss = 0.04664392\n",
      "Iteration 174, loss = 0.04654695\n",
      "Iteration 175, loss = 0.04645091\n",
      "Iteration 176, loss = 0.04635577\n",
      "Iteration 177, loss = 0.04626153\n",
      "Iteration 178, loss = 0.04616818\n",
      "Iteration 179, loss = 0.04607568\n",
      "Iteration 180, loss = 0.04598405\n",
      "Iteration 181, loss = 0.04589325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 6.08383987\n",
      "Iteration 3, loss = 1.15466405\n",
      "Iteration 4, loss = 1.30663148\n",
      "Iteration 5, loss = 1.11799620\n",
      "Iteration 6, loss = 0.71332946\n",
      "Iteration 7, loss = 0.91302855\n",
      "Iteration 8, loss = 0.54700897\n",
      "Iteration 9, loss = 0.59083764\n",
      "Iteration 10, loss = 0.53380108\n",
      "Iteration 11, loss = 0.49563637\n",
      "Iteration 12, loss = 0.42841994\n",
      "Iteration 13, loss = 0.40741116\n",
      "Iteration 14, loss = 0.38311392\n",
      "Iteration 15, loss = 0.35830108\n",
      "Iteration 16, loss = 0.34155182\n",
      "Iteration 17, loss = 0.30852803\n",
      "Iteration 18, loss = 0.29182281\n",
      "Iteration 19, loss = 0.26585173\n",
      "Iteration 20, loss = 0.24331080\n",
      "Iteration 21, loss = 0.22851763\n",
      "Iteration 22, loss = 0.20750054\n",
      "Iteration 23, loss = 0.19865869\n",
      "Iteration 24, loss = 0.18108259\n",
      "Iteration 25, loss = 0.17439476\n",
      "Iteration 26, loss = 0.16205423\n",
      "Iteration 27, loss = 0.15629768\n",
      "Iteration 28, loss = 0.14821076\n",
      "Iteration 29, loss = 0.14319078\n",
      "Iteration 30, loss = 0.13761490\n",
      "Iteration 31, loss = 0.13299670\n",
      "Iteration 32, loss = 0.12878047\n",
      "Iteration 33, loss = 0.12662515\n",
      "Iteration 34, loss = 0.12272644\n",
      "Iteration 35, loss = 0.11934652\n",
      "Iteration 36, loss = 0.11761627\n",
      "Iteration 37, loss = 0.11505631\n",
      "Iteration 38, loss = 0.11370638\n",
      "Iteration 39, loss = 0.11213165\n",
      "Iteration 40, loss = 0.11084473\n",
      "Iteration 41, loss = 0.11001334\n",
      "Iteration 42, loss = 0.10856830\n",
      "Iteration 43, loss = 0.10829417\n",
      "Iteration 44, loss = 0.10667136\n",
      "Iteration 45, loss = 0.10666802\n",
      "Iteration 46, loss = 0.10493836\n",
      "Iteration 47, loss = 0.10495955\n",
      "Iteration 48, loss = 0.10352619\n",
      "Iteration 49, loss = 0.10337543\n",
      "Iteration 50, loss = 0.10247784\n",
      "Iteration 51, loss = 0.10193384\n",
      "Iteration 52, loss = 0.10160011\n",
      "Iteration 53, loss = 0.10073700\n",
      "Iteration 54, loss = 0.10067252\n",
      "Iteration 55, loss = 0.09984498\n",
      "Iteration 56, loss = 0.09963499\n",
      "Iteration 57, loss = 0.09913268\n",
      "Iteration 58, loss = 0.09856866\n",
      "Iteration 59, loss = 0.09836467\n",
      "Iteration 60, loss = 0.09768963\n",
      "Iteration 61, loss = 0.09742200\n",
      "Iteration 62, loss = 0.09699649\n",
      "Iteration 63, loss = 0.09650082\n",
      "Iteration 64, loss = 0.09627602\n",
      "Iteration 65, loss = 0.09578253\n",
      "Iteration 66, loss = 0.09546735\n",
      "Iteration 67, loss = 0.09517371\n",
      "Iteration 68, loss = 0.09473076\n",
      "Iteration 69, loss = 0.09448514\n",
      "Iteration 70, loss = 0.09413661\n",
      "Iteration 71, loss = 0.09374272\n",
      "Iteration 72, loss = 0.09345117\n",
      "Iteration 73, loss = 0.09304261\n",
      "Iteration 74, loss = 0.09249707\n",
      "Iteration 75, loss = 0.09294494\n",
      "Iteration 76, loss = 0.09684682\n",
      "Iteration 77, loss = 0.09828076\n",
      "Iteration 78, loss = 0.09214820\n",
      "Iteration 79, loss = 0.09298191\n",
      "Iteration 80, loss = 0.09487374\n",
      "Iteration 81, loss = 0.09091526\n",
      "Iteration 82, loss = 0.09274762\n",
      "Iteration 83, loss = 0.09352549\n",
      "Iteration 84, loss = 0.09005475\n",
      "Iteration 85, loss = 0.09240598\n",
      "Iteration 86, loss = 0.09251818\n",
      "Iteration 87, loss = 0.08936211\n",
      "Iteration 88, loss = 0.09188118\n",
      "Iteration 89, loss = 0.09168925\n",
      "Iteration 90, loss = 0.08870274\n",
      "Iteration 91, loss = 0.09122148\n",
      "Iteration 92, loss = 0.09095937\n",
      "Iteration 93, loss = 0.08805147\n",
      "Iteration 94, loss = 0.09049037\n",
      "Iteration 95, loss = 0.09030812\n",
      "Iteration 96, loss = 0.08741203\n",
      "Iteration 97, loss = 0.08972732\n",
      "Iteration 98, loss = 0.08971200\n",
      "Iteration 99, loss = 0.08679282\n",
      "Iteration 100, loss = 0.08893670\n",
      "Iteration 101, loss = 0.08914772\n",
      "Iteration 102, loss = 0.08620414\n",
      "Iteration 103, loss = 0.08811887\n",
      "Iteration 104, loss = 0.08858864\n",
      "Iteration 105, loss = 0.08565595\n",
      "Iteration 106, loss = 0.08726780\n",
      "Iteration 107, loss = 0.08800121\n",
      "Iteration 108, loss = 0.08515728\n",
      "Iteration 109, loss = 0.08637716\n",
      "Iteration 110, loss = 0.08734379\n",
      "Iteration 111, loss = 0.08471432\n",
      "Iteration 112, loss = 0.08545059\n",
      "Iteration 113, loss = 0.08657261\n",
      "Iteration 114, loss = 0.08432803\n",
      "Iteration 115, loss = 0.08451702\n",
      "Iteration 116, loss = 0.08565873\n",
      "Iteration 117, loss = 0.08398739\n",
      "Iteration 118, loss = 0.08364665\n",
      "Iteration 119, loss = 0.08461539\n",
      "Iteration 120, loss = 0.08365763\n",
      "Iteration 121, loss = 0.08294847\n",
      "Iteration 122, loss = 0.08353427\n",
      "Iteration 123, loss = 0.08327311\n",
      "Iteration 124, loss = 0.08251122\n",
      "Iteration 125, loss = 0.08257960\n",
      "Iteration 126, loss = 0.08275514\n",
      "Iteration 127, loss = 0.08228595\n",
      "Iteration 128, loss = 0.08191470\n",
      "Iteration 129, loss = 0.08209099\n",
      "Iteration 130, loss = 0.08203343\n",
      "Iteration 131, loss = 0.08155787\n",
      "Iteration 132, loss = 0.08142487\n",
      "Iteration 133, loss = 0.08153520\n",
      "Iteration 134, loss = 0.08131169\n",
      "Iteration 135, loss = 0.08097993\n",
      "Iteration 136, loss = 0.08090857\n",
      "Iteration 137, loss = 0.08091725\n",
      "Iteration 138, loss = 0.08073166\n",
      "Iteration 139, loss = 0.08047022\n",
      "Iteration 140, loss = 0.08038196\n",
      "Iteration 141, loss = 0.08036242\n",
      "Iteration 142, loss = 0.08020605\n",
      "Iteration 143, loss = 0.07999164\n",
      "Iteration 144, loss = 0.07986900\n",
      "Iteration 145, loss = 0.07981927\n",
      "Iteration 146, loss = 0.07972052\n",
      "Iteration 147, loss = 0.07954517\n",
      "Iteration 148, loss = 0.07939264\n",
      "Iteration 149, loss = 0.07930646\n",
      "Iteration 150, loss = 0.07923179\n",
      "Iteration 151, loss = 0.07911494\n",
      "Iteration 152, loss = 0.07896570\n",
      "Iteration 153, loss = 0.07883784\n",
      "Iteration 154, loss = 0.07874686\n",
      "Iteration 155, loss = 0.07866348\n",
      "Iteration 156, loss = 0.07855936\n",
      "Iteration 157, loss = 0.07843352\n",
      "Iteration 158, loss = 0.07831116\n",
      "Iteration 159, loss = 0.07820778\n",
      "Iteration 160, loss = 0.07811898\n",
      "Iteration 161, loss = 0.07802974\n",
      "Iteration 162, loss = 0.07792893\n",
      "Iteration 163, loss = 0.07781963\n",
      "Iteration 164, loss = 0.07770977\n",
      "Iteration 165, loss = 0.07760723\n",
      "Iteration 166, loss = 0.07751333\n",
      "Iteration 167, loss = 0.07742449\n",
      "Iteration 168, loss = 0.07733602\n",
      "Iteration 169, loss = 0.07724442\n",
      "Iteration 170, loss = 0.07714988\n",
      "Iteration 171, loss = 0.07705319\n",
      "Iteration 172, loss = 0.07695685\n",
      "Iteration 173, loss = 0.07686203\n",
      "Iteration 174, loss = 0.07676961\n",
      "Iteration 175, loss = 0.07667958\n",
      "Iteration 176, loss = 0.07659166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.67599084\n",
      "Iteration 3, loss = 1.66382714\n",
      "Iteration 4, loss = 1.02755535\n",
      "Iteration 5, loss = 1.24755732\n",
      "Iteration 6, loss = 0.62250028\n",
      "Iteration 7, loss = 0.56591440\n",
      "Iteration 8, loss = 0.50405036\n",
      "Iteration 9, loss = 0.45915505\n",
      "Iteration 10, loss = 0.42908856\n",
      "Iteration 11, loss = 0.40417721\n",
      "Iteration 12, loss = 0.38124751\n",
      "Iteration 13, loss = 0.35856057\n",
      "Iteration 14, loss = 0.33520741\n",
      "Iteration 15, loss = 0.31109027\n",
      "Iteration 16, loss = 0.28712078\n",
      "Iteration 17, loss = 0.26600662\n",
      "Iteration 18, loss = 0.28884814\n",
      "Iteration 19, loss = 0.87787156\n",
      "Iteration 20, loss = 2.39955618\n",
      "Iteration 21, loss = 0.89196916\n",
      "Iteration 22, loss = 0.37683979\n",
      "Iteration 23, loss = 0.36084103\n",
      "Iteration 24, loss = 0.34492340\n",
      "Iteration 25, loss = 0.33234927\n",
      "Iteration 26, loss = 0.32334704\n",
      "Iteration 27, loss = 0.31259661\n",
      "Iteration 28, loss = 0.30065919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.64766522\n",
      "Iteration 3, loss = 1.67573023\n",
      "Iteration 4, loss = 1.10824182\n",
      "Iteration 5, loss = 1.32746669\n",
      "Iteration 6, loss = 0.63640802\n",
      "Iteration 7, loss = 0.57045700\n",
      "Iteration 8, loss = 0.50602812\n",
      "Iteration 9, loss = 0.46554345\n",
      "Iteration 10, loss = 0.43416931\n",
      "Iteration 11, loss = 0.40712887\n",
      "Iteration 12, loss = 0.38459684\n",
      "Iteration 13, loss = 0.36286785\n",
      "Iteration 14, loss = 0.34772144\n",
      "Iteration 15, loss = 0.37294608\n",
      "Iteration 16, loss = 0.63402179\n",
      "Iteration 17, loss = 1.26514328\n",
      "Iteration 18, loss = 0.35514532\n",
      "Iteration 19, loss = 0.35436159\n",
      "Iteration 20, loss = 0.30288022\n",
      "Iteration 21, loss = 0.28077313\n",
      "Iteration 22, loss = 0.25894989\n",
      "Iteration 23, loss = 0.24094905\n",
      "Iteration 24, loss = 0.24393650\n",
      "Iteration 25, loss = 0.33849918\n",
      "Iteration 26, loss = 0.93269866\n",
      "Iteration 27, loss = 1.12160194\n",
      "Iteration 28, loss = 0.94099772\n",
      "Iteration 29, loss = 0.30436148\n",
      "Iteration 30, loss = 0.30361014\n",
      "Iteration 31, loss = 0.26891062\n",
      "Iteration 32, loss = 0.25202211\n",
      "Iteration 33, loss = 0.23919603\n",
      "Iteration 34, loss = 0.22757772\n",
      "Iteration 35, loss = 0.21441803\n",
      "Iteration 36, loss = 0.20064846\n",
      "Iteration 37, loss = 0.18719178\n",
      "Iteration 38, loss = 0.17472211\n",
      "Iteration 39, loss = 0.16364942\n",
      "Iteration 40, loss = 0.15407345\n",
      "Iteration 41, loss = 0.14592514\n",
      "Iteration 42, loss = 0.13900108\n",
      "Iteration 43, loss = 0.13318023\n",
      "Iteration 44, loss = 0.12826631\n",
      "Iteration 45, loss = 0.12446755\n",
      "Iteration 46, loss = 0.12264766\n",
      "Iteration 47, loss = 0.12935973\n",
      "Iteration 48, loss = 0.18685187\n",
      "Iteration 49, loss = 0.54996575\n",
      "Iteration 50, loss = 3.62433992\n",
      "Iteration 51, loss = 1.34195099\n",
      "Iteration 52, loss = 1.03531063\n",
      "Iteration 53, loss = 0.55409471\n",
      "Iteration 54, loss = 0.41337382\n",
      "Iteration 55, loss = 0.31620999\n",
      "Iteration 56, loss = 0.32566866\n",
      "Iteration 57, loss = 0.31148464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.64759435\n",
      "Iteration 3, loss = 1.63600662\n",
      "Iteration 4, loss = 0.96085698\n",
      "Iteration 5, loss = 1.24752546\n",
      "Iteration 6, loss = 0.59860221\n",
      "Iteration 7, loss = 0.53089514\n",
      "Iteration 8, loss = 0.47626814\n",
      "Iteration 9, loss = 0.43459843\n",
      "Iteration 10, loss = 0.40207171\n",
      "Iteration 11, loss = 0.37381350\n",
      "Iteration 12, loss = 0.34633597\n",
      "Iteration 13, loss = 0.31863072\n",
      "Iteration 14, loss = 0.29078979\n",
      "Iteration 15, loss = 0.26443792\n",
      "Iteration 16, loss = 0.25965791\n",
      "Iteration 17, loss = 0.51046760\n",
      "Iteration 18, loss = 2.04010419\n",
      "Iteration 19, loss = 1.46018142\n",
      "Iteration 20, loss = 0.31487176\n",
      "Iteration 21, loss = 0.37425393\n",
      "Iteration 22, loss = 0.31886444\n",
      "Iteration 23, loss = 0.30228609\n",
      "Iteration 24, loss = 0.28705367\n",
      "Iteration 25, loss = 0.26540087\n",
      "Iteration 26, loss = 0.24223563\n",
      "Iteration 27, loss = 0.22112071\n",
      "Iteration 28, loss = 0.20083785\n",
      "Iteration 29, loss = 0.18219865\n",
      "Iteration 30, loss = 0.16663620\n",
      "Iteration 31, loss = 0.15357092\n",
      "Iteration 32, loss = 0.14534559\n",
      "Iteration 33, loss = 0.15053737\n",
      "Iteration 34, loss = 0.23006052\n",
      "Iteration 35, loss = 0.99247626\n",
      "Iteration 36, loss = 2.28671439\n",
      "Iteration 37, loss = 0.59135897\n",
      "Iteration 38, loss = 0.40836012\n",
      "Iteration 39, loss = 0.43563949\n",
      "Iteration 40, loss = 0.42715767\n",
      "Iteration 41, loss = 0.43678505\n",
      "Iteration 42, loss = 0.67447107\n",
      "Iteration 43, loss = 1.26935954\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.65717716\n",
      "Iteration 3, loss = 1.66785437\n",
      "Iteration 4, loss = 1.08150368\n",
      "Iteration 5, loss = 1.31520528\n",
      "Iteration 6, loss = 0.62841080\n",
      "Iteration 7, loss = 0.57017333\n",
      "Iteration 8, loss = 0.50090639\n",
      "Iteration 9, loss = 0.45465352\n",
      "Iteration 10, loss = 0.42007807\n",
      "Iteration 11, loss = 0.39121145\n",
      "Iteration 12, loss = 0.36364414\n",
      "Iteration 13, loss = 0.33460731\n",
      "Iteration 14, loss = 0.30750242\n",
      "Iteration 15, loss = 0.28460195\n",
      "Iteration 16, loss = 0.32373448\n",
      "Iteration 17, loss = 0.94442950\n",
      "Iteration 18, loss = 1.76362653\n",
      "Iteration 19, loss = 0.83534130\n",
      "Iteration 20, loss = 0.36770251\n",
      "Iteration 21, loss = 0.35629030\n",
      "Iteration 22, loss = 0.33393929\n",
      "Iteration 23, loss = 0.32061634\n",
      "Iteration 24, loss = 0.30668412\n",
      "Iteration 25, loss = 0.28726206\n",
      "Iteration 26, loss = 0.26329254\n",
      "Iteration 27, loss = 0.24106423\n",
      "Iteration 28, loss = 0.21828965\n",
      "Iteration 29, loss = 0.19605082\n",
      "Iteration 30, loss = 0.17627854\n",
      "Iteration 31, loss = 0.16020549\n",
      "Iteration 32, loss = 0.14712198\n",
      "Iteration 33, loss = 0.13741154\n",
      "Iteration 34, loss = 0.13332888\n",
      "Iteration 35, loss = 0.16321947\n",
      "Iteration 36, loss = 0.48829853\n",
      "Iteration 37, loss = 2.41380078\n",
      "Iteration 38, loss = 0.57022494\n",
      "Iteration 39, loss = 0.31202435\n",
      "Iteration 40, loss = 0.27189806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 0.20808029\n",
      "Iteration 42, loss = 0.21235561\n",
      "Iteration 43, loss = 0.22808467\n",
      "Iteration 44, loss = 0.34956662\n",
      "Iteration 45, loss = 0.51386688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.69296771\n",
      "Iteration 3, loss = 1.69870405\n",
      "Iteration 4, loss = 1.07511183\n",
      "Iteration 5, loss = 1.29173821\n",
      "Iteration 6, loss = 0.64161300\n",
      "Iteration 7, loss = 0.57844565\n",
      "Iteration 8, loss = 0.51077914\n",
      "Iteration 9, loss = 0.46695122\n",
      "Iteration 10, loss = 0.43674951\n",
      "Iteration 11, loss = 0.41172133\n",
      "Iteration 12, loss = 0.38863373\n",
      "Iteration 13, loss = 0.36565544\n",
      "Iteration 14, loss = 0.34143505\n",
      "Iteration 15, loss = 0.31600080\n",
      "Iteration 16, loss = 0.28994208\n",
      "Iteration 17, loss = 0.26469160\n",
      "Iteration 18, loss = 0.24284936\n",
      "Iteration 19, loss = 0.26474688\n",
      "Iteration 20, loss = 0.89887048\n",
      "Iteration 21, loss = 2.94742519\n",
      "Iteration 22, loss = 0.84966473\n",
      "Iteration 23, loss = 0.41305030\n",
      "Iteration 24, loss = 0.33577150\n",
      "Iteration 25, loss = 0.33553256\n",
      "Iteration 26, loss = 0.32059915\n",
      "Iteration 27, loss = 0.31029415\n",
      "Iteration 28, loss = 0.29946781\n",
      "Iteration 29, loss = 0.28837906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.23656474\n",
      "Iteration 3, loss = 1.08521818\n",
      "Iteration 4, loss = 0.99173175\n",
      "Iteration 5, loss = 0.92634460\n",
      "Iteration 6, loss = 0.86402220\n",
      "Iteration 7, loss = 0.79344095\n",
      "Iteration 8, loss = 0.72720746\n",
      "Iteration 9, loss = 0.67654418\n",
      "Iteration 10, loss = 0.64203449\n",
      "Iteration 11, loss = 0.61214574\n",
      "Iteration 12, loss = 0.57961916\n",
      "Iteration 13, loss = 0.54765644\n",
      "Iteration 14, loss = 0.52000057\n",
      "Iteration 15, loss = 0.49565715\n",
      "Iteration 16, loss = 0.47302438\n",
      "Iteration 17, loss = 0.45423104\n",
      "Iteration 18, loss = 0.43932644\n",
      "Iteration 19, loss = 0.42383172\n",
      "Iteration 20, loss = 0.40647242\n",
      "Iteration 21, loss = 0.39155911\n",
      "Iteration 22, loss = 0.37847471\n",
      "Iteration 23, loss = 0.36497322\n",
      "Iteration 24, loss = 0.35211110\n",
      "Iteration 25, loss = 0.34109328\n",
      "Iteration 26, loss = 0.32960785\n",
      "Iteration 27, loss = 0.31703690\n",
      "Iteration 28, loss = 0.30512955\n",
      "Iteration 29, loss = 0.29365742\n",
      "Iteration 30, loss = 0.28276194\n",
      "Iteration 31, loss = 0.27337063\n",
      "Iteration 32, loss = 0.26420430\n",
      "Iteration 33, loss = 0.25454451\n",
      "Iteration 34, loss = 0.24542835\n",
      "Iteration 35, loss = 0.23627314\n",
      "Iteration 36, loss = 0.22710923\n",
      "Iteration 37, loss = 0.21855408\n",
      "Iteration 38, loss = 0.20985864\n",
      "Iteration 39, loss = 0.20184381\n",
      "Iteration 40, loss = 0.19376746\n",
      "Iteration 41, loss = 0.18627064\n",
      "Iteration 42, loss = 0.17872624\n",
      "Iteration 43, loss = 0.17173684\n",
      "Iteration 44, loss = 0.16475789\n",
      "Iteration 45, loss = 0.15834162\n",
      "Iteration 46, loss = 0.15202776\n",
      "Iteration 47, loss = 0.14620721\n",
      "Iteration 48, loss = 0.14079918\n",
      "Iteration 49, loss = 0.13552457\n",
      "Iteration 50, loss = 0.13075457\n",
      "Iteration 51, loss = 0.12636088\n",
      "Iteration 52, loss = 0.12213378\n",
      "Iteration 53, loss = 0.11826484\n",
      "Iteration 54, loss = 0.11481922\n",
      "Iteration 55, loss = 0.11159466\n",
      "Iteration 56, loss = 0.10859276\n",
      "Iteration 57, loss = 0.10577605\n",
      "Iteration 58, loss = 0.10324844\n",
      "Iteration 59, loss = 0.10096668\n",
      "Iteration 60, loss = 0.09893525\n",
      "Iteration 61, loss = 0.09715764\n",
      "Iteration 62, loss = 0.09552589\n",
      "Iteration 63, loss = 0.09390750\n",
      "Iteration 64, loss = 0.09227456\n",
      "Iteration 65, loss = 0.09097479\n",
      "Iteration 66, loss = 0.08998537\n",
      "Iteration 67, loss = 0.08901841\n",
      "Iteration 68, loss = 0.08794594\n",
      "Iteration 69, loss = 0.08691272\n",
      "Iteration 70, loss = 0.08619262\n",
      "Iteration 71, loss = 0.08562549\n",
      "Iteration 72, loss = 0.08495766\n",
      "Iteration 73, loss = 0.08421943\n",
      "Iteration 74, loss = 0.08360353\n",
      "Iteration 75, loss = 0.08317685\n",
      "Iteration 76, loss = 0.08280599\n",
      "Iteration 77, loss = 0.08235367\n",
      "Iteration 78, loss = 0.08185807\n",
      "Iteration 79, loss = 0.08142873\n",
      "Iteration 80, loss = 0.08111805\n",
      "Iteration 81, loss = 0.08086957\n",
      "Iteration 82, loss = 0.08059883\n",
      "Iteration 83, loss = 0.08028049\n",
      "Iteration 84, loss = 0.07993860\n",
      "Iteration 85, loss = 0.07964116\n",
      "Iteration 86, loss = 0.07941097\n",
      "Iteration 87, loss = 0.07922736\n",
      "Iteration 88, loss = 0.07906092\n",
      "Iteration 89, loss = 0.07888376\n",
      "Iteration 90, loss = 0.07868841\n",
      "Iteration 91, loss = 0.07845890\n",
      "Iteration 92, loss = 0.07822450\n",
      "Iteration 93, loss = 0.07800585\n",
      "Iteration 94, loss = 0.07781941\n",
      "Iteration 95, loss = 0.07766352\n",
      "Iteration 96, loss = 0.07754009\n",
      "Iteration 97, loss = 0.07746897\n",
      "Iteration 98, loss = 0.07743010\n",
      "Iteration 99, loss = 0.07742005\n",
      "Iteration 100, loss = 0.07732720\n",
      "Iteration 101, loss = 0.07711546\n",
      "Iteration 102, loss = 0.07677597\n",
      "Iteration 103, loss = 0.07653876\n",
      "Iteration 104, loss = 0.07648244\n",
      "Iteration 105, loss = 0.07649843\n",
      "Iteration 106, loss = 0.07644182\n",
      "Iteration 107, loss = 0.07622748\n",
      "Iteration 108, loss = 0.07599544\n",
      "Iteration 109, loss = 0.07587182\n",
      "Iteration 110, loss = 0.07584843\n",
      "Iteration 111, loss = 0.07581634\n",
      "Iteration 112, loss = 0.07568508\n",
      "Iteration 113, loss = 0.07550552\n",
      "Iteration 114, loss = 0.07536309\n",
      "Iteration 115, loss = 0.07529670\n",
      "Iteration 116, loss = 0.07525973\n",
      "Iteration 117, loss = 0.07518441\n",
      "Iteration 118, loss = 0.07506250\n",
      "Iteration 119, loss = 0.07492365\n",
      "Iteration 120, loss = 0.07481633\n",
      "Iteration 121, loss = 0.07474849\n",
      "Iteration 122, loss = 0.07469501\n",
      "Iteration 123, loss = 0.07462911\n",
      "Iteration 124, loss = 0.07453561\n",
      "Iteration 125, loss = 0.07442805\n",
      "Iteration 126, loss = 0.07432247\n",
      "Iteration 127, loss = 0.07423257\n",
      "Iteration 128, loss = 0.07415929\n",
      "Iteration 129, loss = 0.07409650\n",
      "Iteration 130, loss = 0.07403787\n",
      "Iteration 131, loss = 0.07397896\n",
      "Iteration 132, loss = 0.07392285\n",
      "Iteration 133, loss = 0.07386567\n",
      "Iteration 134, loss = 0.07381750\n",
      "Iteration 135, loss = 0.07376636\n",
      "Iteration 136, loss = 0.07372539\n",
      "Iteration 137, loss = 0.07366580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.23360476\n",
      "Iteration 3, loss = 1.08067148\n",
      "Iteration 4, loss = 0.99422921\n",
      "Iteration 5, loss = 0.93026402\n",
      "Iteration 6, loss = 0.86747655\n",
      "Iteration 7, loss = 0.79695092\n",
      "Iteration 8, loss = 0.73196230\n",
      "Iteration 9, loss = 0.68323897\n",
      "Iteration 10, loss = 0.64836308\n",
      "Iteration 11, loss = 0.61717858\n",
      "Iteration 12, loss = 0.58354468\n",
      "Iteration 13, loss = 0.55113819\n",
      "Iteration 14, loss = 0.52297465\n",
      "Iteration 15, loss = 0.49866760\n",
      "Iteration 16, loss = 0.47637679\n",
      "Iteration 17, loss = 0.45813719\n",
      "Iteration 18, loss = 0.44297493\n",
      "Iteration 19, loss = 0.42702268\n",
      "Iteration 20, loss = 0.40897183\n",
      "Iteration 21, loss = 0.39243168\n",
      "Iteration 22, loss = 0.37858073\n",
      "Iteration 23, loss = 0.36433778\n",
      "Iteration 24, loss = 0.35142268\n",
      "Iteration 25, loss = 0.34011110\n",
      "Iteration 26, loss = 0.32730154\n",
      "Iteration 27, loss = 0.31402532\n",
      "Iteration 28, loss = 0.30177826\n",
      "Iteration 29, loss = 0.29016321\n",
      "Iteration 30, loss = 0.27980892\n",
      "Iteration 31, loss = 0.27019054\n",
      "Iteration 32, loss = 0.26009744\n",
      "Iteration 33, loss = 0.24996014\n",
      "Iteration 34, loss = 0.24046113\n",
      "Iteration 35, loss = 0.23063286\n",
      "Iteration 36, loss = 0.22128655\n",
      "Iteration 37, loss = 0.21213039\n",
      "Iteration 38, loss = 0.20322658\n",
      "Iteration 39, loss = 0.19479131\n",
      "Iteration 40, loss = 0.18645891\n",
      "Iteration 41, loss = 0.17871559\n",
      "Iteration 42, loss = 0.17092842\n",
      "Iteration 43, loss = 0.16377282\n",
      "Iteration 44, loss = 0.15669556\n",
      "Iteration 45, loss = 0.15011016\n",
      "Iteration 46, loss = 0.14380154\n",
      "Iteration 47, loss = 0.13784375\n",
      "Iteration 48, loss = 0.13243616\n",
      "Iteration 49, loss = 0.12722206\n",
      "Iteration 50, loss = 0.12247042\n",
      "Iteration 51, loss = 0.11814235\n",
      "Iteration 52, loss = 0.11400919\n",
      "Iteration 53, loss = 0.11021465\n",
      "Iteration 54, loss = 0.10680631\n",
      "Iteration 55, loss = 0.10367424\n",
      "Iteration 56, loss = 0.10080572\n",
      "Iteration 57, loss = 0.09810657\n",
      "Iteration 58, loss = 0.09565566\n",
      "Iteration 59, loss = 0.09351010\n",
      "Iteration 60, loss = 0.09159301\n",
      "Iteration 61, loss = 0.08990614\n",
      "Iteration 62, loss = 0.08837751\n",
      "Iteration 63, loss = 0.08693785\n",
      "Iteration 64, loss = 0.08547163\n",
      "Iteration 65, loss = 0.08423656\n",
      "Iteration 66, loss = 0.08330173\n",
      "Iteration 67, loss = 0.08246651\n",
      "Iteration 68, loss = 0.08158116\n",
      "Iteration 69, loss = 0.08066492\n",
      "Iteration 70, loss = 0.07995462\n",
      "Iteration 71, loss = 0.07944355\n",
      "Iteration 72, loss = 0.07894449\n",
      "Iteration 73, loss = 0.07837202\n",
      "Iteration 74, loss = 0.07780139\n",
      "Iteration 75, loss = 0.07737403\n",
      "Iteration 76, loss = 0.07706880\n",
      "Iteration 77, loss = 0.07677162\n",
      "Iteration 78, loss = 0.07642263\n",
      "Iteration 79, loss = 0.07604611\n",
      "Iteration 80, loss = 0.07573761\n",
      "Iteration 81, loss = 0.07552149\n",
      "Iteration 82, loss = 0.07535297\n",
      "Iteration 83, loss = 0.07517691\n",
      "Iteration 84, loss = 0.07495400\n",
      "Iteration 85, loss = 0.07471488\n",
      "Iteration 86, loss = 0.07450109\n",
      "Iteration 87, loss = 0.07434113\n",
      "Iteration 88, loss = 0.07422333\n",
      "Iteration 89, loss = 0.07411976\n",
      "Iteration 90, loss = 0.07401313\n",
      "Iteration 91, loss = 0.07387961\n",
      "Iteration 92, loss = 0.07372978\n",
      "Iteration 93, loss = 0.07356961\n",
      "Iteration 94, loss = 0.07342561\n",
      "Iteration 95, loss = 0.07330673\n",
      "Iteration 96, loss = 0.07321125\n",
      "Iteration 97, loss = 0.07313163\n",
      "Iteration 98, loss = 0.07306029\n",
      "Iteration 99, loss = 0.07299644\n",
      "Iteration 100, loss = 0.07292621\n",
      "Iteration 101, loss = 0.07285272\n",
      "Iteration 102, loss = 0.07274803\n",
      "Iteration 103, loss = 0.07262745\n",
      "Iteration 104, loss = 0.07249064\n",
      "Iteration 105, loss = 0.07236800\n",
      "Iteration 106, loss = 0.07227025\n",
      "Iteration 107, loss = 0.07219725\n",
      "Iteration 108, loss = 0.07214077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109, loss = 0.07209018\n",
      "Iteration 110, loss = 0.07204155\n",
      "Iteration 111, loss = 0.07197932\n",
      "Iteration 112, loss = 0.07190766\n",
      "Iteration 113, loss = 0.07181032\n",
      "Iteration 114, loss = 0.07170470\n",
      "Iteration 115, loss = 0.07159637\n",
      "Iteration 116, loss = 0.07150077\n",
      "Iteration 117, loss = 0.07142188\n",
      "Iteration 118, loss = 0.07135735\n",
      "Iteration 119, loss = 0.07130286\n",
      "Iteration 120, loss = 0.07125598\n",
      "Iteration 121, loss = 0.07121863\n",
      "Iteration 122, loss = 0.07118913\n",
      "Iteration 123, loss = 0.07117951\n",
      "Iteration 124, loss = 0.07116596\n",
      "Iteration 125, loss = 0.07115870\n",
      "Iteration 126, loss = 0.07107817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.23533066\n",
      "Iteration 3, loss = 1.08499961\n",
      "Iteration 4, loss = 0.98847314\n",
      "Iteration 5, loss = 0.92115040\n",
      "Iteration 6, loss = 0.86012722\n",
      "Iteration 7, loss = 0.79200685\n",
      "Iteration 8, loss = 0.72818655\n",
      "Iteration 9, loss = 0.67950369\n",
      "Iteration 10, loss = 0.64416378\n",
      "Iteration 11, loss = 0.61117829\n",
      "Iteration 12, loss = 0.57574545\n",
      "Iteration 13, loss = 0.54232525\n",
      "Iteration 14, loss = 0.51350241\n",
      "Iteration 15, loss = 0.48801572\n",
      "Iteration 16, loss = 0.46482474\n",
      "Iteration 17, loss = 0.44553659\n",
      "Iteration 18, loss = 0.43011852\n",
      "Iteration 19, loss = 0.41492458\n",
      "Iteration 20, loss = 0.39716313\n",
      "Iteration 21, loss = 0.37898477\n",
      "Iteration 22, loss = 0.36429464\n",
      "Iteration 23, loss = 0.35030410\n",
      "Iteration 24, loss = 0.33646605\n",
      "Iteration 25, loss = 0.32432543\n",
      "Iteration 26, loss = 0.31190733\n",
      "Iteration 27, loss = 0.29875064\n",
      "Iteration 28, loss = 0.28575383\n",
      "Iteration 29, loss = 0.27349325\n",
      "Iteration 30, loss = 0.26160296\n",
      "Iteration 31, loss = 0.25114300\n",
      "Iteration 32, loss = 0.24112610\n",
      "Iteration 33, loss = 0.23067815\n",
      "Iteration 34, loss = 0.22051937\n",
      "Iteration 35, loss = 0.21066495\n",
      "Iteration 36, loss = 0.20066968\n",
      "Iteration 37, loss = 0.19136187\n",
      "Iteration 38, loss = 0.18208193\n",
      "Iteration 39, loss = 0.17335564\n",
      "Iteration 40, loss = 0.16473375\n",
      "Iteration 41, loss = 0.15668717\n",
      "Iteration 42, loss = 0.14887578\n",
      "Iteration 43, loss = 0.14145726\n",
      "Iteration 44, loss = 0.13431938\n",
      "Iteration 45, loss = 0.12761456\n",
      "Iteration 46, loss = 0.12124208\n",
      "Iteration 47, loss = 0.11536862\n",
      "Iteration 48, loss = 0.10979182\n",
      "Iteration 49, loss = 0.10470698\n",
      "Iteration 50, loss = 0.09993947\n",
      "Iteration 51, loss = 0.09553557\n",
      "Iteration 52, loss = 0.09158286\n",
      "Iteration 53, loss = 0.08785205\n",
      "Iteration 54, loss = 0.08444013\n",
      "Iteration 55, loss = 0.08138745\n",
      "Iteration 56, loss = 0.07861312\n",
      "Iteration 57, loss = 0.07605989\n",
      "Iteration 58, loss = 0.07379095\n",
      "Iteration 59, loss = 0.07177511\n",
      "Iteration 60, loss = 0.06997401\n",
      "Iteration 61, loss = 0.06834804\n",
      "Iteration 62, loss = 0.06686773\n",
      "Iteration 63, loss = 0.06555732\n",
      "Iteration 64, loss = 0.06440388\n",
      "Iteration 65, loss = 0.06338646\n",
      "Iteration 66, loss = 0.06250029\n",
      "Iteration 67, loss = 0.06173084\n",
      "Iteration 68, loss = 0.06102335\n",
      "Iteration 69, loss = 0.06036788\n",
      "Iteration 70, loss = 0.05977556\n",
      "Iteration 71, loss = 0.05927563\n",
      "Iteration 72, loss = 0.05886315\n",
      "Iteration 73, loss = 0.05849985\n",
      "Iteration 74, loss = 0.05815639\n",
      "Iteration 75, loss = 0.05781808\n",
      "Iteration 76, loss = 0.05750661\n",
      "Iteration 77, loss = 0.05724598\n",
      "Iteration 78, loss = 0.05702606\n",
      "Iteration 79, loss = 0.05682585\n",
      "Iteration 80, loss = 0.05662238\n",
      "Iteration 81, loss = 0.05641939\n",
      "Iteration 82, loss = 0.05623354\n",
      "Iteration 83, loss = 0.05607522\n",
      "Iteration 84, loss = 0.05593669\n",
      "Iteration 85, loss = 0.05580354\n",
      "Iteration 86, loss = 0.05566662\n",
      "Iteration 87, loss = 0.05552666\n",
      "Iteration 88, loss = 0.05539287\n",
      "Iteration 89, loss = 0.05527234\n",
      "Iteration 90, loss = 0.05516418\n",
      "Iteration 91, loss = 0.05506332\n",
      "Iteration 92, loss = 0.05496263\n",
      "Iteration 93, loss = 0.05485929\n",
      "Iteration 94, loss = 0.05475453\n",
      "Iteration 95, loss = 0.05465234\n",
      "Iteration 96, loss = 0.05455558\n",
      "Iteration 97, loss = 0.05446464\n",
      "Iteration 98, loss = 0.05437809\n",
      "Iteration 99, loss = 0.05429406\n",
      "Iteration 100, loss = 0.05421099\n",
      "Iteration 101, loss = 0.05412793\n",
      "Iteration 102, loss = 0.05404467\n",
      "Iteration 103, loss = 0.05396138\n",
      "Iteration 104, loss = 0.05387835\n",
      "Iteration 105, loss = 0.05379632\n",
      "Iteration 106, loss = 0.05371573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.23473898\n",
      "Iteration 3, loss = 1.07985780\n",
      "Iteration 4, loss = 0.99117602\n",
      "Iteration 5, loss = 0.92601102\n",
      "Iteration 6, loss = 0.86234976\n",
      "Iteration 7, loss = 0.79102258\n",
      "Iteration 8, loss = 0.72454497\n",
      "Iteration 9, loss = 0.67399180\n",
      "Iteration 10, loss = 0.63877006\n",
      "Iteration 11, loss = 0.60786464\n",
      "Iteration 12, loss = 0.57407537\n",
      "Iteration 13, loss = 0.54076063\n",
      "Iteration 14, loss = 0.51202583\n",
      "Iteration 15, loss = 0.48755250\n",
      "Iteration 16, loss = 0.46450532\n",
      "Iteration 17, loss = 0.44499361\n",
      "Iteration 18, loss = 0.42829841\n",
      "Iteration 19, loss = 0.41060506\n",
      "Iteration 20, loss = 0.39317345\n",
      "Iteration 21, loss = 0.37766282\n",
      "Iteration 22, loss = 0.36307684\n",
      "Iteration 23, loss = 0.34840919\n",
      "Iteration 24, loss = 0.33490982\n",
      "Iteration 25, loss = 0.32256317\n",
      "Iteration 26, loss = 0.30933145\n",
      "Iteration 27, loss = 0.29531756\n",
      "Iteration 28, loss = 0.28209711\n",
      "Iteration 29, loss = 0.27063646\n",
      "Iteration 30, loss = 0.26005356\n",
      "Iteration 31, loss = 0.24977669\n",
      "Iteration 32, loss = 0.23953283\n",
      "Iteration 33, loss = 0.22924070\n",
      "Iteration 34, loss = 0.21919274\n",
      "Iteration 35, loss = 0.20922366\n",
      "Iteration 36, loss = 0.19947487\n",
      "Iteration 37, loss = 0.19009142\n",
      "Iteration 38, loss = 0.18090846\n",
      "Iteration 39, loss = 0.17217592\n",
      "Iteration 40, loss = 0.16369817\n",
      "Iteration 41, loss = 0.15563512\n",
      "Iteration 42, loss = 0.14781953\n",
      "Iteration 43, loss = 0.14032470\n",
      "Iteration 44, loss = 0.13313918\n",
      "Iteration 45, loss = 0.12638810\n",
      "Iteration 46, loss = 0.11993468\n",
      "Iteration 47, loss = 0.11388096\n",
      "Iteration 48, loss = 0.10820550\n",
      "Iteration 49, loss = 0.10285513\n",
      "Iteration 50, loss = 0.09790777\n",
      "Iteration 51, loss = 0.09331576\n",
      "Iteration 52, loss = 0.08901920\n",
      "Iteration 53, loss = 0.08508260\n",
      "Iteration 54, loss = 0.08144257\n",
      "Iteration 55, loss = 0.07809942\n",
      "Iteration 56, loss = 0.07505628\n",
      "Iteration 57, loss = 0.07227293\n",
      "Iteration 58, loss = 0.06981408\n",
      "Iteration 59, loss = 0.06753853\n",
      "Iteration 60, loss = 0.06543945\n",
      "Iteration 61, loss = 0.06344834\n",
      "Iteration 62, loss = 0.06172226\n",
      "Iteration 63, loss = 0.06023697\n",
      "Iteration 64, loss = 0.05886582\n",
      "Iteration 65, loss = 0.05754750\n",
      "Iteration 66, loss = 0.05628040\n",
      "Iteration 67, loss = 0.05518451\n",
      "Iteration 68, loss = 0.05426147\n",
      "Iteration 69, loss = 0.05343879\n",
      "Iteration 70, loss = 0.05265088\n",
      "Iteration 71, loss = 0.05184591\n",
      "Iteration 72, loss = 0.05110315\n",
      "Iteration 73, loss = 0.05047957\n",
      "Iteration 74, loss = 0.04996080\n",
      "Iteration 75, loss = 0.04950022\n",
      "Iteration 76, loss = 0.04903320\n",
      "Iteration 77, loss = 0.04855186\n",
      "Iteration 78, loss = 0.04807371\n",
      "Iteration 79, loss = 0.04766218\n",
      "Iteration 80, loss = 0.04733028\n",
      "Iteration 81, loss = 0.04704965\n",
      "Iteration 82, loss = 0.04679130\n",
      "Iteration 83, loss = 0.04651271\n",
      "Iteration 84, loss = 0.04621162\n",
      "Iteration 85, loss = 0.04589521\n",
      "Iteration 86, loss = 0.04561574\n",
      "Iteration 87, loss = 0.04539111\n",
      "Iteration 88, loss = 0.04520855\n",
      "Iteration 89, loss = 0.04504789\n",
      "Iteration 90, loss = 0.04488226\n",
      "Iteration 91, loss = 0.04470357\n",
      "Iteration 92, loss = 0.04448659\n",
      "Iteration 93, loss = 0.04426386\n",
      "Iteration 94, loss = 0.04406065\n",
      "Iteration 95, loss = 0.04389702\n",
      "Iteration 96, loss = 0.04376678\n",
      "Iteration 97, loss = 0.04365432\n",
      "Iteration 98, loss = 0.04354909\n",
      "Iteration 99, loss = 0.04342887\n",
      "Iteration 100, loss = 0.04329427\n",
      "Iteration 101, loss = 0.04312854\n",
      "Iteration 102, loss = 0.04296120\n",
      "Iteration 103, loss = 0.04280889\n",
      "Iteration 104, loss = 0.04268546\n",
      "Iteration 105, loss = 0.04258744\n",
      "Iteration 106, loss = 0.04250342\n",
      "Iteration 107, loss = 0.04242847\n",
      "Iteration 108, loss = 0.04234824\n",
      "Iteration 109, loss = 0.04226233\n",
      "Iteration 110, loss = 0.04214979\n",
      "Iteration 111, loss = 0.04202435\n",
      "Iteration 112, loss = 0.04189019\n",
      "Iteration 113, loss = 0.04177077\n",
      "Iteration 114, loss = 0.04167401\n",
      "Iteration 115, loss = 0.04159770\n",
      "Iteration 116, loss = 0.04153435\n",
      "Iteration 117, loss = 0.04147576\n",
      "Iteration 118, loss = 0.04141908\n",
      "Iteration 119, loss = 0.04135088\n",
      "Iteration 120, loss = 0.04127556\n",
      "Iteration 121, loss = 0.04117911\n",
      "Iteration 122, loss = 0.04107739\n",
      "Iteration 123, loss = 0.04097484\n",
      "Iteration 124, loss = 0.04088466\n",
      "Iteration 125, loss = 0.04081008\n",
      "Iteration 126, loss = 0.04074758\n",
      "Iteration 127, loss = 0.04069447\n",
      "Iteration 128, loss = 0.04064581\n",
      "Iteration 129, loss = 0.04060159\n",
      "Iteration 130, loss = 0.04055509\n",
      "Iteration 131, loss = 0.04051135\n",
      "Iteration 132, loss = 0.04045482\n",
      "Iteration 133, loss = 0.04039470\n",
      "Iteration 134, loss = 0.04031492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.24126068\n",
      "Iteration 3, loss = 1.08003592\n",
      "Iteration 4, loss = 0.99074829\n",
      "Iteration 5, loss = 0.92572514\n",
      "Iteration 6, loss = 0.86200977\n",
      "Iteration 7, loss = 0.79018830\n",
      "Iteration 8, loss = 0.72314797\n",
      "Iteration 9, loss = 0.67223048\n",
      "Iteration 10, loss = 0.63761892\n",
      "Iteration 11, loss = 0.60748155\n",
      "Iteration 12, loss = 0.57480771\n",
      "Iteration 13, loss = 0.54322351\n",
      "Iteration 14, loss = 0.51570977\n",
      "Iteration 15, loss = 0.49079658\n",
      "Iteration 16, loss = 0.46879340\n",
      "Iteration 17, loss = 0.45211262\n",
      "Iteration 18, loss = 0.43808782\n",
      "Iteration 19, loss = 0.42196414\n",
      "Iteration 20, loss = 0.40476936\n",
      "Iteration 21, loss = 0.39067168\n",
      "Iteration 22, loss = 0.37735072\n",
      "Iteration 23, loss = 0.36370710\n",
      "Iteration 24, loss = 0.35198873\n",
      "Iteration 25, loss = 0.34140726\n",
      "Iteration 26, loss = 0.32954658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.31790075\n",
      "Iteration 28, loss = 0.30719520\n",
      "Iteration 29, loss = 0.29571552\n",
      "Iteration 30, loss = 0.28432361\n",
      "Iteration 31, loss = 0.27358505\n",
      "Iteration 32, loss = 0.26333058\n",
      "Iteration 33, loss = 0.25381338\n",
      "Iteration 34, loss = 0.24496929\n",
      "Iteration 35, loss = 0.23560205\n",
      "Iteration 36, loss = 0.22649009\n",
      "Iteration 37, loss = 0.21769176\n",
      "Iteration 38, loss = 0.20894313\n",
      "Iteration 39, loss = 0.20084348\n",
      "Iteration 40, loss = 0.19272588\n",
      "Iteration 41, loss = 0.18523505\n",
      "Iteration 42, loss = 0.17771984\n",
      "Iteration 43, loss = 0.17078927\n",
      "Iteration 44, loss = 0.16386945\n",
      "Iteration 45, loss = 0.15751986\n",
      "Iteration 46, loss = 0.15120906\n",
      "Iteration 47, loss = 0.14547107\n",
      "Iteration 48, loss = 0.13994591\n",
      "Iteration 49, loss = 0.13476314\n",
      "Iteration 50, loss = 0.13007937\n",
      "Iteration 51, loss = 0.12557895\n",
      "Iteration 52, loss = 0.12140029\n",
      "Iteration 53, loss = 0.11768898\n",
      "Iteration 54, loss = 0.11423461\n",
      "Iteration 55, loss = 0.11090947\n",
      "Iteration 56, loss = 0.10792415\n",
      "Iteration 57, loss = 0.10530496\n",
      "Iteration 58, loss = 0.10291689\n",
      "Iteration 59, loss = 0.10069614\n",
      "Iteration 60, loss = 0.09856394\n",
      "Iteration 61, loss = 0.09662404\n",
      "Iteration 62, loss = 0.09492928\n",
      "Iteration 63, loss = 0.09345696\n",
      "Iteration 64, loss = 0.09216177\n",
      "Iteration 65, loss = 0.09095931\n",
      "Iteration 66, loss = 0.08975817\n",
      "Iteration 67, loss = 0.08853454\n",
      "Iteration 68, loss = 0.08749964\n",
      "Iteration 69, loss = 0.08671491\n",
      "Iteration 70, loss = 0.08605243\n",
      "Iteration 71, loss = 0.08536945\n",
      "Iteration 72, loss = 0.08458904\n",
      "Iteration 73, loss = 0.08386164\n",
      "Iteration 74, loss = 0.08331282\n",
      "Iteration 75, loss = 0.08290470\n",
      "Iteration 76, loss = 0.08252192\n",
      "Iteration 77, loss = 0.08206187\n",
      "Iteration 78, loss = 0.08155246\n",
      "Iteration 79, loss = 0.08108642\n",
      "Iteration 80, loss = 0.08074123\n",
      "Iteration 81, loss = 0.08048770\n",
      "Iteration 82, loss = 0.08024293\n",
      "Iteration 83, loss = 0.07995865\n",
      "Iteration 84, loss = 0.07961452\n",
      "Iteration 85, loss = 0.07928076\n",
      "Iteration 86, loss = 0.07900922\n",
      "Iteration 87, loss = 0.07880824\n",
      "Iteration 88, loss = 0.07864860\n",
      "Iteration 89, loss = 0.07849338\n",
      "Iteration 90, loss = 0.07832118\n",
      "Iteration 91, loss = 0.07810692\n",
      "Iteration 92, loss = 0.07787483\n",
      "Iteration 93, loss = 0.07765187\n",
      "Iteration 94, loss = 0.07746752\n",
      "Iteration 95, loss = 0.07732295\n",
      "Iteration 96, loss = 0.07720533\n",
      "Iteration 97, loss = 0.07710246\n",
      "Iteration 98, loss = 0.07699939\n",
      "Iteration 99, loss = 0.07689395\n",
      "Iteration 100, loss = 0.07675905\n",
      "Iteration 101, loss = 0.07660413\n",
      "Iteration 102, loss = 0.07642431\n",
      "Iteration 103, loss = 0.07625321\n",
      "Iteration 104, loss = 0.07610705\n",
      "Iteration 105, loss = 0.07599184\n",
      "Iteration 106, loss = 0.07590035\n",
      "Iteration 107, loss = 0.07582245\n",
      "Iteration 108, loss = 0.07575302\n",
      "Iteration 109, loss = 0.07568055\n",
      "Iteration 110, loss = 0.07560601\n",
      "Iteration 111, loss = 0.07550391\n",
      "Iteration 112, loss = 0.07538288\n",
      "Iteration 113, loss = 0.07523506\n",
      "Iteration 114, loss = 0.07509008\n",
      "Iteration 115, loss = 0.07496290\n",
      "Iteration 116, loss = 0.07486322\n",
      "Iteration 117, loss = 0.07478695\n",
      "Iteration 118, loss = 0.07472524\n",
      "Iteration 119, loss = 0.07467098\n",
      "Iteration 120, loss = 0.07461422\n",
      "Iteration 121, loss = 0.07455665\n",
      "Iteration 122, loss = 0.07448028\n",
      "Iteration 123, loss = 0.07439324\n",
      "Iteration 124, loss = 0.07428245\n",
      "Iteration 125, loss = 0.07416768\n",
      "Iteration 126, loss = 0.07405239\n",
      "Iteration 127, loss = 0.07394993\n",
      "Iteration 128, loss = 0.07386205\n",
      "Iteration 129, loss = 0.07378727\n",
      "Iteration 130, loss = 0.07372266\n",
      "Iteration 131, loss = 0.07366758\n",
      "Iteration 132, loss = 0.07362742\n",
      "Iteration 133, loss = 0.07361093\n",
      "Iteration 134, loss = 0.07364653\n",
      "Iteration 135, loss = 0.07372477\n",
      "Iteration 136, loss = 0.07385793\n",
      "Iteration 137, loss = 0.07381074\n",
      "Iteration 138, loss = 0.07357509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.35470632\n",
      "Iteration 3, loss = 1.20781357\n",
      "Iteration 4, loss = 1.09587557\n",
      "Iteration 5, loss = 1.03315165\n",
      "Iteration 6, loss = 0.98464934\n",
      "Iteration 7, loss = 0.92354956\n",
      "Iteration 8, loss = 0.85782788\n",
      "Iteration 9, loss = 0.79882466\n",
      "Iteration 10, loss = 0.74802128\n",
      "Iteration 11, loss = 0.70342928\n",
      "Iteration 12, loss = 0.66459326\n",
      "Iteration 13, loss = 0.63102446\n",
      "Iteration 14, loss = 0.60216382\n",
      "Iteration 15, loss = 0.57663847\n",
      "Iteration 16, loss = 0.55364079\n",
      "Iteration 17, loss = 0.53281372\n",
      "Iteration 18, loss = 0.51406284\n",
      "Iteration 19, loss = 0.49723402\n",
      "Iteration 20, loss = 0.48210990\n",
      "Iteration 21, loss = 0.46843593\n",
      "Iteration 22, loss = 0.45594942\n",
      "Iteration 23, loss = 0.44440323\n",
      "Iteration 24, loss = 0.43370869\n",
      "Iteration 25, loss = 0.42356145\n",
      "Iteration 26, loss = 0.41456710\n",
      "Iteration 27, loss = 0.40653777\n",
      "Iteration 28, loss = 0.39899485\n",
      "Iteration 29, loss = 0.39180649\n",
      "Iteration 30, loss = 0.38489487\n",
      "Iteration 31, loss = 0.37813147\n",
      "Iteration 32, loss = 0.37156679\n",
      "Iteration 33, loss = 0.36523220\n",
      "Iteration 34, loss = 0.35913518\n",
      "Iteration 35, loss = 0.35320117\n",
      "Iteration 36, loss = 0.34770778\n",
      "Iteration 37, loss = 0.34261430\n",
      "Iteration 38, loss = 0.33782372\n",
      "Iteration 39, loss = 0.33323810\n",
      "Iteration 40, loss = 0.32878406\n",
      "Iteration 41, loss = 0.32445831\n",
      "Iteration 42, loss = 0.32024272\n",
      "Iteration 43, loss = 0.31613889\n",
      "Iteration 44, loss = 0.31214770\n",
      "Iteration 45, loss = 0.30826688\n",
      "Iteration 46, loss = 0.30447437\n",
      "Iteration 47, loss = 0.30076215\n",
      "Iteration 48, loss = 0.29712786\n",
      "Iteration 49, loss = 0.29357146\n",
      "Iteration 50, loss = 0.29009289\n",
      "Iteration 51, loss = 0.28669362\n",
      "Iteration 52, loss = 0.28337217\n",
      "Iteration 53, loss = 0.28012181\n",
      "Iteration 54, loss = 0.27694409\n",
      "Iteration 55, loss = 0.27383114\n",
      "Iteration 56, loss = 0.27077981\n",
      "Iteration 57, loss = 0.26779569\n",
      "Iteration 58, loss = 0.26488105\n",
      "Iteration 59, loss = 0.26200837\n",
      "Iteration 60, loss = 0.25917071\n",
      "Iteration 61, loss = 0.25634901\n",
      "Iteration 62, loss = 0.25354144\n",
      "Iteration 63, loss = 0.25072014\n",
      "Iteration 64, loss = 0.24787371\n",
      "Iteration 65, loss = 0.24504640\n",
      "Iteration 66, loss = 0.24218877\n",
      "Iteration 67, loss = 0.23922628\n",
      "Iteration 68, loss = 0.23667674\n",
      "Iteration 69, loss = 0.23444236\n",
      "Iteration 70, loss = 0.23234349\n",
      "Iteration 71, loss = 0.23031154\n",
      "Iteration 72, loss = 0.22830967\n",
      "Iteration 73, loss = 0.22633099\n",
      "Iteration 74, loss = 0.22437875\n",
      "Iteration 75, loss = 0.22245522\n",
      "Iteration 76, loss = 0.22056326\n",
      "Iteration 77, loss = 0.21869545\n",
      "Iteration 78, loss = 0.21685534\n",
      "Iteration 79, loss = 0.21504427\n",
      "Iteration 80, loss = 0.21326380\n",
      "Iteration 81, loss = 0.21151474\n",
      "Iteration 82, loss = 0.20979742\n",
      "Iteration 83, loss = 0.20810975\n",
      "Iteration 84, loss = 0.20645220\n",
      "Iteration 85, loss = 0.20482562\n",
      "Iteration 86, loss = 0.20322813\n",
      "Iteration 87, loss = 0.20166329\n",
      "Iteration 88, loss = 0.20012990\n",
      "Iteration 89, loss = 0.19864089\n",
      "Iteration 90, loss = 0.19718277\n",
      "Iteration 91, loss = 0.19575930\n",
      "Iteration 92, loss = 0.19437535\n",
      "Iteration 93, loss = 0.19302272\n",
      "Iteration 94, loss = 0.19169834\n",
      "Iteration 95, loss = 0.19039742\n",
      "Iteration 96, loss = 0.18911877\n",
      "Iteration 97, loss = 0.18786130\n",
      "Iteration 98, loss = 0.18662491\n",
      "Iteration 99, loss = 0.18540946\n",
      "Iteration 100, loss = 0.18421483\n",
      "Iteration 101, loss = 0.18304223\n",
      "Iteration 102, loss = 0.18189437\n",
      "Iteration 103, loss = 0.18076842\n",
      "Iteration 104, loss = 0.17966327\n",
      "Iteration 105, loss = 0.17857889\n",
      "Iteration 106, loss = 0.17751566\n",
      "Iteration 107, loss = 0.17647227\n",
      "Iteration 108, loss = 0.17545046\n",
      "Iteration 109, loss = 0.17444579\n",
      "Iteration 110, loss = 0.17345814\n",
      "Iteration 111, loss = 0.17248731\n",
      "Iteration 112, loss = 0.17153381\n",
      "Iteration 113, loss = 0.17059709\n",
      "Iteration 114, loss = 0.16967832\n",
      "Iteration 115, loss = 0.16877558\n",
      "Iteration 116, loss = 0.16788849\n",
      "Iteration 117, loss = 0.16701660\n",
      "Iteration 118, loss = 0.16615952\n",
      "Iteration 119, loss = 0.16531696\n",
      "Iteration 120, loss = 0.16448862\n",
      "Iteration 121, loss = 0.16367414\n",
      "Iteration 122, loss = 0.16287323\n",
      "Iteration 123, loss = 0.16208564\n",
      "Iteration 124, loss = 0.16131142\n",
      "Iteration 125, loss = 0.16055011\n",
      "Iteration 126, loss = 0.15980103\n",
      "Iteration 127, loss = 0.15906431\n",
      "Iteration 128, loss = 0.15833978\n",
      "Iteration 129, loss = 0.15762712\n",
      "Iteration 130, loss = 0.15692569\n",
      "Iteration 131, loss = 0.15623582\n",
      "Iteration 132, loss = 0.15555761\n",
      "Iteration 133, loss = 0.15488966\n",
      "Iteration 134, loss = 0.15423252\n",
      "Iteration 135, loss = 0.15358580\n",
      "Iteration 136, loss = 0.15294896\n",
      "Iteration 137, loss = 0.15232217\n",
      "Iteration 138, loss = 0.15170463\n",
      "Iteration 139, loss = 0.15109633\n",
      "Iteration 140, loss = 0.15049815\n",
      "Iteration 141, loss = 0.14990787\n",
      "Iteration 142, loss = 0.14932722\n",
      "Iteration 143, loss = 0.14875553\n",
      "Iteration 144, loss = 0.14819220\n",
      "Iteration 145, loss = 0.14763718\n",
      "Iteration 146, loss = 0.14709045\n",
      "Iteration 147, loss = 0.14655166\n",
      "Iteration 148, loss = 0.14602083\n",
      "Iteration 149, loss = 0.14549780\n",
      "Iteration 150, loss = 0.14498286\n",
      "Iteration 151, loss = 0.14447593\n",
      "Iteration 152, loss = 0.14397604\n",
      "Iteration 153, loss = 0.14348312\n",
      "Iteration 154, loss = 0.14299706\n",
      "Iteration 155, loss = 0.14251803\n",
      "Iteration 156, loss = 0.14204526\n",
      "Iteration 157, loss = 0.14157943\n",
      "Iteration 158, loss = 0.14112018\n",
      "Iteration 159, loss = 0.14066717\n",
      "Iteration 160, loss = 0.14022035\n",
      "Iteration 161, loss = 0.13977966\n",
      "Iteration 162, loss = 0.13934494\n",
      "Iteration 163, loss = 0.13891598\n",
      "Iteration 164, loss = 0.13849298\n",
      "Iteration 165, loss = 0.13807596\n",
      "Iteration 166, loss = 0.13766265\n",
      "Iteration 167, loss = 0.13725359\n",
      "Iteration 168, loss = 0.13684990\n",
      "Iteration 169, loss = 0.13645128\n",
      "Iteration 170, loss = 0.13605801\n",
      "Iteration 171, loss = 0.13567015\n",
      "Iteration 172, loss = 0.13528622\n",
      "Iteration 173, loss = 0.13490682\n",
      "Iteration 174, loss = 0.13453607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 175, loss = 0.13417061\n",
      "Iteration 176, loss = 0.13381003\n",
      "Iteration 177, loss = 0.13345494\n",
      "Iteration 178, loss = 0.13310473\n",
      "Iteration 179, loss = 0.13275903\n",
      "Iteration 180, loss = 0.13241778\n",
      "Iteration 181, loss = 0.13208044\n",
      "Iteration 182, loss = 0.13174707\n",
      "Iteration 183, loss = 0.13141711\n",
      "Iteration 184, loss = 0.13109111\n",
      "Iteration 185, loss = 0.13076852\n",
      "Iteration 186, loss = 0.13044963\n",
      "Iteration 187, loss = 0.13013455\n",
      "Iteration 188, loss = 0.12982313\n",
      "Iteration 189, loss = 0.12951553\n",
      "Iteration 190, loss = 0.12921158\n",
      "Iteration 191, loss = 0.12891101\n",
      "Iteration 192, loss = 0.12861436\n",
      "Iteration 193, loss = 0.12832075\n",
      "Iteration 194, loss = 0.12803107\n",
      "Iteration 195, loss = 0.12774445\n",
      "Iteration 196, loss = 0.12746117\n",
      "Iteration 197, loss = 0.12718105\n",
      "Iteration 198, loss = 0.12690398\n",
      "Iteration 199, loss = 0.12663012\n",
      "Iteration 200, loss = 0.12635923\n",
      "Iteration 201, loss = 0.12609126\n",
      "Iteration 202, loss = 0.12582630\n",
      "Iteration 203, loss = 0.12556422\n",
      "Iteration 204, loss = 0.12530483\n",
      "Iteration 205, loss = 0.12504828\n",
      "Iteration 206, loss = 0.12479469\n",
      "Iteration 207, loss = 0.12454364\n",
      "Iteration 208, loss = 0.12429523\n",
      "Iteration 209, loss = 0.12404942\n",
      "Iteration 210, loss = 0.12380631\n",
      "Iteration 211, loss = 0.12356571\n",
      "Iteration 212, loss = 0.12332755\n",
      "Iteration 213, loss = 0.12309198\n",
      "Iteration 214, loss = 0.12285878\n",
      "Iteration 215, loss = 0.12262804\n",
      "Iteration 216, loss = 0.12239980\n",
      "Iteration 217, loss = 0.12217376\n",
      "Iteration 218, loss = 0.12195004\n",
      "Iteration 219, loss = 0.12172857\n",
      "Iteration 220, loss = 0.12150937\n",
      "Iteration 221, loss = 0.12129230\n",
      "Iteration 222, loss = 0.12107742\n",
      "Iteration 223, loss = 0.12086468\n",
      "Iteration 224, loss = 0.12065404\n",
      "Iteration 225, loss = 0.12044550\n",
      "Iteration 226, loss = 0.12023907\n",
      "Iteration 227, loss = 0.12003451\n",
      "Iteration 228, loss = 0.11983208\n",
      "Iteration 229, loss = 0.11963149\n",
      "Iteration 230, loss = 0.11943288\n",
      "Iteration 231, loss = 0.11923620\n",
      "Iteration 232, loss = 0.11904134\n",
      "Iteration 233, loss = 0.11884838\n",
      "Iteration 234, loss = 0.11865719\n",
      "Iteration 235, loss = 0.11846777\n",
      "Iteration 236, loss = 0.11828020\n",
      "Iteration 237, loss = 0.11809430\n",
      "Iteration 238, loss = 0.11791010\n",
      "Iteration 239, loss = 0.11772768\n",
      "Iteration 240, loss = 0.11754684\n",
      "Iteration 241, loss = 0.11736772\n",
      "Iteration 242, loss = 0.11719017\n",
      "Iteration 243, loss = 0.11701429\n",
      "Iteration 244, loss = 0.11683994\n",
      "Iteration 245, loss = 0.11666715\n",
      "Iteration 246, loss = 0.11649598\n",
      "Iteration 247, loss = 0.11632625\n",
      "Iteration 248, loss = 0.11615801\n",
      "Iteration 249, loss = 0.11599131\n",
      "Iteration 250, loss = 0.11582604\n",
      "Iteration 251, loss = 0.11566232\n",
      "Iteration 252, loss = 0.11549990\n",
      "Iteration 253, loss = 0.11533901\n",
      "Iteration 254, loss = 0.11517948\n",
      "Iteration 255, loss = 0.11502121\n",
      "Iteration 256, loss = 0.11486443\n",
      "Iteration 257, loss = 0.11470886\n",
      "Iteration 258, loss = 0.11455468\n",
      "Iteration 259, loss = 0.11440173\n",
      "Iteration 260, loss = 0.11425009\n",
      "Iteration 261, loss = 0.11409969\n",
      "Iteration 262, loss = 0.11395055\n",
      "Iteration 263, loss = 0.11380264\n",
      "Iteration 264, loss = 0.11365602\n",
      "Iteration 265, loss = 0.11351050\n",
      "Iteration 266, loss = 0.11336653\n",
      "Iteration 267, loss = 0.11322373\n",
      "Iteration 268, loss = 0.11308199\n",
      "Iteration 269, loss = 0.11294136\n",
      "Iteration 270, loss = 0.11280183\n",
      "Iteration 271, loss = 0.11266336\n",
      "Iteration 272, loss = 0.11252619\n",
      "Iteration 273, loss = 0.11238981\n",
      "Iteration 274, loss = 0.11225490\n",
      "Iteration 275, loss = 0.11212092\n",
      "Iteration 276, loss = 0.11198805\n",
      "Iteration 277, loss = 0.11185610\n",
      "Iteration 278, loss = 0.11172536\n",
      "Iteration 279, loss = 0.11159554\n",
      "Iteration 280, loss = 0.11146681\n",
      "Iteration 281, loss = 0.11133910\n",
      "Iteration 282, loss = 0.11121231\n",
      "Iteration 283, loss = 0.11108656\n",
      "Iteration 284, loss = 0.11096180\n",
      "Iteration 285, loss = 0.11083798\n",
      "Iteration 286, loss = 0.11071513\n",
      "Iteration 287, loss = 0.11059319\n",
      "Iteration 288, loss = 0.11047211\n",
      "Iteration 289, loss = 0.11035208\n",
      "Iteration 290, loss = 0.11023287\n",
      "Iteration 291, loss = 0.11011458\n",
      "Iteration 292, loss = 0.10999719\n",
      "Iteration 293, loss = 0.10988068\n",
      "Iteration 294, loss = 0.10976497\n",
      "Iteration 295, loss = 0.10965020\n",
      "Iteration 296, loss = 0.10953622\n",
      "Iteration 297, loss = 0.10942319\n",
      "Iteration 298, loss = 0.10931087\n",
      "Iteration 299, loss = 0.10919938\n",
      "Iteration 300, loss = 0.10908870\n",
      "Iteration 301, loss = 0.10897891\n",
      "Iteration 302, loss = 0.10886990\n",
      "Iteration 303, loss = 0.10876161\n",
      "Iteration 304, loss = 0.10865409\n",
      "Iteration 305, loss = 0.10854747\n",
      "Iteration 306, loss = 0.10844146\n",
      "Iteration 307, loss = 0.10833627\n",
      "Iteration 308, loss = 0.10823188\n",
      "Iteration 309, loss = 0.10812824\n",
      "Iteration 310, loss = 0.10802518\n",
      "Iteration 311, loss = 0.10792296\n",
      "Iteration 312, loss = 0.10782136\n",
      "Iteration 313, loss = 0.10772067\n",
      "Iteration 314, loss = 0.10762056\n",
      "Iteration 315, loss = 0.10752112\n",
      "Iteration 316, loss = 0.10742231\n",
      "Iteration 317, loss = 0.10732426\n",
      "Iteration 318, loss = 0.10722697\n",
      "Iteration 319, loss = 0.10713017\n",
      "Iteration 320, loss = 0.10703415\n",
      "Iteration 321, loss = 0.10693879\n",
      "Iteration 322, loss = 0.10684394\n",
      "Iteration 323, loss = 0.10674981\n",
      "Iteration 324, loss = 0.10665638\n",
      "Iteration 325, loss = 0.10656355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.35012459\n",
      "Iteration 3, loss = 1.20900168\n",
      "Iteration 4, loss = 1.10086237\n",
      "Iteration 5, loss = 1.03876308\n",
      "Iteration 6, loss = 0.99083615\n",
      "Iteration 7, loss = 0.93024279\n",
      "Iteration 8, loss = 0.86428215\n",
      "Iteration 9, loss = 0.80562693\n",
      "Iteration 10, loss = 0.75430211\n",
      "Iteration 11, loss = 0.70945946\n",
      "Iteration 12, loss = 0.67056661\n",
      "Iteration 13, loss = 0.63705574\n",
      "Iteration 14, loss = 0.60808229\n",
      "Iteration 15, loss = 0.58211260\n",
      "Iteration 16, loss = 0.55850394\n",
      "Iteration 17, loss = 0.53705944\n",
      "Iteration 18, loss = 0.51762158\n",
      "Iteration 19, loss = 0.49991340\n",
      "Iteration 20, loss = 0.48376981\n",
      "Iteration 21, loss = 0.46895494\n",
      "Iteration 22, loss = 0.45571524\n",
      "Iteration 23, loss = 0.44433375\n",
      "Iteration 24, loss = 0.43395336\n",
      "Iteration 25, loss = 0.42426073\n",
      "Iteration 26, loss = 0.41511635\n",
      "Iteration 27, loss = 0.40645602\n",
      "Iteration 28, loss = 0.39825066\n",
      "Iteration 29, loss = 0.39049060\n",
      "Iteration 30, loss = 0.38314781\n",
      "Iteration 31, loss = 0.37611210\n",
      "Iteration 32, loss = 0.36934534\n",
      "Iteration 33, loss = 0.36282573\n",
      "Iteration 34, loss = 0.35658307\n",
      "Iteration 35, loss = 0.35040402\n",
      "Iteration 36, loss = 0.34447724\n",
      "Iteration 37, loss = 0.33904305\n",
      "Iteration 38, loss = 0.33400918\n",
      "Iteration 39, loss = 0.32922628\n",
      "Iteration 40, loss = 0.32460449\n",
      "Iteration 41, loss = 0.32010577\n",
      "Iteration 42, loss = 0.31571082\n",
      "Iteration 43, loss = 0.31142252\n",
      "Iteration 44, loss = 0.30723407\n",
      "Iteration 45, loss = 0.30314207\n",
      "Iteration 46, loss = 0.29914297\n",
      "Iteration 47, loss = 0.29523291\n",
      "Iteration 48, loss = 0.29141137\n",
      "Iteration 49, loss = 0.28767401\n",
      "Iteration 50, loss = 0.28402520\n",
      "Iteration 51, loss = 0.28046607\n",
      "Iteration 52, loss = 0.27698528\n",
      "Iteration 53, loss = 0.27358048\n",
      "Iteration 54, loss = 0.27024918\n",
      "Iteration 55, loss = 0.26699300\n",
      "Iteration 56, loss = 0.26380857\n",
      "Iteration 57, loss = 0.26069278\n",
      "Iteration 58, loss = 0.25764765\n",
      "Iteration 59, loss = 0.25467243\n",
      "Iteration 60, loss = 0.25175805\n",
      "Iteration 61, loss = 0.24888636\n",
      "Iteration 62, loss = 0.24604138\n",
      "Iteration 63, loss = 0.24321776\n",
      "Iteration 64, loss = 0.24039348\n",
      "Iteration 65, loss = 0.23753796\n",
      "Iteration 66, loss = 0.23461711\n",
      "Iteration 67, loss = 0.23176033\n",
      "Iteration 68, loss = 0.22879824\n",
      "Iteration 69, loss = 0.22609271\n",
      "Iteration 70, loss = 0.22381805\n",
      "Iteration 71, loss = 0.22167460\n",
      "Iteration 72, loss = 0.21960892\n",
      "Iteration 73, loss = 0.21757768\n",
      "Iteration 74, loss = 0.21557171\n",
      "Iteration 75, loss = 0.21359190\n",
      "Iteration 76, loss = 0.21164013\n",
      "Iteration 77, loss = 0.20971786\n",
      "Iteration 78, loss = 0.20782755\n",
      "Iteration 79, loss = 0.20596924\n",
      "Iteration 80, loss = 0.20414332\n",
      "Iteration 81, loss = 0.20234947\n",
      "Iteration 82, loss = 0.20058790\n",
      "Iteration 83, loss = 0.19885834\n",
      "Iteration 84, loss = 0.19716176\n",
      "Iteration 85, loss = 0.19549620\n",
      "Iteration 86, loss = 0.19386197\n",
      "Iteration 87, loss = 0.19225844\n",
      "Iteration 88, loss = 0.19068992\n",
      "Iteration 89, loss = 0.18915504\n",
      "Iteration 90, loss = 0.18766334\n",
      "Iteration 91, loss = 0.18620372\n",
      "Iteration 92, loss = 0.18478370\n",
      "Iteration 93, loss = 0.18340159\n",
      "Iteration 94, loss = 0.18204834\n",
      "Iteration 95, loss = 0.18072312\n",
      "Iteration 96, loss = 0.17942939\n",
      "Iteration 97, loss = 0.17815947\n",
      "Iteration 98, loss = 0.17691143\n",
      "Iteration 99, loss = 0.17568369\n",
      "Iteration 100, loss = 0.17447681\n",
      "Iteration 101, loss = 0.17329199\n",
      "Iteration 102, loss = 0.17213303\n",
      "Iteration 103, loss = 0.17099702\n",
      "Iteration 104, loss = 0.16988489\n",
      "Iteration 105, loss = 0.16879487\n",
      "Iteration 106, loss = 0.16772499\n",
      "Iteration 107, loss = 0.16667561\n",
      "Iteration 108, loss = 0.16564578\n",
      "Iteration 109, loss = 0.16463538\n",
      "Iteration 110, loss = 0.16364446\n",
      "Iteration 111, loss = 0.16267258\n",
      "Iteration 112, loss = 0.16171913\n",
      "Iteration 113, loss = 0.16078305\n",
      "Iteration 114, loss = 0.15986437\n",
      "Iteration 115, loss = 0.15896257\n",
      "Iteration 116, loss = 0.15807677\n",
      "Iteration 117, loss = 0.15720668\n",
      "Iteration 118, loss = 0.15635250\n",
      "Iteration 119, loss = 0.15551382\n",
      "Iteration 120, loss = 0.15468949\n",
      "Iteration 121, loss = 0.15387958\n",
      "Iteration 122, loss = 0.15308399\n",
      "Iteration 123, loss = 0.15230233\n",
      "Iteration 124, loss = 0.15153464\n",
      "Iteration 125, loss = 0.15077966\n",
      "Iteration 126, loss = 0.15003624\n",
      "Iteration 127, loss = 0.14930613\n",
      "Iteration 128, loss = 0.14858807\n",
      "Iteration 129, loss = 0.14788346\n",
      "Iteration 130, loss = 0.14719111\n",
      "Iteration 131, loss = 0.14651084\n",
      "Iteration 132, loss = 0.14584216\n",
      "Iteration 133, loss = 0.14518556\n",
      "Iteration 134, loss = 0.14454005\n",
      "Iteration 135, loss = 0.14390544\n",
      "Iteration 136, loss = 0.14328112\n",
      "Iteration 137, loss = 0.14266701\n",
      "Iteration 138, loss = 0.14206306\n",
      "Iteration 139, loss = 0.14146913\n",
      "Iteration 140, loss = 0.14088465\n",
      "Iteration 141, loss = 0.14030968\n",
      "Iteration 142, loss = 0.13974393\n",
      "Iteration 143, loss = 0.13918694\n",
      "Iteration 144, loss = 0.13863913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 145, loss = 0.13809939\n",
      "Iteration 146, loss = 0.13756793\n",
      "Iteration 147, loss = 0.13704297\n",
      "Iteration 148, loss = 0.13652359\n",
      "Iteration 149, loss = 0.13601198\n",
      "Iteration 150, loss = 0.13550782\n",
      "Iteration 151, loss = 0.13501137\n",
      "Iteration 152, loss = 0.13452137\n",
      "Iteration 153, loss = 0.13403761\n",
      "Iteration 154, loss = 0.13356123\n",
      "Iteration 155, loss = 0.13309205\n",
      "Iteration 156, loss = 0.13263086\n",
      "Iteration 157, loss = 0.13218091\n",
      "Iteration 158, loss = 0.13174033\n",
      "Iteration 159, loss = 0.13130876\n",
      "Iteration 160, loss = 0.13088039\n",
      "Iteration 161, loss = 0.13045409\n",
      "Iteration 162, loss = 0.13003115\n",
      "Iteration 163, loss = 0.12961347\n",
      "Iteration 164, loss = 0.12920399\n",
      "Iteration 165, loss = 0.12880263\n",
      "Iteration 166, loss = 0.12840475\n",
      "Iteration 167, loss = 0.12801328\n",
      "Iteration 168, loss = 0.12762485\n",
      "Iteration 169, loss = 0.12725320\n",
      "Iteration 170, loss = 0.12688527\n",
      "Iteration 171, loss = 0.12652119\n",
      "Iteration 172, loss = 0.12616040\n",
      "Iteration 173, loss = 0.12580321\n",
      "Iteration 174, loss = 0.12544946\n",
      "Iteration 175, loss = 0.12509938\n",
      "Iteration 176, loss = 0.12475415\n",
      "Iteration 177, loss = 0.12441269\n",
      "Iteration 178, loss = 0.12407612\n",
      "Iteration 179, loss = 0.12374663\n",
      "Iteration 180, loss = 0.12342019\n",
      "Iteration 181, loss = 0.12309907\n",
      "Iteration 182, loss = 0.12278228\n",
      "Iteration 183, loss = 0.12246793\n",
      "Iteration 184, loss = 0.12215957\n",
      "Iteration 185, loss = 0.12185273\n",
      "Iteration 186, loss = 0.12155101\n",
      "Iteration 187, loss = 0.12125276\n",
      "Iteration 188, loss = 0.12095828\n",
      "Iteration 189, loss = 0.12066788\n",
      "Iteration 190, loss = 0.12037974\n",
      "Iteration 191, loss = 0.12009583\n",
      "Iteration 192, loss = 0.11981541\n",
      "Iteration 193, loss = 0.11953825\n",
      "Iteration 194, loss = 0.11926393\n",
      "Iteration 195, loss = 0.11899318\n",
      "Iteration 196, loss = 0.11872607\n",
      "Iteration 197, loss = 0.11846141\n",
      "Iteration 198, loss = 0.11820019\n",
      "Iteration 199, loss = 0.11794179\n",
      "Iteration 200, loss = 0.11768671\n",
      "Iteration 201, loss = 0.11743425\n",
      "Iteration 202, loss = 0.11718476\n",
      "Iteration 203, loss = 0.11693800\n",
      "Iteration 204, loss = 0.11669445\n",
      "Iteration 205, loss = 0.11645310\n",
      "Iteration 206, loss = 0.11621460\n",
      "Iteration 207, loss = 0.11597905\n",
      "Iteration 208, loss = 0.11574576\n",
      "Iteration 209, loss = 0.11551518\n",
      "Iteration 210, loss = 0.11528709\n",
      "Iteration 211, loss = 0.11506168\n",
      "Iteration 212, loss = 0.11483842\n",
      "Iteration 213, loss = 0.11461767\n",
      "Iteration 214, loss = 0.11439952\n",
      "Iteration 215, loss = 0.11418335\n",
      "Iteration 216, loss = 0.11396972\n",
      "Iteration 217, loss = 0.11375829\n",
      "Iteration 218, loss = 0.11354890\n",
      "Iteration 219, loss = 0.11334230\n",
      "Iteration 220, loss = 0.11313728\n",
      "Iteration 221, loss = 0.11293433\n",
      "Iteration 222, loss = 0.11273400\n",
      "Iteration 223, loss = 0.11253521\n",
      "Iteration 224, loss = 0.11233902\n",
      "Iteration 225, loss = 0.11214436\n",
      "Iteration 226, loss = 0.11195207\n",
      "Iteration 227, loss = 0.11176142\n",
      "Iteration 228, loss = 0.11157244\n",
      "Iteration 229, loss = 0.11138618\n",
      "Iteration 230, loss = 0.11120107\n",
      "Iteration 231, loss = 0.11101784\n",
      "Iteration 232, loss = 0.11083645\n",
      "Iteration 233, loss = 0.11065735\n",
      "Iteration 234, loss = 0.11047938\n",
      "Iteration 235, loss = 0.11030341\n",
      "Iteration 236, loss = 0.11012895\n",
      "Iteration 237, loss = 0.10995660\n",
      "Iteration 238, loss = 0.10978537\n",
      "Iteration 239, loss = 0.10961661\n",
      "Iteration 240, loss = 0.10944871\n",
      "Iteration 241, loss = 0.10928244\n",
      "Iteration 242, loss = 0.10911797\n",
      "Iteration 243, loss = 0.10895506\n",
      "Iteration 244, loss = 0.10879344\n",
      "Iteration 245, loss = 0.10863374\n",
      "Iteration 246, loss = 0.10847506\n",
      "Iteration 247, loss = 0.10831854\n",
      "Iteration 248, loss = 0.10816275\n",
      "Iteration 249, loss = 0.10800878\n",
      "Iteration 250, loss = 0.10785587\n",
      "Iteration 251, loss = 0.10770493\n",
      "Iteration 252, loss = 0.10755469\n",
      "Iteration 253, loss = 0.10740657\n",
      "Iteration 254, loss = 0.10725923\n",
      "Iteration 255, loss = 0.10711308\n",
      "Iteration 256, loss = 0.10696880\n",
      "Iteration 257, loss = 0.10682519\n",
      "Iteration 258, loss = 0.10668364\n",
      "Iteration 259, loss = 0.10654250\n",
      "Iteration 260, loss = 0.10640308\n",
      "Iteration 261, loss = 0.10626458\n",
      "Iteration 262, loss = 0.10612746\n",
      "Iteration 263, loss = 0.10599147\n",
      "Iteration 264, loss = 0.10585655\n",
      "Iteration 265, loss = 0.10572319\n",
      "Iteration 266, loss = 0.10559048\n",
      "Iteration 267, loss = 0.10545922\n",
      "Iteration 268, loss = 0.10532900\n",
      "Iteration 269, loss = 0.10519985\n",
      "Iteration 270, loss = 0.10507188\n",
      "Iteration 271, loss = 0.10494476\n",
      "Iteration 272, loss = 0.10481922\n",
      "Iteration 273, loss = 0.10469400\n",
      "Iteration 274, loss = 0.10457043\n",
      "Iteration 275, loss = 0.10444757\n",
      "Iteration 276, loss = 0.10432561\n",
      "Iteration 277, loss = 0.10420517\n",
      "Iteration 278, loss = 0.10408507\n",
      "Iteration 279, loss = 0.10396631\n",
      "Iteration 280, loss = 0.10384853\n",
      "Iteration 281, loss = 0.10373150\n",
      "Iteration 282, loss = 0.10361553\n",
      "Iteration 283, loss = 0.10350058\n",
      "Iteration 284, loss = 0.10338630\n",
      "Iteration 285, loss = 0.10327328\n",
      "Iteration 286, loss = 0.10316078\n",
      "Iteration 287, loss = 0.10304924\n",
      "Iteration 288, loss = 0.10293902\n",
      "Iteration 289, loss = 0.10282923\n",
      "Iteration 290, loss = 0.10272017\n",
      "Iteration 291, loss = 0.10261265\n",
      "Iteration 292, loss = 0.10250501\n",
      "Iteration 293, loss = 0.10239916\n",
      "Iteration 294, loss = 0.10229361\n",
      "Iteration 295, loss = 0.10218875\n",
      "Iteration 296, loss = 0.10208459\n",
      "Iteration 297, loss = 0.10198214\n",
      "Iteration 298, loss = 0.10187925\n",
      "Iteration 299, loss = 0.10177802\n",
      "Iteration 300, loss = 0.10167719\n",
      "Iteration 301, loss = 0.10157700\n",
      "Iteration 302, loss = 0.10147760\n",
      "Iteration 303, loss = 0.10137924\n",
      "Iteration 304, loss = 0.10128127\n",
      "Iteration 305, loss = 0.10118423\n",
      "Iteration 306, loss = 0.10108786\n",
      "Iteration 307, loss = 0.10099205\n",
      "Iteration 308, loss = 0.10089722\n",
      "Iteration 309, loss = 0.10080286\n",
      "Iteration 310, loss = 0.10070915\n",
      "Iteration 311, loss = 0.10061638\n",
      "Iteration 312, loss = 0.10052416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.35341740\n",
      "Iteration 3, loss = 1.20792366\n",
      "Iteration 4, loss = 1.09572296\n",
      "Iteration 5, loss = 1.03049913\n",
      "Iteration 6, loss = 0.97987460\n",
      "Iteration 7, loss = 0.91687234\n",
      "Iteration 8, loss = 0.84857942\n",
      "Iteration 9, loss = 0.78833216\n",
      "Iteration 10, loss = 0.73587537\n",
      "Iteration 11, loss = 0.69134850\n",
      "Iteration 12, loss = 0.65293663\n",
      "Iteration 13, loss = 0.61983510\n",
      "Iteration 14, loss = 0.59080553\n",
      "Iteration 15, loss = 0.56502403\n",
      "Iteration 16, loss = 0.54176874\n",
      "Iteration 17, loss = 0.52071651\n",
      "Iteration 18, loss = 0.50171320\n",
      "Iteration 19, loss = 0.48467500\n",
      "Iteration 20, loss = 0.46922343\n",
      "Iteration 21, loss = 0.45512917\n",
      "Iteration 22, loss = 0.44237131\n",
      "Iteration 23, loss = 0.43056063\n",
      "Iteration 24, loss = 0.41936744\n",
      "Iteration 25, loss = 0.40873882\n",
      "Iteration 26, loss = 0.39912983\n",
      "Iteration 27, loss = 0.39060738\n",
      "Iteration 28, loss = 0.38256480\n",
      "Iteration 29, loss = 0.37483506\n",
      "Iteration 30, loss = 0.36735091\n",
      "Iteration 31, loss = 0.36003122\n",
      "Iteration 32, loss = 0.35287234\n",
      "Iteration 33, loss = 0.34597079\n",
      "Iteration 34, loss = 0.33921183\n",
      "Iteration 35, loss = 0.33285729\n",
      "Iteration 36, loss = 0.32708105\n",
      "Iteration 37, loss = 0.32171098\n",
      "Iteration 38, loss = 0.31660789\n",
      "Iteration 39, loss = 0.31163589\n",
      "Iteration 40, loss = 0.30677245\n",
      "Iteration 41, loss = 0.30203617\n",
      "Iteration 42, loss = 0.29741090\n",
      "Iteration 43, loss = 0.29289233\n",
      "Iteration 44, loss = 0.28849403\n",
      "Iteration 45, loss = 0.28420021\n",
      "Iteration 46, loss = 0.28000177\n",
      "Iteration 47, loss = 0.27589779\n",
      "Iteration 48, loss = 0.27188732\n",
      "Iteration 49, loss = 0.26796930\n",
      "Iteration 50, loss = 0.26414680\n",
      "Iteration 51, loss = 0.26041650\n",
      "Iteration 52, loss = 0.25677904\n",
      "Iteration 53, loss = 0.25322279\n",
      "Iteration 54, loss = 0.24974658\n",
      "Iteration 55, loss = 0.24634674\n",
      "Iteration 56, loss = 0.24302609\n",
      "Iteration 57, loss = 0.23978408\n",
      "Iteration 58, loss = 0.23660400\n",
      "Iteration 59, loss = 0.23348343\n",
      "Iteration 60, loss = 0.23040738\n",
      "Iteration 61, loss = 0.22735336\n",
      "Iteration 62, loss = 0.22430125\n",
      "Iteration 63, loss = 0.22121824\n",
      "Iteration 64, loss = 0.21813218\n",
      "Iteration 65, loss = 0.21509115\n",
      "Iteration 66, loss = 0.21201185\n",
      "Iteration 67, loss = 0.20911860\n",
      "Iteration 68, loss = 0.20663741\n",
      "Iteration 69, loss = 0.20432730\n",
      "Iteration 70, loss = 0.20210528\n",
      "Iteration 71, loss = 0.19992370\n",
      "Iteration 72, loss = 0.19777302\n",
      "Iteration 73, loss = 0.19565534\n",
      "Iteration 74, loss = 0.19357067\n",
      "Iteration 75, loss = 0.19151997\n",
      "Iteration 76, loss = 0.18950307\n",
      "Iteration 77, loss = 0.18752290\n",
      "Iteration 78, loss = 0.18557973\n",
      "Iteration 79, loss = 0.18367345\n",
      "Iteration 80, loss = 0.18180541\n",
      "Iteration 81, loss = 0.17997302\n",
      "Iteration 82, loss = 0.17817802\n",
      "Iteration 83, loss = 0.17641804\n",
      "Iteration 84, loss = 0.17469308\n",
      "Iteration 85, loss = 0.17300381\n",
      "Iteration 86, loss = 0.17134969\n",
      "Iteration 87, loss = 0.16973395\n",
      "Iteration 88, loss = 0.16815940\n",
      "Iteration 89, loss = 0.16662593\n",
      "Iteration 90, loss = 0.16513121\n",
      "Iteration 91, loss = 0.16367807\n",
      "Iteration 92, loss = 0.16226381\n",
      "Iteration 93, loss = 0.16088411\n",
      "Iteration 94, loss = 0.15953091\n",
      "Iteration 95, loss = 0.15820259\n",
      "Iteration 96, loss = 0.15689776\n",
      "Iteration 97, loss = 0.15561646\n",
      "Iteration 98, loss = 0.15435827\n",
      "Iteration 99, loss = 0.15312581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, loss = 0.15191745\n",
      "Iteration 101, loss = 0.15073656\n",
      "Iteration 102, loss = 0.14958031\n",
      "Iteration 103, loss = 0.14844779\n",
      "Iteration 104, loss = 0.14734048\n",
      "Iteration 105, loss = 0.14625568\n",
      "Iteration 106, loss = 0.14519289\n",
      "Iteration 107, loss = 0.14415222\n",
      "Iteration 108, loss = 0.14313060\n",
      "Iteration 109, loss = 0.14212794\n",
      "Iteration 110, loss = 0.14114429\n",
      "Iteration 111, loss = 0.14017996\n",
      "Iteration 112, loss = 0.13923516\n",
      "Iteration 113, loss = 0.13830808\n",
      "Iteration 114, loss = 0.13739885\n",
      "Iteration 115, loss = 0.13650708\n",
      "Iteration 116, loss = 0.13563184\n",
      "Iteration 117, loss = 0.13477255\n",
      "Iteration 118, loss = 0.13392909\n",
      "Iteration 119, loss = 0.13310096\n",
      "Iteration 120, loss = 0.13228768\n",
      "Iteration 121, loss = 0.13149039\n",
      "Iteration 122, loss = 0.13070624\n",
      "Iteration 123, loss = 0.12993590\n",
      "Iteration 124, loss = 0.12917936\n",
      "Iteration 125, loss = 0.12843686\n",
      "Iteration 126, loss = 0.12770827\n",
      "Iteration 127, loss = 0.12699229\n",
      "Iteration 128, loss = 0.12628908\n",
      "Iteration 129, loss = 0.12559896\n",
      "Iteration 130, loss = 0.12492003\n",
      "Iteration 131, loss = 0.12425345\n",
      "Iteration 132, loss = 0.12359865\n",
      "Iteration 133, loss = 0.12295551\n",
      "Iteration 134, loss = 0.12232301\n",
      "Iteration 135, loss = 0.12170136\n",
      "Iteration 136, loss = 0.12109054\n",
      "Iteration 137, loss = 0.12049031\n",
      "Iteration 138, loss = 0.11990066\n",
      "Iteration 139, loss = 0.11932023\n",
      "Iteration 140, loss = 0.11874986\n",
      "Iteration 141, loss = 0.11818853\n",
      "Iteration 142, loss = 0.11763662\n",
      "Iteration 143, loss = 0.11709353\n",
      "Iteration 144, loss = 0.11655915\n",
      "Iteration 145, loss = 0.11603359\n",
      "Iteration 146, loss = 0.11551502\n",
      "Iteration 147, loss = 0.11500279\n",
      "Iteration 148, loss = 0.11449769\n",
      "Iteration 149, loss = 0.11400059\n",
      "Iteration 150, loss = 0.11351097\n",
      "Iteration 151, loss = 0.11302887\n",
      "Iteration 152, loss = 0.11255198\n",
      "Iteration 153, loss = 0.11208424\n",
      "Iteration 154, loss = 0.11162768\n",
      "Iteration 155, loss = 0.11117909\n",
      "Iteration 156, loss = 0.11073951\n",
      "Iteration 157, loss = 0.11030791\n",
      "Iteration 158, loss = 0.10988285\n",
      "Iteration 159, loss = 0.10946344\n",
      "Iteration 160, loss = 0.10904935\n",
      "Iteration 161, loss = 0.10864041\n",
      "Iteration 162, loss = 0.10823646\n",
      "Iteration 163, loss = 0.10783797\n",
      "Iteration 164, loss = 0.10744542\n",
      "Iteration 165, loss = 0.10705827\n",
      "Iteration 166, loss = 0.10667647\n",
      "Iteration 167, loss = 0.10630021\n",
      "Iteration 168, loss = 0.10592956\n",
      "Iteration 169, loss = 0.10556495\n",
      "Iteration 170, loss = 0.10520548\n",
      "Iteration 171, loss = 0.10485101\n",
      "Iteration 172, loss = 0.10450125\n",
      "Iteration 173, loss = 0.10415630\n",
      "Iteration 174, loss = 0.10381578\n",
      "Iteration 175, loss = 0.10347986\n",
      "Iteration 176, loss = 0.10314832\n",
      "Iteration 177, loss = 0.10282106\n",
      "Iteration 178, loss = 0.10249817\n",
      "Iteration 179, loss = 0.10217934\n",
      "Iteration 180, loss = 0.10186474\n",
      "Iteration 181, loss = 0.10155408\n",
      "Iteration 182, loss = 0.10124773\n",
      "Iteration 183, loss = 0.10094511\n",
      "Iteration 184, loss = 0.10064629\n",
      "Iteration 185, loss = 0.10035133\n",
      "Iteration 186, loss = 0.10005986\n",
      "Iteration 187, loss = 0.09977202\n",
      "Iteration 188, loss = 0.09948756\n",
      "Iteration 189, loss = 0.09920673\n",
      "Iteration 190, loss = 0.09892922\n",
      "Iteration 191, loss = 0.09865507\n",
      "Iteration 192, loss = 0.09838436\n",
      "Iteration 193, loss = 0.09811679\n",
      "Iteration 194, loss = 0.09785251\n",
      "Iteration 195, loss = 0.09759131\n",
      "Iteration 196, loss = 0.09733335\n",
      "Iteration 197, loss = 0.09707833\n",
      "Iteration 198, loss = 0.09682637\n",
      "Iteration 199, loss = 0.09657756\n",
      "Iteration 200, loss = 0.09633162\n",
      "Iteration 201, loss = 0.09608860\n",
      "Iteration 202, loss = 0.09584832\n",
      "Iteration 203, loss = 0.09561084\n",
      "Iteration 204, loss = 0.09537609\n",
      "Iteration 205, loss = 0.09514401\n",
      "Iteration 206, loss = 0.09491464\n",
      "Iteration 207, loss = 0.09468775\n",
      "Iteration 208, loss = 0.09446348\n",
      "Iteration 209, loss = 0.09424169\n",
      "Iteration 210, loss = 0.09402240\n",
      "Iteration 211, loss = 0.09380555\n",
      "Iteration 212, loss = 0.09359103\n",
      "Iteration 213, loss = 0.09337897\n",
      "Iteration 214, loss = 0.09316914\n",
      "Iteration 215, loss = 0.09296165\n",
      "Iteration 216, loss = 0.09275635\n",
      "Iteration 217, loss = 0.09255331\n",
      "Iteration 218, loss = 0.09235241\n",
      "Iteration 219, loss = 0.09215369\n",
      "Iteration 220, loss = 0.09195708\n",
      "Iteration 221, loss = 0.09176251\n",
      "Iteration 222, loss = 0.09157010\n",
      "Iteration 223, loss = 0.09137957\n",
      "Iteration 224, loss = 0.09119111\n",
      "Iteration 225, loss = 0.09100454\n",
      "Iteration 226, loss = 0.09081996\n",
      "Iteration 227, loss = 0.09063723\n",
      "Iteration 228, loss = 0.09045640\n",
      "Iteration 229, loss = 0.09027740\n",
      "Iteration 230, loss = 0.09010021\n",
      "Iteration 231, loss = 0.08992485\n",
      "Iteration 232, loss = 0.08975118\n",
      "Iteration 233, loss = 0.08957935\n",
      "Iteration 234, loss = 0.08940913\n",
      "Iteration 235, loss = 0.08924065\n",
      "Iteration 236, loss = 0.08907380\n",
      "Iteration 237, loss = 0.08890863\n",
      "Iteration 238, loss = 0.08874501\n",
      "Iteration 239, loss = 0.08858308\n",
      "Iteration 240, loss = 0.08842262\n",
      "Iteration 241, loss = 0.08826383\n",
      "Iteration 242, loss = 0.08810647\n",
      "Iteration 243, loss = 0.08795060\n",
      "Iteration 244, loss = 0.08779632\n",
      "Iteration 245, loss = 0.08764353\n",
      "Iteration 246, loss = 0.08749223\n",
      "Iteration 247, loss = 0.08734239\n",
      "Iteration 248, loss = 0.08719396\n",
      "Iteration 249, loss = 0.08704689\n",
      "Iteration 250, loss = 0.08690126\n",
      "Iteration 251, loss = 0.08675691\n",
      "Iteration 252, loss = 0.08661397\n",
      "Iteration 253, loss = 0.08647227\n",
      "Iteration 254, loss = 0.08633192\n",
      "Iteration 255, loss = 0.08619281\n",
      "Iteration 256, loss = 0.08605501\n",
      "Iteration 257, loss = 0.08591841\n",
      "Iteration 258, loss = 0.08578310\n",
      "Iteration 259, loss = 0.08564895\n",
      "Iteration 260, loss = 0.08551604\n",
      "Iteration 261, loss = 0.08538433\n",
      "Iteration 262, loss = 0.08525384\n",
      "Iteration 263, loss = 0.08512455\n",
      "Iteration 264, loss = 0.08499648\n",
      "Iteration 265, loss = 0.08486947\n",
      "Iteration 266, loss = 0.08474364\n",
      "Iteration 267, loss = 0.08461885\n",
      "Iteration 268, loss = 0.08449521\n",
      "Iteration 269, loss = 0.08437262\n",
      "Iteration 270, loss = 0.08425114\n",
      "Iteration 271, loss = 0.08413066\n",
      "Iteration 272, loss = 0.08401127\n",
      "Iteration 273, loss = 0.08389283\n",
      "Iteration 274, loss = 0.08377542\n",
      "Iteration 275, loss = 0.08365897\n",
      "Iteration 276, loss = 0.08354356\n",
      "Iteration 277, loss = 0.08342908\n",
      "Iteration 278, loss = 0.08331556\n",
      "Iteration 279, loss = 0.08320296\n",
      "Iteration 280, loss = 0.08309136\n",
      "Iteration 281, loss = 0.08298061\n",
      "Iteration 282, loss = 0.08287080\n",
      "Iteration 283, loss = 0.08276193\n",
      "Iteration 284, loss = 0.08265390\n",
      "Iteration 285, loss = 0.08254675\n",
      "Iteration 286, loss = 0.08244051\n",
      "Iteration 287, loss = 0.08233515\n",
      "Iteration 288, loss = 0.08223060\n",
      "Iteration 289, loss = 0.08212686\n",
      "Iteration 290, loss = 0.08202394\n",
      "Iteration 291, loss = 0.08192184\n",
      "Iteration 292, loss = 0.08182055\n",
      "Iteration 293, loss = 0.08172003\n",
      "Iteration 294, loss = 0.08162038\n",
      "Iteration 295, loss = 0.08152158\n",
      "Iteration 296, loss = 0.08142348\n",
      "Iteration 297, loss = 0.08132614\n",
      "Iteration 298, loss = 0.08122955\n",
      "Iteration 299, loss = 0.08113368\n",
      "Iteration 300, loss = 0.08103870\n",
      "Iteration 301, loss = 0.08094421\n",
      "Iteration 302, loss = 0.08085054\n",
      "Iteration 303, loss = 0.08075757\n",
      "Iteration 304, loss = 0.08066529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.35081329\n",
      "Iteration 3, loss = 1.20634404\n",
      "Iteration 4, loss = 1.09588858\n",
      "Iteration 5, loss = 1.03436606\n",
      "Iteration 6, loss = 0.98636287\n",
      "Iteration 7, loss = 0.92558714\n",
      "Iteration 8, loss = 0.85905698\n",
      "Iteration 9, loss = 0.79921516\n",
      "Iteration 10, loss = 0.74703155\n",
      "Iteration 11, loss = 0.70167012\n",
      "Iteration 12, loss = 0.66213993\n",
      "Iteration 13, loss = 0.62830065\n",
      "Iteration 14, loss = 0.59876019\n",
      "Iteration 15, loss = 0.57219540\n",
      "Iteration 16, loss = 0.54813945\n",
      "Iteration 17, loss = 0.52639799\n",
      "Iteration 18, loss = 0.50675463\n",
      "Iteration 19, loss = 0.48917597\n",
      "Iteration 20, loss = 0.47346996\n",
      "Iteration 21, loss = 0.45923127\n",
      "Iteration 22, loss = 0.44636099\n",
      "Iteration 23, loss = 0.43466868\n",
      "Iteration 24, loss = 0.42393555\n",
      "Iteration 25, loss = 0.41402427\n",
      "Iteration 26, loss = 0.40472035\n",
      "Iteration 27, loss = 0.39589570\n",
      "Iteration 28, loss = 0.38750504\n",
      "Iteration 29, loss = 0.37950980\n",
      "Iteration 30, loss = 0.37191231\n",
      "Iteration 31, loss = 0.36465816\n",
      "Iteration 32, loss = 0.35758512\n",
      "Iteration 33, loss = 0.35073756\n",
      "Iteration 34, loss = 0.34415114\n",
      "Iteration 35, loss = 0.33774096\n",
      "Iteration 36, loss = 0.33159905\n",
      "Iteration 37, loss = 0.32592278\n",
      "Iteration 38, loss = 0.32058089\n",
      "Iteration 39, loss = 0.31547493\n",
      "Iteration 40, loss = 0.31055636\n",
      "Iteration 41, loss = 0.30578601\n",
      "Iteration 42, loss = 0.30114241\n",
      "Iteration 43, loss = 0.29660994\n",
      "Iteration 44, loss = 0.29219302\n",
      "Iteration 45, loss = 0.28787858\n",
      "Iteration 46, loss = 0.28366461\n",
      "Iteration 47, loss = 0.27954762\n",
      "Iteration 48, loss = 0.27552846\n",
      "Iteration 49, loss = 0.27160022\n",
      "Iteration 50, loss = 0.26776245\n",
      "Iteration 51, loss = 0.26401291\n",
      "Iteration 52, loss = 0.26035239\n",
      "Iteration 53, loss = 0.25677601\n",
      "Iteration 54, loss = 0.25328101\n",
      "Iteration 55, loss = 0.24986475\n",
      "Iteration 56, loss = 0.24653024\n",
      "Iteration 57, loss = 0.24327113\n",
      "Iteration 58, loss = 0.24008942\n",
      "Iteration 59, loss = 0.23696657\n",
      "Iteration 60, loss = 0.23389278\n",
      "Iteration 61, loss = 0.23087033\n",
      "Iteration 62, loss = 0.22789559\n",
      "Iteration 63, loss = 0.22491541\n",
      "Iteration 64, loss = 0.22192968\n",
      "Iteration 65, loss = 0.21887045\n",
      "Iteration 66, loss = 0.21586672\n",
      "Iteration 67, loss = 0.21273358\n",
      "Iteration 68, loss = 0.20972317\n",
      "Iteration 69, loss = 0.20724295\n",
      "Iteration 70, loss = 0.20500360\n",
      "Iteration 71, loss = 0.20280532\n",
      "Iteration 72, loss = 0.20064537\n",
      "Iteration 73, loss = 0.19852171\n",
      "Iteration 74, loss = 0.19642858\n",
      "Iteration 75, loss = 0.19436620\n",
      "Iteration 76, loss = 0.19233616\n",
      "Iteration 77, loss = 0.19033906\n",
      "Iteration 78, loss = 0.18837567\n",
      "Iteration 79, loss = 0.18644579\n",
      "Iteration 80, loss = 0.18454907\n",
      "Iteration 81, loss = 0.18268809\n",
      "Iteration 82, loss = 0.18086145\n",
      "Iteration 83, loss = 0.17906931\n",
      "Iteration 84, loss = 0.17731228\n",
      "Iteration 85, loss = 0.17558874\n",
      "Iteration 86, loss = 0.17390307\n",
      "Iteration 87, loss = 0.17225057\n",
      "Iteration 88, loss = 0.17063130\n",
      "Iteration 89, loss = 0.16904463\n",
      "Iteration 90, loss = 0.16749403\n",
      "Iteration 91, loss = 0.16599957\n",
      "Iteration 92, loss = 0.16454567\n",
      "Iteration 93, loss = 0.16312876\n",
      "Iteration 94, loss = 0.16174899\n",
      "Iteration 95, loss = 0.16038637\n",
      "Iteration 96, loss = 0.15903780\n",
      "Iteration 97, loss = 0.15771124\n",
      "Iteration 98, loss = 0.15641157\n",
      "Iteration 99, loss = 0.15514060\n",
      "Iteration 100, loss = 0.15389856\n",
      "Iteration 101, loss = 0.15268247\n",
      "Iteration 102, loss = 0.15149064\n",
      "Iteration 103, loss = 0.15032258\n",
      "Iteration 104, loss = 0.14917660\n",
      "Iteration 105, loss = 0.14805206\n",
      "Iteration 106, loss = 0.14694905\n",
      "Iteration 107, loss = 0.14586588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, loss = 0.14480205\n",
      "Iteration 109, loss = 0.14375873\n",
      "Iteration 110, loss = 0.14273482\n",
      "Iteration 111, loss = 0.14172974\n",
      "Iteration 112, loss = 0.14074356\n",
      "Iteration 113, loss = 0.13977515\n",
      "Iteration 114, loss = 0.13882403\n",
      "Iteration 115, loss = 0.13788967\n",
      "Iteration 116, loss = 0.13697208\n",
      "Iteration 117, loss = 0.13607085\n",
      "Iteration 118, loss = 0.13518623\n",
      "Iteration 119, loss = 0.13431658\n",
      "Iteration 120, loss = 0.13346123\n",
      "Iteration 121, loss = 0.13262005\n",
      "Iteration 122, loss = 0.13179336\n",
      "Iteration 123, loss = 0.13098037\n",
      "Iteration 124, loss = 0.13018104\n",
      "Iteration 125, loss = 0.12939497\n",
      "Iteration 126, loss = 0.12862198\n",
      "Iteration 127, loss = 0.12786154\n",
      "Iteration 128, loss = 0.12711349\n",
      "Iteration 129, loss = 0.12637770\n",
      "Iteration 130, loss = 0.12565424\n",
      "Iteration 131, loss = 0.12494320\n",
      "Iteration 132, loss = 0.12424356\n",
      "Iteration 133, loss = 0.12355547\n",
      "Iteration 134, loss = 0.12287792\n",
      "Iteration 135, loss = 0.12221092\n",
      "Iteration 136, loss = 0.12155418\n",
      "Iteration 137, loss = 0.12090825\n",
      "Iteration 138, loss = 0.12027202\n",
      "Iteration 139, loss = 0.11964536\n",
      "Iteration 140, loss = 0.11902897\n",
      "Iteration 141, loss = 0.11842137\n",
      "Iteration 142, loss = 0.11782265\n",
      "Iteration 143, loss = 0.11723315\n",
      "Iteration 144, loss = 0.11665135\n",
      "Iteration 145, loss = 0.11607502\n",
      "Iteration 146, loss = 0.11550704\n",
      "Iteration 147, loss = 0.11494737\n",
      "Iteration 148, loss = 0.11439579\n",
      "Iteration 149, loss = 0.11385228\n",
      "Iteration 150, loss = 0.11331428\n",
      "Iteration 151, loss = 0.11278831\n",
      "Iteration 152, loss = 0.11227179\n",
      "Iteration 153, loss = 0.11176304\n",
      "Iteration 154, loss = 0.11126409\n",
      "Iteration 155, loss = 0.11077230\n",
      "Iteration 156, loss = 0.11028834\n",
      "Iteration 157, loss = 0.10981053\n",
      "Iteration 158, loss = 0.10933848\n",
      "Iteration 159, loss = 0.10887288\n",
      "Iteration 160, loss = 0.10841350\n",
      "Iteration 161, loss = 0.10796054\n",
      "Iteration 162, loss = 0.10751283\n",
      "Iteration 163, loss = 0.10707100\n",
      "Iteration 164, loss = 0.10663464\n",
      "Iteration 165, loss = 0.10620400\n",
      "Iteration 166, loss = 0.10577910\n",
      "Iteration 167, loss = 0.10535992\n",
      "Iteration 168, loss = 0.10494617\n",
      "Iteration 169, loss = 0.10453689\n",
      "Iteration 170, loss = 0.10413278\n",
      "Iteration 171, loss = 0.10373377\n",
      "Iteration 172, loss = 0.10333957\n",
      "Iteration 173, loss = 0.10295058\n",
      "Iteration 174, loss = 0.10256636\n",
      "Iteration 175, loss = 0.10218747\n",
      "Iteration 176, loss = 0.10181316\n",
      "Iteration 177, loss = 0.10144352\n",
      "Iteration 178, loss = 0.10107841\n",
      "Iteration 179, loss = 0.10071778\n",
      "Iteration 180, loss = 0.10036153\n",
      "Iteration 181, loss = 0.10000940\n",
      "Iteration 182, loss = 0.09966147\n",
      "Iteration 183, loss = 0.09931752\n",
      "Iteration 184, loss = 0.09897773\n",
      "Iteration 185, loss = 0.09864157\n",
      "Iteration 186, loss = 0.09830929\n",
      "Iteration 187, loss = 0.09798098\n",
      "Iteration 188, loss = 0.09765631\n",
      "Iteration 189, loss = 0.09733549\n",
      "Iteration 190, loss = 0.09701836\n",
      "Iteration 191, loss = 0.09670493\n",
      "Iteration 192, loss = 0.09639491\n",
      "Iteration 193, loss = 0.09608827\n",
      "Iteration 194, loss = 0.09578512\n",
      "Iteration 195, loss = 0.09548527\n",
      "Iteration 196, loss = 0.09518871\n",
      "Iteration 197, loss = 0.09489547\n",
      "Iteration 198, loss = 0.09460548\n",
      "Iteration 199, loss = 0.09431858\n",
      "Iteration 200, loss = 0.09403474\n",
      "Iteration 201, loss = 0.09375409\n",
      "Iteration 202, loss = 0.09347625\n",
      "Iteration 203, loss = 0.09320150\n",
      "Iteration 204, loss = 0.09292960\n",
      "Iteration 205, loss = 0.09266053\n",
      "Iteration 206, loss = 0.09239425\n",
      "Iteration 207, loss = 0.09213074\n",
      "Iteration 208, loss = 0.09187012\n",
      "Iteration 209, loss = 0.09161193\n",
      "Iteration 210, loss = 0.09135647\n",
      "Iteration 211, loss = 0.09110377\n",
      "Iteration 212, loss = 0.09085360\n",
      "Iteration 213, loss = 0.09060592\n",
      "Iteration 214, loss = 0.09036069\n",
      "Iteration 215, loss = 0.09011810\n",
      "Iteration 216, loss = 0.08987774\n",
      "Iteration 217, loss = 0.08963985\n",
      "Iteration 218, loss = 0.08940435\n",
      "Iteration 219, loss = 0.08917119\n",
      "Iteration 220, loss = 0.08894041\n",
      "Iteration 221, loss = 0.08871189\n",
      "Iteration 222, loss = 0.08848557\n",
      "Iteration 223, loss = 0.08826160\n",
      "Iteration 224, loss = 0.08803959\n",
      "Iteration 225, loss = 0.08781982\n",
      "Iteration 226, loss = 0.08760208\n",
      "Iteration 227, loss = 0.08738658\n",
      "Iteration 228, loss = 0.08717294\n",
      "Iteration 229, loss = 0.08696152\n",
      "Iteration 230, loss = 0.08675185\n",
      "Iteration 231, loss = 0.08654427\n",
      "Iteration 232, loss = 0.08633869\n",
      "Iteration 233, loss = 0.08613494\n",
      "Iteration 234, loss = 0.08593304\n",
      "Iteration 235, loss = 0.08573298\n",
      "Iteration 236, loss = 0.08553487\n",
      "Iteration 237, loss = 0.08533848\n",
      "Iteration 238, loss = 0.08514386\n",
      "Iteration 239, loss = 0.08495101\n",
      "Iteration 240, loss = 0.08475990\n",
      "Iteration 241, loss = 0.08457052\n",
      "Iteration 242, loss = 0.08438292\n",
      "Iteration 243, loss = 0.08419681\n",
      "Iteration 244, loss = 0.08401247\n",
      "Iteration 245, loss = 0.08382969\n",
      "Iteration 246, loss = 0.08364853\n",
      "Iteration 247, loss = 0.08346894\n",
      "Iteration 248, loss = 0.08329090\n",
      "Iteration 249, loss = 0.08311437\n",
      "Iteration 250, loss = 0.08293939\n",
      "Iteration 251, loss = 0.08276585\n",
      "Iteration 252, loss = 0.08259396\n",
      "Iteration 253, loss = 0.08242332\n",
      "Iteration 254, loss = 0.08225421\n",
      "Iteration 255, loss = 0.08208654\n",
      "Iteration 256, loss = 0.08192027\n",
      "Iteration 257, loss = 0.08175548\n",
      "Iteration 258, loss = 0.08159194\n",
      "Iteration 259, loss = 0.08142978\n",
      "Iteration 260, loss = 0.08126897\n",
      "Iteration 261, loss = 0.08110961\n",
      "Iteration 262, loss = 0.08095134\n",
      "Iteration 263, loss = 0.08079446\n",
      "Iteration 264, loss = 0.08063886\n",
      "Iteration 265, loss = 0.08048452\n",
      "Iteration 266, loss = 0.08033142\n",
      "Iteration 267, loss = 0.08017955\n",
      "Iteration 268, loss = 0.08002889\n",
      "Iteration 269, loss = 0.07987942\n",
      "Iteration 270, loss = 0.07973115\n",
      "Iteration 271, loss = 0.07958403\n",
      "Iteration 272, loss = 0.07943809\n",
      "Iteration 273, loss = 0.07929340\n",
      "Iteration 274, loss = 0.07914968\n",
      "Iteration 275, loss = 0.07900713\n",
      "Iteration 276, loss = 0.07886572\n",
      "Iteration 277, loss = 0.07872539\n",
      "Iteration 278, loss = 0.07858614\n",
      "Iteration 279, loss = 0.07844795\n",
      "Iteration 280, loss = 0.07831082\n",
      "Iteration 281, loss = 0.07817472\n",
      "Iteration 282, loss = 0.07803963\n",
      "Iteration 283, loss = 0.07790556\n",
      "Iteration 284, loss = 0.07777263\n",
      "Iteration 285, loss = 0.07764049\n",
      "Iteration 286, loss = 0.07750943\n",
      "Iteration 287, loss = 0.07737934\n",
      "Iteration 288, loss = 0.07725019\n",
      "Iteration 289, loss = 0.07712199\n",
      "Iteration 290, loss = 0.07699473\n",
      "Iteration 291, loss = 0.07686837\n",
      "Iteration 292, loss = 0.07674292\n",
      "Iteration 293, loss = 0.07661837\n",
      "Iteration 294, loss = 0.07649473\n",
      "Iteration 295, loss = 0.07637202\n",
      "Iteration 296, loss = 0.07625016\n",
      "Iteration 297, loss = 0.07612917\n",
      "Iteration 298, loss = 0.07600902\n",
      "Iteration 299, loss = 0.07588971\n",
      "Iteration 300, loss = 0.07577125\n",
      "Iteration 301, loss = 0.07565360\n",
      "Iteration 302, loss = 0.07553680\n",
      "Iteration 303, loss = 0.07542082\n",
      "Iteration 304, loss = 0.07530564\n",
      "Iteration 305, loss = 0.07519130\n",
      "Iteration 306, loss = 0.07507775\n",
      "Iteration 307, loss = 0.07496495\n",
      "Iteration 308, loss = 0.07485292\n",
      "Iteration 309, loss = 0.07474165\n",
      "Iteration 310, loss = 0.07463114\n",
      "Iteration 311, loss = 0.07452138\n",
      "Iteration 312, loss = 0.07441234\n",
      "Iteration 313, loss = 0.07430404\n",
      "Iteration 314, loss = 0.07419645\n",
      "Iteration 315, loss = 0.07408958\n",
      "Iteration 316, loss = 0.07398361\n",
      "Iteration 317, loss = 0.07387809\n",
      "Iteration 318, loss = 0.07377343\n",
      "Iteration 319, loss = 0.07366946\n",
      "Iteration 320, loss = 0.07356618\n",
      "Iteration 321, loss = 0.07346357\n",
      "Iteration 322, loss = 0.07336162\n",
      "Iteration 323, loss = 0.07326033\n",
      "Iteration 324, loss = 0.07315969\n",
      "Iteration 325, loss = 0.07305973\n",
      "Iteration 326, loss = 0.07296043\n",
      "Iteration 327, loss = 0.07286177\n",
      "Iteration 328, loss = 0.07276377\n",
      "Iteration 329, loss = 0.07266638\n",
      "Iteration 330, loss = 0.07256963\n",
      "Iteration 331, loss = 0.07247349\n",
      "Iteration 332, loss = 0.07237796\n",
      "Iteration 333, loss = 0.07228304\n",
      "Iteration 334, loss = 0.07218871\n",
      "Iteration 335, loss = 0.07209497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.35744472\n",
      "Iteration 3, loss = 1.20940635\n",
      "Iteration 4, loss = 1.09712272\n",
      "Iteration 5, loss = 1.03381891\n",
      "Iteration 6, loss = 0.98350683\n",
      "Iteration 7, loss = 0.92040085\n",
      "Iteration 8, loss = 0.85320129\n",
      "Iteration 9, loss = 0.79446959\n",
      "Iteration 10, loss = 0.74332522\n",
      "Iteration 11, loss = 0.69870310\n",
      "Iteration 12, loss = 0.65916375\n",
      "Iteration 13, loss = 0.62480745\n",
      "Iteration 14, loss = 0.59515636\n",
      "Iteration 15, loss = 0.56888986\n",
      "Iteration 16, loss = 0.54593845\n",
      "Iteration 17, loss = 0.52544265\n",
      "Iteration 18, loss = 0.50709081\n",
      "Iteration 19, loss = 0.49040893\n",
      "Iteration 20, loss = 0.47551439\n",
      "Iteration 21, loss = 0.46204720\n",
      "Iteration 22, loss = 0.44988746\n",
      "Iteration 23, loss = 0.43889724\n",
      "Iteration 24, loss = 0.42867830\n",
      "Iteration 25, loss = 0.41893191\n",
      "Iteration 26, loss = 0.40971321\n",
      "Iteration 27, loss = 0.40094440\n",
      "Iteration 28, loss = 0.39294392\n",
      "Iteration 29, loss = 0.38557536\n",
      "Iteration 30, loss = 0.37853734\n",
      "Iteration 31, loss = 0.37163590\n",
      "Iteration 32, loss = 0.36490931\n",
      "Iteration 33, loss = 0.35843449\n",
      "Iteration 34, loss = 0.35212064\n",
      "Iteration 35, loss = 0.34618578\n",
      "Iteration 36, loss = 0.34077935\n",
      "Iteration 37, loss = 0.33571316\n",
      "Iteration 38, loss = 0.33091453\n",
      "Iteration 39, loss = 0.32627591\n",
      "Iteration 40, loss = 0.32177626\n",
      "Iteration 41, loss = 0.31737469\n",
      "Iteration 42, loss = 0.31307075\n",
      "Iteration 43, loss = 0.30886050\n",
      "Iteration 44, loss = 0.30475812\n",
      "Iteration 45, loss = 0.30074607\n",
      "Iteration 46, loss = 0.29682260\n",
      "Iteration 47, loss = 0.29299729\n",
      "Iteration 48, loss = 0.28925696\n",
      "Iteration 49, loss = 0.28560525\n",
      "Iteration 50, loss = 0.28204906\n",
      "Iteration 51, loss = 0.27857394\n",
      "Iteration 52, loss = 0.27517780\n",
      "Iteration 53, loss = 0.27186074\n",
      "Iteration 54, loss = 0.26862095\n",
      "Iteration 55, loss = 0.26544997\n",
      "Iteration 56, loss = 0.26235333\n",
      "Iteration 57, loss = 0.25933831\n",
      "Iteration 58, loss = 0.25639552\n",
      "Iteration 59, loss = 0.25353204\n",
      "Iteration 60, loss = 0.25073029\n",
      "Iteration 61, loss = 0.24797534\n",
      "Iteration 62, loss = 0.24524530\n",
      "Iteration 63, loss = 0.24250526\n",
      "Iteration 64, loss = 0.23974098\n",
      "Iteration 65, loss = 0.23697382\n",
      "Iteration 66, loss = 0.23424714\n",
      "Iteration 67, loss = 0.23153104\n",
      "Iteration 68, loss = 0.22874424\n",
      "Iteration 69, loss = 0.22619210\n",
      "Iteration 70, loss = 0.22394680\n",
      "Iteration 71, loss = 0.22186048\n",
      "Iteration 72, loss = 0.21986026\n",
      "Iteration 73, loss = 0.21789135\n",
      "Iteration 74, loss = 0.21594888\n",
      "Iteration 75, loss = 0.21403330\n",
      "Iteration 76, loss = 0.21214780\n",
      "Iteration 77, loss = 0.21029467\n",
      "Iteration 78, loss = 0.20847288\n",
      "Iteration 79, loss = 0.20668310\n",
      "Iteration 80, loss = 0.20492582\n",
      "Iteration 81, loss = 0.20320161\n",
      "Iteration 82, loss = 0.20150953\n",
      "Iteration 83, loss = 0.19984941\n",
      "Iteration 84, loss = 0.19822130\n",
      "Iteration 85, loss = 0.19662437\n",
      "Iteration 86, loss = 0.19505743\n",
      "Iteration 87, loss = 0.19352041\n",
      "Iteration 88, loss = 0.19202080\n",
      "Iteration 89, loss = 0.19056210\n",
      "Iteration 90, loss = 0.18914448\n",
      "Iteration 91, loss = 0.18776399\n",
      "Iteration 92, loss = 0.18641386\n",
      "Iteration 93, loss = 0.18509974\n",
      "Iteration 94, loss = 0.18381045\n",
      "Iteration 95, loss = 0.18254609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 96, loss = 0.18130446\n",
      "Iteration 97, loss = 0.18008113\n",
      "Iteration 98, loss = 0.17888213\n",
      "Iteration 99, loss = 0.17770731\n",
      "Iteration 100, loss = 0.17655496\n",
      "Iteration 101, loss = 0.17542484\n",
      "Iteration 102, loss = 0.17432179\n",
      "Iteration 103, loss = 0.17324119\n",
      "Iteration 104, loss = 0.17218146\n",
      "Iteration 105, loss = 0.17114217\n",
      "Iteration 106, loss = 0.17012309\n",
      "Iteration 107, loss = 0.16912368\n",
      "Iteration 108, loss = 0.16814384\n",
      "Iteration 109, loss = 0.16718081\n",
      "Iteration 110, loss = 0.16623578\n",
      "Iteration 111, loss = 0.16530875\n",
      "Iteration 112, loss = 0.16439893\n",
      "Iteration 113, loss = 0.16350552\n",
      "Iteration 114, loss = 0.16262834\n",
      "Iteration 115, loss = 0.16176758\n",
      "Iteration 116, loss = 0.16092153\n",
      "Iteration 117, loss = 0.16009085\n",
      "Iteration 118, loss = 0.15927481\n",
      "Iteration 119, loss = 0.15847314\n",
      "Iteration 120, loss = 0.15768572\n",
      "Iteration 121, loss = 0.15691220\n",
      "Iteration 122, loss = 0.15615208\n",
      "Iteration 123, loss = 0.15540520\n",
      "Iteration 124, loss = 0.15467138\n",
      "Iteration 125, loss = 0.15395020\n",
      "Iteration 126, loss = 0.15324137\n",
      "Iteration 127, loss = 0.15254486\n",
      "Iteration 128, loss = 0.15186144\n",
      "Iteration 129, loss = 0.15118833\n",
      "Iteration 130, loss = 0.15052759\n",
      "Iteration 131, loss = 0.14987776\n",
      "Iteration 132, loss = 0.14923901\n",
      "Iteration 133, loss = 0.14861096\n",
      "Iteration 134, loss = 0.14799382\n",
      "Iteration 135, loss = 0.14738690\n",
      "Iteration 136, loss = 0.14678977\n",
      "Iteration 137, loss = 0.14620234\n",
      "Iteration 138, loss = 0.14562545\n",
      "Iteration 139, loss = 0.14505763\n",
      "Iteration 140, loss = 0.14449992\n",
      "Iteration 141, loss = 0.14395155\n",
      "Iteration 142, loss = 0.14341252\n",
      "Iteration 143, loss = 0.14288172\n",
      "Iteration 144, loss = 0.14236052\n",
      "Iteration 145, loss = 0.14184716\n",
      "Iteration 146, loss = 0.14134258\n",
      "Iteration 147, loss = 0.14084801\n",
      "Iteration 148, loss = 0.14036327\n",
      "Iteration 149, loss = 0.13988514\n",
      "Iteration 150, loss = 0.13941432\n",
      "Iteration 151, loss = 0.13894961\n",
      "Iteration 152, loss = 0.13849169\n",
      "Iteration 153, loss = 0.13804037\n",
      "Iteration 154, loss = 0.13759535\n",
      "Iteration 155, loss = 0.13715678\n",
      "Iteration 156, loss = 0.13672460\n",
      "Iteration 157, loss = 0.13630054\n",
      "Iteration 158, loss = 0.13588256\n",
      "Iteration 159, loss = 0.13547110\n",
      "Iteration 160, loss = 0.13506474\n",
      "Iteration 161, loss = 0.13466452\n",
      "Iteration 162, loss = 0.13426958\n",
      "Iteration 163, loss = 0.13388040\n",
      "Iteration 164, loss = 0.13349659\n",
      "Iteration 165, loss = 0.13311835\n",
      "Iteration 166, loss = 0.13274529\n",
      "Iteration 167, loss = 0.13237727\n",
      "Iteration 168, loss = 0.13201455\n",
      "Iteration 169, loss = 0.13165665\n",
      "Iteration 170, loss = 0.13130458\n",
      "Iteration 171, loss = 0.13095576\n",
      "Iteration 172, loss = 0.13061265\n",
      "Iteration 173, loss = 0.13027364\n",
      "Iteration 174, loss = 0.12993892\n",
      "Iteration 175, loss = 0.12960950\n",
      "Iteration 176, loss = 0.12928335\n",
      "Iteration 177, loss = 0.12896203\n",
      "Iteration 178, loss = 0.12864469\n",
      "Iteration 179, loss = 0.12833141\n",
      "Iteration 180, loss = 0.12802209\n",
      "Iteration 181, loss = 0.12771727\n",
      "Iteration 182, loss = 0.12741575\n",
      "Iteration 183, loss = 0.12711828\n",
      "Iteration 184, loss = 0.12682485\n",
      "Iteration 185, loss = 0.12653479\n",
      "Iteration 186, loss = 0.12624824\n",
      "Iteration 187, loss = 0.12596515\n",
      "Iteration 188, loss = 0.12568564\n",
      "Iteration 189, loss = 0.12540966\n",
      "Iteration 190, loss = 0.12513691\n",
      "Iteration 191, loss = 0.12486739\n",
      "Iteration 192, loss = 0.12460165\n",
      "Iteration 193, loss = 0.12433829\n",
      "Iteration 194, loss = 0.12407842\n",
      "Iteration 195, loss = 0.12382193\n",
      "Iteration 196, loss = 0.12356821\n",
      "Iteration 197, loss = 0.12331740\n",
      "Iteration 198, loss = 0.12306945\n",
      "Iteration 199, loss = 0.12282430\n",
      "Iteration 200, loss = 0.12258245\n",
      "Iteration 201, loss = 0.12234271\n",
      "Iteration 202, loss = 0.12210597\n",
      "Iteration 203, loss = 0.12187218\n",
      "Iteration 204, loss = 0.12164081\n",
      "Iteration 205, loss = 0.12141204\n",
      "Iteration 206, loss = 0.12118580\n",
      "Iteration 207, loss = 0.12096202\n",
      "Iteration 208, loss = 0.12074126\n",
      "Iteration 209, loss = 0.12052213\n",
      "Iteration 210, loss = 0.12030582\n",
      "Iteration 211, loss = 0.12009198\n",
      "Iteration 212, loss = 0.11988033\n",
      "Iteration 213, loss = 0.11967090\n",
      "Iteration 214, loss = 0.11946365\n",
      "Iteration 215, loss = 0.11925857\n",
      "Iteration 216, loss = 0.11905618\n",
      "Iteration 217, loss = 0.11885523\n",
      "Iteration 218, loss = 0.11865667\n",
      "Iteration 219, loss = 0.11846046\n",
      "Iteration 220, loss = 0.11826600\n",
      "Iteration 221, loss = 0.11807361\n",
      "Iteration 222, loss = 0.11788318\n",
      "Iteration 223, loss = 0.11769467\n",
      "Iteration 224, loss = 0.11750849\n",
      "Iteration 225, loss = 0.11732368\n",
      "Iteration 226, loss = 0.11714096\n",
      "Iteration 227, loss = 0.11696004\n",
      "Iteration 228, loss = 0.11678120\n",
      "Iteration 229, loss = 0.11660394\n",
      "Iteration 230, loss = 0.11642841\n",
      "Iteration 231, loss = 0.11625459\n",
      "Iteration 232, loss = 0.11608241\n",
      "Iteration 233, loss = 0.11591241\n",
      "Iteration 234, loss = 0.11574341\n",
      "Iteration 235, loss = 0.11557631\n",
      "Iteration 236, loss = 0.11541105\n",
      "Iteration 237, loss = 0.11524719\n",
      "Iteration 238, loss = 0.11508494\n",
      "Iteration 239, loss = 0.11492422\n",
      "Iteration 240, loss = 0.11476500\n",
      "Iteration 241, loss = 0.11460747\n",
      "Iteration 242, loss = 0.11445128\n",
      "Iteration 243, loss = 0.11429657\n",
      "Iteration 244, loss = 0.11414329\n",
      "Iteration 245, loss = 0.11399147\n",
      "Iteration 246, loss = 0.11384118\n",
      "Iteration 247, loss = 0.11369218\n",
      "Iteration 248, loss = 0.11354448\n",
      "Iteration 249, loss = 0.11339810\n",
      "Iteration 250, loss = 0.11325325\n",
      "Iteration 251, loss = 0.11310956\n",
      "Iteration 252, loss = 0.11296717\n",
      "Iteration 253, loss = 0.11282602\n",
      "Iteration 254, loss = 0.11268628\n",
      "Iteration 255, loss = 0.11254775\n",
      "Iteration 256, loss = 0.11241039\n",
      "Iteration 257, loss = 0.11227421\n",
      "Iteration 258, loss = 0.11213920\n",
      "Iteration 259, loss = 0.11200556\n",
      "Iteration 260, loss = 0.11187293\n",
      "Iteration 261, loss = 0.11174143\n",
      "Iteration 262, loss = 0.11161110\n",
      "Iteration 263, loss = 0.11148184\n",
      "Iteration 264, loss = 0.11135390\n",
      "Iteration 265, loss = 0.11122691\n",
      "Iteration 266, loss = 0.11110098\n",
      "Iteration 267, loss = 0.11097607\n",
      "Iteration 268, loss = 0.11085219\n",
      "Iteration 269, loss = 0.11072983\n",
      "Iteration 270, loss = 0.11060777\n",
      "Iteration 271, loss = 0.11048704\n",
      "Iteration 272, loss = 0.11036737\n",
      "Iteration 273, loss = 0.11024874\n",
      "Iteration 274, loss = 0.11013101\n",
      "Iteration 275, loss = 0.11001422\n",
      "Iteration 276, loss = 0.10989832\n",
      "Iteration 277, loss = 0.10978335\n",
      "Iteration 278, loss = 0.10966939\n",
      "Iteration 279, loss = 0.10955641\n",
      "Iteration 280, loss = 0.10944427\n",
      "Iteration 281, loss = 0.10933301\n",
      "Iteration 282, loss = 0.10922260\n",
      "Iteration 283, loss = 0.10911334\n",
      "Iteration 284, loss = 0.10900459\n",
      "Iteration 285, loss = 0.10889683\n",
      "Iteration 286, loss = 0.10878987\n",
      "Iteration 287, loss = 0.10868374\n",
      "Iteration 288, loss = 0.10857865\n",
      "Iteration 289, loss = 0.10847422\n",
      "Iteration 290, loss = 0.10837055\n",
      "Iteration 291, loss = 0.10826765\n",
      "Iteration 292, loss = 0.10816552\n",
      "Iteration 293, loss = 0.10806427\n",
      "Iteration 294, loss = 0.10796379\n",
      "Iteration 295, loss = 0.10786401\n",
      "Iteration 296, loss = 0.10776496\n",
      "Iteration 297, loss = 0.10766662\n",
      "Iteration 298, loss = 0.10756924\n",
      "Iteration 299, loss = 0.10747235\n",
      "Iteration 300, loss = 0.10737625\n",
      "Iteration 301, loss = 0.10728083\n",
      "Iteration 302, loss = 0.10718608\n",
      "Iteration 303, loss = 0.10709221\n",
      "Iteration 304, loss = 0.10699886\n",
      "Iteration 305, loss = 0.10690622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.45807064\n",
      "Iteration 3, loss = 1.42935304\n",
      "Iteration 4, loss = 1.40157758\n",
      "Iteration 5, loss = 1.37470964\n",
      "Iteration 6, loss = 1.34868508\n",
      "Iteration 7, loss = 1.32335864\n",
      "Iteration 8, loss = 1.29871510\n",
      "Iteration 9, loss = 1.27492256\n",
      "Iteration 10, loss = 1.25202889\n",
      "Iteration 11, loss = 1.22995304\n",
      "Iteration 12, loss = 1.20875052\n",
      "Iteration 13, loss = 1.18867466\n",
      "Iteration 14, loss = 1.16962964\n",
      "Iteration 15, loss = 1.15138124\n",
      "Iteration 16, loss = 1.13389030\n",
      "Iteration 17, loss = 1.11701469\n",
      "Iteration 18, loss = 1.10069723\n",
      "Iteration 19, loss = 1.08493152\n",
      "Iteration 20, loss = 1.06973471\n",
      "Iteration 21, loss = 1.05511956\n",
      "Iteration 22, loss = 1.04098713\n",
      "Iteration 23, loss = 1.02725460\n",
      "Iteration 24, loss = 1.01390784\n",
      "Iteration 25, loss = 1.00095351\n",
      "Iteration 26, loss = 0.98832568\n",
      "Iteration 27, loss = 0.97597344\n",
      "Iteration 28, loss = 0.96393407\n",
      "Iteration 29, loss = 0.95212521\n",
      "Iteration 30, loss = 0.94054710\n",
      "Iteration 31, loss = 0.92918203\n",
      "Iteration 32, loss = 0.91800562\n",
      "Iteration 33, loss = 0.90707109\n",
      "Iteration 34, loss = 0.89644862\n",
      "Iteration 35, loss = 0.88614569\n",
      "Iteration 36, loss = 0.87609101\n",
      "Iteration 37, loss = 0.86630510\n",
      "Iteration 38, loss = 0.85670379\n",
      "Iteration 39, loss = 0.84732725\n",
      "Iteration 40, loss = 0.83820194\n",
      "Iteration 41, loss = 0.82935671\n",
      "Iteration 42, loss = 0.82066423\n",
      "Iteration 43, loss = 0.81210105\n",
      "Iteration 44, loss = 0.80369451\n",
      "Iteration 45, loss = 0.79549796\n",
      "Iteration 46, loss = 0.78746996\n",
      "Iteration 47, loss = 0.77958665\n",
      "Iteration 48, loss = 0.77184473\n",
      "Iteration 49, loss = 0.76425638\n",
      "Iteration 50, loss = 0.75681548\n",
      "Iteration 51, loss = 0.74949882\n",
      "Iteration 52, loss = 0.74231560\n",
      "Iteration 53, loss = 0.73528830\n",
      "Iteration 54, loss = 0.72838821\n",
      "Iteration 55, loss = 0.72161492\n",
      "Iteration 56, loss = 0.71497457\n",
      "Iteration 57, loss = 0.70849049\n",
      "Iteration 58, loss = 0.70214668\n",
      "Iteration 59, loss = 0.69594734\n",
      "Iteration 60, loss = 0.68988243\n",
      "Iteration 61, loss = 0.68396632\n",
      "Iteration 62, loss = 0.67820156\n",
      "Iteration 63, loss = 0.67257988\n",
      "Iteration 64, loss = 0.66708098\n",
      "Iteration 65, loss = 0.66167641\n",
      "Iteration 66, loss = 0.65634942\n",
      "Iteration 67, loss = 0.65109608\n",
      "Iteration 68, loss = 0.64591614\n",
      "Iteration 69, loss = 0.64079300\n",
      "Iteration 70, loss = 0.63572519\n",
      "Iteration 71, loss = 0.63072618\n",
      "Iteration 72, loss = 0.62578227\n",
      "Iteration 73, loss = 0.62089962\n",
      "Iteration 74, loss = 0.61608369\n",
      "Iteration 75, loss = 0.61133723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.60664089\n",
      "Iteration 77, loss = 0.60197879\n",
      "Iteration 78, loss = 0.59737483\n",
      "Iteration 79, loss = 0.59278571\n",
      "Iteration 80, loss = 0.58825733\n",
      "Iteration 81, loss = 0.58377501\n",
      "Iteration 82, loss = 0.57925890\n",
      "Iteration 83, loss = 0.57479490\n",
      "Iteration 84, loss = 0.57014835\n",
      "Iteration 85, loss = 0.56545264\n",
      "Iteration 86, loss = 0.56098833\n",
      "Iteration 87, loss = 0.55663484\n",
      "Iteration 88, loss = 0.55234315\n",
      "Iteration 89, loss = 0.54809910\n",
      "Iteration 90, loss = 0.54397932\n",
      "Iteration 91, loss = 0.53998574\n",
      "Iteration 92, loss = 0.53615195\n",
      "Iteration 93, loss = 0.53239329\n",
      "Iteration 94, loss = 0.52871235\n",
      "Iteration 95, loss = 0.52508188\n",
      "Iteration 96, loss = 0.52153168\n",
      "Iteration 97, loss = 0.51797991\n",
      "Iteration 98, loss = 0.51442640\n",
      "Iteration 99, loss = 0.51090590\n",
      "Iteration 100, loss = 0.50747168\n",
      "Iteration 101, loss = 0.50412760\n",
      "Iteration 102, loss = 0.50083052\n",
      "Iteration 103, loss = 0.49755791\n",
      "Iteration 104, loss = 0.49431074\n",
      "Iteration 105, loss = 0.49109223\n",
      "Iteration 106, loss = 0.48790345\n",
      "Iteration 107, loss = 0.48474027\n",
      "Iteration 108, loss = 0.48160977\n",
      "Iteration 109, loss = 0.47851483\n",
      "Iteration 110, loss = 0.47545353\n",
      "Iteration 111, loss = 0.47241607\n",
      "Iteration 112, loss = 0.46940594\n",
      "Iteration 113, loss = 0.46642490\n",
      "Iteration 114, loss = 0.46347545\n",
      "Iteration 115, loss = 0.46055858\n",
      "Iteration 116, loss = 0.45766997\n",
      "Iteration 117, loss = 0.45481451\n",
      "Iteration 118, loss = 0.45198233\n",
      "Iteration 119, loss = 0.44917588\n",
      "Iteration 120, loss = 0.44639538\n",
      "Iteration 121, loss = 0.44363969\n",
      "Iteration 122, loss = 0.44090661\n",
      "Iteration 123, loss = 0.43818378\n",
      "Iteration 124, loss = 0.43545664\n",
      "Iteration 125, loss = 0.43271596\n",
      "Iteration 126, loss = 0.42996144\n",
      "Iteration 127, loss = 0.42717201\n",
      "Iteration 128, loss = 0.42437936\n",
      "Iteration 129, loss = 0.42160246\n",
      "Iteration 130, loss = 0.41878005\n",
      "Iteration 131, loss = 0.41593507\n",
      "Iteration 132, loss = 0.41306881\n",
      "Iteration 133, loss = 0.41022061\n",
      "Iteration 134, loss = 0.40739409\n",
      "Iteration 135, loss = 0.40468696\n",
      "Iteration 136, loss = 0.40211644\n",
      "Iteration 137, loss = 0.39965288\n",
      "Iteration 138, loss = 0.39728102\n",
      "Iteration 139, loss = 0.39501162\n",
      "Iteration 140, loss = 0.39276149\n",
      "Iteration 141, loss = 0.39055388\n",
      "Iteration 142, loss = 0.38837257\n",
      "Iteration 143, loss = 0.38620641\n",
      "Iteration 144, loss = 0.38404765\n",
      "Iteration 145, loss = 0.38188954\n",
      "Iteration 146, loss = 0.37974162\n",
      "Iteration 147, loss = 0.37759739\n",
      "Iteration 148, loss = 0.37545538\n",
      "Iteration 149, loss = 0.37331514\n",
      "Iteration 150, loss = 0.37117819\n",
      "Iteration 151, loss = 0.36904929\n",
      "Iteration 152, loss = 0.36693684\n",
      "Iteration 153, loss = 0.36483001\n",
      "Iteration 154, loss = 0.36273992\n",
      "Iteration 155, loss = 0.36065941\n",
      "Iteration 156, loss = 0.35859332\n",
      "Iteration 157, loss = 0.35655147\n",
      "Iteration 158, loss = 0.35452879\n",
      "Iteration 159, loss = 0.35252777\n",
      "Iteration 160, loss = 0.35055313\n",
      "Iteration 161, loss = 0.34859764\n",
      "Iteration 162, loss = 0.34665603\n",
      "Iteration 163, loss = 0.34472723\n",
      "Iteration 164, loss = 0.34281114\n",
      "Iteration 165, loss = 0.34091050\n",
      "Iteration 166, loss = 0.33902229\n",
      "Iteration 167, loss = 0.33714515\n",
      "Iteration 168, loss = 0.33528029\n",
      "Iteration 169, loss = 0.33342645\n",
      "Iteration 170, loss = 0.33158565\n",
      "Iteration 171, loss = 0.32975684\n",
      "Iteration 172, loss = 0.32793941\n",
      "Iteration 173, loss = 0.32613704\n",
      "Iteration 174, loss = 0.32435055\n",
      "Iteration 175, loss = 0.32257773\n",
      "Iteration 176, loss = 0.32081489\n",
      "Iteration 177, loss = 0.31906243\n",
      "Iteration 178, loss = 0.31732051\n",
      "Iteration 179, loss = 0.31558867\n",
      "Iteration 180, loss = 0.31386810\n",
      "Iteration 181, loss = 0.31215829\n",
      "Iteration 182, loss = 0.31046094\n",
      "Iteration 183, loss = 0.30877370\n",
      "Iteration 184, loss = 0.30709597\n",
      "Iteration 185, loss = 0.30542919\n",
      "Iteration 186, loss = 0.30376941\n",
      "Iteration 187, loss = 0.30211807\n",
      "Iteration 188, loss = 0.30047853\n",
      "Iteration 189, loss = 0.29885116\n",
      "Iteration 190, loss = 0.29723777\n",
      "Iteration 191, loss = 0.29563855\n",
      "Iteration 192, loss = 0.29405252\n",
      "Iteration 193, loss = 0.29247659\n",
      "Iteration 194, loss = 0.29091206\n",
      "Iteration 195, loss = 0.28935841\n",
      "Iteration 196, loss = 0.28781540\n",
      "Iteration 197, loss = 0.28628287\n",
      "Iteration 198, loss = 0.28475927\n",
      "Iteration 199, loss = 0.28324492\n",
      "Iteration 200, loss = 0.28174180\n",
      "Iteration 201, loss = 0.28024921\n",
      "Iteration 202, loss = 0.27876703\n",
      "Iteration 203, loss = 0.27729545\n",
      "Iteration 204, loss = 0.27583386\n",
      "Iteration 205, loss = 0.27438228\n",
      "Iteration 206, loss = 0.27294190\n",
      "Iteration 207, loss = 0.27151292\n",
      "Iteration 208, loss = 0.27009400\n",
      "Iteration 209, loss = 0.26868590\n",
      "Iteration 210, loss = 0.26728862\n",
      "Iteration 211, loss = 0.26590042\n",
      "Iteration 212, loss = 0.26452376\n",
      "Iteration 213, loss = 0.26315734\n",
      "Iteration 214, loss = 0.26180063\n",
      "Iteration 215, loss = 0.26045456\n",
      "Iteration 216, loss = 0.25911894\n",
      "Iteration 217, loss = 0.25779291\n",
      "Iteration 218, loss = 0.25647804\n",
      "Iteration 219, loss = 0.25517259\n",
      "Iteration 220, loss = 0.25387640\n",
      "Iteration 221, loss = 0.25259037\n",
      "Iteration 222, loss = 0.25131483\n",
      "Iteration 223, loss = 0.25004840\n",
      "Iteration 224, loss = 0.24879297\n",
      "Iteration 225, loss = 0.24754679\n",
      "Iteration 226, loss = 0.24631024\n",
      "Iteration 227, loss = 0.24508387\n",
      "Iteration 228, loss = 0.24386666\n",
      "Iteration 229, loss = 0.24265916\n",
      "Iteration 230, loss = 0.24146095\n",
      "Iteration 231, loss = 0.24027217\n",
      "Iteration 232, loss = 0.23909260\n",
      "Iteration 233, loss = 0.23792242\n",
      "Iteration 234, loss = 0.23676214\n",
      "Iteration 235, loss = 0.23561091\n",
      "Iteration 236, loss = 0.23446882\n",
      "Iteration 237, loss = 0.23333558\n",
      "Iteration 238, loss = 0.23221124\n",
      "Iteration 239, loss = 0.23109605\n",
      "Iteration 240, loss = 0.22998984\n",
      "Iteration 241, loss = 0.22889240\n",
      "Iteration 242, loss = 0.22780379\n",
      "Iteration 243, loss = 0.22672384\n",
      "Iteration 244, loss = 0.22565252\n",
      "Iteration 245, loss = 0.22459034\n",
      "Iteration 246, loss = 0.22353691\n",
      "Iteration 247, loss = 0.22249199\n",
      "Iteration 248, loss = 0.22145548\n",
      "Iteration 249, loss = 0.22042697\n",
      "Iteration 250, loss = 0.21940671\n",
      "Iteration 251, loss = 0.21839473\n",
      "Iteration 252, loss = 0.21739108\n",
      "Iteration 253, loss = 0.21639548\n",
      "Iteration 254, loss = 0.21540787\n",
      "Iteration 255, loss = 0.21442831\n",
      "Iteration 256, loss = 0.21345700\n",
      "Iteration 257, loss = 0.21249324\n",
      "Iteration 258, loss = 0.21153775\n",
      "Iteration 259, loss = 0.21059017\n",
      "Iteration 260, loss = 0.20965048\n",
      "Iteration 261, loss = 0.20871808\n",
      "Iteration 262, loss = 0.20779413\n",
      "Iteration 263, loss = 0.20687722\n",
      "Iteration 264, loss = 0.20596766\n",
      "Iteration 265, loss = 0.20506646\n",
      "Iteration 266, loss = 0.20417198\n",
      "Iteration 267, loss = 0.20328414\n",
      "Iteration 268, loss = 0.20240465\n",
      "Iteration 269, loss = 0.20153192\n",
      "Iteration 270, loss = 0.20066584\n",
      "Iteration 271, loss = 0.19980727\n",
      "Iteration 272, loss = 0.19895553\n",
      "Iteration 273, loss = 0.19811121\n",
      "Iteration 274, loss = 0.19727419\n",
      "Iteration 275, loss = 0.19644347\n",
      "Iteration 276, loss = 0.19561942\n",
      "Iteration 277, loss = 0.19480284\n",
      "Iteration 278, loss = 0.19399324\n",
      "Iteration 279, loss = 0.19318980\n",
      "Iteration 280, loss = 0.19239303\n",
      "Iteration 281, loss = 0.19160263\n",
      "Iteration 282, loss = 0.19081780\n",
      "Iteration 283, loss = 0.19003646\n",
      "Iteration 284, loss = 0.18926203\n",
      "Iteration 285, loss = 0.18849264\n",
      "Iteration 286, loss = 0.18772746\n",
      "Iteration 287, loss = 0.18696905\n",
      "Iteration 288, loss = 0.18621533\n",
      "Iteration 289, loss = 0.18546464\n",
      "Iteration 290, loss = 0.18472340\n",
      "Iteration 291, loss = 0.18398867\n",
      "Iteration 292, loss = 0.18325872\n",
      "Iteration 293, loss = 0.18253437\n",
      "Iteration 294, loss = 0.18181517\n",
      "Iteration 295, loss = 0.18110003\n",
      "Iteration 296, loss = 0.18038763\n",
      "Iteration 297, loss = 0.17967437\n",
      "Iteration 298, loss = 0.17896220\n",
      "Iteration 299, loss = 0.17825225\n",
      "Iteration 300, loss = 0.17754423\n",
      "Iteration 301, loss = 0.17683020\n",
      "Iteration 302, loss = 0.17610940\n",
      "Iteration 303, loss = 0.17538720\n",
      "Iteration 304, loss = 0.17466229\n",
      "Iteration 305, loss = 0.17393938\n",
      "Iteration 306, loss = 0.17322533\n",
      "Iteration 307, loss = 0.17251530\n",
      "Iteration 308, loss = 0.17180445\n",
      "Iteration 309, loss = 0.17108526\n",
      "Iteration 310, loss = 0.17039311\n",
      "Iteration 311, loss = 0.16969640\n",
      "Iteration 312, loss = 0.16897165\n",
      "Iteration 313, loss = 0.16822948\n",
      "Iteration 314, loss = 0.16747904\n",
      "Iteration 315, loss = 0.16673206\n",
      "Iteration 316, loss = 0.16599959\n",
      "Iteration 317, loss = 0.16529580\n",
      "Iteration 318, loss = 0.16462949\n",
      "Iteration 319, loss = 0.16400073\n",
      "Iteration 320, loss = 0.16342474\n",
      "Iteration 321, loss = 0.16287928\n",
      "Iteration 322, loss = 0.16235070\n",
      "Iteration 323, loss = 0.16182563\n",
      "Iteration 324, loss = 0.16128911\n",
      "Iteration 325, loss = 0.16077313\n",
      "Iteration 326, loss = 0.16026369\n",
      "Iteration 327, loss = 0.15975080\n",
      "Iteration 328, loss = 0.15923670\n",
      "Iteration 329, loss = 0.15872135\n",
      "Iteration 330, loss = 0.15820425\n",
      "Iteration 331, loss = 0.15768700\n",
      "Iteration 332, loss = 0.15717429\n",
      "Iteration 333, loss = 0.15666395\n",
      "Iteration 334, loss = 0.15615009\n",
      "Iteration 335, loss = 0.15563754\n",
      "Iteration 336, loss = 0.15512909\n",
      "Iteration 337, loss = 0.15462340\n",
      "Iteration 338, loss = 0.15411859\n",
      "Iteration 339, loss = 0.15361462\n",
      "Iteration 340, loss = 0.15311210\n",
      "Iteration 341, loss = 0.15261171\n",
      "Iteration 342, loss = 0.15211482\n",
      "Iteration 343, loss = 0.15162259\n",
      "Iteration 344, loss = 0.15113399\n",
      "Iteration 345, loss = 0.15064745\n",
      "Iteration 346, loss = 0.15016320\n",
      "Iteration 347, loss = 0.14968247\n",
      "Iteration 348, loss = 0.14920562\n",
      "Iteration 349, loss = 0.14873206\n",
      "Iteration 350, loss = 0.14826062\n",
      "Iteration 351, loss = 0.14779253\n",
      "Iteration 352, loss = 0.14732720\n",
      "Iteration 353, loss = 0.14686441\n",
      "Iteration 354, loss = 0.14640496\n",
      "Iteration 355, loss = 0.14594833\n",
      "Iteration 356, loss = 0.14549597\n",
      "Iteration 357, loss = 0.14504582\n",
      "Iteration 358, loss = 0.14459789\n",
      "Iteration 359, loss = 0.14415269\n",
      "Iteration 360, loss = 0.14370927\n",
      "Iteration 361, loss = 0.14326778\n",
      "Iteration 362, loss = 0.14282886\n",
      "Iteration 363, loss = 0.14239243\n",
      "Iteration 364, loss = 0.14195874\n",
      "Iteration 365, loss = 0.14152754\n",
      "Iteration 366, loss = 0.14109847\n",
      "Iteration 367, loss = 0.14067164\n",
      "Iteration 368, loss = 0.14024939\n",
      "Iteration 369, loss = 0.13983001\n",
      "Iteration 370, loss = 0.13941344\n",
      "Iteration 371, loss = 0.13899941\n",
      "Iteration 372, loss = 0.13858796\n",
      "Iteration 373, loss = 0.13817890\n",
      "Iteration 374, loss = 0.13777259\n",
      "Iteration 375, loss = 0.13736920\n",
      "Iteration 376, loss = 0.13696836\n",
      "Iteration 377, loss = 0.13657012\n",
      "Iteration 378, loss = 0.13617467\n",
      "Iteration 379, loss = 0.13578211\n",
      "Iteration 380, loss = 0.13539208\n",
      "Iteration 381, loss = 0.13500483\n",
      "Iteration 382, loss = 0.13462045\n",
      "Iteration 383, loss = 0.13423877\n",
      "Iteration 384, loss = 0.13385976\n",
      "Iteration 385, loss = 0.13348359\n",
      "Iteration 386, loss = 0.13311025\n",
      "Iteration 387, loss = 0.13273949\n",
      "Iteration 388, loss = 0.13237169\n",
      "Iteration 389, loss = 0.13200660\n",
      "Iteration 390, loss = 0.13164403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 391, loss = 0.13128411\n",
      "Iteration 392, loss = 0.13092654\n",
      "Iteration 393, loss = 0.13057139\n",
      "Iteration 394, loss = 0.13021819\n",
      "Iteration 395, loss = 0.12986683\n",
      "Iteration 396, loss = 0.12951712\n",
      "Iteration 397, loss = 0.12916919\n",
      "Iteration 398, loss = 0.12882273\n",
      "Iteration 399, loss = 0.12847745\n",
      "Iteration 400, loss = 0.12813328\n",
      "Iteration 401, loss = 0.12779021\n",
      "Iteration 402, loss = 0.12744813\n",
      "Iteration 403, loss = 0.12710710\n",
      "Iteration 404, loss = 0.12676719\n",
      "Iteration 405, loss = 0.12642839\n",
      "Iteration 406, loss = 0.12609060\n",
      "Iteration 407, loss = 0.12575386\n",
      "Iteration 408, loss = 0.12541824\n",
      "Iteration 409, loss = 0.12508362\n",
      "Iteration 410, loss = 0.12474998\n",
      "Iteration 411, loss = 0.12441753\n",
      "Iteration 412, loss = 0.12408642\n",
      "Iteration 413, loss = 0.12375641\n",
      "Iteration 414, loss = 0.12342779\n",
      "Iteration 415, loss = 0.12310044\n",
      "Iteration 416, loss = 0.12277443\n",
      "Iteration 417, loss = 0.12244978\n",
      "Iteration 418, loss = 0.12212656\n",
      "Iteration 419, loss = 0.12180499\n",
      "Iteration 420, loss = 0.12148484\n",
      "Iteration 421, loss = 0.12116625\n",
      "Iteration 422, loss = 0.12084918\n",
      "Iteration 423, loss = 0.12053341\n",
      "Iteration 424, loss = 0.12021938\n",
      "Iteration 425, loss = 0.11990682\n",
      "Iteration 426, loss = 0.11959547\n",
      "Iteration 427, loss = 0.11928513\n",
      "Iteration 428, loss = 0.11897599\n",
      "Iteration 429, loss = 0.11866792\n",
      "Iteration 430, loss = 0.11836090\n",
      "Iteration 431, loss = 0.11805495\n",
      "Iteration 432, loss = 0.11775077\n",
      "Iteration 433, loss = 0.11744758\n",
      "Iteration 434, loss = 0.11714612\n",
      "Iteration 435, loss = 0.11684573\n",
      "Iteration 436, loss = 0.11654628\n",
      "Iteration 437, loss = 0.11624779\n",
      "Iteration 438, loss = 0.11595005\n",
      "Iteration 439, loss = 0.11565341\n",
      "Iteration 440, loss = 0.11535762\n",
      "Iteration 441, loss = 0.11506295\n",
      "Iteration 442, loss = 0.11476951\n",
      "Iteration 443, loss = 0.11447726\n",
      "Iteration 444, loss = 0.11418633\n",
      "Iteration 445, loss = 0.11389629\n",
      "Iteration 446, loss = 0.11360747\n",
      "Iteration 447, loss = 0.11331980\n",
      "Iteration 448, loss = 0.11303331\n",
      "Iteration 449, loss = 0.11274821\n",
      "Iteration 450, loss = 0.11246449\n",
      "Iteration 451, loss = 0.11218238\n",
      "Iteration 452, loss = 0.11190168\n",
      "Iteration 453, loss = 0.11162240\n",
      "Iteration 454, loss = 0.11134526\n",
      "Iteration 455, loss = 0.11106990\n",
      "Iteration 456, loss = 0.11079601\n",
      "Iteration 457, loss = 0.11052358\n",
      "Iteration 458, loss = 0.11025272\n",
      "Iteration 459, loss = 0.10998384\n",
      "Iteration 460, loss = 0.10971673\n",
      "Iteration 461, loss = 0.10945128\n",
      "Iteration 462, loss = 0.10918771\n",
      "Iteration 463, loss = 0.10892606\n",
      "Iteration 464, loss = 0.10866623\n",
      "Iteration 465, loss = 0.10840843\n",
      "Iteration 466, loss = 0.10815264\n",
      "Iteration 467, loss = 0.10789893\n",
      "Iteration 468, loss = 0.10764723\n",
      "Iteration 469, loss = 0.10739742\n",
      "Iteration 470, loss = 0.10714957\n",
      "Iteration 471, loss = 0.10690368\n",
      "Iteration 472, loss = 0.10665973\n",
      "Iteration 473, loss = 0.10641791\n",
      "Iteration 474, loss = 0.10617825\n",
      "Iteration 475, loss = 0.10594057\n",
      "Iteration 476, loss = 0.10570497\n",
      "Iteration 477, loss = 0.10547142\n",
      "Iteration 478, loss = 0.10523989\n",
      "Iteration 479, loss = 0.10501046\n",
      "Iteration 480, loss = 0.10478306\n",
      "Iteration 481, loss = 0.10455780\n",
      "Iteration 482, loss = 0.10433460\n",
      "Iteration 483, loss = 0.10411337\n",
      "Iteration 484, loss = 0.10389420\n",
      "Iteration 485, loss = 0.10367722\n",
      "Iteration 486, loss = 0.10346236\n",
      "Iteration 487, loss = 0.10324956\n",
      "Iteration 488, loss = 0.10303875\n",
      "Iteration 489, loss = 0.10282997\n",
      "Iteration 490, loss = 0.10262317\n",
      "Iteration 491, loss = 0.10241841\n",
      "Iteration 492, loss = 0.10221571\n",
      "Iteration 493, loss = 0.10201501\n",
      "Iteration 494, loss = 0.10181622\n",
      "Iteration 495, loss = 0.10161941\n",
      "Iteration 496, loss = 0.10142412\n",
      "Iteration 497, loss = 0.10123025\n",
      "Iteration 498, loss = 0.10103799\n",
      "Iteration 499, loss = 0.10084737\n",
      "Iteration 500, loss = 0.10065846\n",
      "Iteration 501, loss = 0.10047136\n",
      "Iteration 502, loss = 0.10028584\n",
      "Iteration 503, loss = 0.10010193\n",
      "Iteration 504, loss = 0.09991967\n",
      "Iteration 505, loss = 0.09974024\n",
      "Iteration 506, loss = 0.09956279\n",
      "Iteration 507, loss = 0.09938717\n",
      "Iteration 508, loss = 0.09921323\n",
      "Iteration 509, loss = 0.09904121\n",
      "Iteration 510, loss = 0.09887106\n",
      "Iteration 511, loss = 0.09870289\n",
      "Iteration 512, loss = 0.09853651\n",
      "Iteration 513, loss = 0.09837185\n",
      "Iteration 514, loss = 0.09820881\n",
      "Iteration 515, loss = 0.09804742\n",
      "Iteration 516, loss = 0.09788765\n",
      "Iteration 517, loss = 0.09772948\n",
      "Iteration 518, loss = 0.09757321\n",
      "Iteration 519, loss = 0.09741845\n",
      "Iteration 520, loss = 0.09726515\n",
      "Iteration 521, loss = 0.09711325\n",
      "Iteration 522, loss = 0.09696284\n",
      "Iteration 523, loss = 0.09681393\n",
      "Iteration 524, loss = 0.09666644\n",
      "Iteration 525, loss = 0.09652058\n",
      "Iteration 526, loss = 0.09637614\n",
      "Iteration 527, loss = 0.09623318\n",
      "Iteration 528, loss = 0.09609149\n",
      "Iteration 529, loss = 0.09595119\n",
      "Iteration 530, loss = 0.09581217\n",
      "Iteration 531, loss = 0.09567476\n",
      "Iteration 532, loss = 0.09553832\n",
      "Iteration 533, loss = 0.09540355\n",
      "Iteration 534, loss = 0.09526986\n",
      "Iteration 535, loss = 0.09513760\n",
      "Iteration 536, loss = 0.09500653\n",
      "Iteration 537, loss = 0.09487697\n",
      "Iteration 538, loss = 0.09474858\n",
      "Iteration 539, loss = 0.09462164\n",
      "Iteration 540, loss = 0.09449584\n",
      "Iteration 541, loss = 0.09437175\n",
      "Iteration 542, loss = 0.09424833\n",
      "Iteration 543, loss = 0.09412660\n",
      "Iteration 544, loss = 0.09400543\n",
      "Iteration 545, loss = 0.09388609\n",
      "Iteration 546, loss = 0.09376700\n",
      "Iteration 547, loss = 0.09364968\n",
      "Iteration 548, loss = 0.09353306\n",
      "Iteration 549, loss = 0.09341775\n",
      "Iteration 550, loss = 0.09330328\n",
      "Iteration 551, loss = 0.09319002\n",
      "Iteration 552, loss = 0.09307771\n",
      "Iteration 553, loss = 0.09296653\n",
      "Iteration 554, loss = 0.09285640\n",
      "Iteration 555, loss = 0.09274727\n",
      "Iteration 556, loss = 0.09263904\n",
      "Iteration 557, loss = 0.09253190\n",
      "Iteration 558, loss = 0.09242560\n",
      "Iteration 559, loss = 0.09232030\n",
      "Iteration 560, loss = 0.09221586\n",
      "Iteration 561, loss = 0.09211244\n",
      "Iteration 562, loss = 0.09200988\n",
      "Iteration 563, loss = 0.09190822\n",
      "Iteration 564, loss = 0.09180748\n",
      "Iteration 565, loss = 0.09170758\n",
      "Iteration 566, loss = 0.09160860\n",
      "Iteration 567, loss = 0.09151060\n",
      "Iteration 568, loss = 0.09141350\n",
      "Iteration 569, loss = 0.09131731\n",
      "Iteration 570, loss = 0.09122207\n",
      "Iteration 571, loss = 0.09112761\n",
      "Iteration 572, loss = 0.09103397\n",
      "Iteration 573, loss = 0.09094113\n",
      "Iteration 574, loss = 0.09084913\n",
      "Iteration 575, loss = 0.09075786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.44829403\n",
      "Iteration 3, loss = 1.42029541\n",
      "Iteration 4, loss = 1.39318462\n",
      "Iteration 5, loss = 1.36701846\n",
      "Iteration 6, loss = 1.34160451\n",
      "Iteration 7, loss = 1.31684159\n",
      "Iteration 8, loss = 1.29274792\n",
      "Iteration 9, loss = 1.26945737\n",
      "Iteration 10, loss = 1.24705978\n",
      "Iteration 11, loss = 1.22550787\n",
      "Iteration 12, loss = 1.20479259\n",
      "Iteration 13, loss = 1.18516225\n",
      "Iteration 14, loss = 1.16656161\n",
      "Iteration 15, loss = 1.14870233\n",
      "Iteration 16, loss = 1.13150719\n",
      "Iteration 17, loss = 1.11486448\n",
      "Iteration 18, loss = 1.09876003\n",
      "Iteration 19, loss = 1.08319824\n",
      "Iteration 20, loss = 1.06832635\n",
      "Iteration 21, loss = 1.05409252\n",
      "Iteration 22, loss = 1.04032650\n",
      "Iteration 23, loss = 1.02696246\n",
      "Iteration 24, loss = 1.01407218\n",
      "Iteration 25, loss = 1.00150299\n",
      "Iteration 26, loss = 0.98918104\n",
      "Iteration 27, loss = 0.97712368\n",
      "Iteration 28, loss = 0.96533392\n",
      "Iteration 29, loss = 0.95377553\n",
      "Iteration 30, loss = 0.94249394\n",
      "Iteration 31, loss = 0.93144504\n",
      "Iteration 32, loss = 0.92059150\n",
      "Iteration 33, loss = 0.90995191\n",
      "Iteration 34, loss = 0.89958853\n",
      "Iteration 35, loss = 0.88947540\n",
      "Iteration 36, loss = 0.87957441\n",
      "Iteration 37, loss = 0.86991875\n",
      "Iteration 38, loss = 0.86045471\n",
      "Iteration 39, loss = 0.85123926\n",
      "Iteration 40, loss = 0.84224884\n",
      "Iteration 41, loss = 0.83348241\n",
      "Iteration 42, loss = 0.82488246\n",
      "Iteration 43, loss = 0.81639812\n",
      "Iteration 44, loss = 0.80809412\n",
      "Iteration 45, loss = 0.79998216\n",
      "Iteration 46, loss = 0.79203494\n",
      "Iteration 47, loss = 0.78423638\n",
      "Iteration 48, loss = 0.77656046\n",
      "Iteration 49, loss = 0.76900835\n",
      "Iteration 50, loss = 0.76156816\n",
      "Iteration 51, loss = 0.75422717\n",
      "Iteration 52, loss = 0.74700400\n",
      "Iteration 53, loss = 0.73991132\n",
      "Iteration 54, loss = 0.73294245\n",
      "Iteration 55, loss = 0.72606671\n",
      "Iteration 56, loss = 0.71929340\n",
      "Iteration 57, loss = 0.71265399\n",
      "Iteration 58, loss = 0.70616233\n",
      "Iteration 59, loss = 0.69982722\n",
      "Iteration 60, loss = 0.69366489\n",
      "Iteration 61, loss = 0.68768801\n",
      "Iteration 62, loss = 0.68188091\n",
      "Iteration 63, loss = 0.67623364\n",
      "Iteration 64, loss = 0.67072018\n",
      "Iteration 65, loss = 0.66530892\n",
      "Iteration 66, loss = 0.65998882\n",
      "Iteration 67, loss = 0.65475847\n",
      "Iteration 68, loss = 0.64960544\n",
      "Iteration 69, loss = 0.64448878\n",
      "Iteration 70, loss = 0.63932522\n",
      "Iteration 71, loss = 0.63415469\n",
      "Iteration 72, loss = 0.62893063\n",
      "Iteration 73, loss = 0.62359068\n",
      "Iteration 74, loss = 0.61834293\n",
      "Iteration 75, loss = 0.61330749\n",
      "Iteration 76, loss = 0.60835569\n",
      "Iteration 77, loss = 0.60350679\n",
      "Iteration 78, loss = 0.59876682\n",
      "Iteration 79, loss = 0.59413954\n",
      "Iteration 80, loss = 0.58963311\n",
      "Iteration 81, loss = 0.58530770\n",
      "Iteration 82, loss = 0.58108174\n",
      "Iteration 83, loss = 0.57694470\n",
      "Iteration 84, loss = 0.57288514\n",
      "Iteration 85, loss = 0.56889097\n",
      "Iteration 86, loss = 0.56492062\n",
      "Iteration 87, loss = 0.56097355\n",
      "Iteration 88, loss = 0.55705845\n",
      "Iteration 89, loss = 0.55320558\n",
      "Iteration 90, loss = 0.54941907\n",
      "Iteration 91, loss = 0.54566937\n",
      "Iteration 92, loss = 0.54195673\n",
      "Iteration 93, loss = 0.53828795\n",
      "Iteration 94, loss = 0.53465753\n",
      "Iteration 95, loss = 0.53106206\n",
      "Iteration 96, loss = 0.52750386\n",
      "Iteration 97, loss = 0.52394044\n",
      "Iteration 98, loss = 0.52037199\n",
      "Iteration 99, loss = 0.51680442\n",
      "Iteration 100, loss = 0.51326936\n",
      "Iteration 101, loss = 0.50983210\n",
      "Iteration 102, loss = 0.50647970\n",
      "Iteration 103, loss = 0.50317074\n",
      "Iteration 104, loss = 0.49988810\n",
      "Iteration 105, loss = 0.49662961\n",
      "Iteration 106, loss = 0.49340307\n",
      "Iteration 107, loss = 0.49020386\n",
      "Iteration 108, loss = 0.48703471\n",
      "Iteration 109, loss = 0.48389133\n",
      "Iteration 110, loss = 0.48076778\n",
      "Iteration 111, loss = 0.47767347\n",
      "Iteration 112, loss = 0.47460701\n",
      "Iteration 113, loss = 0.47155855\n",
      "Iteration 114, loss = 0.46853113\n",
      "Iteration 115, loss = 0.46552085\n",
      "Iteration 116, loss = 0.46253154\n",
      "Iteration 117, loss = 0.45956414\n",
      "Iteration 118, loss = 0.45661019\n",
      "Iteration 119, loss = 0.45367089\n",
      "Iteration 120, loss = 0.45073185\n",
      "Iteration 121, loss = 0.44778847\n",
      "Iteration 122, loss = 0.44484136\n",
      "Iteration 123, loss = 0.44187695\n",
      "Iteration 124, loss = 0.43886461\n",
      "Iteration 125, loss = 0.43582071\n",
      "Iteration 126, loss = 0.43279740\n",
      "Iteration 127, loss = 0.42970812\n",
      "Iteration 128, loss = 0.42664726\n",
      "Iteration 129, loss = 0.42354090\n",
      "Iteration 130, loss = 0.42041211\n",
      "Iteration 131, loss = 0.41733570\n",
      "Iteration 132, loss = 0.41439339\n",
      "Iteration 133, loss = 0.41157583\n",
      "Iteration 134, loss = 0.40897280\n",
      "Iteration 135, loss = 0.40647489\n",
      "Iteration 136, loss = 0.40406275\n",
      "Iteration 137, loss = 0.40168095\n",
      "Iteration 138, loss = 0.39935046\n",
      "Iteration 139, loss = 0.39705396\n",
      "Iteration 140, loss = 0.39474726\n",
      "Iteration 141, loss = 0.39243858\n",
      "Iteration 142, loss = 0.39012766\n",
      "Iteration 143, loss = 0.38782180\n",
      "Iteration 144, loss = 0.38552253\n",
      "Iteration 145, loss = 0.38322665\n",
      "Iteration 146, loss = 0.38094592\n",
      "Iteration 147, loss = 0.37866807\n",
      "Iteration 148, loss = 0.37640245\n",
      "Iteration 149, loss = 0.37414100\n",
      "Iteration 150, loss = 0.37189365\n",
      "Iteration 151, loss = 0.36965327\n",
      "Iteration 152, loss = 0.36743509\n",
      "Iteration 153, loss = 0.36524070\n",
      "Iteration 154, loss = 0.36307141\n",
      "Iteration 155, loss = 0.36092684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 156, loss = 0.35879917\n",
      "Iteration 157, loss = 0.35669030\n",
      "Iteration 158, loss = 0.35459692\n",
      "Iteration 159, loss = 0.35252425\n",
      "Iteration 160, loss = 0.35046591\n",
      "Iteration 161, loss = 0.34842492\n",
      "Iteration 162, loss = 0.34639700\n",
      "Iteration 163, loss = 0.34438344\n",
      "Iteration 164, loss = 0.34237993\n",
      "Iteration 165, loss = 0.34039919\n",
      "Iteration 166, loss = 0.33843113\n",
      "Iteration 167, loss = 0.33646828\n",
      "Iteration 168, loss = 0.33451479\n",
      "Iteration 169, loss = 0.33257377\n",
      "Iteration 170, loss = 0.33064545\n",
      "Iteration 171, loss = 0.32872977\n",
      "Iteration 172, loss = 0.32682390\n",
      "Iteration 173, loss = 0.32492726\n",
      "Iteration 174, loss = 0.32304025\n",
      "Iteration 175, loss = 0.32116319\n",
      "Iteration 176, loss = 0.31929706\n",
      "Iteration 177, loss = 0.31744101\n",
      "Iteration 178, loss = 0.31559413\n",
      "Iteration 179, loss = 0.31375864\n",
      "Iteration 180, loss = 0.31193339\n",
      "Iteration 181, loss = 0.31012710\n",
      "Iteration 182, loss = 0.30833644\n",
      "Iteration 183, loss = 0.30655744\n",
      "Iteration 184, loss = 0.30479094\n",
      "Iteration 185, loss = 0.30303910\n",
      "Iteration 186, loss = 0.30130668\n",
      "Iteration 187, loss = 0.29958547\n",
      "Iteration 188, loss = 0.29787614\n",
      "Iteration 189, loss = 0.29617497\n",
      "Iteration 190, loss = 0.29447864\n",
      "Iteration 191, loss = 0.29279244\n",
      "Iteration 192, loss = 0.29111807\n",
      "Iteration 193, loss = 0.28945511\n",
      "Iteration 194, loss = 0.28780870\n",
      "Iteration 195, loss = 0.28617552\n",
      "Iteration 196, loss = 0.28455418\n",
      "Iteration 197, loss = 0.28294368\n",
      "Iteration 198, loss = 0.28134407\n",
      "Iteration 199, loss = 0.27975630\n",
      "Iteration 200, loss = 0.27817929\n",
      "Iteration 201, loss = 0.27661286\n",
      "Iteration 202, loss = 0.27505718\n",
      "Iteration 203, loss = 0.27351252\n",
      "Iteration 204, loss = 0.27197794\n",
      "Iteration 205, loss = 0.27045476\n",
      "Iteration 206, loss = 0.26894467\n",
      "Iteration 207, loss = 0.26744681\n",
      "Iteration 208, loss = 0.26595951\n",
      "Iteration 209, loss = 0.26448354\n",
      "Iteration 210, loss = 0.26301865\n",
      "Iteration 211, loss = 0.26156414\n",
      "Iteration 212, loss = 0.26011954\n",
      "Iteration 213, loss = 0.25868661\n",
      "Iteration 214, loss = 0.25726429\n",
      "Iteration 215, loss = 0.25585268\n",
      "Iteration 216, loss = 0.25445276\n",
      "Iteration 217, loss = 0.25306329\n",
      "Iteration 218, loss = 0.25168425\n",
      "Iteration 219, loss = 0.25031606\n",
      "Iteration 220, loss = 0.24895826\n",
      "Iteration 221, loss = 0.24761080\n",
      "Iteration 222, loss = 0.24627393\n",
      "Iteration 223, loss = 0.24494963\n",
      "Iteration 224, loss = 0.24363613\n",
      "Iteration 225, loss = 0.24233352\n",
      "Iteration 226, loss = 0.24104143\n",
      "Iteration 227, loss = 0.23975944\n",
      "Iteration 228, loss = 0.23848770\n",
      "Iteration 229, loss = 0.23722617\n",
      "Iteration 230, loss = 0.23597488\n",
      "Iteration 231, loss = 0.23473429\n",
      "Iteration 232, loss = 0.23350343\n",
      "Iteration 233, loss = 0.23228202\n",
      "Iteration 234, loss = 0.23107000\n",
      "Iteration 235, loss = 0.22986785\n",
      "Iteration 236, loss = 0.22867543\n",
      "Iteration 237, loss = 0.22749253\n",
      "Iteration 238, loss = 0.22632016\n",
      "Iteration 239, loss = 0.22515711\n",
      "Iteration 240, loss = 0.22400338\n",
      "Iteration 241, loss = 0.22285903\n",
      "Iteration 242, loss = 0.22172557\n",
      "Iteration 243, loss = 0.22060037\n",
      "Iteration 244, loss = 0.21948409\n",
      "Iteration 245, loss = 0.21837763\n",
      "Iteration 246, loss = 0.21728078\n",
      "Iteration 247, loss = 0.21619304\n",
      "Iteration 248, loss = 0.21511398\n",
      "Iteration 249, loss = 0.21404445\n",
      "Iteration 250, loss = 0.21298349\n",
      "Iteration 251, loss = 0.21193188\n",
      "Iteration 252, loss = 0.21088876\n",
      "Iteration 253, loss = 0.20985423\n",
      "Iteration 254, loss = 0.20882825\n",
      "Iteration 255, loss = 0.20781114\n",
      "Iteration 256, loss = 0.20680382\n",
      "Iteration 257, loss = 0.20580458\n",
      "Iteration 258, loss = 0.20481341\n",
      "Iteration 259, loss = 0.20383026\n",
      "Iteration 260, loss = 0.20285656\n",
      "Iteration 261, loss = 0.20189010\n",
      "Iteration 262, loss = 0.20093131\n",
      "Iteration 263, loss = 0.19998089\n",
      "Iteration 264, loss = 0.19903839\n",
      "Iteration 265, loss = 0.19810420\n",
      "Iteration 266, loss = 0.19717682\n",
      "Iteration 267, loss = 0.19625492\n",
      "Iteration 268, loss = 0.19533925\n",
      "Iteration 269, loss = 0.19443062\n",
      "Iteration 270, loss = 0.19352833\n",
      "Iteration 271, loss = 0.19263177\n",
      "Iteration 272, loss = 0.19173911\n",
      "Iteration 273, loss = 0.19085259\n",
      "Iteration 274, loss = 0.18997254\n",
      "Iteration 275, loss = 0.18909679\n",
      "Iteration 276, loss = 0.18822364\n",
      "Iteration 277, loss = 0.18735632\n",
      "Iteration 278, loss = 0.18649537\n",
      "Iteration 279, loss = 0.18563651\n",
      "Iteration 280, loss = 0.18477466\n",
      "Iteration 281, loss = 0.18391579\n",
      "Iteration 282, loss = 0.18306117\n",
      "Iteration 283, loss = 0.18219046\n",
      "Iteration 284, loss = 0.18131453\n",
      "Iteration 285, loss = 0.18043046\n",
      "Iteration 286, loss = 0.17953560\n",
      "Iteration 287, loss = 0.17865070\n",
      "Iteration 288, loss = 0.17776537\n",
      "Iteration 289, loss = 0.17687681\n",
      "Iteration 290, loss = 0.17599775\n",
      "Iteration 291, loss = 0.17515364\n",
      "Iteration 292, loss = 0.17428573\n",
      "Iteration 293, loss = 0.17339455\n",
      "Iteration 294, loss = 0.17248434\n",
      "Iteration 295, loss = 0.17157843\n",
      "Iteration 296, loss = 0.17070964\n",
      "Iteration 297, loss = 0.16987279\n",
      "Iteration 298, loss = 0.16908287\n",
      "Iteration 299, loss = 0.16832211\n",
      "Iteration 300, loss = 0.16763732\n",
      "Iteration 301, loss = 0.16699571\n",
      "Iteration 302, loss = 0.16636146\n",
      "Iteration 303, loss = 0.16573215\n",
      "Iteration 304, loss = 0.16511519\n",
      "Iteration 305, loss = 0.16449982\n",
      "Iteration 306, loss = 0.16388514\n",
      "Iteration 307, loss = 0.16327237\n",
      "Iteration 308, loss = 0.16266080\n",
      "Iteration 309, loss = 0.16205066\n",
      "Iteration 310, loss = 0.16144201\n",
      "Iteration 311, loss = 0.16083618\n",
      "Iteration 312, loss = 0.16023240\n",
      "Iteration 313, loss = 0.15963191\n",
      "Iteration 314, loss = 0.15903391\n",
      "Iteration 315, loss = 0.15843905\n",
      "Iteration 316, loss = 0.15784864\n",
      "Iteration 317, loss = 0.15726293\n",
      "Iteration 318, loss = 0.15668081\n",
      "Iteration 319, loss = 0.15610294\n",
      "Iteration 320, loss = 0.15552777\n",
      "Iteration 321, loss = 0.15496093\n",
      "Iteration 322, loss = 0.15440224\n",
      "Iteration 323, loss = 0.15384781\n",
      "Iteration 324, loss = 0.15329788\n",
      "Iteration 325, loss = 0.15275346\n",
      "Iteration 326, loss = 0.15221531\n",
      "Iteration 327, loss = 0.15168321\n",
      "Iteration 328, loss = 0.15115770\n",
      "Iteration 329, loss = 0.15063655\n",
      "Iteration 330, loss = 0.15011484\n",
      "Iteration 331, loss = 0.14957434\n",
      "Iteration 332, loss = 0.14906955\n",
      "Iteration 333, loss = 0.14857597\n",
      "Iteration 334, loss = 0.14808108\n",
      "Iteration 335, loss = 0.14758619\n",
      "Iteration 336, loss = 0.14709192\n",
      "Iteration 337, loss = 0.14659851\n",
      "Iteration 338, loss = 0.14610561\n",
      "Iteration 339, loss = 0.14561416\n",
      "Iteration 340, loss = 0.14512522\n",
      "Iteration 341, loss = 0.14463866\n",
      "Iteration 342, loss = 0.14415687\n",
      "Iteration 343, loss = 0.14368561\n",
      "Iteration 344, loss = 0.14320837\n",
      "Iteration 345, loss = 0.14273327\n",
      "Iteration 346, loss = 0.14226636\n",
      "Iteration 347, loss = 0.14180317\n",
      "Iteration 348, loss = 0.14134212\n",
      "Iteration 349, loss = 0.14088308\n",
      "Iteration 350, loss = 0.14042555\n",
      "Iteration 351, loss = 0.13996982\n",
      "Iteration 352, loss = 0.13951631\n",
      "Iteration 353, loss = 0.13906495\n",
      "Iteration 354, loss = 0.13861621\n",
      "Iteration 355, loss = 0.13817066\n",
      "Iteration 356, loss = 0.13772829\n",
      "Iteration 357, loss = 0.13728761\n",
      "Iteration 358, loss = 0.13684814\n",
      "Iteration 359, loss = 0.13641222\n",
      "Iteration 360, loss = 0.13597900\n",
      "Iteration 361, loss = 0.13554865\n",
      "Iteration 362, loss = 0.13512045\n",
      "Iteration 363, loss = 0.13469427\n",
      "Iteration 364, loss = 0.13427037\n",
      "Iteration 365, loss = 0.13384925\n",
      "Iteration 366, loss = 0.13343095\n",
      "Iteration 367, loss = 0.13301544\n",
      "Iteration 368, loss = 0.13260181\n",
      "Iteration 369, loss = 0.13219053\n",
      "Iteration 370, loss = 0.13178217\n",
      "Iteration 371, loss = 0.13137647\n",
      "Iteration 372, loss = 0.13097314\n",
      "Iteration 373, loss = 0.13057226\n",
      "Iteration 374, loss = 0.13017405\n",
      "Iteration 375, loss = 0.12977855\n",
      "Iteration 376, loss = 0.12938564\n",
      "Iteration 377, loss = 0.12899523\n",
      "Iteration 378, loss = 0.12860759\n",
      "Iteration 379, loss = 0.12822258\n",
      "Iteration 380, loss = 0.12783976\n",
      "Iteration 381, loss = 0.12745983\n",
      "Iteration 382, loss = 0.12708262\n",
      "Iteration 383, loss = 0.12670803\n",
      "Iteration 384, loss = 0.12633598\n",
      "Iteration 385, loss = 0.12596646\n",
      "Iteration 386, loss = 0.12559954\n",
      "Iteration 387, loss = 0.12523519\n",
      "Iteration 388, loss = 0.12487335\n",
      "Iteration 389, loss = 0.12451411\n",
      "Iteration 390, loss = 0.12415761\n",
      "Iteration 391, loss = 0.12380322\n",
      "Iteration 392, loss = 0.12345177\n",
      "Iteration 393, loss = 0.12310265\n",
      "Iteration 394, loss = 0.12275540\n",
      "Iteration 395, loss = 0.12241074\n",
      "Iteration 396, loss = 0.12206790\n",
      "Iteration 397, loss = 0.12172667\n",
      "Iteration 398, loss = 0.12138705\n",
      "Iteration 399, loss = 0.12104909\n",
      "Iteration 400, loss = 0.12071231\n",
      "Iteration 401, loss = 0.12037659\n",
      "Iteration 402, loss = 0.12004184\n",
      "Iteration 403, loss = 0.11970808\n",
      "Iteration 404, loss = 0.11937554\n",
      "Iteration 405, loss = 0.11904431\n",
      "Iteration 406, loss = 0.11871401\n",
      "Iteration 407, loss = 0.11838508\n",
      "Iteration 408, loss = 0.11805728\n",
      "Iteration 409, loss = 0.11773013\n",
      "Iteration 410, loss = 0.11740377\n",
      "Iteration 411, loss = 0.11707854\n",
      "Iteration 412, loss = 0.11675453\n",
      "Iteration 413, loss = 0.11643193\n",
      "Iteration 414, loss = 0.11611034\n",
      "Iteration 415, loss = 0.11578983\n",
      "Iteration 416, loss = 0.11547077\n",
      "Iteration 417, loss = 0.11515295\n",
      "Iteration 418, loss = 0.11483643\n",
      "Iteration 419, loss = 0.11452133\n",
      "Iteration 420, loss = 0.11420769\n",
      "Iteration 421, loss = 0.11389550\n",
      "Iteration 422, loss = 0.11358475\n",
      "Iteration 423, loss = 0.11327544\n",
      "Iteration 424, loss = 0.11296756\n",
      "Iteration 425, loss = 0.11266143\n",
      "Iteration 426, loss = 0.11235621\n",
      "Iteration 427, loss = 0.11205217\n",
      "Iteration 428, loss = 0.11174925\n",
      "Iteration 429, loss = 0.11144749\n",
      "Iteration 430, loss = 0.11114700\n",
      "Iteration 431, loss = 0.11084789\n",
      "Iteration 432, loss = 0.11054990\n",
      "Iteration 433, loss = 0.11025286\n",
      "Iteration 434, loss = 0.10995673\n",
      "Iteration 435, loss = 0.10966147\n",
      "Iteration 436, loss = 0.10936718\n",
      "Iteration 437, loss = 0.10907368\n",
      "Iteration 438, loss = 0.10878114\n",
      "Iteration 439, loss = 0.10848973\n",
      "Iteration 440, loss = 0.10819904\n",
      "Iteration 441, loss = 0.10790911\n",
      "Iteration 442, loss = 0.10762054\n",
      "Iteration 443, loss = 0.10733311\n",
      "Iteration 444, loss = 0.10704684\n",
      "Iteration 445, loss = 0.10676157\n",
      "Iteration 446, loss = 0.10647748\n",
      "Iteration 447, loss = 0.10619459\n",
      "Iteration 448, loss = 0.10591299\n",
      "Iteration 449, loss = 0.10563274\n",
      "Iteration 450, loss = 0.10535392\n",
      "Iteration 451, loss = 0.10507658\n",
      "Iteration 452, loss = 0.10480073\n",
      "Iteration 453, loss = 0.10452644\n",
      "Iteration 454, loss = 0.10425380\n",
      "Iteration 455, loss = 0.10398269\n",
      "Iteration 456, loss = 0.10371340\n",
      "Iteration 457, loss = 0.10344586\n",
      "Iteration 458, loss = 0.10318005\n",
      "Iteration 459, loss = 0.10291611\n",
      "Iteration 460, loss = 0.10265404\n",
      "Iteration 461, loss = 0.10239376\n",
      "Iteration 462, loss = 0.10213541\n",
      "Iteration 463, loss = 0.10187905\n",
      "Iteration 464, loss = 0.10162468\n",
      "Iteration 465, loss = 0.10137226\n",
      "Iteration 466, loss = 0.10112206\n",
      "Iteration 467, loss = 0.10087383\n",
      "Iteration 468, loss = 0.10062787\n",
      "Iteration 469, loss = 0.10038393\n",
      "Iteration 470, loss = 0.10014210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 471, loss = 0.09990235\n",
      "Iteration 472, loss = 0.09966468\n",
      "Iteration 473, loss = 0.09942919\n",
      "Iteration 474, loss = 0.09919575\n",
      "Iteration 475, loss = 0.09896447\n",
      "Iteration 476, loss = 0.09873534\n",
      "Iteration 477, loss = 0.09850833\n",
      "Iteration 478, loss = 0.09828343\n",
      "Iteration 479, loss = 0.09806071\n",
      "Iteration 480, loss = 0.09784015\n",
      "Iteration 481, loss = 0.09762169\n",
      "Iteration 482, loss = 0.09740547\n",
      "Iteration 483, loss = 0.09719147\n",
      "Iteration 484, loss = 0.09697962\n",
      "Iteration 485, loss = 0.09676985\n",
      "Iteration 486, loss = 0.09656216\n",
      "Iteration 487, loss = 0.09635658\n",
      "Iteration 488, loss = 0.09615307\n",
      "Iteration 489, loss = 0.09595165\n",
      "Iteration 490, loss = 0.09575230\n",
      "Iteration 491, loss = 0.09555527\n",
      "Iteration 492, loss = 0.09536027\n",
      "Iteration 493, loss = 0.09516728\n",
      "Iteration 494, loss = 0.09497650\n",
      "Iteration 495, loss = 0.09478765\n",
      "Iteration 496, loss = 0.09460069\n",
      "Iteration 497, loss = 0.09441559\n",
      "Iteration 498, loss = 0.09423241\n",
      "Iteration 499, loss = 0.09405120\n",
      "Iteration 500, loss = 0.09387186\n",
      "Iteration 501, loss = 0.09369428\n",
      "Iteration 502, loss = 0.09351874\n",
      "Iteration 503, loss = 0.09334507\n",
      "Iteration 504, loss = 0.09317333\n",
      "Iteration 505, loss = 0.09300332\n",
      "Iteration 506, loss = 0.09283526\n",
      "Iteration 507, loss = 0.09266887\n",
      "Iteration 508, loss = 0.09250430\n",
      "Iteration 509, loss = 0.09234159\n",
      "Iteration 510, loss = 0.09218056\n",
      "Iteration 511, loss = 0.09202127\n",
      "Iteration 512, loss = 0.09186364\n",
      "Iteration 513, loss = 0.09170794\n",
      "Iteration 514, loss = 0.09155389\n",
      "Iteration 515, loss = 0.09140141\n",
      "Iteration 516, loss = 0.09125057\n",
      "Iteration 517, loss = 0.09110130\n",
      "Iteration 518, loss = 0.09095370\n",
      "Iteration 519, loss = 0.09080772\n",
      "Iteration 520, loss = 0.09066334\n",
      "Iteration 521, loss = 0.09052047\n",
      "Iteration 522, loss = 0.09037912\n",
      "Iteration 523, loss = 0.09023933\n",
      "Iteration 524, loss = 0.09010099\n",
      "Iteration 525, loss = 0.08996414\n",
      "Iteration 526, loss = 0.08982881\n",
      "Iteration 527, loss = 0.08969497\n",
      "Iteration 528, loss = 0.08956251\n",
      "Iteration 529, loss = 0.08943149\n",
      "Iteration 530, loss = 0.08930179\n",
      "Iteration 531, loss = 0.08917343\n",
      "Iteration 532, loss = 0.08904645\n",
      "Iteration 533, loss = 0.08892083\n",
      "Iteration 534, loss = 0.08879649\n",
      "Iteration 535, loss = 0.08867344\n",
      "Iteration 536, loss = 0.08855166\n",
      "Iteration 537, loss = 0.08843112\n",
      "Iteration 538, loss = 0.08831190\n",
      "Iteration 539, loss = 0.08819396\n",
      "Iteration 540, loss = 0.08807685\n",
      "Iteration 541, loss = 0.08796048\n",
      "Iteration 542, loss = 0.08784486\n",
      "Iteration 543, loss = 0.08773012\n",
      "Iteration 544, loss = 0.08761619\n",
      "Iteration 545, loss = 0.08750317\n",
      "Iteration 546, loss = 0.08739101\n",
      "Iteration 547, loss = 0.08727972\n",
      "Iteration 548, loss = 0.08716946\n",
      "Iteration 549, loss = 0.08706015\n",
      "Iteration 550, loss = 0.08695258\n",
      "Iteration 551, loss = 0.08684702\n",
      "Iteration 552, loss = 0.08674260\n",
      "Iteration 553, loss = 0.08663933\n",
      "Iteration 554, loss = 0.08653736\n",
      "Iteration 555, loss = 0.08643662\n",
      "Iteration 556, loss = 0.08633689\n",
      "Iteration 557, loss = 0.08623825\n",
      "Iteration 558, loss = 0.08614097\n",
      "Iteration 559, loss = 0.08604459\n",
      "Iteration 560, loss = 0.08594921\n",
      "Iteration 561, loss = 0.08585459\n",
      "Iteration 562, loss = 0.08576094\n",
      "Iteration 563, loss = 0.08566812\n",
      "Iteration 564, loss = 0.08557609\n",
      "Iteration 565, loss = 0.08548496\n",
      "Iteration 566, loss = 0.08539469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.45483051\n",
      "Iteration 3, loss = 1.42637119\n",
      "Iteration 4, loss = 1.39878816\n",
      "Iteration 5, loss = 1.37212407\n",
      "Iteration 6, loss = 1.34629895\n",
      "Iteration 7, loss = 1.32115228\n",
      "Iteration 8, loss = 1.29672099\n",
      "Iteration 9, loss = 1.27308988\n",
      "Iteration 10, loss = 1.25027877\n",
      "Iteration 11, loss = 1.22824449\n",
      "Iteration 12, loss = 1.20705794\n",
      "Iteration 13, loss = 1.18695462\n",
      "Iteration 14, loss = 1.16791924\n",
      "Iteration 15, loss = 1.14969120\n",
      "Iteration 16, loss = 1.13215846\n",
      "Iteration 17, loss = 1.11524287\n",
      "Iteration 18, loss = 1.09889439\n",
      "Iteration 19, loss = 1.08301134\n",
      "Iteration 20, loss = 1.06758345\n",
      "Iteration 21, loss = 1.05273809\n",
      "Iteration 22, loss = 1.03839828\n",
      "Iteration 23, loss = 1.02448386\n",
      "Iteration 24, loss = 1.01102205\n",
      "Iteration 25, loss = 0.99787970\n",
      "Iteration 26, loss = 0.98505212\n",
      "Iteration 27, loss = 0.97257278\n",
      "Iteration 28, loss = 0.96044402\n",
      "Iteration 29, loss = 0.94860421\n",
      "Iteration 30, loss = 0.93707605\n",
      "Iteration 31, loss = 0.92580660\n",
      "Iteration 32, loss = 0.91479272\n",
      "Iteration 33, loss = 0.90402010\n",
      "Iteration 34, loss = 0.89355373\n",
      "Iteration 35, loss = 0.88338896\n",
      "Iteration 36, loss = 0.87351350\n",
      "Iteration 37, loss = 0.86400795\n",
      "Iteration 38, loss = 0.85482124\n",
      "Iteration 39, loss = 0.84602083\n",
      "Iteration 40, loss = 0.83749757\n",
      "Iteration 41, loss = 0.82926818\n",
      "Iteration 42, loss = 0.82122401\n",
      "Iteration 43, loss = 0.81338129\n",
      "Iteration 44, loss = 0.80574439\n",
      "Iteration 45, loss = 0.79831726\n",
      "Iteration 46, loss = 0.79106192\n",
      "Iteration 47, loss = 0.78395270\n",
      "Iteration 48, loss = 0.77698523\n",
      "Iteration 49, loss = 0.77015433\n",
      "Iteration 50, loss = 0.76343826\n",
      "Iteration 51, loss = 0.75682193\n",
      "Iteration 52, loss = 0.75031441\n",
      "Iteration 53, loss = 0.74390690\n",
      "Iteration 54, loss = 0.73759176\n",
      "Iteration 55, loss = 0.73133584\n",
      "Iteration 56, loss = 0.72513261\n",
      "Iteration 57, loss = 0.71898659\n",
      "Iteration 58, loss = 0.71286586\n",
      "Iteration 59, loss = 0.70674674\n",
      "Iteration 60, loss = 0.70062448\n",
      "Iteration 61, loss = 0.69448752\n",
      "Iteration 62, loss = 0.68835783\n",
      "Iteration 63, loss = 0.68221830\n",
      "Iteration 64, loss = 0.67607921\n",
      "Iteration 65, loss = 0.67000337\n",
      "Iteration 66, loss = 0.66397680\n",
      "Iteration 67, loss = 0.65802378\n",
      "Iteration 68, loss = 0.65213357\n",
      "Iteration 69, loss = 0.64630491\n",
      "Iteration 70, loss = 0.64052892\n",
      "Iteration 71, loss = 0.63483577\n",
      "Iteration 72, loss = 0.62921175\n",
      "Iteration 73, loss = 0.62368768\n",
      "Iteration 74, loss = 0.61825399\n",
      "Iteration 75, loss = 0.61292028\n",
      "Iteration 76, loss = 0.60771728\n",
      "Iteration 77, loss = 0.60270860\n",
      "Iteration 78, loss = 0.59785327\n",
      "Iteration 79, loss = 0.59313472\n",
      "Iteration 80, loss = 0.58854443\n",
      "Iteration 81, loss = 0.58404863\n",
      "Iteration 82, loss = 0.57963280\n",
      "Iteration 83, loss = 0.57525391\n",
      "Iteration 84, loss = 0.57094341\n",
      "Iteration 85, loss = 0.56669964\n",
      "Iteration 86, loss = 0.56251259\n",
      "Iteration 87, loss = 0.55837329\n",
      "Iteration 88, loss = 0.55427766\n",
      "Iteration 89, loss = 0.55023127\n",
      "Iteration 90, loss = 0.54624442\n",
      "Iteration 91, loss = 0.54231563\n",
      "Iteration 92, loss = 0.53844719\n",
      "Iteration 93, loss = 0.53463412\n",
      "Iteration 94, loss = 0.53087071\n",
      "Iteration 95, loss = 0.52713868\n",
      "Iteration 96, loss = 0.52341503\n",
      "Iteration 97, loss = 0.51969532\n",
      "Iteration 98, loss = 0.51598508\n",
      "Iteration 99, loss = 0.51230417\n",
      "Iteration 100, loss = 0.50871444\n",
      "Iteration 101, loss = 0.50522235\n",
      "Iteration 102, loss = 0.50178322\n",
      "Iteration 103, loss = 0.49837506\n",
      "Iteration 104, loss = 0.49499590\n",
      "Iteration 105, loss = 0.49164715\n",
      "Iteration 106, loss = 0.48833143\n",
      "Iteration 107, loss = 0.48504539\n",
      "Iteration 108, loss = 0.48178988\n",
      "Iteration 109, loss = 0.47856407\n",
      "Iteration 110, loss = 0.47536649\n",
      "Iteration 111, loss = 0.47219340\n",
      "Iteration 112, loss = 0.46904547\n",
      "Iteration 113, loss = 0.46592113\n",
      "Iteration 114, loss = 0.46281570\n",
      "Iteration 115, loss = 0.45972129\n",
      "Iteration 116, loss = 0.45663697\n",
      "Iteration 117, loss = 0.45357159\n",
      "Iteration 118, loss = 0.45051542\n",
      "Iteration 119, loss = 0.44744782\n",
      "Iteration 120, loss = 0.44435978\n",
      "Iteration 121, loss = 0.44125838\n",
      "Iteration 122, loss = 0.43810107\n",
      "Iteration 123, loss = 0.43494940\n",
      "Iteration 124, loss = 0.43178621\n",
      "Iteration 125, loss = 0.42855243\n",
      "Iteration 126, loss = 0.42531792\n",
      "Iteration 127, loss = 0.42212317\n",
      "Iteration 128, loss = 0.41893802\n",
      "Iteration 129, loss = 0.41585362\n",
      "Iteration 130, loss = 0.41285324\n",
      "Iteration 131, loss = 0.41007267\n",
      "Iteration 132, loss = 0.40746026\n",
      "Iteration 133, loss = 0.40495871\n",
      "Iteration 134, loss = 0.40250049\n",
      "Iteration 135, loss = 0.40007779\n",
      "Iteration 136, loss = 0.39768742\n",
      "Iteration 137, loss = 0.39530276\n",
      "Iteration 138, loss = 0.39290105\n",
      "Iteration 139, loss = 0.39050619\n",
      "Iteration 140, loss = 0.38811349\n",
      "Iteration 141, loss = 0.38571720\n",
      "Iteration 142, loss = 0.38332866\n",
      "Iteration 143, loss = 0.38094816\n",
      "Iteration 144, loss = 0.37857158\n",
      "Iteration 145, loss = 0.37621400\n",
      "Iteration 146, loss = 0.37386393\n",
      "Iteration 147, loss = 0.37152357\n",
      "Iteration 148, loss = 0.36919455\n",
      "Iteration 149, loss = 0.36687317\n",
      "Iteration 150, loss = 0.36457244\n",
      "Iteration 151, loss = 0.36230138\n",
      "Iteration 152, loss = 0.36004806\n",
      "Iteration 153, loss = 0.35780991\n",
      "Iteration 154, loss = 0.35559396\n",
      "Iteration 155, loss = 0.35340240\n",
      "Iteration 156, loss = 0.35122417\n",
      "Iteration 157, loss = 0.34906200\n",
      "Iteration 158, loss = 0.34692624\n",
      "Iteration 159, loss = 0.34481660\n",
      "Iteration 160, loss = 0.34273050\n",
      "Iteration 161, loss = 0.34066047\n",
      "Iteration 162, loss = 0.33859718\n",
      "Iteration 163, loss = 0.33655338\n",
      "Iteration 164, loss = 0.33452274\n",
      "Iteration 165, loss = 0.33249764\n",
      "Iteration 166, loss = 0.33048340\n",
      "Iteration 167, loss = 0.32848011\n",
      "Iteration 168, loss = 0.32648677\n",
      "Iteration 169, loss = 0.32450781\n",
      "Iteration 170, loss = 0.32254660\n",
      "Iteration 171, loss = 0.32059971\n",
      "Iteration 172, loss = 0.31866419\n",
      "Iteration 173, loss = 0.31673830\n",
      "Iteration 174, loss = 0.31482505\n",
      "Iteration 175, loss = 0.31292572\n",
      "Iteration 176, loss = 0.31103328\n",
      "Iteration 177, loss = 0.30915183\n",
      "Iteration 178, loss = 0.30727917\n",
      "Iteration 179, loss = 0.30541699\n",
      "Iteration 180, loss = 0.30356468\n",
      "Iteration 181, loss = 0.30172129\n",
      "Iteration 182, loss = 0.29988731\n",
      "Iteration 183, loss = 0.29806471\n",
      "Iteration 184, loss = 0.29625533\n",
      "Iteration 185, loss = 0.29445612\n",
      "Iteration 186, loss = 0.29267128\n",
      "Iteration 187, loss = 0.29089972\n",
      "Iteration 188, loss = 0.28914046\n",
      "Iteration 189, loss = 0.28739427\n",
      "Iteration 190, loss = 0.28566294\n",
      "Iteration 191, loss = 0.28394583\n",
      "Iteration 192, loss = 0.28223899\n",
      "Iteration 193, loss = 0.28053978\n",
      "Iteration 194, loss = 0.27885009\n",
      "Iteration 195, loss = 0.27717008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 196, loss = 0.27549989\n",
      "Iteration 197, loss = 0.27384098\n",
      "Iteration 198, loss = 0.27219556\n",
      "Iteration 199, loss = 0.27056202\n",
      "Iteration 200, loss = 0.26893976\n",
      "Iteration 201, loss = 0.26732822\n",
      "Iteration 202, loss = 0.26572778\n",
      "Iteration 203, loss = 0.26413882\n",
      "Iteration 204, loss = 0.26256270\n",
      "Iteration 205, loss = 0.26099718\n",
      "Iteration 206, loss = 0.25944246\n",
      "Iteration 207, loss = 0.25789881\n",
      "Iteration 208, loss = 0.25636581\n",
      "Iteration 209, loss = 0.25484392\n",
      "Iteration 210, loss = 0.25333265\n",
      "Iteration 211, loss = 0.25183209\n",
      "Iteration 212, loss = 0.25034251\n",
      "Iteration 213, loss = 0.24886336\n",
      "Iteration 214, loss = 0.24739560\n",
      "Iteration 215, loss = 0.24593821\n",
      "Iteration 216, loss = 0.24449109\n",
      "Iteration 217, loss = 0.24305442\n",
      "Iteration 218, loss = 0.24162899\n",
      "Iteration 219, loss = 0.24021442\n",
      "Iteration 220, loss = 0.23881048\n",
      "Iteration 221, loss = 0.23741699\n",
      "Iteration 222, loss = 0.23603441\n",
      "Iteration 223, loss = 0.23466242\n",
      "Iteration 224, loss = 0.23330026\n",
      "Iteration 225, loss = 0.23194825\n",
      "Iteration 226, loss = 0.23060641\n",
      "Iteration 227, loss = 0.22927504\n",
      "Iteration 228, loss = 0.22795349\n",
      "Iteration 229, loss = 0.22664194\n",
      "Iteration 230, loss = 0.22534103\n",
      "Iteration 231, loss = 0.22404994\n",
      "Iteration 232, loss = 0.22276855\n",
      "Iteration 233, loss = 0.22149700\n",
      "Iteration 234, loss = 0.22023533\n",
      "Iteration 235, loss = 0.21898334\n",
      "Iteration 236, loss = 0.21774119\n",
      "Iteration 237, loss = 0.21650825\n",
      "Iteration 238, loss = 0.21528503\n",
      "Iteration 239, loss = 0.21407185\n",
      "Iteration 240, loss = 0.21286836\n",
      "Iteration 241, loss = 0.21167439\n",
      "Iteration 242, loss = 0.21048990\n",
      "Iteration 243, loss = 0.20931497\n",
      "Iteration 244, loss = 0.20814925\n",
      "Iteration 245, loss = 0.20699346\n",
      "Iteration 246, loss = 0.20584663\n",
      "Iteration 247, loss = 0.20470871\n",
      "Iteration 248, loss = 0.20357963\n",
      "Iteration 249, loss = 0.20245939\n",
      "Iteration 250, loss = 0.20134817\n",
      "Iteration 251, loss = 0.20024574\n",
      "Iteration 252, loss = 0.19915282\n",
      "Iteration 253, loss = 0.19806831\n",
      "Iteration 254, loss = 0.19699236\n",
      "Iteration 255, loss = 0.19592525\n",
      "Iteration 256, loss = 0.19486669\n",
      "Iteration 257, loss = 0.19381659\n",
      "Iteration 258, loss = 0.19277525\n",
      "Iteration 259, loss = 0.19174223\n",
      "Iteration 260, loss = 0.19071743\n",
      "Iteration 261, loss = 0.18970120\n",
      "Iteration 262, loss = 0.18869212\n",
      "Iteration 263, loss = 0.18769133\n",
      "Iteration 264, loss = 0.18669877\n",
      "Iteration 265, loss = 0.18571418\n",
      "Iteration 266, loss = 0.18473505\n",
      "Iteration 267, loss = 0.18376246\n",
      "Iteration 268, loss = 0.18279552\n",
      "Iteration 269, loss = 0.18183513\n",
      "Iteration 270, loss = 0.18088273\n",
      "Iteration 271, loss = 0.17993504\n",
      "Iteration 272, loss = 0.17899282\n",
      "Iteration 273, loss = 0.17806036\n",
      "Iteration 274, loss = 0.17713662\n",
      "Iteration 275, loss = 0.17621810\n",
      "Iteration 276, loss = 0.17530650\n",
      "Iteration 277, loss = 0.17440215\n",
      "Iteration 278, loss = 0.17350192\n",
      "Iteration 279, loss = 0.17259534\n",
      "Iteration 280, loss = 0.17168740\n",
      "Iteration 281, loss = 0.17077963\n",
      "Iteration 282, loss = 0.16987105\n",
      "Iteration 283, loss = 0.16895376\n",
      "Iteration 284, loss = 0.16803749\n",
      "Iteration 285, loss = 0.16711685\n",
      "Iteration 286, loss = 0.16619905\n",
      "Iteration 287, loss = 0.16528109\n",
      "Iteration 288, loss = 0.16436066\n",
      "Iteration 289, loss = 0.16343315\n",
      "Iteration 290, loss = 0.16251354\n",
      "Iteration 291, loss = 0.16160676\n",
      "Iteration 292, loss = 0.16069881\n",
      "Iteration 293, loss = 0.15977971\n",
      "Iteration 294, loss = 0.15884026\n",
      "Iteration 295, loss = 0.15790715\n",
      "Iteration 296, loss = 0.15696382\n",
      "Iteration 297, loss = 0.15604064\n",
      "Iteration 298, loss = 0.15516386\n",
      "Iteration 299, loss = 0.15432652\n",
      "Iteration 300, loss = 0.15357896\n",
      "Iteration 301, loss = 0.15286591\n",
      "Iteration 302, loss = 0.15216115\n",
      "Iteration 303, loss = 0.15146150\n",
      "Iteration 304, loss = 0.15076847\n",
      "Iteration 305, loss = 0.15007557\n",
      "Iteration 306, loss = 0.14938143\n",
      "Iteration 307, loss = 0.14868847\n",
      "Iteration 308, loss = 0.14799535\n",
      "Iteration 309, loss = 0.14730234\n",
      "Iteration 310, loss = 0.14660953\n",
      "Iteration 311, loss = 0.14591745\n",
      "Iteration 312, loss = 0.14522656\n",
      "Iteration 313, loss = 0.14453683\n",
      "Iteration 314, loss = 0.14384951\n",
      "Iteration 315, loss = 0.14316428\n",
      "Iteration 316, loss = 0.14248229\n",
      "Iteration 317, loss = 0.14180372\n",
      "Iteration 318, loss = 0.14112861\n",
      "Iteration 319, loss = 0.14045672\n",
      "Iteration 320, loss = 0.13978862\n",
      "Iteration 321, loss = 0.13912474\n",
      "Iteration 322, loss = 0.13846608\n",
      "Iteration 323, loss = 0.13781622\n",
      "Iteration 324, loss = 0.13717049\n",
      "Iteration 325, loss = 0.13652918\n",
      "Iteration 326, loss = 0.13589425\n",
      "Iteration 327, loss = 0.13526543\n",
      "Iteration 328, loss = 0.13464143\n",
      "Iteration 329, loss = 0.13402287\n",
      "Iteration 330, loss = 0.13341070\n",
      "Iteration 331, loss = 0.13280338\n",
      "Iteration 332, loss = 0.13220072\n",
      "Iteration 333, loss = 0.13160278\n",
      "Iteration 334, loss = 0.13101302\n",
      "Iteration 335, loss = 0.13042794\n",
      "Iteration 336, loss = 0.12984734\n",
      "Iteration 337, loss = 0.12927155\n",
      "Iteration 338, loss = 0.12870014\n",
      "Iteration 339, loss = 0.12813253\n",
      "Iteration 340, loss = 0.12756898\n",
      "Iteration 341, loss = 0.12700885\n",
      "Iteration 342, loss = 0.12645299\n",
      "Iteration 343, loss = 0.12590244\n",
      "Iteration 344, loss = 0.12535611\n",
      "Iteration 345, loss = 0.12481470\n",
      "Iteration 346, loss = 0.12427765\n",
      "Iteration 347, loss = 0.12374480\n",
      "Iteration 348, loss = 0.12321828\n",
      "Iteration 349, loss = 0.12269636\n",
      "Iteration 350, loss = 0.12217873\n",
      "Iteration 351, loss = 0.12166631\n",
      "Iteration 352, loss = 0.12115768\n",
      "Iteration 353, loss = 0.12065316\n",
      "Iteration 354, loss = 0.12015282\n",
      "Iteration 355, loss = 0.11965701\n",
      "Iteration 356, loss = 0.11916524\n",
      "Iteration 357, loss = 0.11867759\n",
      "Iteration 358, loss = 0.11819469\n",
      "Iteration 359, loss = 0.11771593\n",
      "Iteration 360, loss = 0.11724127\n",
      "Iteration 361, loss = 0.11677092\n",
      "Iteration 362, loss = 0.11630502\n",
      "Iteration 363, loss = 0.11584296\n",
      "Iteration 364, loss = 0.11538492\n",
      "Iteration 365, loss = 0.11493138\n",
      "Iteration 366, loss = 0.11448206\n",
      "Iteration 367, loss = 0.11403665\n",
      "Iteration 368, loss = 0.11359533\n",
      "Iteration 369, loss = 0.11315859\n",
      "Iteration 370, loss = 0.11272520\n",
      "Iteration 371, loss = 0.11229584\n",
      "Iteration 372, loss = 0.11187059\n",
      "Iteration 373, loss = 0.11144907\n",
      "Iteration 374, loss = 0.11103174\n",
      "Iteration 375, loss = 0.11061794\n",
      "Iteration 376, loss = 0.11020796\n",
      "Iteration 377, loss = 0.10980198\n",
      "Iteration 378, loss = 0.10939954\n",
      "Iteration 379, loss = 0.10900061\n",
      "Iteration 380, loss = 0.10860555\n",
      "Iteration 381, loss = 0.10821421\n",
      "Iteration 382, loss = 0.10782607\n",
      "Iteration 383, loss = 0.10744187\n",
      "Iteration 384, loss = 0.10706119\n",
      "Iteration 385, loss = 0.10668371\n",
      "Iteration 386, loss = 0.10630965\n",
      "Iteration 387, loss = 0.10593903\n",
      "Iteration 388, loss = 0.10557224\n",
      "Iteration 389, loss = 0.10520813\n",
      "Iteration 390, loss = 0.10484758\n",
      "Iteration 391, loss = 0.10449014\n",
      "Iteration 392, loss = 0.10413567\n",
      "Iteration 393, loss = 0.10378404\n",
      "Iteration 394, loss = 0.10343515\n",
      "Iteration 395, loss = 0.10308854\n",
      "Iteration 396, loss = 0.10274426\n",
      "Iteration 397, loss = 0.10240241\n",
      "Iteration 398, loss = 0.10206239\n",
      "Iteration 399, loss = 0.10172408\n",
      "Iteration 400, loss = 0.10138736\n",
      "Iteration 401, loss = 0.10105202\n",
      "Iteration 402, loss = 0.10071805\n",
      "Iteration 403, loss = 0.10038555\n",
      "Iteration 404, loss = 0.10005449\n",
      "Iteration 405, loss = 0.09972482\n",
      "Iteration 406, loss = 0.09939642\n",
      "Iteration 407, loss = 0.09906923\n",
      "Iteration 408, loss = 0.09874342\n",
      "Iteration 409, loss = 0.09841891\n",
      "Iteration 410, loss = 0.09809567\n",
      "Iteration 411, loss = 0.09777426\n",
      "Iteration 412, loss = 0.09745345\n",
      "Iteration 413, loss = 0.09713452\n",
      "Iteration 414, loss = 0.09681691\n",
      "Iteration 415, loss = 0.09650081\n",
      "Iteration 416, loss = 0.09618623\n",
      "Iteration 417, loss = 0.09587321\n",
      "Iteration 418, loss = 0.09556160\n",
      "Iteration 419, loss = 0.09525179\n",
      "Iteration 420, loss = 0.09494343\n",
      "Iteration 421, loss = 0.09463667\n",
      "Iteration 422, loss = 0.09433152\n",
      "Iteration 423, loss = 0.09402786\n",
      "Iteration 424, loss = 0.09372590\n",
      "Iteration 425, loss = 0.09342514\n",
      "Iteration 426, loss = 0.09312549\n",
      "Iteration 427, loss = 0.09282698\n",
      "Iteration 428, loss = 0.09252978\n",
      "Iteration 429, loss = 0.09223373\n",
      "Iteration 430, loss = 0.09193887\n",
      "Iteration 431, loss = 0.09164523\n",
      "Iteration 432, loss = 0.09135303\n",
      "Iteration 433, loss = 0.09106226\n",
      "Iteration 434, loss = 0.09077254\n",
      "Iteration 435, loss = 0.09048367\n",
      "Iteration 436, loss = 0.09019577\n",
      "Iteration 437, loss = 0.08990874\n",
      "Iteration 438, loss = 0.08962273\n",
      "Iteration 439, loss = 0.08933760\n",
      "Iteration 440, loss = 0.08905382\n",
      "Iteration 441, loss = 0.08877121\n",
      "Iteration 442, loss = 0.08848945\n",
      "Iteration 443, loss = 0.08820864\n",
      "Iteration 444, loss = 0.08792922\n",
      "Iteration 445, loss = 0.08765098\n",
      "Iteration 446, loss = 0.08737397\n",
      "Iteration 447, loss = 0.08709825\n",
      "Iteration 448, loss = 0.08682390\n",
      "Iteration 449, loss = 0.08655094\n",
      "Iteration 450, loss = 0.08627943\n",
      "Iteration 451, loss = 0.08600936\n",
      "Iteration 452, loss = 0.08574085\n",
      "Iteration 453, loss = 0.08547391\n",
      "Iteration 454, loss = 0.08520872\n",
      "Iteration 455, loss = 0.08494516\n",
      "Iteration 456, loss = 0.08468328\n",
      "Iteration 457, loss = 0.08442316\n",
      "Iteration 458, loss = 0.08416474\n",
      "Iteration 459, loss = 0.08390821\n",
      "Iteration 460, loss = 0.08365350\n",
      "Iteration 461, loss = 0.08340068\n",
      "Iteration 462, loss = 0.08314977\n",
      "Iteration 463, loss = 0.08290083\n",
      "Iteration 464, loss = 0.08265386\n",
      "Iteration 465, loss = 0.08240888\n",
      "Iteration 466, loss = 0.08216592\n",
      "Iteration 467, loss = 0.08192511\n",
      "Iteration 468, loss = 0.08168644\n",
      "Iteration 469, loss = 0.08144982\n",
      "Iteration 470, loss = 0.08121528\n",
      "Iteration 471, loss = 0.08098277\n",
      "Iteration 472, loss = 0.08075231\n",
      "Iteration 473, loss = 0.08052396\n",
      "Iteration 474, loss = 0.08029767\n",
      "Iteration 475, loss = 0.08007350\n",
      "Iteration 476, loss = 0.07985160\n",
      "Iteration 477, loss = 0.07963183\n",
      "Iteration 478, loss = 0.07941414\n",
      "Iteration 479, loss = 0.07919855\n",
      "Iteration 480, loss = 0.07898503\n",
      "Iteration 481, loss = 0.07877358\n",
      "Iteration 482, loss = 0.07856419\n",
      "Iteration 483, loss = 0.07835695\n",
      "Iteration 484, loss = 0.07815178\n",
      "Iteration 485, loss = 0.07794866\n",
      "Iteration 486, loss = 0.07774758\n",
      "Iteration 487, loss = 0.07754854\n",
      "Iteration 488, loss = 0.07735162\n",
      "Iteration 489, loss = 0.07715671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 490, loss = 0.07696378\n",
      "Iteration 491, loss = 0.07677290\n",
      "Iteration 492, loss = 0.07658402\n",
      "Iteration 493, loss = 0.07639710\n",
      "Iteration 494, loss = 0.07621210\n",
      "Iteration 495, loss = 0.07602908\n",
      "Iteration 496, loss = 0.07584800\n",
      "Iteration 497, loss = 0.07566882\n",
      "Iteration 498, loss = 0.07549151\n",
      "Iteration 499, loss = 0.07531608\n",
      "Iteration 500, loss = 0.07514249\n",
      "Iteration 501, loss = 0.07497079\n",
      "Iteration 502, loss = 0.07480096\n",
      "Iteration 503, loss = 0.07463292\n",
      "Iteration 504, loss = 0.07446665\n",
      "Iteration 505, loss = 0.07430217\n",
      "Iteration 506, loss = 0.07413946\n",
      "Iteration 507, loss = 0.07397853\n",
      "Iteration 508, loss = 0.07381937\n",
      "Iteration 509, loss = 0.07366199\n",
      "Iteration 510, loss = 0.07350640\n",
      "Iteration 511, loss = 0.07335243\n",
      "Iteration 512, loss = 0.07320007\n",
      "Iteration 513, loss = 0.07304920\n",
      "Iteration 514, loss = 0.07290014\n",
      "Iteration 515, loss = 0.07275269\n",
      "Iteration 516, loss = 0.07260690\n",
      "Iteration 517, loss = 0.07246269\n",
      "Iteration 518, loss = 0.07231998\n",
      "Iteration 519, loss = 0.07217885\n",
      "Iteration 520, loss = 0.07203918\n",
      "Iteration 521, loss = 0.07190103\n",
      "Iteration 522, loss = 0.07176442\n",
      "Iteration 523, loss = 0.07162920\n",
      "Iteration 524, loss = 0.07149545\n",
      "Iteration 525, loss = 0.07136316\n",
      "Iteration 526, loss = 0.07123224\n",
      "Iteration 527, loss = 0.07110273\n",
      "Iteration 528, loss = 0.07097458\n",
      "Iteration 529, loss = 0.07084777\n",
      "Iteration 530, loss = 0.07072230\n",
      "Iteration 531, loss = 0.07059819\n",
      "Iteration 532, loss = 0.07047539\n",
      "Iteration 533, loss = 0.07035392\n",
      "Iteration 534, loss = 0.07023374\n",
      "Iteration 535, loss = 0.07011486\n",
      "Iteration 536, loss = 0.06999723\n",
      "Iteration 537, loss = 0.06988080\n",
      "Iteration 538, loss = 0.06976558\n",
      "Iteration 539, loss = 0.06965155\n",
      "Iteration 540, loss = 0.06953870\n",
      "Iteration 541, loss = 0.06942702\n",
      "Iteration 542, loss = 0.06931653\n",
      "Iteration 543, loss = 0.06920715\n",
      "Iteration 544, loss = 0.06909888\n",
      "Iteration 545, loss = 0.06899181\n",
      "Iteration 546, loss = 0.06888571\n",
      "Iteration 547, loss = 0.06878066\n",
      "Iteration 548, loss = 0.06867666\n",
      "Iteration 549, loss = 0.06857377\n",
      "Iteration 550, loss = 0.06847185\n",
      "Iteration 551, loss = 0.06837099\n",
      "Iteration 552, loss = 0.06827112\n",
      "Iteration 553, loss = 0.06817227\n",
      "Iteration 554, loss = 0.06807390\n",
      "Iteration 555, loss = 0.06797531\n",
      "Iteration 556, loss = 0.06787722\n",
      "Iteration 557, loss = 0.06777968\n",
      "Iteration 558, loss = 0.06768273\n",
      "Iteration 559, loss = 0.06758640\n",
      "Iteration 560, loss = 0.06749071\n",
      "Iteration 561, loss = 0.06739579\n",
      "Iteration 562, loss = 0.06730154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.45168950\n",
      "Iteration 3, loss = 1.42335152\n",
      "Iteration 4, loss = 1.39594052\n",
      "Iteration 5, loss = 1.36945525\n",
      "Iteration 6, loss = 1.34380149\n",
      "Iteration 7, loss = 1.31885283\n",
      "Iteration 8, loss = 1.29466654\n",
      "Iteration 9, loss = 1.27125206\n",
      "Iteration 10, loss = 1.24871342\n",
      "Iteration 11, loss = 1.22693336\n",
      "Iteration 12, loss = 1.20603022\n",
      "Iteration 13, loss = 1.18619227\n",
      "Iteration 14, loss = 1.16746238\n",
      "Iteration 15, loss = 1.14948880\n",
      "Iteration 16, loss = 1.13216282\n",
      "Iteration 17, loss = 1.11543218\n",
      "Iteration 18, loss = 1.09924615\n",
      "Iteration 19, loss = 1.08352541\n",
      "Iteration 20, loss = 1.06830879\n",
      "Iteration 21, loss = 1.05369307\n",
      "Iteration 22, loss = 1.03962029\n",
      "Iteration 23, loss = 1.02594869\n",
      "Iteration 24, loss = 1.01268808\n",
      "Iteration 25, loss = 0.99983181\n",
      "Iteration 26, loss = 0.98732277\n",
      "Iteration 27, loss = 0.97507210\n",
      "Iteration 28, loss = 0.96309377\n",
      "Iteration 29, loss = 0.95134013\n",
      "Iteration 30, loss = 0.93987363\n",
      "Iteration 31, loss = 0.92865771\n",
      "Iteration 32, loss = 0.91764115\n",
      "Iteration 33, loss = 0.90685870\n",
      "Iteration 34, loss = 0.89634869\n",
      "Iteration 35, loss = 0.88610453\n",
      "Iteration 36, loss = 0.87609683\n",
      "Iteration 37, loss = 0.86637053\n",
      "Iteration 38, loss = 0.85681569\n",
      "Iteration 39, loss = 0.84750451\n",
      "Iteration 40, loss = 0.83843194\n",
      "Iteration 41, loss = 0.82958076\n",
      "Iteration 42, loss = 0.82086480\n",
      "Iteration 43, loss = 0.81227508\n",
      "Iteration 44, loss = 0.80385458\n",
      "Iteration 45, loss = 0.79564564\n",
      "Iteration 46, loss = 0.78760906\n",
      "Iteration 47, loss = 0.77973089\n",
      "Iteration 48, loss = 0.77197040\n",
      "Iteration 49, loss = 0.76432860\n",
      "Iteration 50, loss = 0.75681042\n",
      "Iteration 51, loss = 0.74942025\n",
      "Iteration 52, loss = 0.74214876\n",
      "Iteration 53, loss = 0.73502238\n",
      "Iteration 54, loss = 0.72801973\n",
      "Iteration 55, loss = 0.72115165\n",
      "Iteration 56, loss = 0.71440587\n",
      "Iteration 57, loss = 0.70779676\n",
      "Iteration 58, loss = 0.70135803\n",
      "Iteration 59, loss = 0.69507749\n",
      "Iteration 60, loss = 0.68898399\n",
      "Iteration 61, loss = 0.68304475\n",
      "Iteration 62, loss = 0.67723474\n",
      "Iteration 63, loss = 0.67156703\n",
      "Iteration 64, loss = 0.66601069\n",
      "Iteration 65, loss = 0.66051157\n",
      "Iteration 66, loss = 0.65508824\n",
      "Iteration 67, loss = 0.64974372\n",
      "Iteration 68, loss = 0.64448911\n",
      "Iteration 69, loss = 0.63930870\n",
      "Iteration 70, loss = 0.63417428\n",
      "Iteration 71, loss = 0.62908935\n",
      "Iteration 72, loss = 0.62406236\n",
      "Iteration 73, loss = 0.61908810\n",
      "Iteration 74, loss = 0.61416724\n",
      "Iteration 75, loss = 0.60929781\n",
      "Iteration 76, loss = 0.60447128\n",
      "Iteration 77, loss = 0.59969171\n",
      "Iteration 78, loss = 0.59497561\n",
      "Iteration 79, loss = 0.59027823\n",
      "Iteration 80, loss = 0.58562180\n",
      "Iteration 81, loss = 0.58097622\n",
      "Iteration 82, loss = 0.57638947\n",
      "Iteration 83, loss = 0.57181431\n",
      "Iteration 84, loss = 0.56719533\n",
      "Iteration 85, loss = 0.56264413\n",
      "Iteration 86, loss = 0.55793168\n",
      "Iteration 87, loss = 0.55318565\n",
      "Iteration 88, loss = 0.54862862\n",
      "Iteration 89, loss = 0.54410857\n",
      "Iteration 90, loss = 0.53967411\n",
      "Iteration 91, loss = 0.53531774\n",
      "Iteration 92, loss = 0.53111761\n",
      "Iteration 93, loss = 0.52708184\n",
      "Iteration 94, loss = 0.52317839\n",
      "Iteration 95, loss = 0.51935989\n",
      "Iteration 96, loss = 0.51559160\n",
      "Iteration 97, loss = 0.51187348\n",
      "Iteration 98, loss = 0.50816224\n",
      "Iteration 99, loss = 0.50448526\n",
      "Iteration 100, loss = 0.50089899\n",
      "Iteration 101, loss = 0.49741384\n",
      "Iteration 102, loss = 0.49397697\n",
      "Iteration 103, loss = 0.49055805\n",
      "Iteration 104, loss = 0.48714666\n",
      "Iteration 105, loss = 0.48374958\n",
      "Iteration 106, loss = 0.48037867\n",
      "Iteration 107, loss = 0.47704252\n",
      "Iteration 108, loss = 0.47374047\n",
      "Iteration 109, loss = 0.47046884\n",
      "Iteration 110, loss = 0.46723358\n",
      "Iteration 111, loss = 0.46402605\n",
      "Iteration 112, loss = 0.46083318\n",
      "Iteration 113, loss = 0.45766329\n",
      "Iteration 114, loss = 0.45451713\n",
      "Iteration 115, loss = 0.45138782\n",
      "Iteration 116, loss = 0.44827542\n",
      "Iteration 117, loss = 0.44517799\n",
      "Iteration 118, loss = 0.44209521\n",
      "Iteration 119, loss = 0.43901024\n",
      "Iteration 120, loss = 0.43589526\n",
      "Iteration 121, loss = 0.43274670\n",
      "Iteration 122, loss = 0.42955296\n",
      "Iteration 123, loss = 0.42632568\n",
      "Iteration 124, loss = 0.42312450\n",
      "Iteration 125, loss = 0.41991482\n",
      "Iteration 126, loss = 0.41667956\n",
      "Iteration 127, loss = 0.41343090\n",
      "Iteration 128, loss = 0.41024515\n",
      "Iteration 129, loss = 0.40713552\n",
      "Iteration 130, loss = 0.40414847\n",
      "Iteration 131, loss = 0.40133330\n",
      "Iteration 132, loss = 0.39870607\n",
      "Iteration 133, loss = 0.39611976\n",
      "Iteration 134, loss = 0.39357468\n",
      "Iteration 135, loss = 0.39109345\n",
      "Iteration 136, loss = 0.38865539\n",
      "Iteration 137, loss = 0.38624573\n",
      "Iteration 138, loss = 0.38386100\n",
      "Iteration 139, loss = 0.38146055\n",
      "Iteration 140, loss = 0.37910260\n",
      "Iteration 141, loss = 0.37674976\n",
      "Iteration 142, loss = 0.37438697\n",
      "Iteration 143, loss = 0.37202516\n",
      "Iteration 144, loss = 0.36969757\n",
      "Iteration 145, loss = 0.36735154\n",
      "Iteration 146, loss = 0.36502108\n",
      "Iteration 147, loss = 0.36272673\n",
      "Iteration 148, loss = 0.36043582\n",
      "Iteration 149, loss = 0.35814645\n",
      "Iteration 150, loss = 0.35588379\n",
      "Iteration 151, loss = 0.35365012\n",
      "Iteration 152, loss = 0.35143121\n",
      "Iteration 153, loss = 0.34923381\n",
      "Iteration 154, loss = 0.34707912\n",
      "Iteration 155, loss = 0.34494161\n",
      "Iteration 156, loss = 0.34281331\n",
      "Iteration 157, loss = 0.34071291\n",
      "Iteration 158, loss = 0.33863853\n",
      "Iteration 159, loss = 0.33657221\n",
      "Iteration 160, loss = 0.33451865\n",
      "Iteration 161, loss = 0.33248806\n",
      "Iteration 162, loss = 0.33046691\n",
      "Iteration 163, loss = 0.32845293\n",
      "Iteration 164, loss = 0.32646001\n",
      "Iteration 165, loss = 0.32448281\n",
      "Iteration 166, loss = 0.32251629\n",
      "Iteration 167, loss = 0.32055826\n",
      "Iteration 168, loss = 0.31861293\n",
      "Iteration 169, loss = 0.31668675\n",
      "Iteration 170, loss = 0.31477148\n",
      "Iteration 171, loss = 0.31286614\n",
      "Iteration 172, loss = 0.31097162\n",
      "Iteration 173, loss = 0.30909501\n",
      "Iteration 174, loss = 0.30723103\n",
      "Iteration 175, loss = 0.30537756\n",
      "Iteration 176, loss = 0.30353456\n",
      "Iteration 177, loss = 0.30170341\n",
      "Iteration 178, loss = 0.29988482\n",
      "Iteration 179, loss = 0.29807663\n",
      "Iteration 180, loss = 0.29627873\n",
      "Iteration 181, loss = 0.29449256\n",
      "Iteration 182, loss = 0.29271779\n",
      "Iteration 183, loss = 0.29095339\n",
      "Iteration 184, loss = 0.28920226\n",
      "Iteration 185, loss = 0.28746039\n",
      "Iteration 186, loss = 0.28572575\n",
      "Iteration 187, loss = 0.28400079\n",
      "Iteration 188, loss = 0.28228449\n",
      "Iteration 189, loss = 0.28057830\n",
      "Iteration 190, loss = 0.27888276\n",
      "Iteration 191, loss = 0.27719931\n",
      "Iteration 192, loss = 0.27552888\n",
      "Iteration 193, loss = 0.27387112\n",
      "Iteration 194, loss = 0.27222866\n",
      "Iteration 195, loss = 0.27060014\n",
      "Iteration 196, loss = 0.26898422\n",
      "Iteration 197, loss = 0.26737993\n",
      "Iteration 198, loss = 0.26578495\n",
      "Iteration 199, loss = 0.26419993\n",
      "Iteration 200, loss = 0.26262269\n",
      "Iteration 201, loss = 0.26105472\n",
      "Iteration 202, loss = 0.25949947\n",
      "Iteration 203, loss = 0.25795577\n",
      "Iteration 204, loss = 0.25642350\n",
      "Iteration 205, loss = 0.25490357\n",
      "Iteration 206, loss = 0.25339619\n",
      "Iteration 207, loss = 0.25189911\n",
      "Iteration 208, loss = 0.25041257\n",
      "Iteration 209, loss = 0.24893641\n",
      "Iteration 210, loss = 0.24747039\n",
      "Iteration 211, loss = 0.24601473\n",
      "Iteration 212, loss = 0.24456919\n",
      "Iteration 213, loss = 0.24313370\n",
      "Iteration 214, loss = 0.24170898\n",
      "Iteration 215, loss = 0.24029487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 216, loss = 0.23889105\n",
      "Iteration 217, loss = 0.23750221\n",
      "Iteration 218, loss = 0.23612344\n",
      "Iteration 219, loss = 0.23475372\n",
      "Iteration 220, loss = 0.23339274\n",
      "Iteration 221, loss = 0.23204231\n",
      "Iteration 222, loss = 0.23070440\n",
      "Iteration 223, loss = 0.22937617\n",
      "Iteration 224, loss = 0.22805811\n",
      "Iteration 225, loss = 0.22675002\n",
      "Iteration 226, loss = 0.22545169\n",
      "Iteration 227, loss = 0.22416351\n",
      "Iteration 228, loss = 0.22288511\n",
      "Iteration 229, loss = 0.22161632\n",
      "Iteration 230, loss = 0.22035744\n",
      "Iteration 231, loss = 0.21910959\n",
      "Iteration 232, loss = 0.21787093\n",
      "Iteration 233, loss = 0.21664131\n",
      "Iteration 234, loss = 0.21542121\n",
      "Iteration 235, loss = 0.21421073\n",
      "Iteration 236, loss = 0.21301022\n",
      "Iteration 237, loss = 0.21181887\n",
      "Iteration 238, loss = 0.21063718\n",
      "Iteration 239, loss = 0.20946540\n",
      "Iteration 240, loss = 0.20830294\n",
      "Iteration 241, loss = 0.20714969\n",
      "Iteration 242, loss = 0.20600542\n",
      "Iteration 243, loss = 0.20487025\n",
      "Iteration 244, loss = 0.20374393\n",
      "Iteration 245, loss = 0.20262648\n",
      "Iteration 246, loss = 0.20151811\n",
      "Iteration 247, loss = 0.20041847\n",
      "Iteration 248, loss = 0.19932757\n",
      "Iteration 249, loss = 0.19824547\n",
      "Iteration 250, loss = 0.19717320\n",
      "Iteration 251, loss = 0.19610914\n",
      "Iteration 252, loss = 0.19505335\n",
      "Iteration 253, loss = 0.19400620\n",
      "Iteration 254, loss = 0.19296775\n",
      "Iteration 255, loss = 0.19193762\n",
      "Iteration 256, loss = 0.19091573\n",
      "Iteration 257, loss = 0.18990212\n",
      "Iteration 258, loss = 0.18889666\n",
      "Iteration 259, loss = 0.18789950\n",
      "Iteration 260, loss = 0.18691047\n",
      "Iteration 261, loss = 0.18592890\n",
      "Iteration 262, loss = 0.18495343\n",
      "Iteration 263, loss = 0.18398560\n",
      "Iteration 264, loss = 0.18302375\n",
      "Iteration 265, loss = 0.18206806\n",
      "Iteration 266, loss = 0.18111976\n",
      "Iteration 267, loss = 0.18017817\n",
      "Iteration 268, loss = 0.17924377\n",
      "Iteration 269, loss = 0.17831607\n",
      "Iteration 270, loss = 0.17739361\n",
      "Iteration 271, loss = 0.17647775\n",
      "Iteration 272, loss = 0.17556733\n",
      "Iteration 273, loss = 0.17466325\n",
      "Iteration 274, loss = 0.17376001\n",
      "Iteration 275, loss = 0.17285899\n",
      "Iteration 276, loss = 0.17195469\n",
      "Iteration 277, loss = 0.17104927\n",
      "Iteration 278, loss = 0.17014500\n",
      "Iteration 279, loss = 0.16923140\n",
      "Iteration 280, loss = 0.16831053\n",
      "Iteration 281, loss = 0.16738066\n",
      "Iteration 282, loss = 0.16644148\n",
      "Iteration 283, loss = 0.16550423\n",
      "Iteration 284, loss = 0.16456649\n",
      "Iteration 285, loss = 0.16361474\n",
      "Iteration 286, loss = 0.16265479\n",
      "Iteration 287, loss = 0.16169349\n",
      "Iteration 288, loss = 0.16070403\n",
      "Iteration 289, loss = 0.15970475\n",
      "Iteration 290, loss = 0.15870865\n",
      "Iteration 291, loss = 0.15774015\n",
      "Iteration 292, loss = 0.15682294\n",
      "Iteration 293, loss = 0.15597067\n",
      "Iteration 294, loss = 0.15517701\n",
      "Iteration 295, loss = 0.15446679\n",
      "Iteration 296, loss = 0.15377227\n",
      "Iteration 297, loss = 0.15308727\n",
      "Iteration 298, loss = 0.15240657\n",
      "Iteration 299, loss = 0.15172647\n",
      "Iteration 300, loss = 0.15104164\n",
      "Iteration 301, loss = 0.15035524\n",
      "Iteration 302, loss = 0.14966945\n",
      "Iteration 303, loss = 0.14898270\n",
      "Iteration 304, loss = 0.14829465\n",
      "Iteration 305, loss = 0.14760643\n",
      "Iteration 306, loss = 0.14691860\n",
      "Iteration 307, loss = 0.14623142\n",
      "Iteration 308, loss = 0.14554504\n",
      "Iteration 309, loss = 0.14486031\n",
      "Iteration 310, loss = 0.14417795\n",
      "Iteration 311, loss = 0.14349711\n",
      "Iteration 312, loss = 0.14281878\n",
      "Iteration 313, loss = 0.14214313\n",
      "Iteration 314, loss = 0.14146975\n",
      "Iteration 315, loss = 0.14079932\n",
      "Iteration 316, loss = 0.14013321\n",
      "Iteration 317, loss = 0.13947217\n",
      "Iteration 318, loss = 0.13881665\n",
      "Iteration 319, loss = 0.13816759\n",
      "Iteration 320, loss = 0.13752340\n",
      "Iteration 321, loss = 0.13688490\n",
      "Iteration 322, loss = 0.13625263\n",
      "Iteration 323, loss = 0.13562456\n",
      "Iteration 324, loss = 0.13500074\n",
      "Iteration 325, loss = 0.13438306\n",
      "Iteration 326, loss = 0.13377616\n",
      "Iteration 327, loss = 0.13317691\n",
      "Iteration 328, loss = 0.13258063\n",
      "Iteration 329, loss = 0.13198544\n",
      "Iteration 330, loss = 0.13139185\n",
      "Iteration 331, loss = 0.13080001\n",
      "Iteration 332, loss = 0.13021024\n",
      "Iteration 333, loss = 0.12962285\n",
      "Iteration 334, loss = 0.12903923\n",
      "Iteration 335, loss = 0.12846190\n",
      "Iteration 336, loss = 0.12789199\n",
      "Iteration 337, loss = 0.12732756\n",
      "Iteration 338, loss = 0.12676741\n",
      "Iteration 339, loss = 0.12621162\n",
      "Iteration 340, loss = 0.12565996\n",
      "Iteration 341, loss = 0.12511197\n",
      "Iteration 342, loss = 0.12456748\n",
      "Iteration 343, loss = 0.12402638\n",
      "Iteration 344, loss = 0.12348926\n",
      "Iteration 345, loss = 0.12295616\n",
      "Iteration 346, loss = 0.12242660\n",
      "Iteration 347, loss = 0.12190085\n",
      "Iteration 348, loss = 0.12138008\n",
      "Iteration 349, loss = 0.12086352\n",
      "Iteration 350, loss = 0.12035083\n",
      "Iteration 351, loss = 0.11984212\n",
      "Iteration 352, loss = 0.11933742\n",
      "Iteration 353, loss = 0.11883663\n",
      "Iteration 354, loss = 0.11834043\n",
      "Iteration 355, loss = 0.11784876\n",
      "Iteration 356, loss = 0.11736074\n",
      "Iteration 357, loss = 0.11687635\n",
      "Iteration 358, loss = 0.11639556\n",
      "Iteration 359, loss = 0.11591845\n",
      "Iteration 360, loss = 0.11544499\n",
      "Iteration 361, loss = 0.11497547\n",
      "Iteration 362, loss = 0.11451049\n",
      "Iteration 363, loss = 0.11404932\n",
      "Iteration 364, loss = 0.11359183\n",
      "Iteration 365, loss = 0.11313797\n",
      "Iteration 366, loss = 0.11268797\n",
      "Iteration 367, loss = 0.11224155\n",
      "Iteration 368, loss = 0.11179867\n",
      "Iteration 369, loss = 0.11135954\n",
      "Iteration 370, loss = 0.11092402\n",
      "Iteration 371, loss = 0.11049185\n",
      "Iteration 372, loss = 0.11006334\n",
      "Iteration 373, loss = 0.10963836\n",
      "Iteration 374, loss = 0.10921699\n",
      "Iteration 375, loss = 0.10879942\n",
      "Iteration 376, loss = 0.10838522\n",
      "Iteration 377, loss = 0.10797443\n",
      "Iteration 378, loss = 0.10756691\n",
      "Iteration 379, loss = 0.10716270\n",
      "Iteration 380, loss = 0.10676192\n",
      "Iteration 381, loss = 0.10636451\n",
      "Iteration 382, loss = 0.10597047\n",
      "Iteration 383, loss = 0.10557972\n",
      "Iteration 384, loss = 0.10519215\n",
      "Iteration 385, loss = 0.10480779\n",
      "Iteration 386, loss = 0.10442659\n",
      "Iteration 387, loss = 0.10404834\n",
      "Iteration 388, loss = 0.10367334\n",
      "Iteration 389, loss = 0.10330144\n",
      "Iteration 390, loss = 0.10293237\n",
      "Iteration 391, loss = 0.10256647\n",
      "Iteration 392, loss = 0.10220329\n",
      "Iteration 393, loss = 0.10184272\n",
      "Iteration 394, loss = 0.10148472\n",
      "Iteration 395, loss = 0.10112899\n",
      "Iteration 396, loss = 0.10077547\n",
      "Iteration 397, loss = 0.10042381\n",
      "Iteration 398, loss = 0.10007407\n",
      "Iteration 399, loss = 0.09972587\n",
      "Iteration 400, loss = 0.09937909\n",
      "Iteration 401, loss = 0.09903365\n",
      "Iteration 402, loss = 0.09868947\n",
      "Iteration 403, loss = 0.09834663\n",
      "Iteration 404, loss = 0.09800507\n",
      "Iteration 405, loss = 0.09766477\n",
      "Iteration 406, loss = 0.09732551\n",
      "Iteration 407, loss = 0.09698730\n",
      "Iteration 408, loss = 0.09665009\n",
      "Iteration 409, loss = 0.09631406\n",
      "Iteration 410, loss = 0.09597898\n",
      "Iteration 411, loss = 0.09564519\n",
      "Iteration 412, loss = 0.09531255\n",
      "Iteration 413, loss = 0.09498111\n",
      "Iteration 414, loss = 0.09465085\n",
      "Iteration 415, loss = 0.09432183\n",
      "Iteration 416, loss = 0.09399408\n",
      "Iteration 417, loss = 0.09366761\n",
      "Iteration 418, loss = 0.09334246\n",
      "Iteration 419, loss = 0.09301884\n",
      "Iteration 420, loss = 0.09269644\n",
      "Iteration 421, loss = 0.09237560\n",
      "Iteration 422, loss = 0.09205609\n",
      "Iteration 423, loss = 0.09173768\n",
      "Iteration 424, loss = 0.09142089\n",
      "Iteration 425, loss = 0.09110539\n",
      "Iteration 426, loss = 0.09079093\n",
      "Iteration 427, loss = 0.09047758\n",
      "Iteration 428, loss = 0.09016538\n",
      "Iteration 429, loss = 0.08985436\n",
      "Iteration 430, loss = 0.08954425\n",
      "Iteration 431, loss = 0.08923531\n",
      "Iteration 432, loss = 0.08892728\n",
      "Iteration 433, loss = 0.08862007\n",
      "Iteration 434, loss = 0.08831364\n",
      "Iteration 435, loss = 0.08800798\n",
      "Iteration 436, loss = 0.08770305\n",
      "Iteration 437, loss = 0.08739885\n",
      "Iteration 438, loss = 0.08709560\n",
      "Iteration 439, loss = 0.08679293\n",
      "Iteration 440, loss = 0.08649106\n",
      "Iteration 441, loss = 0.08619004\n",
      "Iteration 442, loss = 0.08588981\n",
      "Iteration 443, loss = 0.08559046\n",
      "Iteration 444, loss = 0.08529183\n",
      "Iteration 445, loss = 0.08499420\n",
      "Iteration 446, loss = 0.08469758\n",
      "Iteration 447, loss = 0.08440204\n",
      "Iteration 448, loss = 0.08410731\n",
      "Iteration 449, loss = 0.08381383\n",
      "Iteration 450, loss = 0.08352140\n",
      "Iteration 451, loss = 0.08323005\n",
      "Iteration 452, loss = 0.08294000\n",
      "Iteration 453, loss = 0.08265125\n",
      "Iteration 454, loss = 0.08236384\n",
      "Iteration 455, loss = 0.08207781\n",
      "Iteration 456, loss = 0.08179330\n",
      "Iteration 457, loss = 0.08151027\n",
      "Iteration 458, loss = 0.08122874\n",
      "Iteration 459, loss = 0.08094871\n",
      "Iteration 460, loss = 0.08067022\n",
      "Iteration 461, loss = 0.08039333\n",
      "Iteration 462, loss = 0.08011812\n",
      "Iteration 463, loss = 0.07984460\n",
      "Iteration 464, loss = 0.07957274\n",
      "Iteration 465, loss = 0.07930259\n",
      "Iteration 466, loss = 0.07903418\n",
      "Iteration 467, loss = 0.07876769\n",
      "Iteration 468, loss = 0.07850286\n",
      "Iteration 469, loss = 0.07823989\n",
      "Iteration 470, loss = 0.07797876\n",
      "Iteration 471, loss = 0.07771945\n",
      "Iteration 472, loss = 0.07746199\n",
      "Iteration 473, loss = 0.07720646\n",
      "Iteration 474, loss = 0.07695276\n",
      "Iteration 475, loss = 0.07670108\n",
      "Iteration 476, loss = 0.07645124\n",
      "Iteration 477, loss = 0.07620329\n",
      "Iteration 478, loss = 0.07595741\n",
      "Iteration 479, loss = 0.07571329\n",
      "Iteration 480, loss = 0.07547126\n",
      "Iteration 481, loss = 0.07523123\n",
      "Iteration 482, loss = 0.07499303\n",
      "Iteration 483, loss = 0.07475689\n",
      "Iteration 484, loss = 0.07452271\n",
      "Iteration 485, loss = 0.07429057\n",
      "Iteration 486, loss = 0.07406075\n",
      "Iteration 487, loss = 0.07383292\n",
      "Iteration 488, loss = 0.07360705\n",
      "Iteration 489, loss = 0.07338305\n",
      "Iteration 490, loss = 0.07316091\n",
      "Iteration 491, loss = 0.07294085\n",
      "Iteration 492, loss = 0.07272268\n",
      "Iteration 493, loss = 0.07250639\n",
      "Iteration 494, loss = 0.07229193\n",
      "Iteration 495, loss = 0.07207944\n",
      "Iteration 496, loss = 0.07186883\n",
      "Iteration 497, loss = 0.07166016\n",
      "Iteration 498, loss = 0.07145337\n",
      "Iteration 499, loss = 0.07124840\n",
      "Iteration 500, loss = 0.07104532\n",
      "Iteration 501, loss = 0.07084405\n",
      "Iteration 502, loss = 0.07064460\n",
      "Iteration 503, loss = 0.07044700\n",
      "Iteration 504, loss = 0.07025120\n",
      "Iteration 505, loss = 0.07005717\n",
      "Iteration 506, loss = 0.06986488\n",
      "Iteration 507, loss = 0.06967431\n",
      "Iteration 508, loss = 0.06948549\n",
      "Iteration 509, loss = 0.06929834\n",
      "Iteration 510, loss = 0.06911289\n",
      "Iteration 511, loss = 0.06892912\n",
      "Iteration 512, loss = 0.06874702\n",
      "Iteration 513, loss = 0.06856657\n",
      "Iteration 514, loss = 0.06838779\n",
      "Iteration 515, loss = 0.06821070\n",
      "Iteration 516, loss = 0.06803518\n",
      "Iteration 517, loss = 0.06786117\n",
      "Iteration 518, loss = 0.06768872\n",
      "Iteration 519, loss = 0.06751783\n",
      "Iteration 520, loss = 0.06734848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 521, loss = 0.06718064\n",
      "Iteration 522, loss = 0.06701432\n",
      "Iteration 523, loss = 0.06684946\n",
      "Iteration 524, loss = 0.06668607\n",
      "Iteration 525, loss = 0.06652413\n",
      "Iteration 526, loss = 0.06636361\n",
      "Iteration 527, loss = 0.06620451\n",
      "Iteration 528, loss = 0.06604681\n",
      "Iteration 529, loss = 0.06589050\n",
      "Iteration 530, loss = 0.06573554\n",
      "Iteration 531, loss = 0.06558196\n",
      "Iteration 532, loss = 0.06542969\n",
      "Iteration 533, loss = 0.06527877\n",
      "Iteration 534, loss = 0.06512924\n",
      "Iteration 535, loss = 0.06498101\n",
      "Iteration 536, loss = 0.06483407\n",
      "Iteration 537, loss = 0.06468839\n",
      "Iteration 538, loss = 0.06454400\n",
      "Iteration 539, loss = 0.06440086\n",
      "Iteration 540, loss = 0.06425894\n",
      "Iteration 541, loss = 0.06411824\n",
      "Iteration 542, loss = 0.06397872\n",
      "Iteration 543, loss = 0.06384039\n",
      "Iteration 544, loss = 0.06370323\n",
      "Iteration 545, loss = 0.06356721\n",
      "Iteration 546, loss = 0.06343234\n",
      "Iteration 547, loss = 0.06329865\n",
      "Iteration 548, loss = 0.06316600\n",
      "Iteration 549, loss = 0.06303450\n",
      "Iteration 550, loss = 0.06290410\n",
      "Iteration 551, loss = 0.06277482\n",
      "Iteration 552, loss = 0.06264662\n",
      "Iteration 553, loss = 0.06251949\n",
      "Iteration 554, loss = 0.06239339\n",
      "Iteration 555, loss = 0.06226832\n",
      "Iteration 556, loss = 0.06214427\n",
      "Iteration 557, loss = 0.06202123\n",
      "Iteration 558, loss = 0.06189923\n",
      "Iteration 559, loss = 0.06177824\n",
      "Iteration 560, loss = 0.06165826\n",
      "Iteration 561, loss = 0.06153928\n",
      "Iteration 562, loss = 0.06142126\n",
      "Iteration 563, loss = 0.06130421\n",
      "Iteration 564, loss = 0.06118814\n",
      "Iteration 565, loss = 0.06107297\n",
      "Iteration 566, loss = 0.06095883\n",
      "Iteration 567, loss = 0.06084552\n",
      "Iteration 568, loss = 0.06073326\n",
      "Iteration 569, loss = 0.06062194\n",
      "Iteration 570, loss = 0.06051152\n",
      "Iteration 571, loss = 0.06040198\n",
      "Iteration 572, loss = 0.06029336\n",
      "Iteration 573, loss = 0.06018556\n",
      "Iteration 574, loss = 0.06007860\n",
      "Iteration 575, loss = 0.05997257\n",
      "Iteration 576, loss = 0.05986729\n",
      "Iteration 577, loss = 0.05976289\n",
      "Iteration 578, loss = 0.05965921\n",
      "Iteration 579, loss = 0.05955645\n",
      "Iteration 580, loss = 0.05945436\n",
      "Iteration 581, loss = 0.05935315\n",
      "Iteration 582, loss = 0.05925261\n",
      "Iteration 583, loss = 0.05915292\n",
      "Iteration 584, loss = 0.05905391\n",
      "Iteration 585, loss = 0.05895586\n",
      "Iteration 586, loss = 0.05885851\n",
      "Iteration 587, loss = 0.05876182\n",
      "Iteration 588, loss = 0.05866598\n",
      "Iteration 589, loss = 0.05857072\n",
      "Iteration 590, loss = 0.05847631\n",
      "Iteration 591, loss = 0.05838251\n",
      "Iteration 592, loss = 0.05828947\n",
      "Iteration 593, loss = 0.05819714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.46211109\n",
      "Iteration 3, loss = 1.43328691\n",
      "Iteration 4, loss = 1.40544449\n",
      "Iteration 5, loss = 1.37858816\n",
      "Iteration 6, loss = 1.35254150\n",
      "Iteration 7, loss = 1.32720236\n",
      "Iteration 8, loss = 1.30252209\n",
      "Iteration 9, loss = 1.27860234\n",
      "Iteration 10, loss = 1.25556334\n",
      "Iteration 11, loss = 1.23331567\n",
      "Iteration 12, loss = 1.21192025\n",
      "Iteration 13, loss = 1.19158888\n",
      "Iteration 14, loss = 1.17230811\n",
      "Iteration 15, loss = 1.15379169\n",
      "Iteration 16, loss = 1.13598303\n",
      "Iteration 17, loss = 1.11878158\n",
      "Iteration 18, loss = 1.10218775\n",
      "Iteration 19, loss = 1.08624380\n",
      "Iteration 20, loss = 1.07089268\n",
      "Iteration 21, loss = 1.05617065\n",
      "Iteration 22, loss = 1.04196642\n",
      "Iteration 23, loss = 1.02811644\n",
      "Iteration 24, loss = 1.01467924\n",
      "Iteration 25, loss = 1.00158668\n",
      "Iteration 26, loss = 0.98877491\n",
      "Iteration 27, loss = 0.97629072\n",
      "Iteration 28, loss = 0.96415087\n",
      "Iteration 29, loss = 0.95224164\n",
      "Iteration 30, loss = 0.94063007\n",
      "Iteration 31, loss = 0.92923238\n",
      "Iteration 32, loss = 0.91797539\n",
      "Iteration 33, loss = 0.90693822\n",
      "Iteration 34, loss = 0.89623284\n",
      "Iteration 35, loss = 0.88579098\n",
      "Iteration 36, loss = 0.87560845\n",
      "Iteration 37, loss = 0.86570651\n",
      "Iteration 38, loss = 0.85601435\n",
      "Iteration 39, loss = 0.84656563\n",
      "Iteration 40, loss = 0.83737179\n",
      "Iteration 41, loss = 0.82843827\n",
      "Iteration 42, loss = 0.81963826\n",
      "Iteration 43, loss = 0.81098926\n",
      "Iteration 44, loss = 0.80254076\n",
      "Iteration 45, loss = 0.79428922\n",
      "Iteration 46, loss = 0.78619452\n",
      "Iteration 47, loss = 0.77827440\n",
      "Iteration 48, loss = 0.77050580\n",
      "Iteration 49, loss = 0.76286085\n",
      "Iteration 50, loss = 0.75533561\n",
      "Iteration 51, loss = 0.74793558\n",
      "Iteration 52, loss = 0.74066276\n",
      "Iteration 53, loss = 0.73352547\n",
      "Iteration 54, loss = 0.72652042\n",
      "Iteration 55, loss = 0.71963558\n",
      "Iteration 56, loss = 0.71286787\n",
      "Iteration 57, loss = 0.70625429\n",
      "Iteration 58, loss = 0.69978530\n",
      "Iteration 59, loss = 0.69347761\n",
      "Iteration 60, loss = 0.68734825\n",
      "Iteration 61, loss = 0.68136364\n",
      "Iteration 62, loss = 0.67552744\n",
      "Iteration 63, loss = 0.66983245\n",
      "Iteration 64, loss = 0.66429249\n",
      "Iteration 65, loss = 0.65886078\n",
      "Iteration 66, loss = 0.65352884\n",
      "Iteration 67, loss = 0.64828580\n",
      "Iteration 68, loss = 0.64312507\n",
      "Iteration 69, loss = 0.63804733\n",
      "Iteration 70, loss = 0.63304550\n",
      "Iteration 71, loss = 0.62812012\n",
      "Iteration 72, loss = 0.62326627\n",
      "Iteration 73, loss = 0.61843149\n",
      "Iteration 74, loss = 0.61366313\n",
      "Iteration 75, loss = 0.60892530\n",
      "Iteration 76, loss = 0.60409063\n",
      "Iteration 77, loss = 0.59920615\n",
      "Iteration 78, loss = 0.59425684\n",
      "Iteration 79, loss = 0.58920025\n",
      "Iteration 80, loss = 0.58424295\n",
      "Iteration 81, loss = 0.57929902\n",
      "Iteration 82, loss = 0.57442247\n",
      "Iteration 83, loss = 0.56971919\n",
      "Iteration 84, loss = 0.56519247\n",
      "Iteration 85, loss = 0.56082830\n",
      "Iteration 86, loss = 0.55667783\n",
      "Iteration 87, loss = 0.55271716\n",
      "Iteration 88, loss = 0.54884792\n",
      "Iteration 89, loss = 0.54511435\n",
      "Iteration 90, loss = 0.54145429\n",
      "Iteration 91, loss = 0.53784028\n",
      "Iteration 92, loss = 0.53427685\n",
      "Iteration 93, loss = 0.53076957\n",
      "Iteration 94, loss = 0.52731608\n",
      "Iteration 95, loss = 0.52390152\n",
      "Iteration 96, loss = 0.52049526\n",
      "Iteration 97, loss = 0.51708180\n",
      "Iteration 98, loss = 0.51366081\n",
      "Iteration 99, loss = 0.51026645\n",
      "Iteration 100, loss = 0.50694026\n",
      "Iteration 101, loss = 0.50371725\n",
      "Iteration 102, loss = 0.50055530\n",
      "Iteration 103, loss = 0.49742442\n",
      "Iteration 104, loss = 0.49432156\n",
      "Iteration 105, loss = 0.49124523\n",
      "Iteration 106, loss = 0.48819835\n",
      "Iteration 107, loss = 0.48517948\n",
      "Iteration 108, loss = 0.48218806\n",
      "Iteration 109, loss = 0.47922420\n",
      "Iteration 110, loss = 0.47628905\n",
      "Iteration 111, loss = 0.47338238\n",
      "Iteration 112, loss = 0.47050421\n",
      "Iteration 113, loss = 0.46765688\n",
      "Iteration 114, loss = 0.46483724\n",
      "Iteration 115, loss = 0.46204582\n",
      "Iteration 116, loss = 0.45928185\n",
      "Iteration 117, loss = 0.45654454\n",
      "Iteration 118, loss = 0.45382563\n",
      "Iteration 119, loss = 0.45113233\n",
      "Iteration 120, loss = 0.44846196\n",
      "Iteration 121, loss = 0.44580651\n",
      "Iteration 122, loss = 0.44316908\n",
      "Iteration 123, loss = 0.44054448\n",
      "Iteration 124, loss = 0.43793445\n",
      "Iteration 125, loss = 0.43532733\n",
      "Iteration 126, loss = 0.43272672\n",
      "Iteration 127, loss = 0.43013246\n",
      "Iteration 128, loss = 0.42752466\n",
      "Iteration 129, loss = 0.42490465\n",
      "Iteration 130, loss = 0.42225214\n",
      "Iteration 131, loss = 0.41955534\n",
      "Iteration 132, loss = 0.41680470\n",
      "Iteration 133, loss = 0.41401053\n",
      "Iteration 134, loss = 0.41119327\n",
      "Iteration 135, loss = 0.40839052\n",
      "Iteration 136, loss = 0.40556358\n",
      "Iteration 137, loss = 0.40278341\n",
      "Iteration 138, loss = 0.40001776\n",
      "Iteration 139, loss = 0.39735242\n",
      "Iteration 140, loss = 0.39480988\n",
      "Iteration 141, loss = 0.39245350\n",
      "Iteration 142, loss = 0.39021185\n",
      "Iteration 143, loss = 0.38802677\n",
      "Iteration 144, loss = 0.38590054\n",
      "Iteration 145, loss = 0.38383566\n",
      "Iteration 146, loss = 0.38179394\n",
      "Iteration 147, loss = 0.37975185\n",
      "Iteration 148, loss = 0.37771793\n",
      "Iteration 149, loss = 0.37568269\n",
      "Iteration 150, loss = 0.37365149\n",
      "Iteration 151, loss = 0.37161819\n",
      "Iteration 152, loss = 0.36957824\n",
      "Iteration 153, loss = 0.36755327\n",
      "Iteration 154, loss = 0.36552896\n",
      "Iteration 155, loss = 0.36351218\n",
      "Iteration 156, loss = 0.36151187\n",
      "Iteration 157, loss = 0.35951045\n",
      "Iteration 158, loss = 0.35753437\n",
      "Iteration 159, loss = 0.35557872\n",
      "Iteration 160, loss = 0.35363303\n",
      "Iteration 161, loss = 0.35169744\n",
      "Iteration 162, loss = 0.34977810\n",
      "Iteration 163, loss = 0.34787995\n",
      "Iteration 164, loss = 0.34600150\n",
      "Iteration 165, loss = 0.34414736\n",
      "Iteration 166, loss = 0.34230659\n",
      "Iteration 167, loss = 0.34048002\n",
      "Iteration 168, loss = 0.33866962\n",
      "Iteration 169, loss = 0.33687422\n",
      "Iteration 170, loss = 0.33509095\n",
      "Iteration 171, loss = 0.33331410\n",
      "Iteration 172, loss = 0.33154987\n",
      "Iteration 173, loss = 0.32979749\n",
      "Iteration 174, loss = 0.32805510\n",
      "Iteration 175, loss = 0.32632242\n",
      "Iteration 176, loss = 0.32460103\n",
      "Iteration 177, loss = 0.32288955\n",
      "Iteration 178, loss = 0.32118854\n",
      "Iteration 179, loss = 0.31949772\n",
      "Iteration 180, loss = 0.31781730\n",
      "Iteration 181, loss = 0.31614704\n",
      "Iteration 182, loss = 0.31448710\n",
      "Iteration 183, loss = 0.31283685\n",
      "Iteration 184, loss = 0.31119791\n",
      "Iteration 185, loss = 0.30957028\n",
      "Iteration 186, loss = 0.30795403\n",
      "Iteration 187, loss = 0.30634706\n",
      "Iteration 188, loss = 0.30475027\n",
      "Iteration 189, loss = 0.30316515\n",
      "Iteration 190, loss = 0.30158984\n",
      "Iteration 191, loss = 0.30002533\n",
      "Iteration 192, loss = 0.29847275\n",
      "Iteration 193, loss = 0.29693178\n",
      "Iteration 194, loss = 0.29540047\n",
      "Iteration 195, loss = 0.29387884\n",
      "Iteration 196, loss = 0.29236725\n",
      "Iteration 197, loss = 0.29086694\n",
      "Iteration 198, loss = 0.28937647\n",
      "Iteration 199, loss = 0.28789590\n",
      "Iteration 200, loss = 0.28642555\n",
      "Iteration 201, loss = 0.28496485\n",
      "Iteration 202, loss = 0.28351377\n",
      "Iteration 203, loss = 0.28207259\n",
      "Iteration 204, loss = 0.28064134\n",
      "Iteration 205, loss = 0.27921992\n",
      "Iteration 206, loss = 0.27780915\n",
      "Iteration 207, loss = 0.27640816\n",
      "Iteration 208, loss = 0.27501646\n",
      "Iteration 209, loss = 0.27363425\n",
      "Iteration 210, loss = 0.27226239\n",
      "Iteration 211, loss = 0.27089985\n",
      "Iteration 212, loss = 0.26954671\n",
      "Iteration 213, loss = 0.26820349\n",
      "Iteration 214, loss = 0.26686936\n",
      "Iteration 215, loss = 0.26554428\n",
      "Iteration 216, loss = 0.26422991\n",
      "Iteration 217, loss = 0.26292395\n",
      "Iteration 218, loss = 0.26162626\n",
      "Iteration 219, loss = 0.26033964\n",
      "Iteration 220, loss = 0.25906247\n",
      "Iteration 221, loss = 0.25779419\n",
      "Iteration 222, loss = 0.25653574\n",
      "Iteration 223, loss = 0.25528585\n",
      "Iteration 224, loss = 0.25404516\n",
      "Iteration 225, loss = 0.25281496\n",
      "Iteration 226, loss = 0.25159282\n",
      "Iteration 227, loss = 0.25037895\n",
      "Iteration 228, loss = 0.24917566\n",
      "Iteration 229, loss = 0.24798076\n",
      "Iteration 230, loss = 0.24679416\n",
      "Iteration 231, loss = 0.24561635\n",
      "Iteration 232, loss = 0.24444907\n",
      "Iteration 233, loss = 0.24328863\n",
      "Iteration 234, loss = 0.24213753\n",
      "Iteration 235, loss = 0.24099628\n",
      "Iteration 236, loss = 0.23986297\n",
      "Iteration 237, loss = 0.23873750\n",
      "Iteration 238, loss = 0.23762017\n",
      "Iteration 239, loss = 0.23651225\n",
      "Iteration 240, loss = 0.23541284\n",
      "Iteration 241, loss = 0.23432162\n",
      "Iteration 242, loss = 0.23323931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 243, loss = 0.23216378\n",
      "Iteration 244, loss = 0.23109783\n",
      "Iteration 245, loss = 0.23003987\n",
      "Iteration 246, loss = 0.22898962\n",
      "Iteration 247, loss = 0.22794765\n",
      "Iteration 248, loss = 0.22691373\n",
      "Iteration 249, loss = 0.22588796\n",
      "Iteration 250, loss = 0.22487019\n",
      "Iteration 251, loss = 0.22386023\n",
      "Iteration 252, loss = 0.22285808\n",
      "Iteration 253, loss = 0.22186364\n",
      "Iteration 254, loss = 0.22087699\n",
      "Iteration 255, loss = 0.21989812\n",
      "Iteration 256, loss = 0.21892681\n",
      "Iteration 257, loss = 0.21796306\n",
      "Iteration 258, loss = 0.21700675\n",
      "Iteration 259, loss = 0.21605833\n",
      "Iteration 260, loss = 0.21511667\n",
      "Iteration 261, loss = 0.21418276\n",
      "Iteration 262, loss = 0.21325609\n",
      "Iteration 263, loss = 0.21233682\n",
      "Iteration 264, loss = 0.21142534\n",
      "Iteration 265, loss = 0.21052089\n",
      "Iteration 266, loss = 0.20962348\n",
      "Iteration 267, loss = 0.20873349\n",
      "Iteration 268, loss = 0.20785012\n",
      "Iteration 269, loss = 0.20697398\n",
      "Iteration 270, loss = 0.20610461\n",
      "Iteration 271, loss = 0.20524213\n",
      "Iteration 272, loss = 0.20438649\n",
      "Iteration 273, loss = 0.20353765\n",
      "Iteration 274, loss = 0.20269551\n",
      "Iteration 275, loss = 0.20186002\n",
      "Iteration 276, loss = 0.20103148\n",
      "Iteration 277, loss = 0.20020893\n",
      "Iteration 278, loss = 0.19939330\n",
      "Iteration 279, loss = 0.19858409\n",
      "Iteration 280, loss = 0.19778124\n",
      "Iteration 281, loss = 0.19698505\n",
      "Iteration 282, loss = 0.19619426\n",
      "Iteration 283, loss = 0.19540890\n",
      "Iteration 284, loss = 0.19463187\n",
      "Iteration 285, loss = 0.19386058\n",
      "Iteration 286, loss = 0.19309153\n",
      "Iteration 287, loss = 0.19233105\n",
      "Iteration 288, loss = 0.19157705\n",
      "Iteration 289, loss = 0.19082537\n",
      "Iteration 290, loss = 0.19007833\n",
      "Iteration 291, loss = 0.18933777\n",
      "Iteration 292, loss = 0.18860043\n",
      "Iteration 293, loss = 0.18786674\n",
      "Iteration 294, loss = 0.18713776\n",
      "Iteration 295, loss = 0.18641296\n",
      "Iteration 296, loss = 0.18569024\n",
      "Iteration 297, loss = 0.18497145\n",
      "Iteration 298, loss = 0.18425676\n",
      "Iteration 299, loss = 0.18354438\n",
      "Iteration 300, loss = 0.18283606\n",
      "Iteration 301, loss = 0.18213129\n",
      "Iteration 302, loss = 0.18142862\n",
      "Iteration 303, loss = 0.18072840\n",
      "Iteration 304, loss = 0.18003218\n",
      "Iteration 305, loss = 0.17933923\n",
      "Iteration 306, loss = 0.17864869\n",
      "Iteration 307, loss = 0.17796181\n",
      "Iteration 308, loss = 0.17727900\n",
      "Iteration 309, loss = 0.17659828\n",
      "Iteration 310, loss = 0.17591971\n",
      "Iteration 311, loss = 0.17524507\n",
      "Iteration 312, loss = 0.17457464\n",
      "Iteration 313, loss = 0.17390741\n",
      "Iteration 314, loss = 0.17324327\n",
      "Iteration 315, loss = 0.17258269\n",
      "Iteration 316, loss = 0.17192596\n",
      "Iteration 317, loss = 0.17127279\n",
      "Iteration 318, loss = 0.17062253\n",
      "Iteration 319, loss = 0.16997556\n",
      "Iteration 320, loss = 0.16933300\n",
      "Iteration 321, loss = 0.16869398\n",
      "Iteration 322, loss = 0.16805896\n",
      "Iteration 323, loss = 0.16742827\n",
      "Iteration 324, loss = 0.16680131\n",
      "Iteration 325, loss = 0.16617719\n",
      "Iteration 326, loss = 0.16555647\n",
      "Iteration 327, loss = 0.16493976\n",
      "Iteration 328, loss = 0.16432730\n",
      "Iteration 329, loss = 0.16371917\n",
      "Iteration 330, loss = 0.16311416\n",
      "Iteration 331, loss = 0.16251350\n",
      "Iteration 332, loss = 0.16191691\n",
      "Iteration 333, loss = 0.16132433\n",
      "Iteration 334, loss = 0.16073518\n",
      "Iteration 335, loss = 0.16015030\n",
      "Iteration 336, loss = 0.15956962\n",
      "Iteration 337, loss = 0.15899287\n",
      "Iteration 338, loss = 0.15842006\n",
      "Iteration 339, loss = 0.15785118\n",
      "Iteration 340, loss = 0.15728650\n",
      "Iteration 341, loss = 0.15672647\n",
      "Iteration 342, loss = 0.15617018\n",
      "Iteration 343, loss = 0.15561780\n",
      "Iteration 344, loss = 0.15507108\n",
      "Iteration 345, loss = 0.15453055\n",
      "Iteration 346, loss = 0.15399383\n",
      "Iteration 347, loss = 0.15346065\n",
      "Iteration 348, loss = 0.15293118\n",
      "Iteration 349, loss = 0.15240603\n",
      "Iteration 350, loss = 0.15188538\n",
      "Iteration 351, loss = 0.15136862\n",
      "Iteration 352, loss = 0.15085558\n",
      "Iteration 353, loss = 0.15034710\n",
      "Iteration 354, loss = 0.14984334\n",
      "Iteration 355, loss = 0.14934289\n",
      "Iteration 356, loss = 0.14884682\n",
      "Iteration 357, loss = 0.14835478\n",
      "Iteration 358, loss = 0.14786656\n",
      "Iteration 359, loss = 0.14738241\n",
      "Iteration 360, loss = 0.14690273\n",
      "Iteration 361, loss = 0.14642751\n",
      "Iteration 362, loss = 0.14595582\n",
      "Iteration 363, loss = 0.14548672\n",
      "Iteration 364, loss = 0.14502078\n",
      "Iteration 365, loss = 0.14455895\n",
      "Iteration 366, loss = 0.14409960\n",
      "Iteration 367, loss = 0.14364303\n",
      "Iteration 368, loss = 0.14318932\n",
      "Iteration 369, loss = 0.14273798\n",
      "Iteration 370, loss = 0.14228976\n",
      "Iteration 371, loss = 0.14184479\n",
      "Iteration 372, loss = 0.14140245\n",
      "Iteration 373, loss = 0.14096345\n",
      "Iteration 374, loss = 0.14052772\n",
      "Iteration 375, loss = 0.14008895\n",
      "Iteration 376, loss = 0.13964883\n",
      "Iteration 377, loss = 0.13920970\n",
      "Iteration 378, loss = 0.13877253\n",
      "Iteration 379, loss = 0.13833697\n",
      "Iteration 380, loss = 0.13790251\n",
      "Iteration 381, loss = 0.13746370\n",
      "Iteration 382, loss = 0.13703430\n",
      "Iteration 383, loss = 0.13660956\n",
      "Iteration 384, loss = 0.13618746\n",
      "Iteration 385, loss = 0.13576856\n",
      "Iteration 386, loss = 0.13534402\n",
      "Iteration 387, loss = 0.13491959\n",
      "Iteration 388, loss = 0.13450856\n",
      "Iteration 389, loss = 0.13410998\n",
      "Iteration 390, loss = 0.13370280\n",
      "Iteration 391, loss = 0.13328828\n",
      "Iteration 392, loss = 0.13286756\n",
      "Iteration 393, loss = 0.13243835\n",
      "Iteration 394, loss = 0.13200285\n",
      "Iteration 395, loss = 0.13157043\n",
      "Iteration 396, loss = 0.13113525\n",
      "Iteration 397, loss = 0.13071875\n",
      "Iteration 398, loss = 0.13030968\n",
      "Iteration 399, loss = 0.12991225\n",
      "Iteration 400, loss = 0.12953207\n",
      "Iteration 401, loss = 0.12915905\n",
      "Iteration 402, loss = 0.12879724\n",
      "Iteration 403, loss = 0.12845501\n",
      "Iteration 404, loss = 0.12812449\n",
      "Iteration 405, loss = 0.12779481\n",
      "Iteration 406, loss = 0.12746509\n",
      "Iteration 407, loss = 0.12713593\n",
      "Iteration 408, loss = 0.12680764\n",
      "Iteration 409, loss = 0.12647922\n",
      "Iteration 410, loss = 0.12615110\n",
      "Iteration 411, loss = 0.12582437\n",
      "Iteration 412, loss = 0.12549796\n",
      "Iteration 413, loss = 0.12517115\n",
      "Iteration 414, loss = 0.12484419\n",
      "Iteration 415, loss = 0.12451737\n",
      "Iteration 416, loss = 0.12419044\n",
      "Iteration 417, loss = 0.12386379\n",
      "Iteration 418, loss = 0.12353771\n",
      "Iteration 419, loss = 0.12321262\n",
      "Iteration 420, loss = 0.12288957\n",
      "Iteration 421, loss = 0.12256751\n",
      "Iteration 422, loss = 0.12224655\n",
      "Iteration 423, loss = 0.12192641\n",
      "Iteration 424, loss = 0.12160761\n",
      "Iteration 425, loss = 0.12128977\n",
      "Iteration 426, loss = 0.12097341\n",
      "Iteration 427, loss = 0.12065800\n",
      "Iteration 428, loss = 0.12034334\n",
      "Iteration 429, loss = 0.12002956\n",
      "Iteration 430, loss = 0.11971649\n",
      "Iteration 431, loss = 0.11940427\n",
      "Iteration 432, loss = 0.11909347\n",
      "Iteration 433, loss = 0.11878370\n",
      "Iteration 434, loss = 0.11847470\n",
      "Iteration 435, loss = 0.11816631\n",
      "Iteration 436, loss = 0.11785850\n",
      "Iteration 437, loss = 0.11755126\n",
      "Iteration 438, loss = 0.11724466\n",
      "Iteration 439, loss = 0.11693906\n",
      "Iteration 440, loss = 0.11663563\n",
      "Iteration 441, loss = 0.11633394\n",
      "Iteration 442, loss = 0.11603308\n",
      "Iteration 443, loss = 0.11573289\n",
      "Iteration 444, loss = 0.11543356\n",
      "Iteration 445, loss = 0.11513513\n",
      "Iteration 446, loss = 0.11483769\n",
      "Iteration 447, loss = 0.11454122\n",
      "Iteration 448, loss = 0.11424595\n",
      "Iteration 449, loss = 0.11395160\n",
      "Iteration 450, loss = 0.11365847\n",
      "Iteration 451, loss = 0.11336659\n",
      "Iteration 452, loss = 0.11307604\n",
      "Iteration 453, loss = 0.11278675\n",
      "Iteration 454, loss = 0.11249895\n",
      "Iteration 455, loss = 0.11221259\n",
      "Iteration 456, loss = 0.11192784\n",
      "Iteration 457, loss = 0.11164466\n",
      "Iteration 458, loss = 0.11136317\n",
      "Iteration 459, loss = 0.11108342\n",
      "Iteration 460, loss = 0.11080627\n",
      "Iteration 461, loss = 0.11053076\n",
      "Iteration 462, loss = 0.11025694\n",
      "Iteration 463, loss = 0.10998507\n",
      "Iteration 464, loss = 0.10971509\n",
      "Iteration 465, loss = 0.10944695\n",
      "Iteration 466, loss = 0.10918069\n",
      "Iteration 467, loss = 0.10891631\n",
      "Iteration 468, loss = 0.10865397\n",
      "Iteration 469, loss = 0.10839368\n",
      "Iteration 470, loss = 0.10813557\n",
      "Iteration 471, loss = 0.10787947\n",
      "Iteration 472, loss = 0.10762530\n",
      "Iteration 473, loss = 0.10737327\n",
      "Iteration 474, loss = 0.10712372\n",
      "Iteration 475, loss = 0.10687613\n",
      "Iteration 476, loss = 0.10663054\n",
      "Iteration 477, loss = 0.10638690\n",
      "Iteration 478, loss = 0.10614529\n",
      "Iteration 479, loss = 0.10590597\n",
      "Iteration 480, loss = 0.10566884\n",
      "Iteration 481, loss = 0.10543400\n",
      "Iteration 482, loss = 0.10520126\n",
      "Iteration 483, loss = 0.10497058\n",
      "Iteration 484, loss = 0.10474203\n",
      "Iteration 485, loss = 0.10451557\n",
      "Iteration 486, loss = 0.10429119\n",
      "Iteration 487, loss = 0.10406884\n",
      "Iteration 488, loss = 0.10384869\n",
      "Iteration 489, loss = 0.10363068\n",
      "Iteration 490, loss = 0.10341478\n",
      "Iteration 491, loss = 0.10320088\n",
      "Iteration 492, loss = 0.10298902\n",
      "Iteration 493, loss = 0.10277925\n",
      "Iteration 494, loss = 0.10257159\n",
      "Iteration 495, loss = 0.10236591\n",
      "Iteration 496, loss = 0.10216223\n",
      "Iteration 497, loss = 0.10196049\n",
      "Iteration 498, loss = 0.10176087\n",
      "Iteration 499, loss = 0.10156317\n",
      "Iteration 500, loss = 0.10136731\n",
      "Iteration 501, loss = 0.10117352\n",
      "Iteration 502, loss = 0.10098202\n",
      "Iteration 503, loss = 0.10079238\n",
      "Iteration 504, loss = 0.10060463\n",
      "Iteration 505, loss = 0.10041870\n",
      "Iteration 506, loss = 0.10023475\n",
      "Iteration 507, loss = 0.10005266\n",
      "Iteration 508, loss = 0.09987229\n",
      "Iteration 509, loss = 0.09969366\n",
      "Iteration 510, loss = 0.09951680\n",
      "Iteration 511, loss = 0.09934177\n",
      "Iteration 512, loss = 0.09916844\n",
      "Iteration 513, loss = 0.09899690\n",
      "Iteration 514, loss = 0.09882700\n",
      "Iteration 515, loss = 0.09865878\n",
      "Iteration 516, loss = 0.09849225\n",
      "Iteration 517, loss = 0.09832738\n",
      "Iteration 518, loss = 0.09816418\n",
      "Iteration 519, loss = 0.09800261\n",
      "Iteration 520, loss = 0.09784262\n",
      "Iteration 521, loss = 0.09768421\n",
      "Iteration 522, loss = 0.09752736\n",
      "Iteration 523, loss = 0.09737204\n",
      "Iteration 524, loss = 0.09721833\n",
      "Iteration 525, loss = 0.09706611\n",
      "Iteration 526, loss = 0.09691539\n",
      "Iteration 527, loss = 0.09676614\n",
      "Iteration 528, loss = 0.09661833\n",
      "Iteration 529, loss = 0.09647199\n",
      "Iteration 530, loss = 0.09632706\n",
      "Iteration 531, loss = 0.09618346\n",
      "Iteration 532, loss = 0.09604132\n",
      "Iteration 533, loss = 0.09590058\n",
      "Iteration 534, loss = 0.09576119\n",
      "Iteration 535, loss = 0.09562309\n",
      "Iteration 536, loss = 0.09548628\n",
      "Iteration 537, loss = 0.09535076\n",
      "Iteration 538, loss = 0.09521656\n",
      "Iteration 539, loss = 0.09508359\n",
      "Iteration 540, loss = 0.09495197\n",
      "Iteration 541, loss = 0.09482158\n",
      "Iteration 542, loss = 0.09469242\n",
      "Iteration 543, loss = 0.09456447\n",
      "Iteration 544, loss = 0.09443774\n",
      "Iteration 545, loss = 0.09431224\n",
      "Iteration 546, loss = 0.09418789\n",
      "Iteration 547, loss = 0.09406475\n",
      "Iteration 548, loss = 0.09394278\n",
      "Iteration 549, loss = 0.09382194\n",
      "Iteration 550, loss = 0.09370223\n",
      "Iteration 551, loss = 0.09358367\n",
      "Iteration 552, loss = 0.09346621\n",
      "Iteration 553, loss = 0.09334987\n",
      "Iteration 554, loss = 0.09323464\n",
      "Iteration 555, loss = 0.09312047\n",
      "Iteration 556, loss = 0.09300726\n",
      "Iteration 557, loss = 0.09289511\n",
      "Iteration 558, loss = 0.09278403\n",
      "Iteration 559, loss = 0.09267373\n",
      "Iteration 560, loss = 0.09256407\n",
      "Iteration 561, loss = 0.09245505\n",
      "Iteration 562, loss = 0.09234672\n",
      "Iteration 563, loss = 0.09223909\n",
      "Iteration 564, loss = 0.09213213\n",
      "Iteration 565, loss = 0.09202589\n",
      "Iteration 566, loss = 0.09192043\n",
      "Iteration 567, loss = 0.09181568\n",
      "Iteration 568, loss = 0.09171166\n",
      "Iteration 569, loss = 0.09160835\n",
      "Iteration 570, loss = 0.09150596\n",
      "Iteration 571, loss = 0.09140582\n",
      "Iteration 572, loss = 0.09130686\n",
      "Iteration 573, loss = 0.09120891\n",
      "Iteration 574, loss = 0.09111180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 575, loss = 0.09101584\n",
      "Iteration 576, loss = 0.09092080\n",
      "Iteration 577, loss = 0.09082666\n",
      "Iteration 578, loss = 0.09073329\n",
      "Iteration 579, loss = 0.09064075\n",
      "Iteration 580, loss = 0.09054902\n",
      "Iteration 581, loss = 0.09045812\n",
      "Iteration 582, loss = 0.09036803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48762646\n",
      "Iteration 2, loss = 1.47371141\n",
      "Iteration 3, loss = 1.45417536\n",
      "Iteration 4, loss = 1.42995974\n",
      "Iteration 5, loss = 1.40197366\n",
      "Iteration 6, loss = 1.37106191\n",
      "Iteration 7, loss = 1.33814295\n",
      "Iteration 8, loss = 1.30426142\n",
      "Iteration 9, loss = 1.27039389\n",
      "Iteration 10, loss = 1.23772986\n",
      "Iteration 11, loss = 1.20659852\n",
      "Iteration 12, loss = 1.17739010\n",
      "Iteration 13, loss = 1.15049083\n",
      "Iteration 14, loss = 1.12616724\n",
      "Iteration 15, loss = 1.10480581\n",
      "Iteration 16, loss = 1.08609340\n",
      "Iteration 17, loss = 1.06952768\n",
      "Iteration 18, loss = 1.05482189\n",
      "Iteration 19, loss = 1.04163892\n",
      "Iteration 20, loss = 1.02942412\n",
      "Iteration 21, loss = 1.01772555\n",
      "Iteration 22, loss = 1.00615888\n",
      "Iteration 23, loss = 0.99448202\n",
      "Iteration 24, loss = 0.98262362\n",
      "Iteration 25, loss = 0.97059261\n",
      "Iteration 26, loss = 0.95844889\n",
      "Iteration 27, loss = 0.94633258\n",
      "Iteration 28, loss = 0.93428182\n",
      "Iteration 29, loss = 0.92232085\n",
      "Iteration 30, loss = 0.91058077\n",
      "Iteration 31, loss = 0.89909106\n",
      "Iteration 32, loss = 0.88788004\n",
      "Iteration 33, loss = 0.87706972\n",
      "Iteration 34, loss = 0.86678895\n",
      "Iteration 35, loss = 0.85698123\n",
      "Iteration 36, loss = 0.84752930\n",
      "Iteration 37, loss = 0.83834119\n",
      "Iteration 38, loss = 0.82947861\n",
      "Iteration 39, loss = 0.82091319\n",
      "Iteration 40, loss = 0.81255875\n",
      "Iteration 41, loss = 0.80443383\n",
      "Iteration 42, loss = 0.79650816\n",
      "Iteration 43, loss = 0.78888922\n",
      "Iteration 44, loss = 0.78147695\n",
      "Iteration 45, loss = 0.77433013\n",
      "Iteration 46, loss = 0.76745950\n",
      "Iteration 47, loss = 0.76077477\n",
      "Iteration 48, loss = 0.75424616\n",
      "Iteration 49, loss = 0.74788958\n",
      "Iteration 50, loss = 0.74176848\n",
      "Iteration 51, loss = 0.73581701\n",
      "Iteration 52, loss = 0.73002430\n",
      "Iteration 53, loss = 0.72439991\n",
      "Iteration 54, loss = 0.71892218\n",
      "Iteration 55, loss = 0.71358192\n",
      "Iteration 56, loss = 0.70836802\n",
      "Iteration 57, loss = 0.70326033\n",
      "Iteration 58, loss = 0.69826828\n",
      "Iteration 59, loss = 0.69338577\n",
      "Iteration 60, loss = 0.68861731\n",
      "Iteration 61, loss = 0.68398399\n",
      "Iteration 62, loss = 0.67949987\n",
      "Iteration 63, loss = 0.67514052\n",
      "Iteration 64, loss = 0.67087289\n",
      "Iteration 65, loss = 0.66669259\n",
      "Iteration 66, loss = 0.66260665\n",
      "Iteration 67, loss = 0.65860021\n",
      "Iteration 68, loss = 0.65467484\n",
      "Iteration 69, loss = 0.65082778\n",
      "Iteration 70, loss = 0.64705559\n",
      "Iteration 71, loss = 0.64335630\n",
      "Iteration 72, loss = 0.63972524\n",
      "Iteration 73, loss = 0.63616113\n",
      "Iteration 74, loss = 0.63266268\n",
      "Iteration 75, loss = 0.62922854\n",
      "Iteration 76, loss = 0.62585535\n",
      "Iteration 77, loss = 0.62254462\n",
      "Iteration 78, loss = 0.61929200\n",
      "Iteration 79, loss = 0.61609588\n",
      "Iteration 80, loss = 0.61295503\n",
      "Iteration 81, loss = 0.60987098\n",
      "Iteration 82, loss = 0.60683946\n",
      "Iteration 83, loss = 0.60385891\n",
      "Iteration 84, loss = 0.60093047\n",
      "Iteration 85, loss = 0.59805224\n",
      "Iteration 86, loss = 0.59522207\n",
      "Iteration 87, loss = 0.59243910\n",
      "Iteration 88, loss = 0.58970354\n",
      "Iteration 89, loss = 0.58701878\n",
      "Iteration 90, loss = 0.58437900\n",
      "Iteration 91, loss = 0.58178157\n",
      "Iteration 92, loss = 0.57922577\n",
      "Iteration 93, loss = 0.57670997\n",
      "Iteration 94, loss = 0.57423316\n",
      "Iteration 95, loss = 0.57179446\n",
      "Iteration 96, loss = 0.56939293\n",
      "Iteration 97, loss = 0.56702805\n",
      "Iteration 98, loss = 0.56470067\n",
      "Iteration 99, loss = 0.56241096\n",
      "Iteration 100, loss = 0.56015960\n",
      "Iteration 101, loss = 0.55794292\n",
      "Iteration 102, loss = 0.55575984\n",
      "Iteration 103, loss = 0.55360811\n",
      "Iteration 104, loss = 0.55148381\n",
      "Iteration 105, loss = 0.54938916\n",
      "Iteration 106, loss = 0.54732504\n",
      "Iteration 107, loss = 0.54529083\n",
      "Iteration 108, loss = 0.54328549\n",
      "Iteration 109, loss = 0.54130493\n",
      "Iteration 110, loss = 0.53935057\n",
      "Iteration 111, loss = 0.53742229\n",
      "Iteration 112, loss = 0.53552195\n",
      "Iteration 113, loss = 0.53365131\n",
      "Iteration 114, loss = 0.53180514\n",
      "Iteration 115, loss = 0.52998286\n",
      "Iteration 116, loss = 0.52818185\n",
      "Iteration 117, loss = 0.52640275\n",
      "Iteration 118, loss = 0.52464468\n",
      "Iteration 119, loss = 0.52290439\n",
      "Iteration 120, loss = 0.52117453\n",
      "Iteration 121, loss = 0.51945703\n",
      "Iteration 122, loss = 0.51774216\n",
      "Iteration 123, loss = 0.51602753\n",
      "Iteration 124, loss = 0.51432477\n",
      "Iteration 125, loss = 0.51265276\n",
      "Iteration 126, loss = 0.51099064\n",
      "Iteration 127, loss = 0.50933779\n",
      "Iteration 128, loss = 0.50769595\n",
      "Iteration 129, loss = 0.50607668\n",
      "Iteration 130, loss = 0.50446399\n",
      "Iteration 131, loss = 0.50285367\n",
      "Iteration 132, loss = 0.50124484\n",
      "Iteration 133, loss = 0.49965722\n",
      "Iteration 134, loss = 0.49810280\n",
      "Iteration 135, loss = 0.49655827\n",
      "Iteration 136, loss = 0.49502946\n",
      "Iteration 137, loss = 0.49352852\n",
      "Iteration 138, loss = 0.49207845\n",
      "Iteration 139, loss = 0.49066202\n",
      "Iteration 140, loss = 0.48927807\n",
      "Iteration 141, loss = 0.48791806\n",
      "Iteration 142, loss = 0.48659204\n",
      "Iteration 143, loss = 0.48529602\n",
      "Iteration 144, loss = 0.48401602\n",
      "Iteration 145, loss = 0.48274109\n",
      "Iteration 146, loss = 0.48147262\n",
      "Iteration 147, loss = 0.48021112\n",
      "Iteration 148, loss = 0.47895723\n",
      "Iteration 149, loss = 0.47771164\n",
      "Iteration 150, loss = 0.47647452\n",
      "Iteration 151, loss = 0.47524668\n",
      "Iteration 152, loss = 0.47402808\n",
      "Iteration 153, loss = 0.47281901\n",
      "Iteration 154, loss = 0.47161976\n",
      "Iteration 155, loss = 0.47043178\n",
      "Iteration 156, loss = 0.46925676\n",
      "Iteration 157, loss = 0.46809336\n",
      "Iteration 158, loss = 0.46694351\n",
      "Iteration 159, loss = 0.46580998\n",
      "Iteration 160, loss = 0.46469156\n",
      "Iteration 161, loss = 0.46358143\n",
      "Iteration 162, loss = 0.46247949\n",
      "Iteration 163, loss = 0.46138545\n",
      "Iteration 164, loss = 0.46029936\n",
      "Iteration 165, loss = 0.45922128\n",
      "Iteration 166, loss = 0.45815093\n",
      "Iteration 167, loss = 0.45708919\n",
      "Iteration 168, loss = 0.45603892\n",
      "Iteration 169, loss = 0.45499836\n",
      "Iteration 170, loss = 0.45396624\n",
      "Iteration 171, loss = 0.45294296\n",
      "Iteration 172, loss = 0.45192845\n",
      "Iteration 173, loss = 0.45091993\n",
      "Iteration 174, loss = 0.44991859\n",
      "Iteration 175, loss = 0.44892581\n",
      "Iteration 176, loss = 0.44794057\n",
      "Iteration 177, loss = 0.44696374\n",
      "Iteration 178, loss = 0.44599425\n",
      "Iteration 179, loss = 0.44503193\n",
      "Iteration 180, loss = 0.44407685\n",
      "Iteration 181, loss = 0.44312906\n",
      "Iteration 182, loss = 0.44218861\n",
      "Iteration 183, loss = 0.44125514\n",
      "Iteration 184, loss = 0.44032842\n",
      "Iteration 185, loss = 0.43940839\n",
      "Iteration 186, loss = 0.43849492\n",
      "Iteration 187, loss = 0.43758806\n",
      "Iteration 188, loss = 0.43668768\n",
      "Iteration 189, loss = 0.43579387\n",
      "Iteration 190, loss = 0.43490630\n",
      "Iteration 191, loss = 0.43402483\n",
      "Iteration 192, loss = 0.43314939\n",
      "Iteration 193, loss = 0.43227979\n",
      "Iteration 194, loss = 0.43141602\n",
      "Iteration 195, loss = 0.43055791\n",
      "Iteration 196, loss = 0.42970546\n",
      "Iteration 197, loss = 0.42885848\n",
      "Iteration 198, loss = 0.42801696\n",
      "Iteration 199, loss = 0.42718110\n",
      "Iteration 200, loss = 0.42635060\n",
      "Iteration 201, loss = 0.42552533\n",
      "Iteration 202, loss = 0.42470643\n",
      "Iteration 203, loss = 0.42389301\n",
      "Iteration 204, loss = 0.42308463\n",
      "Iteration 205, loss = 0.42228122\n",
      "Iteration 206, loss = 0.42148273\n",
      "Iteration 207, loss = 0.42068911\n",
      "Iteration 208, loss = 0.41990040\n",
      "Iteration 209, loss = 0.41911632\n",
      "Iteration 210, loss = 0.41833706\n",
      "Iteration 211, loss = 0.41756243\n",
      "Iteration 212, loss = 0.41679271\n",
      "Iteration 213, loss = 0.41602772\n",
      "Iteration 214, loss = 0.41526745\n",
      "Iteration 215, loss = 0.41451190\n",
      "Iteration 216, loss = 0.41376058\n",
      "Iteration 217, loss = 0.41301355\n",
      "Iteration 218, loss = 0.41227072\n",
      "Iteration 219, loss = 0.41153208\n",
      "Iteration 220, loss = 0.41079759\n",
      "Iteration 221, loss = 0.41006701\n",
      "Iteration 222, loss = 0.40934041\n",
      "Iteration 223, loss = 0.40861782\n",
      "Iteration 224, loss = 0.40789915\n",
      "Iteration 225, loss = 0.40718434\n",
      "Iteration 226, loss = 0.40647334\n",
      "Iteration 227, loss = 0.40576611\n",
      "Iteration 228, loss = 0.40506258\n",
      "Iteration 229, loss = 0.40436271\n",
      "Iteration 230, loss = 0.40366624\n",
      "Iteration 231, loss = 0.40297245\n",
      "Iteration 232, loss = 0.40228205\n",
      "Iteration 233, loss = 0.40159508\n",
      "Iteration 234, loss = 0.40091146\n",
      "Iteration 235, loss = 0.40023144\n",
      "Iteration 236, loss = 0.39955480\n",
      "Iteration 237, loss = 0.39888144\n",
      "Iteration 238, loss = 0.39821128\n",
      "Iteration 239, loss = 0.39754434\n",
      "Iteration 240, loss = 0.39688056\n",
      "Iteration 241, loss = 0.39621956\n",
      "Iteration 242, loss = 0.39556078\n",
      "Iteration 243, loss = 0.39490498\n",
      "Iteration 244, loss = 0.39425211\n",
      "Iteration 245, loss = 0.39360215\n",
      "Iteration 246, loss = 0.39295514\n",
      "Iteration 247, loss = 0.39231096\n",
      "Iteration 248, loss = 0.39166977\n",
      "Iteration 249, loss = 0.39103143\n",
      "Iteration 250, loss = 0.39039591\n",
      "Iteration 251, loss = 0.38976321\n",
      "Iteration 252, loss = 0.38913333\n",
      "Iteration 253, loss = 0.38850620\n",
      "Iteration 254, loss = 0.38788179\n",
      "Iteration 255, loss = 0.38726005\n",
      "Iteration 256, loss = 0.38664095\n",
      "Iteration 257, loss = 0.38602450\n",
      "Iteration 258, loss = 0.38541115\n",
      "Iteration 259, loss = 0.38480113\n",
      "Iteration 260, loss = 0.38419341\n",
      "Iteration 261, loss = 0.38358815\n",
      "Iteration 262, loss = 0.38298555\n",
      "Iteration 263, loss = 0.38238586\n",
      "Iteration 264, loss = 0.38178870\n",
      "Iteration 265, loss = 0.38119412\n",
      "Iteration 266, loss = 0.38060223\n",
      "Iteration 267, loss = 0.38001263\n",
      "Iteration 268, loss = 0.37942541\n",
      "Iteration 269, loss = 0.37883989\n",
      "Iteration 270, loss = 0.37825602\n",
      "Iteration 271, loss = 0.37767430\n",
      "Iteration 272, loss = 0.37709489\n",
      "Iteration 273, loss = 0.37651777\n",
      "Iteration 274, loss = 0.37594287\n",
      "Iteration 275, loss = 0.37537024\n",
      "Iteration 276, loss = 0.37479985\n",
      "Iteration 277, loss = 0.37423168\n",
      "Iteration 278, loss = 0.37366572\n",
      "Iteration 279, loss = 0.37310197\n",
      "Iteration 280, loss = 0.37254036\n",
      "Iteration 281, loss = 0.37198092\n",
      "Iteration 282, loss = 0.37142362\n",
      "Iteration 283, loss = 0.37086844\n",
      "Iteration 284, loss = 0.37031539\n",
      "Iteration 285, loss = 0.36976442\n",
      "Iteration 286, loss = 0.36921553\n",
      "Iteration 287, loss = 0.36866869\n",
      "Iteration 288, loss = 0.36812421\n",
      "Iteration 289, loss = 0.36758417\n",
      "Iteration 290, loss = 0.36704591\n",
      "Iteration 291, loss = 0.36650839\n",
      "Iteration 292, loss = 0.36597286\n",
      "Iteration 293, loss = 0.36543929\n",
      "Iteration 294, loss = 0.36490776\n",
      "Iteration 295, loss = 0.36437805\n",
      "Iteration 296, loss = 0.36385031\n",
      "Iteration 297, loss = 0.36332436\n",
      "Iteration 298, loss = 0.36279860\n",
      "Iteration 299, loss = 0.36227412\n",
      "Iteration 300, loss = 0.36174751\n",
      "Iteration 301, loss = 0.36121979\n",
      "Iteration 302, loss = 0.36069127\n",
      "Iteration 303, loss = 0.36016359\n",
      "Iteration 304, loss = 0.35963523\n",
      "Iteration 305, loss = 0.35910701\n",
      "Iteration 306, loss = 0.35857964\n",
      "Iteration 307, loss = 0.35805093\n",
      "Iteration 308, loss = 0.35752019\n",
      "Iteration 309, loss = 0.35698811\n",
      "Iteration 310, loss = 0.35646015\n",
      "Iteration 311, loss = 0.35593108\n",
      "Iteration 312, loss = 0.35540235\n",
      "Iteration 313, loss = 0.35487452\n",
      "Iteration 314, loss = 0.35434628\n",
      "Iteration 315, loss = 0.35381617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 316, loss = 0.35328595\n",
      "Iteration 317, loss = 0.35276014\n",
      "Iteration 318, loss = 0.35224130\n",
      "Iteration 319, loss = 0.35172034\n",
      "Iteration 320, loss = 0.35119907\n",
      "Iteration 321, loss = 0.35067489\n",
      "Iteration 322, loss = 0.35015180\n",
      "Iteration 323, loss = 0.34963351\n",
      "Iteration 324, loss = 0.34911838\n",
      "Iteration 325, loss = 0.34860442\n",
      "Iteration 326, loss = 0.34809056\n",
      "Iteration 327, loss = 0.34757759\n",
      "Iteration 328, loss = 0.34706230\n",
      "Iteration 329, loss = 0.34654866\n",
      "Iteration 330, loss = 0.34603226\n",
      "Iteration 331, loss = 0.34551784\n",
      "Iteration 332, loss = 0.34500360\n",
      "Iteration 333, loss = 0.34448663\n",
      "Iteration 334, loss = 0.34396311\n",
      "Iteration 335, loss = 0.34343291\n",
      "Iteration 336, loss = 0.34290578\n",
      "Iteration 337, loss = 0.34238595\n",
      "Iteration 338, loss = 0.34187627\n",
      "Iteration 339, loss = 0.34136858\n",
      "Iteration 340, loss = 0.34086241\n",
      "Iteration 341, loss = 0.34035861\n",
      "Iteration 342, loss = 0.33986496\n",
      "Iteration 343, loss = 0.33938739\n",
      "Iteration 344, loss = 0.33891172\n",
      "Iteration 345, loss = 0.33844287\n",
      "Iteration 346, loss = 0.33797744\n",
      "Iteration 347, loss = 0.33751687\n",
      "Iteration 348, loss = 0.33705886\n",
      "Iteration 349, loss = 0.33660465\n",
      "Iteration 350, loss = 0.33615363\n",
      "Iteration 351, loss = 0.33570469\n",
      "Iteration 352, loss = 0.33525931\n",
      "Iteration 353, loss = 0.33481700\n",
      "Iteration 354, loss = 0.33437709\n",
      "Iteration 355, loss = 0.33394004\n",
      "Iteration 356, loss = 0.33350585\n",
      "Iteration 357, loss = 0.33307365\n",
      "Iteration 358, loss = 0.33264285\n",
      "Iteration 359, loss = 0.33221360\n",
      "Iteration 360, loss = 0.33178564\n",
      "Iteration 361, loss = 0.33136048\n",
      "Iteration 362, loss = 0.33093788\n",
      "Iteration 363, loss = 0.33051681\n",
      "Iteration 364, loss = 0.33009680\n",
      "Iteration 365, loss = 0.32967903\n",
      "Iteration 366, loss = 0.32926242\n",
      "Iteration 367, loss = 0.32884685\n",
      "Iteration 368, loss = 0.32843245\n",
      "Iteration 369, loss = 0.32801912\n",
      "Iteration 370, loss = 0.32760697\n",
      "Iteration 371, loss = 0.32719598\n",
      "Iteration 372, loss = 0.32678596\n",
      "Iteration 373, loss = 0.32637710\n",
      "Iteration 374, loss = 0.32596932\n",
      "Iteration 375, loss = 0.32556253\n",
      "Iteration 376, loss = 0.32515689\n",
      "Iteration 377, loss = 0.32475230\n",
      "Iteration 378, loss = 0.32434876\n",
      "Iteration 379, loss = 0.32394639\n",
      "Iteration 380, loss = 0.32354494\n",
      "Iteration 381, loss = 0.32314457\n",
      "Iteration 382, loss = 0.32274531\n",
      "Iteration 383, loss = 0.32234707\n",
      "Iteration 384, loss = 0.32194979\n",
      "Iteration 385, loss = 0.32155359\n",
      "Iteration 386, loss = 0.32115838\n",
      "Iteration 387, loss = 0.32076430\n",
      "Iteration 388, loss = 0.32037108\n",
      "Iteration 389, loss = 0.31997889\n",
      "Iteration 390, loss = 0.31958774\n",
      "Iteration 391, loss = 0.31919764\n",
      "Iteration 392, loss = 0.31880858\n",
      "Iteration 393, loss = 0.31842043\n",
      "Iteration 394, loss = 0.31803331\n",
      "Iteration 395, loss = 0.31764715\n",
      "Iteration 396, loss = 0.31726194\n",
      "Iteration 397, loss = 0.31687778\n",
      "Iteration 398, loss = 0.31649457\n",
      "Iteration 399, loss = 0.31611236\n",
      "Iteration 400, loss = 0.31573102\n",
      "Iteration 401, loss = 0.31535063\n",
      "Iteration 402, loss = 0.31497122\n",
      "Iteration 403, loss = 0.31459294\n",
      "Iteration 404, loss = 0.31421562\n",
      "Iteration 405, loss = 0.31383919\n",
      "Iteration 406, loss = 0.31346370\n",
      "Iteration 407, loss = 0.31308910\n",
      "Iteration 408, loss = 0.31271538\n",
      "Iteration 409, loss = 0.31234257\n",
      "Iteration 410, loss = 0.31197080\n",
      "Iteration 411, loss = 0.31159997\n",
      "Iteration 412, loss = 0.31123008\n",
      "Iteration 413, loss = 0.31086100\n",
      "Iteration 414, loss = 0.31049288\n",
      "Iteration 415, loss = 0.31012560\n",
      "Iteration 416, loss = 0.30975919\n",
      "Iteration 417, loss = 0.30939371\n",
      "Iteration 418, loss = 0.30902904\n",
      "Iteration 419, loss = 0.30866540\n",
      "Iteration 420, loss = 0.30830276\n",
      "Iteration 421, loss = 0.30794109\n",
      "Iteration 422, loss = 0.30758034\n",
      "Iteration 423, loss = 0.30722039\n",
      "Iteration 424, loss = 0.30686141\n",
      "Iteration 425, loss = 0.30650331\n",
      "Iteration 426, loss = 0.30614615\n",
      "Iteration 427, loss = 0.30578973\n",
      "Iteration 428, loss = 0.30543430\n",
      "Iteration 429, loss = 0.30507966\n",
      "Iteration 430, loss = 0.30472584\n",
      "Iteration 431, loss = 0.30437285\n",
      "Iteration 432, loss = 0.30402069\n",
      "Iteration 433, loss = 0.30366936\n",
      "Iteration 434, loss = 0.30331883\n",
      "Iteration 435, loss = 0.30296911\n",
      "Iteration 436, loss = 0.30262024\n",
      "Iteration 437, loss = 0.30227211\n",
      "Iteration 438, loss = 0.30192479\n",
      "Iteration 439, loss = 0.30157830\n",
      "Iteration 440, loss = 0.30123257\n",
      "Iteration 441, loss = 0.30088762\n",
      "Iteration 442, loss = 0.30054351\n",
      "Iteration 443, loss = 0.30020010\n",
      "Iteration 444, loss = 0.29985752\n",
      "Iteration 445, loss = 0.29951567\n",
      "Iteration 446, loss = 0.29917465\n",
      "Iteration 447, loss = 0.29883432\n",
      "Iteration 448, loss = 0.29849481\n",
      "Iteration 449, loss = 0.29815603\n",
      "Iteration 450, loss = 0.29781800\n",
      "Iteration 451, loss = 0.29748076\n",
      "Iteration 452, loss = 0.29714426\n",
      "Iteration 453, loss = 0.29680848\n",
      "Iteration 454, loss = 0.29647347\n",
      "Iteration 455, loss = 0.29613917\n",
      "Iteration 456, loss = 0.29580565\n",
      "Iteration 457, loss = 0.29547282\n",
      "Iteration 458, loss = 0.29514079\n",
      "Iteration 459, loss = 0.29480959\n",
      "Iteration 460, loss = 0.29447915\n",
      "Iteration 461, loss = 0.29414969\n",
      "Iteration 462, loss = 0.29382113\n",
      "Iteration 463, loss = 0.29349336\n",
      "Iteration 464, loss = 0.29316625\n",
      "Iteration 465, loss = 0.29283987\n",
      "Iteration 466, loss = 0.29251417\n",
      "Iteration 467, loss = 0.29218929\n",
      "Iteration 468, loss = 0.29186501\n",
      "Iteration 469, loss = 0.29154148\n",
      "Iteration 470, loss = 0.29121871\n",
      "Iteration 471, loss = 0.29089661\n",
      "Iteration 472, loss = 0.29057520\n",
      "Iteration 473, loss = 0.29025451\n",
      "Iteration 474, loss = 0.28993452\n",
      "Iteration 475, loss = 0.28961521\n",
      "Iteration 476, loss = 0.28929661\n",
      "Iteration 477, loss = 0.28897866\n",
      "Iteration 478, loss = 0.28866151\n",
      "Iteration 479, loss = 0.28834492\n",
      "Iteration 480, loss = 0.28802905\n",
      "Iteration 481, loss = 0.28771409\n",
      "Iteration 482, loss = 0.28740005\n",
      "Iteration 483, loss = 0.28708650\n",
      "Iteration 484, loss = 0.28677370\n",
      "Iteration 485, loss = 0.28646155\n",
      "Iteration 486, loss = 0.28615013\n",
      "Iteration 487, loss = 0.28583934\n",
      "Iteration 488, loss = 0.28552920\n",
      "Iteration 489, loss = 0.28521973\n",
      "Iteration 490, loss = 0.28491089\n",
      "Iteration 491, loss = 0.28460284\n",
      "Iteration 492, loss = 0.28429528\n",
      "Iteration 493, loss = 0.28398850\n",
      "Iteration 494, loss = 0.28368234\n",
      "Iteration 495, loss = 0.28337696\n",
      "Iteration 496, loss = 0.28307203\n",
      "Iteration 497, loss = 0.28276785\n",
      "Iteration 498, loss = 0.28246430\n",
      "Iteration 499, loss = 0.28216141\n",
      "Iteration 500, loss = 0.28185911\n",
      "Iteration 501, loss = 0.28155766\n",
      "Iteration 502, loss = 0.28125657\n",
      "Iteration 503, loss = 0.28095626\n",
      "Iteration 504, loss = 0.28065653\n",
      "Iteration 505, loss = 0.28035744\n",
      "Iteration 506, loss = 0.28005920\n",
      "Iteration 507, loss = 0.27976161\n",
      "Iteration 508, loss = 0.27946460\n",
      "Iteration 509, loss = 0.27916821\n",
      "Iteration 510, loss = 0.27887249\n",
      "Iteration 511, loss = 0.27857733\n",
      "Iteration 512, loss = 0.27828284\n",
      "Iteration 513, loss = 0.27798895\n",
      "Iteration 514, loss = 0.27769567\n",
      "Iteration 515, loss = 0.27740292\n",
      "Iteration 516, loss = 0.27711066\n",
      "Iteration 517, loss = 0.27681900\n",
      "Iteration 518, loss = 0.27652799\n",
      "Iteration 519, loss = 0.27623749\n",
      "Iteration 520, loss = 0.27594760\n",
      "Iteration 521, loss = 0.27565830\n",
      "Iteration 522, loss = 0.27536960\n",
      "Iteration 523, loss = 0.27508152\n",
      "Iteration 524, loss = 0.27479420\n",
      "Iteration 525, loss = 0.27450743\n",
      "Iteration 526, loss = 0.27422132\n",
      "Iteration 527, loss = 0.27393582\n",
      "Iteration 528, loss = 0.27365094\n",
      "Iteration 529, loss = 0.27336660\n",
      "Iteration 530, loss = 0.27308286\n",
      "Iteration 531, loss = 0.27279972\n",
      "Iteration 532, loss = 0.27251714\n",
      "Iteration 533, loss = 0.27223519\n",
      "Iteration 534, loss = 0.27195376\n",
      "Iteration 535, loss = 0.27167292\n",
      "Iteration 536, loss = 0.27139266\n",
      "Iteration 537, loss = 0.27111293\n",
      "Iteration 538, loss = 0.27083392\n",
      "Iteration 539, loss = 0.27055530\n",
      "Iteration 540, loss = 0.27027734\n",
      "Iteration 541, loss = 0.26999995\n",
      "Iteration 542, loss = 0.26972314\n",
      "Iteration 543, loss = 0.26944686\n",
      "Iteration 544, loss = 0.26917117\n",
      "Iteration 545, loss = 0.26889601\n",
      "Iteration 546, loss = 0.26862141\n",
      "Iteration 547, loss = 0.26834743\n",
      "Iteration 548, loss = 0.26807395\n",
      "Iteration 549, loss = 0.26780105\n",
      "Iteration 550, loss = 0.26752869\n",
      "Iteration 551, loss = 0.26725690\n",
      "Iteration 552, loss = 0.26698566\n",
      "Iteration 553, loss = 0.26671498\n",
      "Iteration 554, loss = 0.26644483\n",
      "Iteration 555, loss = 0.26617524\n",
      "Iteration 556, loss = 0.26590619\n",
      "Iteration 557, loss = 0.26563767\n",
      "Iteration 558, loss = 0.26536981\n",
      "Iteration 559, loss = 0.26510248\n",
      "Iteration 560, loss = 0.26483572\n",
      "Iteration 561, loss = 0.26456955\n",
      "Iteration 562, loss = 0.26430388\n",
      "Iteration 563, loss = 0.26403876\n",
      "Iteration 564, loss = 0.26377419\n",
      "Iteration 565, loss = 0.26351015\n",
      "Iteration 566, loss = 0.26324669\n",
      "Iteration 567, loss = 0.26298371\n",
      "Iteration 568, loss = 0.26272127\n",
      "Iteration 569, loss = 0.26245935\n",
      "Iteration 570, loss = 0.26219802\n",
      "Iteration 571, loss = 0.26193717\n",
      "Iteration 572, loss = 0.26167686\n",
      "Iteration 573, loss = 0.26141709\n",
      "Iteration 574, loss = 0.26115791\n",
      "Iteration 575, loss = 0.26089944\n",
      "Iteration 576, loss = 0.26064156\n",
      "Iteration 577, loss = 0.26038417\n",
      "Iteration 578, loss = 0.26012735\n",
      "Iteration 579, loss = 0.25987103\n",
      "Iteration 580, loss = 0.25961528\n",
      "Iteration 581, loss = 0.25936006\n",
      "Iteration 582, loss = 0.25910532\n",
      "Iteration 583, loss = 0.25885114\n",
      "Iteration 584, loss = 0.25859740\n",
      "Iteration 585, loss = 0.25834433\n",
      "Iteration 586, loss = 0.25809162\n",
      "Iteration 587, loss = 0.25783951\n",
      "Iteration 588, loss = 0.25758789\n",
      "Iteration 589, loss = 0.25733673\n",
      "Iteration 590, loss = 0.25708616\n",
      "Iteration 591, loss = 0.25683607\n",
      "Iteration 592, loss = 0.25658654\n",
      "Iteration 593, loss = 0.25633749\n",
      "Iteration 594, loss = 0.25608897\n",
      "Iteration 595, loss = 0.25584097\n",
      "Iteration 596, loss = 0.25559345\n",
      "Iteration 597, loss = 0.25534643\n",
      "Iteration 598, loss = 0.25509990\n",
      "Iteration 599, loss = 0.25485389\n",
      "Iteration 600, loss = 0.25460837\n",
      "Iteration 601, loss = 0.25436334\n",
      "Iteration 602, loss = 0.25411877\n",
      "Iteration 603, loss = 0.25387474\n",
      "Iteration 604, loss = 0.25363112\n",
      "Iteration 605, loss = 0.25338810\n",
      "Iteration 606, loss = 0.25314556\n",
      "Iteration 607, loss = 0.25290343\n",
      "Iteration 608, loss = 0.25266183\n",
      "Iteration 609, loss = 0.25242069\n",
      "Iteration 610, loss = 0.25218011\n",
      "Iteration 611, loss = 0.25193992\n",
      "Iteration 612, loss = 0.25170030\n",
      "Iteration 613, loss = 0.25146118\n",
      "Iteration 614, loss = 0.25122259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 615, loss = 0.25098448\n",
      "Iteration 616, loss = 0.25074690\n",
      "Iteration 617, loss = 0.25050978\n",
      "Iteration 618, loss = 0.25027318\n",
      "Iteration 619, loss = 0.25003700\n",
      "Iteration 620, loss = 0.24980135\n",
      "Iteration 621, loss = 0.24956614\n",
      "Iteration 622, loss = 0.24933145\n",
      "Iteration 623, loss = 0.24909725\n",
      "Iteration 624, loss = 0.24886350\n",
      "Iteration 625, loss = 0.24863028\n",
      "Iteration 626, loss = 0.24839749\n",
      "Iteration 627, loss = 0.24816517\n",
      "Iteration 628, loss = 0.24793337\n",
      "Iteration 629, loss = 0.24770197\n",
      "Iteration 630, loss = 0.24747104\n",
      "Iteration 631, loss = 0.24724058\n",
      "Iteration 632, loss = 0.24701058\n",
      "Iteration 633, loss = 0.24678102\n",
      "Iteration 634, loss = 0.24655195\n",
      "Iteration 635, loss = 0.24632332\n",
      "Iteration 636, loss = 0.24609520\n",
      "Iteration 637, loss = 0.24586763\n",
      "Iteration 638, loss = 0.24564050\n",
      "Iteration 639, loss = 0.24541384\n",
      "Iteration 640, loss = 0.24518759\n",
      "Iteration 641, loss = 0.24496187\n",
      "Iteration 642, loss = 0.24473653\n",
      "Iteration 643, loss = 0.24451165\n",
      "Iteration 644, loss = 0.24428723\n",
      "Iteration 645, loss = 0.24406323\n",
      "Iteration 646, loss = 0.24383972\n",
      "Iteration 647, loss = 0.24361660\n",
      "Iteration 648, loss = 0.24339395\n",
      "Iteration 649, loss = 0.24317174\n",
      "Iteration 650, loss = 0.24294994\n",
      "Iteration 651, loss = 0.24272866\n",
      "Iteration 652, loss = 0.24250777\n",
      "Iteration 653, loss = 0.24228737\n",
      "Iteration 654, loss = 0.24206738\n",
      "Iteration 655, loss = 0.24184783\n",
      "Iteration 656, loss = 0.24162875\n",
      "Iteration 657, loss = 0.24141005\n",
      "Iteration 658, loss = 0.24119179\n",
      "Iteration 659, loss = 0.24097398\n",
      "Iteration 660, loss = 0.24075657\n",
      "Iteration 661, loss = 0.24053963\n",
      "Iteration 662, loss = 0.24032312\n",
      "Iteration 663, loss = 0.24010709\n",
      "Iteration 664, loss = 0.23989146\n",
      "Iteration 665, loss = 0.23967622\n",
      "Iteration 666, loss = 0.23946140\n",
      "Iteration 667, loss = 0.23924702\n",
      "Iteration 668, loss = 0.23903302\n",
      "Iteration 669, loss = 0.23881950\n",
      "Iteration 670, loss = 0.23860637\n",
      "Iteration 671, loss = 0.23839367\n",
      "Iteration 672, loss = 0.23818138\n",
      "Iteration 673, loss = 0.23796948\n",
      "Iteration 674, loss = 0.23775804\n",
      "Iteration 675, loss = 0.23754700\n",
      "Iteration 676, loss = 0.23733635\n",
      "Iteration 677, loss = 0.23712614\n",
      "Iteration 678, loss = 0.23691632\n",
      "Iteration 679, loss = 0.23670692\n",
      "Iteration 680, loss = 0.23649794\n",
      "Iteration 681, loss = 0.23628933\n",
      "Iteration 682, loss = 0.23608117\n",
      "Iteration 683, loss = 0.23587339\n",
      "Iteration 684, loss = 0.23566601\n",
      "Iteration 685, loss = 0.23545906\n",
      "Iteration 686, loss = 0.23525253\n",
      "Iteration 687, loss = 0.23504646\n",
      "Iteration 688, loss = 0.23484095\n",
      "Iteration 689, loss = 0.23463585\n",
      "Iteration 690, loss = 0.23443116\n",
      "Iteration 691, loss = 0.23422692\n",
      "Iteration 692, loss = 0.23402308\n",
      "Iteration 693, loss = 0.23381965\n",
      "Iteration 694, loss = 0.23361662\n",
      "Iteration 695, loss = 0.23341399\n",
      "Iteration 696, loss = 0.23321176\n",
      "Iteration 697, loss = 0.23300993\n",
      "Iteration 698, loss = 0.23280850\n",
      "Iteration 699, loss = 0.23260750\n",
      "Iteration 700, loss = 0.23240683\n",
      "Iteration 701, loss = 0.23220660\n",
      "Iteration 702, loss = 0.23200674\n",
      "Iteration 703, loss = 0.23180726\n",
      "Iteration 704, loss = 0.23160821\n",
      "Iteration 705, loss = 0.23140950\n",
      "Iteration 706, loss = 0.23121120\n",
      "Iteration 707, loss = 0.23101328\n",
      "Iteration 708, loss = 0.23081574\n",
      "Iteration 709, loss = 0.23061858\n",
      "Iteration 710, loss = 0.23042180\n",
      "Iteration 711, loss = 0.23022543\n",
      "Iteration 712, loss = 0.23002940\n",
      "Iteration 713, loss = 0.22983377\n",
      "Iteration 714, loss = 0.22963852\n",
      "Iteration 715, loss = 0.22944365\n",
      "Iteration 716, loss = 0.22924920\n",
      "Iteration 717, loss = 0.22905513\n",
      "Iteration 718, loss = 0.22886144\n",
      "Iteration 719, loss = 0.22866814\n",
      "Iteration 720, loss = 0.22847524\n",
      "Iteration 721, loss = 0.22828271\n",
      "Iteration 722, loss = 0.22809056\n",
      "Iteration 723, loss = 0.22789879\n",
      "Iteration 724, loss = 0.22770737\n",
      "Iteration 725, loss = 0.22751633\n",
      "Iteration 726, loss = 0.22732568\n",
      "Iteration 727, loss = 0.22713539\n",
      "Iteration 728, loss = 0.22694550\n",
      "Iteration 729, loss = 0.22675596\n",
      "Iteration 730, loss = 0.22656675\n",
      "Iteration 731, loss = 0.22637790\n",
      "Iteration 732, loss = 0.22618941\n",
      "Iteration 733, loss = 0.22600127\n",
      "Iteration 734, loss = 0.22581350\n",
      "Iteration 735, loss = 0.22562608\n",
      "Iteration 736, loss = 0.22543902\n",
      "Iteration 737, loss = 0.22525232\n",
      "Iteration 738, loss = 0.22506599\n",
      "Iteration 739, loss = 0.22488000\n",
      "Iteration 740, loss = 0.22469436\n",
      "Iteration 741, loss = 0.22450909\n",
      "Iteration 742, loss = 0.22432416\n",
      "Iteration 743, loss = 0.22413959\n",
      "Iteration 744, loss = 0.22395538\n",
      "Iteration 745, loss = 0.22377153\n",
      "Iteration 746, loss = 0.22358803\n",
      "Iteration 747, loss = 0.22340488\n",
      "Iteration 748, loss = 0.22322209\n",
      "Iteration 749, loss = 0.22303963\n",
      "Iteration 750, loss = 0.22285754\n",
      "Iteration 751, loss = 0.22267580\n",
      "Iteration 752, loss = 0.22249439\n",
      "Iteration 753, loss = 0.22231328\n",
      "Iteration 754, loss = 0.22213250\n",
      "Iteration 755, loss = 0.22195208\n",
      "Iteration 756, loss = 0.22177198\n",
      "Iteration 757, loss = 0.22159223\n",
      "Iteration 758, loss = 0.22141281\n",
      "Iteration 759, loss = 0.22123374\n",
      "Iteration 760, loss = 0.22105500\n",
      "Iteration 761, loss = 0.22087660\n",
      "Iteration 762, loss = 0.22069854\n",
      "Iteration 763, loss = 0.22052082\n",
      "Iteration 764, loss = 0.22034342\n",
      "Iteration 765, loss = 0.22016637\n",
      "Iteration 766, loss = 0.21998966\n",
      "Iteration 767, loss = 0.21981328\n",
      "Iteration 768, loss = 0.21963724\n",
      "Iteration 769, loss = 0.21946155\n",
      "Iteration 770, loss = 0.21928619\n",
      "Iteration 771, loss = 0.21911117\n",
      "Iteration 772, loss = 0.21893649\n",
      "Iteration 773, loss = 0.21876216\n",
      "Iteration 774, loss = 0.21858824\n",
      "Iteration 775, loss = 0.21841464\n",
      "Iteration 776, loss = 0.21824137\n",
      "Iteration 777, loss = 0.21806838\n",
      "Iteration 778, loss = 0.21789571\n",
      "Iteration 779, loss = 0.21772338\n",
      "Iteration 780, loss = 0.21755138\n",
      "Iteration 781, loss = 0.21737970\n",
      "Iteration 782, loss = 0.21720834\n",
      "Iteration 783, loss = 0.21703730\n",
      "Iteration 784, loss = 0.21686658\n",
      "Iteration 785, loss = 0.21669617\n",
      "Iteration 786, loss = 0.21652609\n",
      "Iteration 787, loss = 0.21635632\n",
      "Iteration 788, loss = 0.21618688\n",
      "Iteration 789, loss = 0.21601775\n",
      "Iteration 790, loss = 0.21584894\n",
      "Iteration 791, loss = 0.21568045\n",
      "Iteration 792, loss = 0.21551227\n",
      "Iteration 793, loss = 0.21534441\n",
      "Iteration 794, loss = 0.21517687\n",
      "Iteration 795, loss = 0.21500964\n",
      "Iteration 796, loss = 0.21484273\n",
      "Iteration 797, loss = 0.21467613\n",
      "Iteration 798, loss = 0.21450985\n",
      "Iteration 799, loss = 0.21434388\n",
      "Iteration 800, loss = 0.21417822\n",
      "Iteration 801, loss = 0.21401288\n",
      "Iteration 802, loss = 0.21384784\n",
      "Iteration 803, loss = 0.21368311\n",
      "Iteration 804, loss = 0.21351871\n",
      "Iteration 805, loss = 0.21335459\n",
      "Iteration 806, loss = 0.21319080\n",
      "Iteration 807, loss = 0.21302731\n",
      "Iteration 808, loss = 0.21286413\n",
      "Iteration 809, loss = 0.21270126\n",
      "Iteration 810, loss = 0.21253870\n",
      "Iteration 811, loss = 0.21237643\n",
      "Iteration 812, loss = 0.21221449\n",
      "Iteration 813, loss = 0.21205284\n",
      "Iteration 814, loss = 0.21189149\n",
      "Iteration 815, loss = 0.21173044\n",
      "Iteration 816, loss = 0.21156972\n",
      "Iteration 817, loss = 0.21140929\n",
      "Iteration 818, loss = 0.21124918\n",
      "Iteration 819, loss = 0.21108936\n",
      "Iteration 820, loss = 0.21092984\n",
      "Iteration 821, loss = 0.21077060\n",
      "Iteration 822, loss = 0.21061163\n",
      "Iteration 823, loss = 0.21045297\n",
      "Iteration 824, loss = 0.21029459\n",
      "Iteration 825, loss = 0.21013651\n",
      "Iteration 826, loss = 0.20997873\n",
      "Iteration 827, loss = 0.20982124\n",
      "Iteration 828, loss = 0.20966405\n",
      "Iteration 829, loss = 0.20950715\n",
      "Iteration 830, loss = 0.20935055\n",
      "Iteration 831, loss = 0.20919424\n",
      "Iteration 832, loss = 0.20903821\n",
      "Iteration 833, loss = 0.20888248\n",
      "Iteration 834, loss = 0.20872704\n",
      "Iteration 835, loss = 0.20857189\n",
      "Iteration 836, loss = 0.20841702\n",
      "Iteration 837, loss = 0.20826244\n",
      "Iteration 838, loss = 0.20810819\n",
      "Iteration 839, loss = 0.20795421\n",
      "Iteration 840, loss = 0.20780051\n",
      "Iteration 841, loss = 0.20764711\n",
      "Iteration 842, loss = 0.20749406\n",
      "Iteration 843, loss = 0.20734130\n",
      "Iteration 844, loss = 0.20718884\n",
      "Iteration 845, loss = 0.20703666\n",
      "Iteration 846, loss = 0.20688476\n",
      "Iteration 847, loss = 0.20673314\n",
      "Iteration 848, loss = 0.20658181\n",
      "Iteration 849, loss = 0.20643078\n",
      "Iteration 850, loss = 0.20628001\n",
      "Iteration 851, loss = 0.20612953\n",
      "Iteration 852, loss = 0.20597934\n",
      "Iteration 853, loss = 0.20582939\n",
      "Iteration 854, loss = 0.20567969\n",
      "Iteration 855, loss = 0.20553027\n",
      "Iteration 856, loss = 0.20538112\n",
      "Iteration 857, loss = 0.20523223\n",
      "Iteration 858, loss = 0.20508362\n",
      "Iteration 859, loss = 0.20493530\n",
      "Iteration 860, loss = 0.20478723\n",
      "Iteration 861, loss = 0.20463944\n",
      "Iteration 862, loss = 0.20449193\n",
      "Iteration 863, loss = 0.20434468\n",
      "Iteration 864, loss = 0.20419770\n",
      "Iteration 865, loss = 0.20405102\n",
      "Iteration 866, loss = 0.20390459\n",
      "Iteration 867, loss = 0.20375844\n",
      "Iteration 868, loss = 0.20361257\n",
      "Iteration 869, loss = 0.20346697\n",
      "Iteration 870, loss = 0.20332164\n",
      "Iteration 871, loss = 0.20317657\n",
      "Iteration 872, loss = 0.20303178\n",
      "Iteration 873, loss = 0.20288724\n",
      "Iteration 874, loss = 0.20274299\n",
      "Iteration 875, loss = 0.20259898\n",
      "Iteration 876, loss = 0.20245525\n",
      "Iteration 877, loss = 0.20231179\n",
      "Iteration 878, loss = 0.20216858\n",
      "Iteration 879, loss = 0.20202565\n",
      "Iteration 880, loss = 0.20188291\n",
      "Iteration 881, loss = 0.20174043\n",
      "Iteration 882, loss = 0.20159820\n",
      "Iteration 883, loss = 0.20145623\n",
      "Iteration 884, loss = 0.20131452\n",
      "Iteration 885, loss = 0.20117307\n",
      "Iteration 886, loss = 0.20103186\n",
      "Iteration 887, loss = 0.20089092\n",
      "Iteration 888, loss = 0.20075024\n",
      "Iteration 889, loss = 0.20060981\n",
      "Iteration 890, loss = 0.20046964\n",
      "Iteration 891, loss = 0.20032971\n",
      "Iteration 892, loss = 0.20019005\n",
      "Iteration 893, loss = 0.20005064\n",
      "Iteration 894, loss = 0.19991148\n",
      "Iteration 895, loss = 0.19977258\n",
      "Iteration 896, loss = 0.19963392\n",
      "Iteration 897, loss = 0.19949553\n",
      "Iteration 898, loss = 0.19935738\n",
      "Iteration 899, loss = 0.19921948\n",
      "Iteration 900, loss = 0.19908185\n",
      "Iteration 901, loss = 0.19894444\n",
      "Iteration 902, loss = 0.19880726\n",
      "Iteration 903, loss = 0.19867022\n",
      "Iteration 904, loss = 0.19853343\n",
      "Iteration 905, loss = 0.19839690\n",
      "Iteration 906, loss = 0.19826067\n",
      "Iteration 907, loss = 0.19812469\n",
      "Iteration 908, loss = 0.19798884\n",
      "Iteration 909, loss = 0.19785318\n",
      "Iteration 910, loss = 0.19771773\n",
      "Iteration 911, loss = 0.19758254\n",
      "Iteration 912, loss = 0.19744759\n",
      "Iteration 913, loss = 0.19731288\n",
      "Iteration 914, loss = 0.19717840\n",
      "Iteration 915, loss = 0.19704417\n",
      "Iteration 916, loss = 0.19691017\n",
      "Iteration 917, loss = 0.19677642\n",
      "Iteration 918, loss = 0.19664292\n",
      "Iteration 919, loss = 0.19650967\n",
      "Iteration 920, loss = 0.19637666\n",
      "Iteration 921, loss = 0.19624388\n",
      "Iteration 922, loss = 0.19611134\n",
      "Iteration 923, loss = 0.19597896\n",
      "Iteration 924, loss = 0.19584676\n",
      "Iteration 925, loss = 0.19571479\n",
      "Iteration 926, loss = 0.19558305\n",
      "Iteration 927, loss = 0.19545156\n",
      "Iteration 928, loss = 0.19532068\n",
      "Iteration 929, loss = 0.19519005\n",
      "Iteration 930, loss = 0.19505965\n",
      "Iteration 931, loss = 0.19492949\n",
      "Iteration 932, loss = 0.19479955\n",
      "Iteration 933, loss = 0.19466985\n",
      "Iteration 934, loss = 0.19454037\n",
      "Iteration 935, loss = 0.19441114\n",
      "Iteration 936, loss = 0.19428213\n",
      "Iteration 937, loss = 0.19415334\n",
      "Iteration 938, loss = 0.19402480\n",
      "Iteration 939, loss = 0.19389647\n",
      "Iteration 940, loss = 0.19376838\n",
      "Iteration 941, loss = 0.19364051\n",
      "Iteration 942, loss = 0.19351287\n",
      "Iteration 943, loss = 0.19338548\n",
      "Iteration 944, loss = 0.19325847\n",
      "Iteration 945, loss = 0.19313170\n",
      "Iteration 946, loss = 0.19300516\n",
      "Iteration 947, loss = 0.19287896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 948, loss = 0.19275311\n",
      "Iteration 949, loss = 0.19262750\n",
      "Iteration 950, loss = 0.19250212\n",
      "Iteration 951, loss = 0.19237699\n",
      "Iteration 952, loss = 0.19225211\n",
      "Iteration 953, loss = 0.19212746\n",
      "Iteration 954, loss = 0.19200304\n",
      "Iteration 955, loss = 0.19187883\n",
      "Iteration 956, loss = 0.19175480\n",
      "Iteration 957, loss = 0.19163095\n",
      "Iteration 958, loss = 0.19150733\n",
      "Iteration 959, loss = 0.19138390\n",
      "Iteration 960, loss = 0.19126070\n",
      "Iteration 961, loss = 0.19113771\n",
      "Iteration 962, loss = 0.19101493\n",
      "Iteration 963, loss = 0.19089235\n",
      "Iteration 964, loss = 0.19077001\n",
      "Iteration 965, loss = 0.19064794\n",
      "Iteration 966, loss = 0.19052609\n",
      "Iteration 967, loss = 0.19040444\n",
      "Iteration 968, loss = 0.19028299\n",
      "Iteration 969, loss = 0.19016175\n",
      "Iteration 970, loss = 0.19004075\n",
      "Iteration 971, loss = 0.18991996\n",
      "Iteration 972, loss = 0.18979940\n",
      "Iteration 973, loss = 0.18967904\n",
      "Iteration 974, loss = 0.18955889\n",
      "Iteration 975, loss = 0.18943896\n",
      "Iteration 976, loss = 0.18931924\n",
      "Iteration 977, loss = 0.18919974\n",
      "Iteration 978, loss = 0.18908045\n",
      "Iteration 979, loss = 0.18896136\n",
      "Iteration 980, loss = 0.18884250\n",
      "Iteration 981, loss = 0.18872383\n",
      "Iteration 982, loss = 0.18860538\n",
      "Iteration 983, loss = 0.18848715\n",
      "Iteration 984, loss = 0.18836911\n",
      "Iteration 985, loss = 0.18825129\n",
      "Iteration 986, loss = 0.18813367\n",
      "Iteration 987, loss = 0.18801627\n",
      "Iteration 988, loss = 0.18789907\n",
      "Iteration 989, loss = 0.18778207\n",
      "Iteration 990, loss = 0.18766529\n",
      "Iteration 991, loss = 0.18754871\n",
      "Iteration 992, loss = 0.18743236\n",
      "Iteration 993, loss = 0.18731621\n",
      "Iteration 994, loss = 0.18720027\n",
      "Iteration 995, loss = 0.18708453\n",
      "Iteration 996, loss = 0.18696898\n",
      "Iteration 997, loss = 0.18685367\n",
      "Iteration 998, loss = 0.18673854\n",
      "Iteration 999, loss = 0.18662363\n",
      "Iteration 1000, loss = 0.18650891\n",
      "Iteration 1, loss = 1.47705517\n",
      "Iteration 2, loss = 1.46378490\n",
      "Iteration 3, loss = 1.44514590\n",
      "Iteration 4, loss = 1.42199086\n",
      "Iteration 5, loss = 1.39519945\n",
      "Iteration 6, loss = 1.36565324\n",
      "Iteration 7, loss = 1.33418575\n",
      "Iteration 8, loss = 1.30176684\n",
      "Iteration 9, loss = 1.26927787\n",
      "Iteration 10, loss = 1.23773357\n",
      "Iteration 11, loss = 1.20769911\n",
      "Iteration 12, loss = 1.17951770\n",
      "Iteration 13, loss = 1.15366073\n",
      "Iteration 14, loss = 1.13024606\n",
      "Iteration 15, loss = 1.10941162\n",
      "Iteration 16, loss = 1.09115060\n",
      "Iteration 17, loss = 1.07497767\n",
      "Iteration 18, loss = 1.06031179\n",
      "Iteration 19, loss = 1.04705854\n",
      "Iteration 20, loss = 1.03492705\n",
      "Iteration 21, loss = 1.02338747\n",
      "Iteration 22, loss = 1.01198774\n",
      "Iteration 23, loss = 1.00046999\n",
      "Iteration 24, loss = 0.98885846\n",
      "Iteration 25, loss = 0.97712599\n",
      "Iteration 26, loss = 0.96531045\n",
      "Iteration 27, loss = 0.95340782\n",
      "Iteration 28, loss = 0.94151458\n",
      "Iteration 29, loss = 0.92982323\n",
      "Iteration 30, loss = 0.91830689\n",
      "Iteration 31, loss = 0.90699844\n",
      "Iteration 32, loss = 0.89598272\n",
      "Iteration 33, loss = 0.88526090\n",
      "Iteration 34, loss = 0.87491600\n",
      "Iteration 35, loss = 0.86492664\n",
      "Iteration 36, loss = 0.85526085\n",
      "Iteration 37, loss = 0.84593614\n",
      "Iteration 38, loss = 0.83698085\n",
      "Iteration 39, loss = 0.82829155\n",
      "Iteration 40, loss = 0.81980507\n",
      "Iteration 41, loss = 0.81152836\n",
      "Iteration 42, loss = 0.80347114\n",
      "Iteration 43, loss = 0.79572433\n",
      "Iteration 44, loss = 0.78828516\n",
      "Iteration 45, loss = 0.78107172\n",
      "Iteration 46, loss = 0.77411473\n",
      "Iteration 47, loss = 0.76741768\n",
      "Iteration 48, loss = 0.76088529\n",
      "Iteration 49, loss = 0.75452093\n",
      "Iteration 50, loss = 0.74836537\n",
      "Iteration 51, loss = 0.74238997\n",
      "Iteration 52, loss = 0.73658476\n",
      "Iteration 53, loss = 0.73093386\n",
      "Iteration 54, loss = 0.72540346\n",
      "Iteration 55, loss = 0.71998162\n",
      "Iteration 56, loss = 0.71467252\n",
      "Iteration 57, loss = 0.70947979\n",
      "Iteration 58, loss = 0.70443749\n",
      "Iteration 59, loss = 0.69954384\n",
      "Iteration 60, loss = 0.69480555\n",
      "Iteration 61, loss = 0.69021244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 62, loss = 0.68574865\n",
      "Iteration 63, loss = 0.68137200\n",
      "Iteration 64, loss = 0.67708366\n",
      "Iteration 65, loss = 0.67287560\n",
      "Iteration 66, loss = 0.66874710\n",
      "Iteration 67, loss = 0.66469599\n",
      "Iteration 68, loss = 0.66071892\n",
      "Iteration 69, loss = 0.65681359\n",
      "Iteration 70, loss = 0.65298225\n",
      "Iteration 71, loss = 0.64921776\n",
      "Iteration 72, loss = 0.64551761\n",
      "Iteration 73, loss = 0.64188409\n",
      "Iteration 74, loss = 0.63831867\n",
      "Iteration 75, loss = 0.63481501\n",
      "Iteration 76, loss = 0.63137642\n",
      "Iteration 77, loss = 0.62799885\n",
      "Iteration 78, loss = 0.62467929\n",
      "Iteration 79, loss = 0.62141375\n",
      "Iteration 80, loss = 0.61820553\n",
      "Iteration 81, loss = 0.61505523\n",
      "Iteration 82, loss = 0.61195412\n",
      "Iteration 83, loss = 0.60890072\n",
      "Iteration 84, loss = 0.60589579\n",
      "Iteration 85, loss = 0.60293784\n",
      "Iteration 86, loss = 0.60003122\n",
      "Iteration 87, loss = 0.59717143\n",
      "Iteration 88, loss = 0.59435225\n",
      "Iteration 89, loss = 0.59157320\n",
      "Iteration 90, loss = 0.58883258\n",
      "Iteration 91, loss = 0.58612751\n",
      "Iteration 92, loss = 0.58345961\n",
      "Iteration 93, loss = 0.58083487\n",
      "Iteration 94, loss = 0.57824354\n",
      "Iteration 95, loss = 0.57568143\n",
      "Iteration 96, loss = 0.57314659\n",
      "Iteration 97, loss = 0.57064984\n",
      "Iteration 98, loss = 0.56817761\n",
      "Iteration 99, loss = 0.56572663\n",
      "Iteration 100, loss = 0.56330742\n",
      "Iteration 101, loss = 0.56092647\n",
      "Iteration 102, loss = 0.55857503\n",
      "Iteration 103, loss = 0.55625519\n",
      "Iteration 104, loss = 0.55395752\n",
      "Iteration 105, loss = 0.55169741\n",
      "Iteration 106, loss = 0.54949913\n",
      "Iteration 107, loss = 0.54733431\n",
      "Iteration 108, loss = 0.54519219\n",
      "Iteration 109, loss = 0.54309259\n",
      "Iteration 110, loss = 0.54102482\n",
      "Iteration 111, loss = 0.53900706\n",
      "Iteration 112, loss = 0.53703187\n",
      "Iteration 113, loss = 0.53508658\n",
      "Iteration 114, loss = 0.53316175\n",
      "Iteration 115, loss = 0.53126736\n",
      "Iteration 116, loss = 0.52939451\n",
      "Iteration 117, loss = 0.52754393\n",
      "Iteration 118, loss = 0.52573015\n",
      "Iteration 119, loss = 0.52393578\n",
      "Iteration 120, loss = 0.52215977\n",
      "Iteration 121, loss = 0.52040166\n",
      "Iteration 122, loss = 0.51866167\n",
      "Iteration 123, loss = 0.51693899\n",
      "Iteration 124, loss = 0.51523531\n",
      "Iteration 125, loss = 0.51355111\n",
      "Iteration 126, loss = 0.51188678\n",
      "Iteration 127, loss = 0.51024677\n",
      "Iteration 128, loss = 0.50863020\n",
      "Iteration 129, loss = 0.50703349\n",
      "Iteration 130, loss = 0.50545497\n",
      "Iteration 131, loss = 0.50389367\n",
      "Iteration 132, loss = 0.50234920\n",
      "Iteration 133, loss = 0.50082120\n",
      "Iteration 134, loss = 0.49930951\n",
      "Iteration 135, loss = 0.49781396\n",
      "Iteration 136, loss = 0.49633469\n",
      "Iteration 137, loss = 0.49487407\n",
      "Iteration 138, loss = 0.49343025\n",
      "Iteration 139, loss = 0.49200206\n",
      "Iteration 140, loss = 0.49058897\n",
      "Iteration 141, loss = 0.48919046\n",
      "Iteration 142, loss = 0.48780657\n",
      "Iteration 143, loss = 0.48643679\n",
      "Iteration 144, loss = 0.48508080\n",
      "Iteration 145, loss = 0.48373840\n",
      "Iteration 146, loss = 0.48240935\n",
      "Iteration 147, loss = 0.48109339\n",
      "Iteration 148, loss = 0.47979079\n",
      "Iteration 149, loss = 0.47850090\n",
      "Iteration 150, loss = 0.47722336\n",
      "Iteration 151, loss = 0.47595791\n",
      "Iteration 152, loss = 0.47470503\n",
      "Iteration 153, loss = 0.47346448\n",
      "Iteration 154, loss = 0.47223553\n",
      "Iteration 155, loss = 0.47101795\n",
      "Iteration 156, loss = 0.46981184\n",
      "Iteration 157, loss = 0.46861720\n",
      "Iteration 158, loss = 0.46743343\n",
      "Iteration 159, loss = 0.46626020\n",
      "Iteration 160, loss = 0.46509748\n",
      "Iteration 161, loss = 0.46394495\n",
      "Iteration 162, loss = 0.46280273\n",
      "Iteration 163, loss = 0.46167041\n",
      "Iteration 164, loss = 0.46054781\n",
      "Iteration 165, loss = 0.45943474\n",
      "Iteration 166, loss = 0.45833105\n",
      "Iteration 167, loss = 0.45723668\n",
      "Iteration 168, loss = 0.45615149\n",
      "Iteration 169, loss = 0.45507529\n",
      "Iteration 170, loss = 0.45400842\n",
      "Iteration 171, loss = 0.45295036\n",
      "Iteration 172, loss = 0.45190093\n",
      "Iteration 173, loss = 0.45085976\n",
      "Iteration 174, loss = 0.44982673\n",
      "Iteration 175, loss = 0.44880190\n",
      "Iteration 176, loss = 0.44778506\n",
      "Iteration 177, loss = 0.44677620\n",
      "Iteration 178, loss = 0.44577467\n",
      "Iteration 179, loss = 0.44478010\n",
      "Iteration 180, loss = 0.44379342\n",
      "Iteration 181, loss = 0.44281388\n",
      "Iteration 182, loss = 0.44184122\n",
      "Iteration 183, loss = 0.44087579\n",
      "Iteration 184, loss = 0.43991782\n",
      "Iteration 185, loss = 0.43896668\n",
      "Iteration 186, loss = 0.43802227\n",
      "Iteration 187, loss = 0.43708500\n",
      "Iteration 188, loss = 0.43615429\n",
      "Iteration 189, loss = 0.43523008\n",
      "Iteration 190, loss = 0.43431235\n",
      "Iteration 191, loss = 0.43340073\n",
      "Iteration 192, loss = 0.43249540\n",
      "Iteration 193, loss = 0.43159623\n",
      "Iteration 194, loss = 0.43070313\n",
      "Iteration 195, loss = 0.42981596\n",
      "Iteration 196, loss = 0.42893462\n",
      "Iteration 197, loss = 0.42805937\n",
      "Iteration 198, loss = 0.42718996\n",
      "Iteration 199, loss = 0.42632614\n",
      "Iteration 200, loss = 0.42546781\n",
      "Iteration 201, loss = 0.42461491\n",
      "Iteration 202, loss = 0.42376733\n",
      "Iteration 203, loss = 0.42292500\n",
      "Iteration 204, loss = 0.42208783\n",
      "Iteration 205, loss = 0.42125575\n",
      "Iteration 206, loss = 0.42042867\n",
      "Iteration 207, loss = 0.41960656\n",
      "Iteration 208, loss = 0.41878945\n",
      "Iteration 209, loss = 0.41797708\n",
      "Iteration 210, loss = 0.41716939\n",
      "Iteration 211, loss = 0.41636631\n",
      "Iteration 212, loss = 0.41556777\n",
      "Iteration 213, loss = 0.41477382\n",
      "Iteration 214, loss = 0.41398523\n",
      "Iteration 215, loss = 0.41320088\n",
      "Iteration 216, loss = 0.41242085\n",
      "Iteration 217, loss = 0.41164511\n",
      "Iteration 218, loss = 0.41087404\n",
      "Iteration 219, loss = 0.41010715\n",
      "Iteration 220, loss = 0.40934446\n",
      "Iteration 221, loss = 0.40858593\n",
      "Iteration 222, loss = 0.40783137\n",
      "Iteration 223, loss = 0.40708086\n",
      "Iteration 224, loss = 0.40633432\n",
      "Iteration 225, loss = 0.40559187\n",
      "Iteration 226, loss = 0.40485343\n",
      "Iteration 227, loss = 0.40411869\n",
      "Iteration 228, loss = 0.40338794\n",
      "Iteration 229, loss = 0.40266113\n",
      "Iteration 230, loss = 0.40193831\n",
      "Iteration 231, loss = 0.40121918\n",
      "Iteration 232, loss = 0.40050378\n",
      "Iteration 233, loss = 0.39979195\n",
      "Iteration 234, loss = 0.39908367\n",
      "Iteration 235, loss = 0.39837898\n",
      "Iteration 236, loss = 0.39767769\n",
      "Iteration 237, loss = 0.39697989\n",
      "Iteration 238, loss = 0.39628550\n",
      "Iteration 239, loss = 0.39559450\n",
      "Iteration 240, loss = 0.39490676\n",
      "Iteration 241, loss = 0.39422235\n",
      "Iteration 242, loss = 0.39354117\n",
      "Iteration 243, loss = 0.39286318\n",
      "Iteration 244, loss = 0.39218836\n",
      "Iteration 245, loss = 0.39151658\n",
      "Iteration 246, loss = 0.39084788\n",
      "Iteration 247, loss = 0.39018247\n",
      "Iteration 248, loss = 0.38952013\n",
      "Iteration 249, loss = 0.38886071\n",
      "Iteration 250, loss = 0.38820346\n",
      "Iteration 251, loss = 0.38754910\n",
      "Iteration 252, loss = 0.38689790\n",
      "Iteration 253, loss = 0.38624981\n",
      "Iteration 254, loss = 0.38560448\n",
      "Iteration 255, loss = 0.38496209\n",
      "Iteration 256, loss = 0.38432233\n",
      "Iteration 257, loss = 0.38368533\n",
      "Iteration 258, loss = 0.38305104\n",
      "Iteration 259, loss = 0.38241945\n",
      "Iteration 260, loss = 0.38179044\n",
      "Iteration 261, loss = 0.38116310\n",
      "Iteration 262, loss = 0.38053858\n",
      "Iteration 263, loss = 0.37991606\n",
      "Iteration 264, loss = 0.37929505\n",
      "Iteration 265, loss = 0.37867635\n",
      "Iteration 266, loss = 0.37805988\n",
      "Iteration 267, loss = 0.37744586\n",
      "Iteration 268, loss = 0.37683419\n",
      "Iteration 269, loss = 0.37622490\n",
      "Iteration 270, loss = 0.37561802\n",
      "Iteration 271, loss = 0.37501331\n",
      "Iteration 272, loss = 0.37441100\n",
      "Iteration 273, loss = 0.37381088\n",
      "Iteration 274, loss = 0.37321312\n",
      "Iteration 275, loss = 0.37261691\n",
      "Iteration 276, loss = 0.37202231\n",
      "Iteration 277, loss = 0.37142914\n",
      "Iteration 278, loss = 0.37083791\n",
      "Iteration 279, loss = 0.37024865\n",
      "Iteration 280, loss = 0.36966155\n",
      "Iteration 281, loss = 0.36907638\n",
      "Iteration 282, loss = 0.36849326\n",
      "Iteration 283, loss = 0.36791243\n",
      "Iteration 284, loss = 0.36733329\n",
      "Iteration 285, loss = 0.36675654\n",
      "Iteration 286, loss = 0.36618172\n",
      "Iteration 287, loss = 0.36560900\n",
      "Iteration 288, loss = 0.36503824\n",
      "Iteration 289, loss = 0.36446962\n",
      "Iteration 290, loss = 0.36390308\n",
      "Iteration 291, loss = 0.36333761\n",
      "Iteration 292, loss = 0.36277379\n",
      "Iteration 293, loss = 0.36221194\n",
      "Iteration 294, loss = 0.36165205\n",
      "Iteration 295, loss = 0.36109733\n",
      "Iteration 296, loss = 0.36054411\n",
      "Iteration 297, loss = 0.35999102\n",
      "Iteration 298, loss = 0.35943974\n",
      "Iteration 299, loss = 0.35889039\n",
      "Iteration 300, loss = 0.35834263\n",
      "Iteration 301, loss = 0.35779508\n",
      "Iteration 302, loss = 0.35724755\n",
      "Iteration 303, loss = 0.35669816\n",
      "Iteration 304, loss = 0.35614433\n",
      "Iteration 305, loss = 0.35559011\n",
      "Iteration 306, loss = 0.35503657\n",
      "Iteration 307, loss = 0.35448375\n",
      "Iteration 308, loss = 0.35393144\n",
      "Iteration 309, loss = 0.35337850\n",
      "Iteration 310, loss = 0.35282373\n",
      "Iteration 311, loss = 0.35227089\n",
      "Iteration 312, loss = 0.35171819\n",
      "Iteration 313, loss = 0.35116653\n",
      "Iteration 314, loss = 0.35061535\n",
      "Iteration 315, loss = 0.35006260\n",
      "Iteration 316, loss = 0.34950860\n",
      "Iteration 317, loss = 0.34895547\n",
      "Iteration 318, loss = 0.34840873\n",
      "Iteration 319, loss = 0.34786407\n",
      "Iteration 320, loss = 0.34731906\n",
      "Iteration 321, loss = 0.34677312\n",
      "Iteration 322, loss = 0.34623124\n",
      "Iteration 323, loss = 0.34569474\n",
      "Iteration 324, loss = 0.34516143\n",
      "Iteration 325, loss = 0.34462887\n",
      "Iteration 326, loss = 0.34409750\n",
      "Iteration 327, loss = 0.34356764\n",
      "Iteration 328, loss = 0.34303536\n",
      "Iteration 329, loss = 0.34250196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 330, loss = 0.34196852\n",
      "Iteration 331, loss = 0.34143042\n",
      "Iteration 332, loss = 0.34089163\n",
      "Iteration 333, loss = 0.34034855\n",
      "Iteration 334, loss = 0.33979865\n",
      "Iteration 335, loss = 0.33924330\n",
      "Iteration 336, loss = 0.33868675\n",
      "Iteration 337, loss = 0.33813855\n",
      "Iteration 338, loss = 0.33759602\n",
      "Iteration 339, loss = 0.33704823\n",
      "Iteration 340, loss = 0.33650858\n",
      "Iteration 341, loss = 0.33597376\n",
      "Iteration 342, loss = 0.33545367\n",
      "Iteration 343, loss = 0.33494200\n",
      "Iteration 344, loss = 0.33443852\n",
      "Iteration 345, loss = 0.33393931\n",
      "Iteration 346, loss = 0.33344291\n",
      "Iteration 347, loss = 0.33295281\n",
      "Iteration 348, loss = 0.33247169\n",
      "Iteration 349, loss = 0.33200004\n",
      "Iteration 350, loss = 0.33153599\n",
      "Iteration 351, loss = 0.33107521\n",
      "Iteration 352, loss = 0.33061750\n",
      "Iteration 353, loss = 0.33016309\n",
      "Iteration 354, loss = 0.32971141\n",
      "Iteration 355, loss = 0.32926116\n",
      "Iteration 356, loss = 0.32881232\n",
      "Iteration 357, loss = 0.32836536\n",
      "Iteration 358, loss = 0.32792188\n",
      "Iteration 359, loss = 0.32748008\n",
      "Iteration 360, loss = 0.32703997\n",
      "Iteration 361, loss = 0.32660082\n",
      "Iteration 362, loss = 0.32616259\n",
      "Iteration 363, loss = 0.32572537\n",
      "Iteration 364, loss = 0.32528916\n",
      "Iteration 365, loss = 0.32485400\n",
      "Iteration 366, loss = 0.32441989\n",
      "Iteration 367, loss = 0.32398683\n",
      "Iteration 368, loss = 0.32355483\n",
      "Iteration 369, loss = 0.32312392\n",
      "Iteration 370, loss = 0.32269418\n",
      "Iteration 371, loss = 0.32226577\n",
      "Iteration 372, loss = 0.32183834\n",
      "Iteration 373, loss = 0.32141206\n",
      "Iteration 374, loss = 0.32098683\n",
      "Iteration 375, loss = 0.32056268\n",
      "Iteration 376, loss = 0.32013960\n",
      "Iteration 377, loss = 0.31971760\n",
      "Iteration 378, loss = 0.31929673\n",
      "Iteration 379, loss = 0.31887692\n",
      "Iteration 380, loss = 0.31845819\n",
      "Iteration 381, loss = 0.31804055\n",
      "Iteration 382, loss = 0.31762392\n",
      "Iteration 383, loss = 0.31720840\n",
      "Iteration 384, loss = 0.31679389\n",
      "Iteration 385, loss = 0.31638044\n",
      "Iteration 386, loss = 0.31596803\n",
      "Iteration 387, loss = 0.31555668\n",
      "Iteration 388, loss = 0.31514663\n",
      "Iteration 389, loss = 0.31473783\n",
      "Iteration 390, loss = 0.31433005\n",
      "Iteration 391, loss = 0.31392339\n",
      "Iteration 392, loss = 0.31351785\n",
      "Iteration 393, loss = 0.31311334\n",
      "Iteration 394, loss = 0.31270986\n",
      "Iteration 395, loss = 0.31230739\n",
      "Iteration 396, loss = 0.31190608\n",
      "Iteration 397, loss = 0.31150600\n",
      "Iteration 398, loss = 0.31110701\n",
      "Iteration 399, loss = 0.31070892\n",
      "Iteration 400, loss = 0.31031173\n",
      "Iteration 401, loss = 0.30991546\n",
      "Iteration 402, loss = 0.30952014\n",
      "Iteration 403, loss = 0.30912572\n",
      "Iteration 404, loss = 0.30873235\n",
      "Iteration 405, loss = 0.30834002\n",
      "Iteration 406, loss = 0.30794865\n",
      "Iteration 407, loss = 0.30755821\n",
      "Iteration 408, loss = 0.30716873\n",
      "Iteration 409, loss = 0.30678015\n",
      "Iteration 410, loss = 0.30639254\n",
      "Iteration 411, loss = 0.30600631\n",
      "Iteration 412, loss = 0.30562091\n",
      "Iteration 413, loss = 0.30523641\n",
      "Iteration 414, loss = 0.30485289\n",
      "Iteration 415, loss = 0.30446988\n",
      "Iteration 416, loss = 0.30408809\n",
      "Iteration 417, loss = 0.30370731\n",
      "Iteration 418, loss = 0.30332724\n",
      "Iteration 419, loss = 0.30294808\n",
      "Iteration 420, loss = 0.30256977\n",
      "Iteration 421, loss = 0.30219235\n",
      "Iteration 422, loss = 0.30181604\n",
      "Iteration 423, loss = 0.30144061\n",
      "Iteration 424, loss = 0.30106604\n",
      "Iteration 425, loss = 0.30069233\n",
      "Iteration 426, loss = 0.30031944\n",
      "Iteration 427, loss = 0.29994741\n",
      "Iteration 428, loss = 0.29957627\n",
      "Iteration 429, loss = 0.29920607\n",
      "Iteration 430, loss = 0.29883672\n",
      "Iteration 431, loss = 0.29846845\n",
      "Iteration 432, loss = 0.29810102\n",
      "Iteration 433, loss = 0.29773442\n",
      "Iteration 434, loss = 0.29736878\n",
      "Iteration 435, loss = 0.29700384\n",
      "Iteration 436, loss = 0.29663989\n",
      "Iteration 437, loss = 0.29627676\n",
      "Iteration 438, loss = 0.29591444\n",
      "Iteration 439, loss = 0.29555301\n",
      "Iteration 440, loss = 0.29519233\n",
      "Iteration 441, loss = 0.29483252\n",
      "Iteration 442, loss = 0.29447355\n",
      "Iteration 443, loss = 0.29411536\n",
      "Iteration 444, loss = 0.29375804\n",
      "Iteration 445, loss = 0.29340149\n",
      "Iteration 446, loss = 0.29304576\n",
      "Iteration 447, loss = 0.29269097\n",
      "Iteration 448, loss = 0.29233682\n",
      "Iteration 449, loss = 0.29198355\n",
      "Iteration 450, loss = 0.29163111\n",
      "Iteration 451, loss = 0.29127944\n",
      "Iteration 452, loss = 0.29092856\n",
      "Iteration 453, loss = 0.29057848\n",
      "Iteration 454, loss = 0.29022920\n",
      "Iteration 455, loss = 0.28988067\n",
      "Iteration 456, loss = 0.28953295\n",
      "Iteration 457, loss = 0.28918601\n",
      "Iteration 458, loss = 0.28883983\n",
      "Iteration 459, loss = 0.28849442\n",
      "Iteration 460, loss = 0.28814981\n",
      "Iteration 461, loss = 0.28780595\n",
      "Iteration 462, loss = 0.28746281\n",
      "Iteration 463, loss = 0.28712047\n",
      "Iteration 464, loss = 0.28677889\n",
      "Iteration 465, loss = 0.28643809\n",
      "Iteration 466, loss = 0.28609798\n",
      "Iteration 467, loss = 0.28575866\n",
      "Iteration 468, loss = 0.28542008\n",
      "Iteration 469, loss = 0.28508244\n",
      "Iteration 470, loss = 0.28474550\n",
      "Iteration 471, loss = 0.28440936\n",
      "Iteration 472, loss = 0.28407401\n",
      "Iteration 473, loss = 0.28373936\n",
      "Iteration 474, loss = 0.28340552\n",
      "Iteration 475, loss = 0.28307233\n",
      "Iteration 476, loss = 0.28274007\n",
      "Iteration 477, loss = 0.28240847\n",
      "Iteration 478, loss = 0.28207772\n",
      "Iteration 479, loss = 0.28174773\n",
      "Iteration 480, loss = 0.28141853\n",
      "Iteration 481, loss = 0.28109008\n",
      "Iteration 482, loss = 0.28076236\n",
      "Iteration 483, loss = 0.28043536\n",
      "Iteration 484, loss = 0.28010911\n",
      "Iteration 485, loss = 0.27978357\n",
      "Iteration 486, loss = 0.27945874\n",
      "Iteration 487, loss = 0.27913463\n",
      "Iteration 488, loss = 0.27881123\n",
      "Iteration 489, loss = 0.27848856\n",
      "Iteration 490, loss = 0.27816657\n",
      "Iteration 491, loss = 0.27784530\n",
      "Iteration 492, loss = 0.27752473\n",
      "Iteration 493, loss = 0.27720486\n",
      "Iteration 494, loss = 0.27688569\n",
      "Iteration 495, loss = 0.27656724\n",
      "Iteration 496, loss = 0.27624946\n",
      "Iteration 497, loss = 0.27593236\n",
      "Iteration 498, loss = 0.27561596\n",
      "Iteration 499, loss = 0.27530023\n",
      "Iteration 500, loss = 0.27498519\n",
      "Iteration 501, loss = 0.27467086\n",
      "Iteration 502, loss = 0.27435716\n",
      "Iteration 503, loss = 0.27404414\n",
      "Iteration 504, loss = 0.27373180\n",
      "Iteration 505, loss = 0.27342025\n",
      "Iteration 506, loss = 0.27310941\n",
      "Iteration 507, loss = 0.27279924\n",
      "Iteration 508, loss = 0.27248973\n",
      "Iteration 509, loss = 0.27218091\n",
      "Iteration 510, loss = 0.27187272\n",
      "Iteration 511, loss = 0.27156522\n",
      "Iteration 512, loss = 0.27125837\n",
      "Iteration 513, loss = 0.27095219\n",
      "Iteration 514, loss = 0.27064666\n",
      "Iteration 515, loss = 0.27034177\n",
      "Iteration 516, loss = 0.27003755\n",
      "Iteration 517, loss = 0.26973396\n",
      "Iteration 518, loss = 0.26943103\n",
      "Iteration 519, loss = 0.26912874\n",
      "Iteration 520, loss = 0.26882709\n",
      "Iteration 521, loss = 0.26852610\n",
      "Iteration 522, loss = 0.26822571\n",
      "Iteration 523, loss = 0.26792600\n",
      "Iteration 524, loss = 0.26762689\n",
      "Iteration 525, loss = 0.26732842\n",
      "Iteration 526, loss = 0.26703059\n",
      "Iteration 527, loss = 0.26673337\n",
      "Iteration 528, loss = 0.26643678\n",
      "Iteration 529, loss = 0.26614097\n",
      "Iteration 530, loss = 0.26584590\n",
      "Iteration 531, loss = 0.26555144\n",
      "Iteration 532, loss = 0.26525768\n",
      "Iteration 533, loss = 0.26496460\n",
      "Iteration 534, loss = 0.26467217\n",
      "Iteration 535, loss = 0.26438035\n",
      "Iteration 536, loss = 0.26408916\n",
      "Iteration 537, loss = 0.26379859\n",
      "Iteration 538, loss = 0.26350866\n",
      "Iteration 539, loss = 0.26321932\n",
      "Iteration 540, loss = 0.26293060\n",
      "Iteration 541, loss = 0.26264248\n",
      "Iteration 542, loss = 0.26235496\n",
      "Iteration 543, loss = 0.26206804\n",
      "Iteration 544, loss = 0.26178173\n",
      "Iteration 545, loss = 0.26149601\n",
      "Iteration 546, loss = 0.26121089\n",
      "Iteration 547, loss = 0.26092639\n",
      "Iteration 548, loss = 0.26064261\n",
      "Iteration 549, loss = 0.26035945\n",
      "Iteration 550, loss = 0.26007689\n",
      "Iteration 551, loss = 0.25979495\n",
      "Iteration 552, loss = 0.25951358\n",
      "Iteration 553, loss = 0.25923282\n",
      "Iteration 554, loss = 0.25895266\n",
      "Iteration 555, loss = 0.25867308\n",
      "Iteration 556, loss = 0.25839407\n",
      "Iteration 557, loss = 0.25811568\n",
      "Iteration 558, loss = 0.25783783\n",
      "Iteration 559, loss = 0.25756060\n",
      "Iteration 560, loss = 0.25728399\n",
      "Iteration 561, loss = 0.25700799\n",
      "Iteration 562, loss = 0.25673253\n",
      "Iteration 563, loss = 0.25645767\n",
      "Iteration 564, loss = 0.25618336\n",
      "Iteration 565, loss = 0.25590965\n",
      "Iteration 566, loss = 0.25563649\n",
      "Iteration 567, loss = 0.25536391\n",
      "Iteration 568, loss = 0.25509190\n",
      "Iteration 569, loss = 0.25482056\n",
      "Iteration 570, loss = 0.25454979\n",
      "Iteration 571, loss = 0.25427959\n",
      "Iteration 572, loss = 0.25400995\n",
      "Iteration 573, loss = 0.25374087\n",
      "Iteration 574, loss = 0.25347236\n",
      "Iteration 575, loss = 0.25320440\n",
      "Iteration 576, loss = 0.25293699\n",
      "Iteration 577, loss = 0.25267019\n",
      "Iteration 578, loss = 0.25240394\n",
      "Iteration 579, loss = 0.25213823\n",
      "Iteration 580, loss = 0.25187307\n",
      "Iteration 581, loss = 0.25160848\n",
      "Iteration 582, loss = 0.25134447\n",
      "Iteration 583, loss = 0.25108101\n",
      "Iteration 584, loss = 0.25081812\n",
      "Iteration 585, loss = 0.25055575\n",
      "Iteration 586, loss = 0.25029394\n",
      "Iteration 587, loss = 0.25003267\n",
      "Iteration 588, loss = 0.24977192\n",
      "Iteration 589, loss = 0.24951171\n",
      "Iteration 590, loss = 0.24925205\n",
      "Iteration 591, loss = 0.24899290\n",
      "Iteration 592, loss = 0.24873432\n",
      "Iteration 593, loss = 0.24847624\n",
      "Iteration 594, loss = 0.24821870\n",
      "Iteration 595, loss = 0.24796168\n",
      "Iteration 596, loss = 0.24770520\n",
      "Iteration 597, loss = 0.24744923\n",
      "Iteration 598, loss = 0.24719380\n",
      "Iteration 599, loss = 0.24693890\n",
      "Iteration 600, loss = 0.24668453\n",
      "Iteration 601, loss = 0.24643068\n",
      "Iteration 602, loss = 0.24617734\n",
      "Iteration 603, loss = 0.24592452\n",
      "Iteration 604, loss = 0.24567222\n",
      "Iteration 605, loss = 0.24542043\n",
      "Iteration 606, loss = 0.24516916\n",
      "Iteration 607, loss = 0.24491841\n",
      "Iteration 608, loss = 0.24466816\n",
      "Iteration 609, loss = 0.24441842\n",
      "Iteration 610, loss = 0.24416919\n",
      "Iteration 611, loss = 0.24392046\n",
      "Iteration 612, loss = 0.24367224\n",
      "Iteration 613, loss = 0.24342463\n",
      "Iteration 614, loss = 0.24317775\n",
      "Iteration 615, loss = 0.24293140\n",
      "Iteration 616, loss = 0.24268555\n",
      "Iteration 617, loss = 0.24244021\n",
      "Iteration 618, loss = 0.24219538\n",
      "Iteration 619, loss = 0.24195123\n",
      "Iteration 620, loss = 0.24170756\n",
      "Iteration 621, loss = 0.24146439\n",
      "Iteration 622, loss = 0.24122170\n",
      "Iteration 623, loss = 0.24097950\n",
      "Iteration 624, loss = 0.24073793\n",
      "Iteration 625, loss = 0.24049681\n",
      "Iteration 626, loss = 0.24025611\n",
      "Iteration 627, loss = 0.24001585\n",
      "Iteration 628, loss = 0.23977631\n",
      "Iteration 629, loss = 0.23953723\n",
      "Iteration 630, loss = 0.23929867\n",
      "Iteration 631, loss = 0.23906057\n",
      "Iteration 632, loss = 0.23882292\n",
      "Iteration 633, loss = 0.23858577\n",
      "Iteration 634, loss = 0.23834908\n",
      "Iteration 635, loss = 0.23811295\n",
      "Iteration 636, loss = 0.23787726\n",
      "Iteration 637, loss = 0.23764198\n",
      "Iteration 638, loss = 0.23740715\n",
      "Iteration 639, loss = 0.23717278\n",
      "Iteration 640, loss = 0.23693899\n",
      "Iteration 641, loss = 0.23670554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 642, loss = 0.23647260\n",
      "Iteration 643, loss = 0.23624016\n",
      "Iteration 644, loss = 0.23600820\n",
      "Iteration 645, loss = 0.23577673\n",
      "Iteration 646, loss = 0.23554574\n",
      "Iteration 647, loss = 0.23531520\n",
      "Iteration 648, loss = 0.23508513\n",
      "Iteration 649, loss = 0.23485559\n",
      "Iteration 650, loss = 0.23462646\n",
      "Iteration 651, loss = 0.23439779\n",
      "Iteration 652, loss = 0.23416967\n",
      "Iteration 653, loss = 0.23394188\n",
      "Iteration 654, loss = 0.23371461\n",
      "Iteration 655, loss = 0.23348790\n",
      "Iteration 656, loss = 0.23326156\n",
      "Iteration 657, loss = 0.23303569\n",
      "Iteration 658, loss = 0.23281028\n",
      "Iteration 659, loss = 0.23258539\n",
      "Iteration 660, loss = 0.23236086\n",
      "Iteration 661, loss = 0.23213682\n",
      "Iteration 662, loss = 0.23191325\n",
      "Iteration 663, loss = 0.23169012\n",
      "Iteration 664, loss = 0.23146745\n",
      "Iteration 665, loss = 0.23124520\n",
      "Iteration 666, loss = 0.23102340\n",
      "Iteration 667, loss = 0.23080209\n",
      "Iteration 668, loss = 0.23058120\n",
      "Iteration 669, loss = 0.23036073\n",
      "Iteration 670, loss = 0.23014075\n",
      "Iteration 671, loss = 0.22992118\n",
      "Iteration 672, loss = 0.22970205\n",
      "Iteration 673, loss = 0.22948337\n",
      "Iteration 674, loss = 0.22926515\n",
      "Iteration 675, loss = 0.22904732\n",
      "Iteration 676, loss = 0.22882994\n",
      "Iteration 677, loss = 0.22861302\n",
      "Iteration 678, loss = 0.22839649\n",
      "Iteration 679, loss = 0.22818039\n",
      "Iteration 680, loss = 0.22796477\n",
      "Iteration 681, loss = 0.22774959\n",
      "Iteration 682, loss = 0.22753481\n",
      "Iteration 683, loss = 0.22732047\n",
      "Iteration 684, loss = 0.22710662\n",
      "Iteration 685, loss = 0.22689315\n",
      "Iteration 686, loss = 0.22668008\n",
      "Iteration 687, loss = 0.22646748\n",
      "Iteration 688, loss = 0.22625533\n",
      "Iteration 689, loss = 0.22604360\n",
      "Iteration 690, loss = 0.22583231\n",
      "Iteration 691, loss = 0.22562145\n",
      "Iteration 692, loss = 0.22541100\n",
      "Iteration 693, loss = 0.22520096\n",
      "Iteration 694, loss = 0.22499140\n",
      "Iteration 695, loss = 0.22478225\n",
      "Iteration 696, loss = 0.22457352\n",
      "Iteration 697, loss = 0.22436529\n",
      "Iteration 698, loss = 0.22415740\n",
      "Iteration 699, loss = 0.22394995\n",
      "Iteration 700, loss = 0.22374295\n",
      "Iteration 701, loss = 0.22353635\n",
      "Iteration 702, loss = 0.22333017\n",
      "Iteration 703, loss = 0.22312439\n",
      "Iteration 704, loss = 0.22291905\n",
      "Iteration 705, loss = 0.22271410\n",
      "Iteration 706, loss = 0.22250958\n",
      "Iteration 707, loss = 0.22230550\n",
      "Iteration 708, loss = 0.22210177\n",
      "Iteration 709, loss = 0.22189848\n",
      "Iteration 710, loss = 0.22169566\n",
      "Iteration 711, loss = 0.22149312\n",
      "Iteration 712, loss = 0.22129109\n",
      "Iteration 713, loss = 0.22108945\n",
      "Iteration 714, loss = 0.22088819\n",
      "Iteration 715, loss = 0.22068735\n",
      "Iteration 716, loss = 0.22048697\n",
      "Iteration 717, loss = 0.22028687\n",
      "Iteration 718, loss = 0.22008726\n",
      "Iteration 719, loss = 0.21988804\n",
      "Iteration 720, loss = 0.21968920\n",
      "Iteration 721, loss = 0.21949074\n",
      "Iteration 722, loss = 0.21929271\n",
      "Iteration 723, loss = 0.21909506\n",
      "Iteration 724, loss = 0.21889778\n",
      "Iteration 725, loss = 0.21870097\n",
      "Iteration 726, loss = 0.21850447\n",
      "Iteration 727, loss = 0.21830838\n",
      "Iteration 728, loss = 0.21811273\n",
      "Iteration 729, loss = 0.21791743\n",
      "Iteration 730, loss = 0.21772250\n",
      "Iteration 731, loss = 0.21752799\n",
      "Iteration 732, loss = 0.21733388\n",
      "Iteration 733, loss = 0.21714019\n",
      "Iteration 734, loss = 0.21694697\n",
      "Iteration 735, loss = 0.21675404\n",
      "Iteration 736, loss = 0.21656158\n",
      "Iteration 737, loss = 0.21636951\n",
      "Iteration 738, loss = 0.21617779\n",
      "Iteration 739, loss = 0.21598646\n",
      "Iteration 740, loss = 0.21579554\n",
      "Iteration 741, loss = 0.21560496\n",
      "Iteration 742, loss = 0.21541478\n",
      "Iteration 743, loss = 0.21522500\n",
      "Iteration 744, loss = 0.21503560\n",
      "Iteration 745, loss = 0.21484651\n",
      "Iteration 746, loss = 0.21465792\n",
      "Iteration 747, loss = 0.21446957\n",
      "Iteration 748, loss = 0.21428170\n",
      "Iteration 749, loss = 0.21409417\n",
      "Iteration 750, loss = 0.21390699\n",
      "Iteration 751, loss = 0.21372016\n",
      "Iteration 752, loss = 0.21353380\n",
      "Iteration 753, loss = 0.21334771\n",
      "Iteration 754, loss = 0.21316205\n",
      "Iteration 755, loss = 0.21297672\n",
      "Iteration 756, loss = 0.21279178\n",
      "Iteration 757, loss = 0.21260720\n",
      "Iteration 758, loss = 0.21242298\n",
      "Iteration 759, loss = 0.21223913\n",
      "Iteration 760, loss = 0.21205566\n",
      "Iteration 761, loss = 0.21187253\n",
      "Iteration 762, loss = 0.21168976\n",
      "Iteration 763, loss = 0.21150741\n",
      "Iteration 764, loss = 0.21132534\n",
      "Iteration 765, loss = 0.21114367\n",
      "Iteration 766, loss = 0.21096236\n",
      "Iteration 767, loss = 0.21078139\n",
      "Iteration 768, loss = 0.21060081\n",
      "Iteration 769, loss = 0.21042055\n",
      "Iteration 770, loss = 0.21024068\n",
      "Iteration 771, loss = 0.21006118\n",
      "Iteration 772, loss = 0.20988202\n",
      "Iteration 773, loss = 0.20970321\n",
      "Iteration 774, loss = 0.20952477\n",
      "Iteration 775, loss = 0.20934665\n",
      "Iteration 776, loss = 0.20916896\n",
      "Iteration 777, loss = 0.20899152\n",
      "Iteration 778, loss = 0.20881450\n",
      "Iteration 779, loss = 0.20863780\n",
      "Iteration 780, loss = 0.20846145\n",
      "Iteration 781, loss = 0.20828545\n",
      "Iteration 782, loss = 0.20810981\n",
      "Iteration 783, loss = 0.20793448\n",
      "Iteration 784, loss = 0.20775956\n",
      "Iteration 785, loss = 0.20758491\n",
      "Iteration 786, loss = 0.20741067\n",
      "Iteration 787, loss = 0.20723676\n",
      "Iteration 788, loss = 0.20706319\n",
      "Iteration 789, loss = 0.20688998\n",
      "Iteration 790, loss = 0.20671711\n",
      "Iteration 791, loss = 0.20654456\n",
      "Iteration 792, loss = 0.20637243\n",
      "Iteration 793, loss = 0.20620052\n",
      "Iteration 794, loss = 0.20602907\n",
      "Iteration 795, loss = 0.20585788\n",
      "Iteration 796, loss = 0.20568702\n",
      "Iteration 797, loss = 0.20551650\n",
      "Iteration 798, loss = 0.20534635\n",
      "Iteration 799, loss = 0.20517648\n",
      "Iteration 800, loss = 0.20500705\n",
      "Iteration 801, loss = 0.20483782\n",
      "Iteration 802, loss = 0.20466903\n",
      "Iteration 803, loss = 0.20450051\n",
      "Iteration 804, loss = 0.20433233\n",
      "Iteration 805, loss = 0.20416448\n",
      "Iteration 806, loss = 0.20399695\n",
      "Iteration 807, loss = 0.20382977\n",
      "Iteration 808, loss = 0.20366290\n",
      "Iteration 809, loss = 0.20349634\n",
      "Iteration 810, loss = 0.20333017\n",
      "Iteration 811, loss = 0.20316425\n",
      "Iteration 812, loss = 0.20299871\n",
      "Iteration 813, loss = 0.20283347\n",
      "Iteration 814, loss = 0.20266857\n",
      "Iteration 815, loss = 0.20250402\n",
      "Iteration 816, loss = 0.20233974\n",
      "Iteration 817, loss = 0.20217585\n",
      "Iteration 818, loss = 0.20201223\n",
      "Iteration 819, loss = 0.20184893\n",
      "Iteration 820, loss = 0.20168599\n",
      "Iteration 821, loss = 0.20152331\n",
      "Iteration 822, loss = 0.20136092\n",
      "Iteration 823, loss = 0.20119890\n",
      "Iteration 824, loss = 0.20103713\n",
      "Iteration 825, loss = 0.20087569\n",
      "Iteration 826, loss = 0.20071458\n",
      "Iteration 827, loss = 0.20055375\n",
      "Iteration 828, loss = 0.20039329\n",
      "Iteration 829, loss = 0.20023305\n",
      "Iteration 830, loss = 0.20007320\n",
      "Iteration 831, loss = 0.19991363\n",
      "Iteration 832, loss = 0.19975433\n",
      "Iteration 833, loss = 0.19959543\n",
      "Iteration 834, loss = 0.19943673\n",
      "Iteration 835, loss = 0.19927842\n",
      "Iteration 836, loss = 0.19912037\n",
      "Iteration 837, loss = 0.19896263\n",
      "Iteration 838, loss = 0.19880524\n",
      "Iteration 839, loss = 0.19864808\n",
      "Iteration 840, loss = 0.19849130\n",
      "Iteration 841, loss = 0.19833478\n",
      "Iteration 842, loss = 0.19817854\n",
      "Iteration 843, loss = 0.19802268\n",
      "Iteration 844, loss = 0.19786703\n",
      "Iteration 845, loss = 0.19771175\n",
      "Iteration 846, loss = 0.19755674\n",
      "Iteration 847, loss = 0.19740205\n",
      "Iteration 848, loss = 0.19724769\n",
      "Iteration 849, loss = 0.19709355\n",
      "Iteration 850, loss = 0.19693981\n",
      "Iteration 851, loss = 0.19678634\n",
      "Iteration 852, loss = 0.19663317\n",
      "Iteration 853, loss = 0.19648036\n",
      "Iteration 854, loss = 0.19632780\n",
      "Iteration 855, loss = 0.19617552\n",
      "Iteration 856, loss = 0.19602362\n",
      "Iteration 857, loss = 0.19587193\n",
      "Iteration 858, loss = 0.19572058\n",
      "Iteration 859, loss = 0.19556951\n",
      "Iteration 860, loss = 0.19541871\n",
      "Iteration 861, loss = 0.19526825\n",
      "Iteration 862, loss = 0.19511804\n",
      "Iteration 863, loss = 0.19496816\n",
      "Iteration 864, loss = 0.19481855\n",
      "Iteration 865, loss = 0.19466929\n",
      "Iteration 866, loss = 0.19452033\n",
      "Iteration 867, loss = 0.19437162\n",
      "Iteration 868, loss = 0.19422322\n",
      "Iteration 869, loss = 0.19407509\n",
      "Iteration 870, loss = 0.19392722\n",
      "Iteration 871, loss = 0.19377970\n",
      "Iteration 872, loss = 0.19363240\n",
      "Iteration 873, loss = 0.19348540\n",
      "Iteration 874, loss = 0.19333870\n",
      "Iteration 875, loss = 0.19319225\n",
      "Iteration 876, loss = 0.19304614\n",
      "Iteration 877, loss = 0.19290024\n",
      "Iteration 878, loss = 0.19275469\n",
      "Iteration 879, loss = 0.19260937\n",
      "Iteration 880, loss = 0.19246433\n",
      "Iteration 881, loss = 0.19231960\n",
      "Iteration 882, loss = 0.19217509\n",
      "Iteration 883, loss = 0.19203093\n",
      "Iteration 884, loss = 0.19188700\n",
      "Iteration 885, loss = 0.19174335\n",
      "Iteration 886, loss = 0.19160000\n",
      "Iteration 887, loss = 0.19145691\n",
      "Iteration 888, loss = 0.19131408\n",
      "Iteration 889, loss = 0.19117155\n",
      "Iteration 890, loss = 0.19102927\n",
      "Iteration 891, loss = 0.19088732\n",
      "Iteration 892, loss = 0.19074557\n",
      "Iteration 893, loss = 0.19060413\n",
      "Iteration 894, loss = 0.19046296\n",
      "Iteration 895, loss = 0.19032202\n",
      "Iteration 896, loss = 0.19018142\n",
      "Iteration 897, loss = 0.19004103\n",
      "Iteration 898, loss = 0.18990098\n",
      "Iteration 899, loss = 0.18976115\n",
      "Iteration 900, loss = 0.18962156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 901, loss = 0.18948232\n",
      "Iteration 902, loss = 0.18934326\n",
      "Iteration 903, loss = 0.18920451\n",
      "Iteration 904, loss = 0.18906600\n",
      "Iteration 905, loss = 0.18892774\n",
      "Iteration 906, loss = 0.18878976\n",
      "Iteration 907, loss = 0.18865200\n",
      "Iteration 908, loss = 0.18851453\n",
      "Iteration 909, loss = 0.18837728\n",
      "Iteration 910, loss = 0.18824034\n",
      "Iteration 911, loss = 0.18810362\n",
      "Iteration 912, loss = 0.18796718\n",
      "Iteration 913, loss = 0.18783099\n",
      "Iteration 914, loss = 0.18769505\n",
      "Iteration 915, loss = 0.18755937\n",
      "Iteration 916, loss = 0.18742396\n",
      "Iteration 917, loss = 0.18728879\n",
      "Iteration 918, loss = 0.18715389\n",
      "Iteration 919, loss = 0.18701923\n",
      "Iteration 920, loss = 0.18688485\n",
      "Iteration 921, loss = 0.18675067\n",
      "Iteration 922, loss = 0.18661681\n",
      "Iteration 923, loss = 0.18648315\n",
      "Iteration 924, loss = 0.18634967\n",
      "Iteration 925, loss = 0.18621640\n",
      "Iteration 926, loss = 0.18608333\n",
      "Iteration 927, loss = 0.18595052\n",
      "Iteration 928, loss = 0.18581793\n",
      "Iteration 929, loss = 0.18568559\n",
      "Iteration 930, loss = 0.18555352\n",
      "Iteration 931, loss = 0.18542163\n",
      "Iteration 932, loss = 0.18529005\n",
      "Iteration 933, loss = 0.18515863\n",
      "Iteration 934, loss = 0.18502754\n",
      "Iteration 935, loss = 0.18489664\n",
      "Iteration 936, loss = 0.18476596\n",
      "Iteration 937, loss = 0.18463560\n",
      "Iteration 938, loss = 0.18450540\n",
      "Iteration 939, loss = 0.18437549\n",
      "Iteration 940, loss = 0.18424579\n",
      "Iteration 941, loss = 0.18411631\n",
      "Iteration 942, loss = 0.18398701\n",
      "Iteration 943, loss = 0.18385794\n",
      "Iteration 944, loss = 0.18372911\n",
      "Iteration 945, loss = 0.18360049\n",
      "Iteration 946, loss = 0.18347215\n",
      "Iteration 947, loss = 0.18334397\n",
      "Iteration 948, loss = 0.18321608\n",
      "Iteration 949, loss = 0.18308840\n",
      "Iteration 950, loss = 0.18296091\n",
      "Iteration 951, loss = 0.18283370\n",
      "Iteration 952, loss = 0.18270664\n",
      "Iteration 953, loss = 0.18257983\n",
      "Iteration 954, loss = 0.18245325\n",
      "Iteration 955, loss = 0.18232675\n",
      "Iteration 956, loss = 0.18220028\n",
      "Iteration 957, loss = 0.18207409\n",
      "Iteration 958, loss = 0.18194802\n",
      "Iteration 959, loss = 0.18182220\n",
      "Iteration 960, loss = 0.18169658\n",
      "Iteration 961, loss = 0.18157114\n",
      "Iteration 962, loss = 0.18144597\n",
      "Iteration 963, loss = 0.18132094\n",
      "Iteration 964, loss = 0.18119620\n",
      "Iteration 965, loss = 0.18107161\n",
      "Iteration 966, loss = 0.18094735\n",
      "Iteration 967, loss = 0.18082346\n",
      "Iteration 968, loss = 0.18069969\n",
      "Iteration 969, loss = 0.18057621\n",
      "Iteration 970, loss = 0.18045289\n",
      "Iteration 971, loss = 0.18032983\n",
      "Iteration 972, loss = 0.18020697\n",
      "Iteration 973, loss = 0.18008436\n",
      "Iteration 974, loss = 0.17996194\n",
      "Iteration 975, loss = 0.17983980\n",
      "Iteration 976, loss = 0.17971784\n",
      "Iteration 977, loss = 0.17959616\n",
      "Iteration 978, loss = 0.17947470\n",
      "Iteration 979, loss = 0.17935339\n",
      "Iteration 980, loss = 0.17923199\n",
      "Iteration 981, loss = 0.17911072\n",
      "Iteration 982, loss = 0.17898969\n",
      "Iteration 983, loss = 0.17886881\n",
      "Iteration 984, loss = 0.17874819\n",
      "Iteration 985, loss = 0.17862771\n",
      "Iteration 986, loss = 0.17850702\n",
      "Iteration 987, loss = 0.17838604\n",
      "Iteration 988, loss = 0.17826518\n",
      "Iteration 989, loss = 0.17814445\n",
      "Iteration 990, loss = 0.17802382\n",
      "Iteration 991, loss = 0.17790340\n",
      "Iteration 992, loss = 0.17778306\n",
      "Iteration 993, loss = 0.17766281\n",
      "Iteration 994, loss = 0.17754255\n",
      "Iteration 995, loss = 0.17742241\n",
      "Iteration 996, loss = 0.17730219\n",
      "Iteration 997, loss = 0.17718185\n",
      "Iteration 998, loss = 0.17706164\n",
      "Iteration 999, loss = 0.17694156\n",
      "Iteration 1000, loss = 0.17682144\n",
      "Iteration 1, loss = 1.48410645\n",
      "Iteration 2, loss = 1.47044523\n",
      "Iteration 3, loss = 1.45126172\n",
      "Iteration 4, loss = 1.42744657\n",
      "Iteration 5, loss = 1.39986533\n",
      "Iteration 6, loss = 1.36941349\n",
      "Iteration 7, loss = 1.33697026\n",
      "Iteration 8, loss = 1.30351419\n",
      "Iteration 9, loss = 1.26994450\n",
      "Iteration 10, loss = 1.23745599\n",
      "Iteration 11, loss = 1.20655649\n",
      "Iteration 12, loss = 1.17753473\n",
      "Iteration 13, loss = 1.15073807\n",
      "Iteration 14, loss = 1.12635486\n",
      "Iteration 15, loss = 1.10464816\n",
      "Iteration 16, loss = 1.08558177\n",
      "Iteration 17, loss = 1.06849209\n",
      "Iteration 18, loss = 1.05329889\n",
      "Iteration 19, loss = 1.03958644\n",
      "Iteration 20, loss = 1.02688454\n",
      "Iteration 21, loss = 1.01474837\n",
      "Iteration 22, loss = 1.00281896\n",
      "Iteration 23, loss = 0.99084484\n",
      "Iteration 24, loss = 0.97875674\n",
      "Iteration 25, loss = 0.96654711\n",
      "Iteration 26, loss = 0.95419554\n",
      "Iteration 27, loss = 0.94172207\n",
      "Iteration 28, loss = 0.92931418\n",
      "Iteration 29, loss = 0.91704159\n",
      "Iteration 30, loss = 0.90499860\n",
      "Iteration 31, loss = 0.89328575\n",
      "Iteration 32, loss = 0.88180968\n",
      "Iteration 33, loss = 0.87060231\n",
      "Iteration 34, loss = 0.85981635\n",
      "Iteration 35, loss = 0.84952670\n",
      "Iteration 36, loss = 0.83966462\n",
      "Iteration 37, loss = 0.83013160\n",
      "Iteration 38, loss = 0.82097497\n",
      "Iteration 39, loss = 0.81211935\n",
      "Iteration 40, loss = 0.80341946\n",
      "Iteration 41, loss = 0.79498465\n",
      "Iteration 42, loss = 0.78685909\n",
      "Iteration 43, loss = 0.77912698\n",
      "Iteration 44, loss = 0.77168587\n",
      "Iteration 45, loss = 0.76454803\n",
      "Iteration 46, loss = 0.75768759\n",
      "Iteration 47, loss = 0.75105299\n",
      "Iteration 48, loss = 0.74458405\n",
      "Iteration 49, loss = 0.73828500\n",
      "Iteration 50, loss = 0.73219850\n",
      "Iteration 51, loss = 0.72627127\n",
      "Iteration 52, loss = 0.72049016\n",
      "Iteration 53, loss = 0.71483656\n",
      "Iteration 54, loss = 0.70929081\n",
      "Iteration 55, loss = 0.70385134\n",
      "Iteration 56, loss = 0.69852853\n",
      "Iteration 57, loss = 0.69332791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 58, loss = 0.68824511\n",
      "Iteration 59, loss = 0.68330438\n",
      "Iteration 60, loss = 0.67849310\n",
      "Iteration 61, loss = 0.67382051\n",
      "Iteration 62, loss = 0.66929240\n",
      "Iteration 63, loss = 0.66489183\n",
      "Iteration 64, loss = 0.66058615\n",
      "Iteration 65, loss = 0.65637297\n",
      "Iteration 66, loss = 0.65224917\n",
      "Iteration 67, loss = 0.64821293\n",
      "Iteration 68, loss = 0.64426451\n",
      "Iteration 69, loss = 0.64039666\n",
      "Iteration 70, loss = 0.63660517\n",
      "Iteration 71, loss = 0.63288073\n",
      "Iteration 72, loss = 0.62921665\n",
      "Iteration 73, loss = 0.62561299\n",
      "Iteration 74, loss = 0.62207065\n",
      "Iteration 75, loss = 0.61859309\n",
      "Iteration 76, loss = 0.61517760\n",
      "Iteration 77, loss = 0.61182525\n",
      "Iteration 78, loss = 0.60853240\n",
      "Iteration 79, loss = 0.60529785\n",
      "Iteration 80, loss = 0.60212420\n",
      "Iteration 81, loss = 0.59900660\n",
      "Iteration 82, loss = 0.59594395\n",
      "Iteration 83, loss = 0.59293413\n",
      "Iteration 84, loss = 0.58997503\n",
      "Iteration 85, loss = 0.58706498\n",
      "Iteration 86, loss = 0.58420285\n",
      "Iteration 87, loss = 0.58138770\n",
      "Iteration 88, loss = 0.57861958\n",
      "Iteration 89, loss = 0.57589885\n",
      "Iteration 90, loss = 0.57322345\n",
      "Iteration 91, loss = 0.57059610\n",
      "Iteration 92, loss = 0.56801265\n",
      "Iteration 93, loss = 0.56547220\n",
      "Iteration 94, loss = 0.56297297\n",
      "Iteration 95, loss = 0.56051064\n",
      "Iteration 96, loss = 0.55808400\n",
      "Iteration 97, loss = 0.55569221\n",
      "Iteration 98, loss = 0.55333546\n",
      "Iteration 99, loss = 0.55101646\n",
      "Iteration 100, loss = 0.54873256\n",
      "Iteration 101, loss = 0.54648201\n",
      "Iteration 102, loss = 0.54426557\n",
      "Iteration 103, loss = 0.54208236\n",
      "Iteration 104, loss = 0.53993119\n",
      "Iteration 105, loss = 0.53781141\n",
      "Iteration 106, loss = 0.53572265\n",
      "Iteration 107, loss = 0.53366244\n",
      "Iteration 108, loss = 0.53162994\n",
      "Iteration 109, loss = 0.52962467\n",
      "Iteration 110, loss = 0.52764433\n",
      "Iteration 111, loss = 0.52568845\n",
      "Iteration 112, loss = 0.52375751\n",
      "Iteration 113, loss = 0.52185170\n",
      "Iteration 114, loss = 0.51996996\n",
      "Iteration 115, loss = 0.51811497\n",
      "Iteration 116, loss = 0.51628244\n",
      "Iteration 117, loss = 0.51446836\n",
      "Iteration 118, loss = 0.51267523\n",
      "Iteration 119, loss = 0.51090070\n",
      "Iteration 120, loss = 0.50914424\n",
      "Iteration 121, loss = 0.50740323\n",
      "Iteration 122, loss = 0.50567426\n",
      "Iteration 123, loss = 0.50395075\n",
      "Iteration 124, loss = 0.50223719\n",
      "Iteration 125, loss = 0.50053170\n",
      "Iteration 126, loss = 0.49883381\n",
      "Iteration 127, loss = 0.49714570\n",
      "Iteration 128, loss = 0.49546733\n",
      "Iteration 129, loss = 0.49378371\n",
      "Iteration 130, loss = 0.49210654\n",
      "Iteration 131, loss = 0.49043681\n",
      "Iteration 132, loss = 0.48878559\n",
      "Iteration 133, loss = 0.48713509\n",
      "Iteration 134, loss = 0.48549645\n",
      "Iteration 135, loss = 0.48390582\n",
      "Iteration 136, loss = 0.48233034\n",
      "Iteration 137, loss = 0.48075760\n",
      "Iteration 138, loss = 0.47921455\n",
      "Iteration 139, loss = 0.47769958\n",
      "Iteration 140, loss = 0.47623291\n",
      "Iteration 141, loss = 0.47479666\n",
      "Iteration 142, loss = 0.47339159\n",
      "Iteration 143, loss = 0.47201892\n",
      "Iteration 144, loss = 0.47068502\n",
      "Iteration 145, loss = 0.46936532\n",
      "Iteration 146, loss = 0.46805401\n",
      "Iteration 147, loss = 0.46675428\n",
      "Iteration 148, loss = 0.46546569\n",
      "Iteration 149, loss = 0.46419005\n",
      "Iteration 150, loss = 0.46292066\n",
      "Iteration 151, loss = 0.46165692\n",
      "Iteration 152, loss = 0.46039955\n",
      "Iteration 153, loss = 0.45914917\n",
      "Iteration 154, loss = 0.45790750\n",
      "Iteration 155, loss = 0.45667977\n",
      "Iteration 156, loss = 0.45546441\n",
      "Iteration 157, loss = 0.45425984\n",
      "Iteration 158, loss = 0.45306647\n",
      "Iteration 159, loss = 0.45188469\n",
      "Iteration 160, loss = 0.45071301\n",
      "Iteration 161, loss = 0.44955129\n",
      "Iteration 162, loss = 0.44839946\n",
      "Iteration 163, loss = 0.44725735\n",
      "Iteration 164, loss = 0.44612517\n",
      "Iteration 165, loss = 0.44500335\n",
      "Iteration 166, loss = 0.44389186\n",
      "Iteration 167, loss = 0.44278947\n",
      "Iteration 168, loss = 0.44169612\n",
      "Iteration 169, loss = 0.44061172\n",
      "Iteration 170, loss = 0.43953598\n",
      "Iteration 171, loss = 0.43846877\n",
      "Iteration 172, loss = 0.43740985\n",
      "Iteration 173, loss = 0.43635903\n",
      "Iteration 174, loss = 0.43531552\n",
      "Iteration 175, loss = 0.43427931\n",
      "Iteration 176, loss = 0.43325027\n",
      "Iteration 177, loss = 0.43222862\n",
      "Iteration 178, loss = 0.43121455\n",
      "Iteration 179, loss = 0.43020765\n",
      "Iteration 180, loss = 0.42920781\n",
      "Iteration 181, loss = 0.42821489\n",
      "Iteration 182, loss = 0.42722877\n",
      "Iteration 183, loss = 0.42624968\n",
      "Iteration 184, loss = 0.42527786\n",
      "Iteration 185, loss = 0.42431262\n",
      "Iteration 186, loss = 0.42335384\n",
      "Iteration 187, loss = 0.42240139\n",
      "Iteration 188, loss = 0.42145523\n",
      "Iteration 189, loss = 0.42051529\n",
      "Iteration 190, loss = 0.41958185\n",
      "Iteration 191, loss = 0.41865438\n",
      "Iteration 192, loss = 0.41773273\n",
      "Iteration 193, loss = 0.41681682\n",
      "Iteration 194, loss = 0.41590672\n",
      "Iteration 195, loss = 0.41500198\n",
      "Iteration 196, loss = 0.41410261\n",
      "Iteration 197, loss = 0.41320892\n",
      "Iteration 198, loss = 0.41232152\n",
      "Iteration 199, loss = 0.41143943\n",
      "Iteration 200, loss = 0.41056257\n",
      "Iteration 201, loss = 0.40969171\n",
      "Iteration 202, loss = 0.40882592\n",
      "Iteration 203, loss = 0.40796549\n",
      "Iteration 204, loss = 0.40711058\n",
      "Iteration 205, loss = 0.40626043\n",
      "Iteration 206, loss = 0.40541511\n",
      "Iteration 207, loss = 0.40457464\n",
      "Iteration 208, loss = 0.40373898\n",
      "Iteration 209, loss = 0.40290805\n",
      "Iteration 210, loss = 0.40208197\n",
      "Iteration 211, loss = 0.40126072\n",
      "Iteration 212, loss = 0.40044419\n",
      "Iteration 213, loss = 0.39963235\n",
      "Iteration 214, loss = 0.39882494\n",
      "Iteration 215, loss = 0.39802208\n",
      "Iteration 216, loss = 0.39722367\n",
      "Iteration 217, loss = 0.39642947\n",
      "Iteration 218, loss = 0.39563968\n",
      "Iteration 219, loss = 0.39485400\n",
      "Iteration 220, loss = 0.39407262\n",
      "Iteration 221, loss = 0.39329536\n",
      "Iteration 222, loss = 0.39252215\n",
      "Iteration 223, loss = 0.39175356\n",
      "Iteration 224, loss = 0.39098888\n",
      "Iteration 225, loss = 0.39022825\n",
      "Iteration 226, loss = 0.38947142\n",
      "Iteration 227, loss = 0.38871851\n",
      "Iteration 228, loss = 0.38796937\n",
      "Iteration 229, loss = 0.38722396\n",
      "Iteration 230, loss = 0.38648222\n",
      "Iteration 231, loss = 0.38574455\n",
      "Iteration 232, loss = 0.38500989\n",
      "Iteration 233, loss = 0.38427825\n",
      "Iteration 234, loss = 0.38355008\n",
      "Iteration 235, loss = 0.38282532\n",
      "Iteration 236, loss = 0.38210399\n",
      "Iteration 237, loss = 0.38138601\n",
      "Iteration 238, loss = 0.38067127\n",
      "Iteration 239, loss = 0.37995984\n",
      "Iteration 240, loss = 0.37925164\n",
      "Iteration 241, loss = 0.37854663\n",
      "Iteration 242, loss = 0.37784471\n",
      "Iteration 243, loss = 0.37714598\n",
      "Iteration 244, loss = 0.37645037\n",
      "Iteration 245, loss = 0.37575795\n",
      "Iteration 246, loss = 0.37506899\n",
      "Iteration 247, loss = 0.37438288\n",
      "Iteration 248, loss = 0.37369983\n",
      "Iteration 249, loss = 0.37301968\n",
      "Iteration 250, loss = 0.37234241\n",
      "Iteration 251, loss = 0.37166808\n",
      "Iteration 252, loss = 0.37099666\n",
      "Iteration 253, loss = 0.37032768\n",
      "Iteration 254, loss = 0.36966066\n",
      "Iteration 255, loss = 0.36899641\n",
      "Iteration 256, loss = 0.36833412\n",
      "Iteration 257, loss = 0.36767383\n",
      "Iteration 258, loss = 0.36701608\n",
      "Iteration 259, loss = 0.36636092\n",
      "Iteration 260, loss = 0.36570818\n",
      "Iteration 261, loss = 0.36505796\n",
      "Iteration 262, loss = 0.36441035\n",
      "Iteration 263, loss = 0.36376502\n",
      "Iteration 264, loss = 0.36312228\n",
      "Iteration 265, loss = 0.36248212\n",
      "Iteration 266, loss = 0.36184478\n",
      "Iteration 267, loss = 0.36120987\n",
      "Iteration 268, loss = 0.36057659\n",
      "Iteration 269, loss = 0.35994503\n",
      "Iteration 270, loss = 0.35931423\n",
      "Iteration 271, loss = 0.35868558\n",
      "Iteration 272, loss = 0.35805765\n",
      "Iteration 273, loss = 0.35743126\n",
      "Iteration 274, loss = 0.35680695\n",
      "Iteration 275, loss = 0.35618474\n",
      "Iteration 276, loss = 0.35556467\n",
      "Iteration 277, loss = 0.35494674\n",
      "Iteration 278, loss = 0.35433077\n",
      "Iteration 279, loss = 0.35371694\n",
      "Iteration 280, loss = 0.35310511\n",
      "Iteration 281, loss = 0.35249537\n",
      "Iteration 282, loss = 0.35188773\n",
      "Iteration 283, loss = 0.35128157\n",
      "Iteration 284, loss = 0.35067679\n",
      "Iteration 285, loss = 0.35007393\n",
      "Iteration 286, loss = 0.34947436\n",
      "Iteration 287, loss = 0.34887934\n",
      "Iteration 288, loss = 0.34828493\n",
      "Iteration 289, loss = 0.34769121\n",
      "Iteration 290, loss = 0.34709932\n",
      "Iteration 291, loss = 0.34650951\n",
      "Iteration 292, loss = 0.34592079\n",
      "Iteration 293, loss = 0.34533317\n",
      "Iteration 294, loss = 0.34474490\n",
      "Iteration 295, loss = 0.34415447\n",
      "Iteration 296, loss = 0.34356395\n",
      "Iteration 297, loss = 0.34297263\n",
      "Iteration 298, loss = 0.34238187\n",
      "Iteration 299, loss = 0.34179169\n",
      "Iteration 300, loss = 0.34119783\n",
      "Iteration 301, loss = 0.34060325\n",
      "Iteration 302, loss = 0.34001172\n",
      "Iteration 303, loss = 0.33941917\n",
      "Iteration 304, loss = 0.33882761\n",
      "Iteration 305, loss = 0.33823616\n",
      "Iteration 306, loss = 0.33764059\n",
      "Iteration 307, loss = 0.33704300\n",
      "Iteration 308, loss = 0.33644602\n",
      "Iteration 309, loss = 0.33584981\n",
      "Iteration 310, loss = 0.33525449\n",
      "Iteration 311, loss = 0.33465914\n",
      "Iteration 312, loss = 0.33406645\n",
      "Iteration 313, loss = 0.33347925\n",
      "Iteration 314, loss = 0.33289546\n",
      "Iteration 315, loss = 0.33231269\n",
      "Iteration 316, loss = 0.33173056\n",
      "Iteration 317, loss = 0.33114958\n",
      "Iteration 318, loss = 0.33056861\n",
      "Iteration 319, loss = 0.32998478\n",
      "Iteration 320, loss = 0.32940143\n",
      "Iteration 321, loss = 0.32881897\n",
      "Iteration 322, loss = 0.32823564\n",
      "Iteration 323, loss = 0.32764960\n",
      "Iteration 324, loss = 0.32706069\n",
      "Iteration 325, loss = 0.32647475\n",
      "Iteration 326, loss = 0.32589825\n",
      "Iteration 327, loss = 0.32532692\n",
      "Iteration 328, loss = 0.32476203\n",
      "Iteration 329, loss = 0.32420264\n",
      "Iteration 330, loss = 0.32364561\n",
      "Iteration 331, loss = 0.32309622\n",
      "Iteration 332, loss = 0.32255068\n",
      "Iteration 333, loss = 0.32201248\n",
      "Iteration 334, loss = 0.32147805\n",
      "Iteration 335, loss = 0.32095002\n",
      "Iteration 336, loss = 0.32042826\n",
      "Iteration 337, loss = 0.31991508\n",
      "Iteration 338, loss = 0.31941052\n",
      "Iteration 339, loss = 0.31891597\n",
      "Iteration 340, loss = 0.31842280\n",
      "Iteration 341, loss = 0.31793300\n",
      "Iteration 342, loss = 0.31744560\n",
      "Iteration 343, loss = 0.31696107\n",
      "Iteration 344, loss = 0.31647809\n",
      "Iteration 345, loss = 0.31599656\n",
      "Iteration 346, loss = 0.31551650\n",
      "Iteration 347, loss = 0.31503861\n",
      "Iteration 348, loss = 0.31456362\n",
      "Iteration 349, loss = 0.31408990\n",
      "Iteration 350, loss = 0.31361777\n",
      "Iteration 351, loss = 0.31314700\n",
      "Iteration 352, loss = 0.31267739\n",
      "Iteration 353, loss = 0.31220867\n",
      "Iteration 354, loss = 0.31174115\n",
      "Iteration 355, loss = 0.31127471\n",
      "Iteration 356, loss = 0.31080950\n",
      "Iteration 357, loss = 0.31034542\n",
      "Iteration 358, loss = 0.30988251\n",
      "Iteration 359, loss = 0.30942070\n",
      "Iteration 360, loss = 0.30896005\n",
      "Iteration 361, loss = 0.30850060\n",
      "Iteration 362, loss = 0.30804225\n",
      "Iteration 363, loss = 0.30758514\n",
      "Iteration 364, loss = 0.30712916\n",
      "Iteration 365, loss = 0.30667435\n",
      "Iteration 366, loss = 0.30622098\n",
      "Iteration 367, loss = 0.30576920\n",
      "Iteration 368, loss = 0.30531872\n",
      "Iteration 369, loss = 0.30486935\n",
      "Iteration 370, loss = 0.30442133\n",
      "Iteration 371, loss = 0.30397440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 372, loss = 0.30352874\n",
      "Iteration 373, loss = 0.30308443\n",
      "Iteration 374, loss = 0.30264151\n",
      "Iteration 375, loss = 0.30219972\n",
      "Iteration 376, loss = 0.30175927\n",
      "Iteration 377, loss = 0.30131994\n",
      "Iteration 378, loss = 0.30088171\n",
      "Iteration 379, loss = 0.30044465\n",
      "Iteration 380, loss = 0.30000859\n",
      "Iteration 381, loss = 0.29957369\n",
      "Iteration 382, loss = 0.29913986\n",
      "Iteration 383, loss = 0.29870710\n",
      "Iteration 384, loss = 0.29827551\n",
      "Iteration 385, loss = 0.29784493\n",
      "Iteration 386, loss = 0.29741539\n",
      "Iteration 387, loss = 0.29698708\n",
      "Iteration 388, loss = 0.29655968\n",
      "Iteration 389, loss = 0.29613342\n",
      "Iteration 390, loss = 0.29570840\n",
      "Iteration 391, loss = 0.29528433\n",
      "Iteration 392, loss = 0.29486141\n",
      "Iteration 393, loss = 0.29443951\n",
      "Iteration 394, loss = 0.29401875\n",
      "Iteration 395, loss = 0.29359897\n",
      "Iteration 396, loss = 0.29318029\n",
      "Iteration 397, loss = 0.29276279\n",
      "Iteration 398, loss = 0.29234631\n",
      "Iteration 399, loss = 0.29193100\n",
      "Iteration 400, loss = 0.29151689\n",
      "Iteration 401, loss = 0.29110394\n",
      "Iteration 402, loss = 0.29069216\n",
      "Iteration 403, loss = 0.29028142\n",
      "Iteration 404, loss = 0.28987171\n",
      "Iteration 405, loss = 0.28946303\n",
      "Iteration 406, loss = 0.28905533\n",
      "Iteration 407, loss = 0.28864867\n",
      "Iteration 408, loss = 0.28824302\n",
      "Iteration 409, loss = 0.28783843\n",
      "Iteration 410, loss = 0.28743471\n",
      "Iteration 411, loss = 0.28703203\n",
      "Iteration 412, loss = 0.28663037\n",
      "Iteration 413, loss = 0.28622963\n",
      "Iteration 414, loss = 0.28582990\n",
      "Iteration 415, loss = 0.28543115\n",
      "Iteration 416, loss = 0.28503337\n",
      "Iteration 417, loss = 0.28463689\n",
      "Iteration 418, loss = 0.28424155\n",
      "Iteration 419, loss = 0.28384706\n",
      "Iteration 420, loss = 0.28345354\n",
      "Iteration 421, loss = 0.28306097\n",
      "Iteration 422, loss = 0.28266927\n",
      "Iteration 423, loss = 0.28227850\n",
      "Iteration 424, loss = 0.28188869\n",
      "Iteration 425, loss = 0.28149972\n",
      "Iteration 426, loss = 0.28111171\n",
      "Iteration 427, loss = 0.28072460\n",
      "Iteration 428, loss = 0.28033840\n",
      "Iteration 429, loss = 0.27995310\n",
      "Iteration 430, loss = 0.27956876\n",
      "Iteration 431, loss = 0.27918520\n",
      "Iteration 432, loss = 0.27880258\n",
      "Iteration 433, loss = 0.27842093\n",
      "Iteration 434, loss = 0.27804001\n",
      "Iteration 435, loss = 0.27766023\n",
      "Iteration 436, loss = 0.27728099\n",
      "Iteration 437, loss = 0.27690297\n",
      "Iteration 438, loss = 0.27652591\n",
      "Iteration 439, loss = 0.27614970\n",
      "Iteration 440, loss = 0.27577435\n",
      "Iteration 441, loss = 0.27539970\n",
      "Iteration 442, loss = 0.27502609\n",
      "Iteration 443, loss = 0.27465355\n",
      "Iteration 444, loss = 0.27428195\n",
      "Iteration 445, loss = 0.27391126\n",
      "Iteration 446, loss = 0.27354142\n",
      "Iteration 447, loss = 0.27317248\n",
      "Iteration 448, loss = 0.27280438\n",
      "Iteration 449, loss = 0.27243716\n",
      "Iteration 450, loss = 0.27207076\n",
      "Iteration 451, loss = 0.27170527\n",
      "Iteration 452, loss = 0.27134060\n",
      "Iteration 453, loss = 0.27097676\n",
      "Iteration 454, loss = 0.27061376\n",
      "Iteration 455, loss = 0.27025159\n",
      "Iteration 456, loss = 0.26989033\n",
      "Iteration 457, loss = 0.26952986\n",
      "Iteration 458, loss = 0.26917019\n",
      "Iteration 459, loss = 0.26881137\n",
      "Iteration 460, loss = 0.26845342\n",
      "Iteration 461, loss = 0.26809624\n",
      "Iteration 462, loss = 0.26773987\n",
      "Iteration 463, loss = 0.26738434\n",
      "Iteration 464, loss = 0.26702961\n",
      "Iteration 465, loss = 0.26667566\n",
      "Iteration 466, loss = 0.26632256\n",
      "Iteration 467, loss = 0.26597026\n",
      "Iteration 468, loss = 0.26561874\n",
      "Iteration 469, loss = 0.26526801\n",
      "Iteration 470, loss = 0.26491808\n",
      "Iteration 471, loss = 0.26456897\n",
      "Iteration 472, loss = 0.26422061\n",
      "Iteration 473, loss = 0.26387306\n",
      "Iteration 474, loss = 0.26352630\n",
      "Iteration 475, loss = 0.26318031\n",
      "Iteration 476, loss = 0.26283510\n",
      "Iteration 477, loss = 0.26249068\n",
      "Iteration 478, loss = 0.26214701\n",
      "Iteration 479, loss = 0.26180414\n",
      "Iteration 480, loss = 0.26146202\n",
      "Iteration 481, loss = 0.26112068\n",
      "Iteration 482, loss = 0.26078008\n",
      "Iteration 483, loss = 0.26044030\n",
      "Iteration 484, loss = 0.26010121\n",
      "Iteration 485, loss = 0.25976295\n",
      "Iteration 486, loss = 0.25942540\n",
      "Iteration 487, loss = 0.25908867\n",
      "Iteration 488, loss = 0.25875269\n",
      "Iteration 489, loss = 0.25841748\n",
      "Iteration 490, loss = 0.25808302\n",
      "Iteration 491, loss = 0.25774932\n",
      "Iteration 492, loss = 0.25741641\n",
      "Iteration 493, loss = 0.25708414\n",
      "Iteration 494, loss = 0.25675266\n",
      "Iteration 495, loss = 0.25642191\n",
      "Iteration 496, loss = 0.25609193\n",
      "Iteration 497, loss = 0.25576262\n",
      "Iteration 498, loss = 0.25543412\n",
      "Iteration 499, loss = 0.25510628\n",
      "Iteration 500, loss = 0.25477922\n",
      "Iteration 501, loss = 0.25445284\n",
      "Iteration 502, loss = 0.25412719\n",
      "Iteration 503, loss = 0.25380225\n",
      "Iteration 504, loss = 0.25347804\n",
      "Iteration 505, loss = 0.25315455\n",
      "Iteration 506, loss = 0.25283176\n",
      "Iteration 507, loss = 0.25250971\n",
      "Iteration 508, loss = 0.25218837\n",
      "Iteration 509, loss = 0.25186770\n",
      "Iteration 510, loss = 0.25154778\n",
      "Iteration 511, loss = 0.25122850\n",
      "Iteration 512, loss = 0.25090994\n",
      "Iteration 513, loss = 0.25059210\n",
      "Iteration 514, loss = 0.25027495\n",
      "Iteration 515, loss = 0.24995853\n",
      "Iteration 516, loss = 0.24964274\n",
      "Iteration 517, loss = 0.24932765\n",
      "Iteration 518, loss = 0.24901325\n",
      "Iteration 519, loss = 0.24869954\n",
      "Iteration 520, loss = 0.24838651\n",
      "Iteration 521, loss = 0.24807417\n",
      "Iteration 522, loss = 0.24776250\n",
      "Iteration 523, loss = 0.24745151\n",
      "Iteration 524, loss = 0.24714130\n",
      "Iteration 525, loss = 0.24683162\n",
      "Iteration 526, loss = 0.24652266\n",
      "Iteration 527, loss = 0.24621462\n",
      "Iteration 528, loss = 0.24590734\n",
      "Iteration 529, loss = 0.24560073\n",
      "Iteration 530, loss = 0.24529479\n",
      "Iteration 531, loss = 0.24498954\n",
      "Iteration 532, loss = 0.24468496\n",
      "Iteration 533, loss = 0.24438106\n",
      "Iteration 534, loss = 0.24407785\n",
      "Iteration 535, loss = 0.24377526\n",
      "Iteration 536, loss = 0.24347337\n",
      "Iteration 537, loss = 0.24317213\n",
      "Iteration 538, loss = 0.24287167\n",
      "Iteration 539, loss = 0.24257202\n",
      "Iteration 540, loss = 0.24227302\n",
      "Iteration 541, loss = 0.24197468\n",
      "Iteration 542, loss = 0.24167708\n",
      "Iteration 543, loss = 0.24138017\n",
      "Iteration 544, loss = 0.24108390\n",
      "Iteration 545, loss = 0.24078829\n",
      "Iteration 546, loss = 0.24049333\n",
      "Iteration 547, loss = 0.24019900\n",
      "Iteration 548, loss = 0.23990532\n",
      "Iteration 549, loss = 0.23961231\n",
      "Iteration 550, loss = 0.23931991\n",
      "Iteration 551, loss = 0.23902815\n",
      "Iteration 552, loss = 0.23873710\n",
      "Iteration 553, loss = 0.23844668\n",
      "Iteration 554, loss = 0.23815687\n",
      "Iteration 555, loss = 0.23786769\n",
      "Iteration 556, loss = 0.23757912\n",
      "Iteration 557, loss = 0.23729119\n",
      "Iteration 558, loss = 0.23700391\n",
      "Iteration 559, loss = 0.23671730\n",
      "Iteration 560, loss = 0.23643136\n",
      "Iteration 561, loss = 0.23614616\n",
      "Iteration 562, loss = 0.23586148\n",
      "Iteration 563, loss = 0.23557749\n",
      "Iteration 564, loss = 0.23529415\n",
      "Iteration 565, loss = 0.23501141\n",
      "Iteration 566, loss = 0.23472930\n",
      "Iteration 567, loss = 0.23444780\n",
      "Iteration 568, loss = 0.23416691\n",
      "Iteration 569, loss = 0.23388663\n",
      "Iteration 570, loss = 0.23360696\n",
      "Iteration 571, loss = 0.23332787\n",
      "Iteration 572, loss = 0.23304943\n",
      "Iteration 573, loss = 0.23277163\n",
      "Iteration 574, loss = 0.23249440\n",
      "Iteration 575, loss = 0.23221780\n",
      "Iteration 576, loss = 0.23194178\n",
      "Iteration 577, loss = 0.23166636\n",
      "Iteration 578, loss = 0.23139152\n",
      "Iteration 579, loss = 0.23111728\n",
      "Iteration 580, loss = 0.23084361\n",
      "Iteration 581, loss = 0.23057057\n",
      "Iteration 582, loss = 0.23029807\n",
      "Iteration 583, loss = 0.23002616\n",
      "Iteration 584, loss = 0.22975483\n",
      "Iteration 585, loss = 0.22948409\n",
      "Iteration 586, loss = 0.22921398\n",
      "Iteration 587, loss = 0.22894452\n",
      "Iteration 588, loss = 0.22867559\n",
      "Iteration 589, loss = 0.22840725\n",
      "Iteration 590, loss = 0.22813949\n",
      "Iteration 591, loss = 0.22787231\n",
      "Iteration 592, loss = 0.22760569\n",
      "Iteration 593, loss = 0.22733965\n",
      "Iteration 594, loss = 0.22707417\n",
      "Iteration 595, loss = 0.22680927\n",
      "Iteration 596, loss = 0.22654490\n",
      "Iteration 597, loss = 0.22628111\n",
      "Iteration 598, loss = 0.22601790\n",
      "Iteration 599, loss = 0.22575533\n",
      "Iteration 600, loss = 0.22549337\n",
      "Iteration 601, loss = 0.22523200\n",
      "Iteration 602, loss = 0.22497118\n",
      "Iteration 603, loss = 0.22471099\n",
      "Iteration 604, loss = 0.22445127\n",
      "Iteration 605, loss = 0.22419212\n",
      "Iteration 606, loss = 0.22393352\n",
      "Iteration 607, loss = 0.22367546\n",
      "Iteration 608, loss = 0.22341802\n",
      "Iteration 609, loss = 0.22316115\n",
      "Iteration 610, loss = 0.22290491\n",
      "Iteration 611, loss = 0.22264921\n",
      "Iteration 612, loss = 0.22239404\n",
      "Iteration 613, loss = 0.22213948\n",
      "Iteration 614, loss = 0.22188537\n",
      "Iteration 615, loss = 0.22163181\n",
      "Iteration 616, loss = 0.22137881\n",
      "Iteration 617, loss = 0.22112637\n",
      "Iteration 618, loss = 0.22087441\n",
      "Iteration 619, loss = 0.22062299\n",
      "Iteration 620, loss = 0.22037210\n",
      "Iteration 621, loss = 0.22012178\n",
      "Iteration 622, loss = 0.21987200\n",
      "Iteration 623, loss = 0.21962274\n",
      "Iteration 624, loss = 0.21937399\n",
      "Iteration 625, loss = 0.21912579\n",
      "Iteration 626, loss = 0.21887809\n",
      "Iteration 627, loss = 0.21863099\n",
      "Iteration 628, loss = 0.21838429\n",
      "Iteration 629, loss = 0.21813820\n",
      "Iteration 630, loss = 0.21789264\n",
      "Iteration 631, loss = 0.21764767\n",
      "Iteration 632, loss = 0.21740322\n",
      "Iteration 633, loss = 0.21715929\n",
      "Iteration 634, loss = 0.21691583\n",
      "Iteration 635, loss = 0.21667290\n",
      "Iteration 636, loss = 0.21643056\n",
      "Iteration 637, loss = 0.21618868\n",
      "Iteration 638, loss = 0.21594731\n",
      "Iteration 639, loss = 0.21570646\n",
      "Iteration 640, loss = 0.21546615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 641, loss = 0.21522631\n",
      "Iteration 642, loss = 0.21498698\n",
      "Iteration 643, loss = 0.21474819\n",
      "Iteration 644, loss = 0.21450998\n",
      "Iteration 645, loss = 0.21427240\n",
      "Iteration 646, loss = 0.21403531\n",
      "Iteration 647, loss = 0.21379879\n",
      "Iteration 648, loss = 0.21356277\n",
      "Iteration 649, loss = 0.21332722\n",
      "Iteration 650, loss = 0.21309218\n",
      "Iteration 651, loss = 0.21285773\n",
      "Iteration 652, loss = 0.21262368\n",
      "Iteration 653, loss = 0.21239013\n",
      "Iteration 654, loss = 0.21215718\n",
      "Iteration 655, loss = 0.21192460\n",
      "Iteration 656, loss = 0.21169256\n",
      "Iteration 657, loss = 0.21146105\n",
      "Iteration 658, loss = 0.21123001\n",
      "Iteration 659, loss = 0.21099943\n",
      "Iteration 660, loss = 0.21076933\n",
      "Iteration 661, loss = 0.21053977\n",
      "Iteration 662, loss = 0.21031066\n",
      "Iteration 663, loss = 0.21008203\n",
      "Iteration 664, loss = 0.20985388\n",
      "Iteration 665, loss = 0.20962626\n",
      "Iteration 666, loss = 0.20939905\n",
      "Iteration 667, loss = 0.20917234\n",
      "Iteration 668, loss = 0.20894615\n",
      "Iteration 669, loss = 0.20872039\n",
      "Iteration 670, loss = 0.20849511\n",
      "Iteration 671, loss = 0.20827030\n",
      "Iteration 672, loss = 0.20804604\n",
      "Iteration 673, loss = 0.20782213\n",
      "Iteration 674, loss = 0.20759874\n",
      "Iteration 675, loss = 0.20737586\n",
      "Iteration 676, loss = 0.20715343\n",
      "Iteration 677, loss = 0.20693143\n",
      "Iteration 678, loss = 0.20670994\n",
      "Iteration 679, loss = 0.20648886\n",
      "Iteration 680, loss = 0.20626833\n",
      "Iteration 681, loss = 0.20604818\n",
      "Iteration 682, loss = 0.20582854\n",
      "Iteration 683, loss = 0.20560943\n",
      "Iteration 684, loss = 0.20539072\n",
      "Iteration 685, loss = 0.20517249\n",
      "Iteration 686, loss = 0.20495471\n",
      "Iteration 687, loss = 0.20473737\n",
      "Iteration 688, loss = 0.20452056\n",
      "Iteration 689, loss = 0.20430412\n",
      "Iteration 690, loss = 0.20408815\n",
      "Iteration 691, loss = 0.20387270\n",
      "Iteration 692, loss = 0.20365762\n",
      "Iteration 693, loss = 0.20344303\n",
      "Iteration 694, loss = 0.20322887\n",
      "Iteration 695, loss = 0.20301517\n",
      "Iteration 696, loss = 0.20280196\n",
      "Iteration 697, loss = 0.20258912\n",
      "Iteration 698, loss = 0.20237676\n",
      "Iteration 699, loss = 0.20216484\n",
      "Iteration 700, loss = 0.20195337\n",
      "Iteration 701, loss = 0.20174239\n",
      "Iteration 702, loss = 0.20153179\n",
      "Iteration 703, loss = 0.20132168\n",
      "Iteration 704, loss = 0.20111201\n",
      "Iteration 705, loss = 0.20090276\n",
      "Iteration 706, loss = 0.20069398\n",
      "Iteration 707, loss = 0.20048560\n",
      "Iteration 708, loss = 0.20027774\n",
      "Iteration 709, loss = 0.20007023\n",
      "Iteration 710, loss = 0.19986317\n",
      "Iteration 711, loss = 0.19965664\n",
      "Iteration 712, loss = 0.19945046\n",
      "Iteration 713, loss = 0.19924473\n",
      "Iteration 714, loss = 0.19903941\n",
      "Iteration 715, loss = 0.19883451\n",
      "Iteration 716, loss = 0.19863010\n",
      "Iteration 717, loss = 0.19842604\n",
      "Iteration 718, loss = 0.19822247\n",
      "Iteration 719, loss = 0.19801929\n",
      "Iteration 720, loss = 0.19781655\n",
      "Iteration 721, loss = 0.19761421\n",
      "Iteration 722, loss = 0.19741229\n",
      "Iteration 723, loss = 0.19721081\n",
      "Iteration 724, loss = 0.19700975\n",
      "Iteration 725, loss = 0.19680913\n",
      "Iteration 726, loss = 0.19660893\n",
      "Iteration 727, loss = 0.19640917\n",
      "Iteration 728, loss = 0.19620986\n",
      "Iteration 729, loss = 0.19601089\n",
      "Iteration 730, loss = 0.19581236\n",
      "Iteration 731, loss = 0.19561429\n",
      "Iteration 732, loss = 0.19541660\n",
      "Iteration 733, loss = 0.19521933\n",
      "Iteration 734, loss = 0.19502248\n",
      "Iteration 735, loss = 0.19482603\n",
      "Iteration 736, loss = 0.19463004\n",
      "Iteration 737, loss = 0.19443437\n",
      "Iteration 738, loss = 0.19423917\n",
      "Iteration 739, loss = 0.19404431\n",
      "Iteration 740, loss = 0.19384994\n",
      "Iteration 741, loss = 0.19365589\n",
      "Iteration 742, loss = 0.19346231\n",
      "Iteration 743, loss = 0.19326907\n",
      "Iteration 744, loss = 0.19307631\n",
      "Iteration 745, loss = 0.19288388\n",
      "Iteration 746, loss = 0.19269187\n",
      "Iteration 747, loss = 0.19250025\n",
      "Iteration 748, loss = 0.19230904\n",
      "Iteration 749, loss = 0.19211825\n",
      "Iteration 750, loss = 0.19192780\n",
      "Iteration 751, loss = 0.19173776\n",
      "Iteration 752, loss = 0.19154812\n",
      "Iteration 753, loss = 0.19135890\n",
      "Iteration 754, loss = 0.19117005\n",
      "Iteration 755, loss = 0.19098156\n",
      "Iteration 756, loss = 0.19079349\n",
      "Iteration 757, loss = 0.19060577\n",
      "Iteration 758, loss = 0.19041853\n",
      "Iteration 759, loss = 0.19023157\n",
      "Iteration 760, loss = 0.19004505\n",
      "Iteration 761, loss = 0.18985889\n",
      "Iteration 762, loss = 0.18967315\n",
      "Iteration 763, loss = 0.18948775\n",
      "Iteration 764, loss = 0.18930278\n",
      "Iteration 765, loss = 0.18911813\n",
      "Iteration 766, loss = 0.18893388\n",
      "Iteration 767, loss = 0.18875008\n",
      "Iteration 768, loss = 0.18856655\n",
      "Iteration 769, loss = 0.18838342\n",
      "Iteration 770, loss = 0.18820074\n",
      "Iteration 771, loss = 0.18801838\n",
      "Iteration 772, loss = 0.18783636\n",
      "Iteration 773, loss = 0.18765472\n",
      "Iteration 774, loss = 0.18747341\n",
      "Iteration 775, loss = 0.18729252\n",
      "Iteration 776, loss = 0.18711202\n",
      "Iteration 777, loss = 0.18693179\n",
      "Iteration 778, loss = 0.18675197\n",
      "Iteration 779, loss = 0.18657259\n",
      "Iteration 780, loss = 0.18639354\n",
      "Iteration 781, loss = 0.18621488\n",
      "Iteration 782, loss = 0.18603657\n",
      "Iteration 783, loss = 0.18585863\n",
      "Iteration 784, loss = 0.18568105\n",
      "Iteration 785, loss = 0.18550386\n",
      "Iteration 786, loss = 0.18532701\n",
      "Iteration 787, loss = 0.18515051\n",
      "Iteration 788, loss = 0.18497440\n",
      "Iteration 789, loss = 0.18479865\n",
      "Iteration 790, loss = 0.18462325\n",
      "Iteration 791, loss = 0.18444820\n",
      "Iteration 792, loss = 0.18427352\n",
      "Iteration 793, loss = 0.18409918\n",
      "Iteration 794, loss = 0.18392525\n",
      "Iteration 795, loss = 0.18375166\n",
      "Iteration 796, loss = 0.18357844\n",
      "Iteration 797, loss = 0.18340562\n",
      "Iteration 798, loss = 0.18323314\n",
      "Iteration 799, loss = 0.18306101\n",
      "Iteration 800, loss = 0.18288926\n",
      "Iteration 801, loss = 0.18271783\n",
      "Iteration 802, loss = 0.18254675\n",
      "Iteration 803, loss = 0.18237605\n",
      "Iteration 804, loss = 0.18220567\n",
      "Iteration 805, loss = 0.18203568\n",
      "Iteration 806, loss = 0.18186597\n",
      "Iteration 807, loss = 0.18169667\n",
      "Iteration 808, loss = 0.18152770\n",
      "Iteration 809, loss = 0.18135906\n",
      "Iteration 810, loss = 0.18119078\n",
      "Iteration 811, loss = 0.18102282\n",
      "Iteration 812, loss = 0.18085526\n",
      "Iteration 813, loss = 0.18068808\n",
      "Iteration 814, loss = 0.18052120\n",
      "Iteration 815, loss = 0.18035468\n",
      "Iteration 816, loss = 0.18018851\n",
      "Iteration 817, loss = 0.18002268\n",
      "Iteration 818, loss = 0.17985720\n",
      "Iteration 819, loss = 0.17969202\n",
      "Iteration 820, loss = 0.17952720\n",
      "Iteration 821, loss = 0.17936273\n",
      "Iteration 822, loss = 0.17919857\n",
      "Iteration 823, loss = 0.17903476\n",
      "Iteration 824, loss = 0.17887128\n",
      "Iteration 825, loss = 0.17870812\n",
      "Iteration 826, loss = 0.17854532\n",
      "Iteration 827, loss = 0.17838285\n",
      "Iteration 828, loss = 0.17822070\n",
      "Iteration 829, loss = 0.17805889\n",
      "Iteration 830, loss = 0.17789741\n",
      "Iteration 831, loss = 0.17773625\n",
      "Iteration 832, loss = 0.17757544\n",
      "Iteration 833, loss = 0.17741492\n",
      "Iteration 834, loss = 0.17725473\n",
      "Iteration 835, loss = 0.17709492\n",
      "Iteration 836, loss = 0.17693538\n",
      "Iteration 837, loss = 0.17677618\n",
      "Iteration 838, loss = 0.17661730\n",
      "Iteration 839, loss = 0.17645873\n",
      "Iteration 840, loss = 0.17630051\n",
      "Iteration 841, loss = 0.17614258\n",
      "Iteration 842, loss = 0.17598498\n",
      "Iteration 843, loss = 0.17582769\n",
      "Iteration 844, loss = 0.17567074\n",
      "Iteration 845, loss = 0.17551409\n",
      "Iteration 846, loss = 0.17535775\n",
      "Iteration 847, loss = 0.17520170\n",
      "Iteration 848, loss = 0.17504601\n",
      "Iteration 849, loss = 0.17489062\n",
      "Iteration 850, loss = 0.17473553\n",
      "Iteration 851, loss = 0.17458074\n",
      "Iteration 852, loss = 0.17442630\n",
      "Iteration 853, loss = 0.17427216\n",
      "Iteration 854, loss = 0.17411831\n",
      "Iteration 855, loss = 0.17396479\n",
      "Iteration 856, loss = 0.17381155\n",
      "Iteration 857, loss = 0.17365864\n",
      "Iteration 858, loss = 0.17350604\n",
      "Iteration 859, loss = 0.17335373\n",
      "Iteration 860, loss = 0.17320174\n",
      "Iteration 861, loss = 0.17305003\n",
      "Iteration 862, loss = 0.17289868\n",
      "Iteration 863, loss = 0.17274757\n",
      "Iteration 864, loss = 0.17259678\n",
      "Iteration 865, loss = 0.17244630\n",
      "Iteration 866, loss = 0.17229614\n",
      "Iteration 867, loss = 0.17214626\n",
      "Iteration 868, loss = 0.17199669\n",
      "Iteration 869, loss = 0.17184740\n",
      "Iteration 870, loss = 0.17169840\n",
      "Iteration 871, loss = 0.17154973\n",
      "Iteration 872, loss = 0.17140135\n",
      "Iteration 873, loss = 0.17125324\n",
      "Iteration 874, loss = 0.17110544\n",
      "Iteration 875, loss = 0.17095796\n",
      "Iteration 876, loss = 0.17081075\n",
      "Iteration 877, loss = 0.17066383\n",
      "Iteration 878, loss = 0.17051720\n",
      "Iteration 879, loss = 0.17037086\n",
      "Iteration 880, loss = 0.17022487\n",
      "Iteration 881, loss = 0.17007909\n",
      "Iteration 882, loss = 0.16993363\n",
      "Iteration 883, loss = 0.16978845\n",
      "Iteration 884, loss = 0.16964360\n",
      "Iteration 885, loss = 0.16949900\n",
      "Iteration 886, loss = 0.16935469\n",
      "Iteration 887, loss = 0.16921066\n",
      "Iteration 888, loss = 0.16906692\n",
      "Iteration 889, loss = 0.16892349\n",
      "Iteration 890, loss = 0.16878034\n",
      "Iteration 891, loss = 0.16863745\n",
      "Iteration 892, loss = 0.16849486\n",
      "Iteration 893, loss = 0.16835252\n",
      "Iteration 894, loss = 0.16821053\n",
      "Iteration 895, loss = 0.16806874\n",
      "Iteration 896, loss = 0.16792727\n",
      "Iteration 897, loss = 0.16778608\n",
      "Iteration 898, loss = 0.16764516\n",
      "Iteration 899, loss = 0.16750452\n",
      "Iteration 900, loss = 0.16736415\n",
      "Iteration 901, loss = 0.16722406\n",
      "Iteration 902, loss = 0.16708424\n",
      "Iteration 903, loss = 0.16694471\n",
      "Iteration 904, loss = 0.16680545\n",
      "Iteration 905, loss = 0.16666645\n",
      "Iteration 906, loss = 0.16652773\n",
      "Iteration 907, loss = 0.16638931\n",
      "Iteration 908, loss = 0.16625113\n",
      "Iteration 909, loss = 0.16611323\n",
      "Iteration 910, loss = 0.16597559\n",
      "Iteration 911, loss = 0.16583826\n",
      "Iteration 912, loss = 0.16570117\n",
      "Iteration 913, loss = 0.16556434\n",
      "Iteration 914, loss = 0.16542777\n",
      "Iteration 915, loss = 0.16529147\n",
      "Iteration 916, loss = 0.16515546\n",
      "Iteration 917, loss = 0.16501970\n",
      "Iteration 918, loss = 0.16488419\n",
      "Iteration 919, loss = 0.16474895\n",
      "Iteration 920, loss = 0.16461397\n",
      "Iteration 921, loss = 0.16447928\n",
      "Iteration 922, loss = 0.16434483\n",
      "Iteration 923, loss = 0.16421064\n",
      "Iteration 924, loss = 0.16407671\n",
      "Iteration 925, loss = 0.16394308\n",
      "Iteration 926, loss = 0.16380966\n",
      "Iteration 927, loss = 0.16367652\n",
      "Iteration 928, loss = 0.16354355\n",
      "Iteration 929, loss = 0.16341079\n",
      "Iteration 930, loss = 0.16327825\n",
      "Iteration 931, loss = 0.16314594\n",
      "Iteration 932, loss = 0.16301388\n",
      "Iteration 933, loss = 0.16288205\n",
      "Iteration 934, loss = 0.16275053\n",
      "Iteration 935, loss = 0.16261919\n",
      "Iteration 936, loss = 0.16248813\n",
      "Iteration 937, loss = 0.16235732\n",
      "Iteration 938, loss = 0.16222677\n",
      "Iteration 939, loss = 0.16209647\n",
      "Iteration 940, loss = 0.16196640\n",
      "Iteration 941, loss = 0.16183658\n",
      "Iteration 942, loss = 0.16170700\n",
      "Iteration 943, loss = 0.16157772\n",
      "Iteration 944, loss = 0.16144862\n",
      "Iteration 945, loss = 0.16131980\n",
      "Iteration 946, loss = 0.16119133\n",
      "Iteration 947, loss = 0.16106338\n",
      "Iteration 948, loss = 0.16093567\n",
      "Iteration 949, loss = 0.16080822\n",
      "Iteration 950, loss = 0.16068100\n",
      "Iteration 951, loss = 0.16055403\n",
      "Iteration 952, loss = 0.16042735\n",
      "Iteration 953, loss = 0.16030085\n",
      "Iteration 954, loss = 0.16017462\n",
      "Iteration 955, loss = 0.16004869\n",
      "Iteration 956, loss = 0.15992293\n",
      "Iteration 957, loss = 0.15979745\n",
      "Iteration 958, loss = 0.15967220\n",
      "Iteration 959, loss = 0.15954719\n",
      "Iteration 960, loss = 0.15942247\n",
      "Iteration 961, loss = 0.15929792\n",
      "Iteration 962, loss = 0.15917363\n",
      "Iteration 963, loss = 0.15904961\n",
      "Iteration 964, loss = 0.15892581\n",
      "Iteration 965, loss = 0.15880225\n",
      "Iteration 966, loss = 0.15867892\n",
      "Iteration 967, loss = 0.15855583\n",
      "Iteration 968, loss = 0.15843301\n",
      "Iteration 969, loss = 0.15831037\n",
      "Iteration 970, loss = 0.15818799\n",
      "Iteration 971, loss = 0.15806586\n",
      "Iteration 972, loss = 0.15794397\n",
      "Iteration 973, loss = 0.15782229\n",
      "Iteration 974, loss = 0.15770085\n",
      "Iteration 975, loss = 0.15757963\n",
      "Iteration 976, loss = 0.15745867\n",
      "Iteration 977, loss = 0.15733792\n",
      "Iteration 978, loss = 0.15721740\n",
      "Iteration 979, loss = 0.15709711\n",
      "Iteration 980, loss = 0.15697709\n",
      "Iteration 981, loss = 0.15685724\n",
      "Iteration 982, loss = 0.15673765\n",
      "Iteration 983, loss = 0.15661828\n",
      "Iteration 984, loss = 0.15649917\n",
      "Iteration 985, loss = 0.15638025\n",
      "Iteration 986, loss = 0.15626156\n",
      "Iteration 987, loss = 0.15614310\n",
      "Iteration 988, loss = 0.15602487\n",
      "Iteration 989, loss = 0.15590687\n",
      "Iteration 990, loss = 0.15578909\n",
      "Iteration 991, loss = 0.15567153\n",
      "Iteration 992, loss = 0.15555423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 993, loss = 0.15543710\n",
      "Iteration 994, loss = 0.15532021\n",
      "Iteration 995, loss = 0.15520354\n",
      "Iteration 996, loss = 0.15508706\n",
      "Iteration 997, loss = 0.15497078\n",
      "Iteration 998, loss = 0.15485470\n",
      "Iteration 999, loss = 0.15473883\n",
      "Iteration 1000, loss = 0.15462321\n",
      "Iteration 1, loss = 1.48086253\n",
      "Iteration 2, loss = 1.46726543\n",
      "Iteration 3, loss = 1.44815339\n",
      "Iteration 4, loss = 1.42442838\n",
      "Iteration 5, loss = 1.39695429\n",
      "Iteration 6, loss = 1.36663728\n",
      "Iteration 7, loss = 1.33439489\n",
      "Iteration 8, loss = 1.30117824\n",
      "Iteration 9, loss = 1.26782317\n",
      "Iteration 10, loss = 1.23552571\n",
      "Iteration 11, loss = 1.20487621\n",
      "Iteration 12, loss = 1.17604309\n",
      "Iteration 13, loss = 1.14957240\n",
      "Iteration 14, loss = 1.12559176\n",
      "Iteration 15, loss = 1.10442006\n",
      "Iteration 16, loss = 1.08594465\n",
      "Iteration 17, loss = 1.06966032\n",
      "Iteration 18, loss = 1.05524199\n",
      "Iteration 19, loss = 1.04218829\n",
      "Iteration 20, loss = 1.03016984\n",
      "Iteration 21, loss = 1.01861807\n",
      "Iteration 22, loss = 1.00713610\n",
      "Iteration 23, loss = 0.99553662\n",
      "Iteration 24, loss = 0.98385790\n",
      "Iteration 25, loss = 0.97207235\n",
      "Iteration 26, loss = 0.96019370\n",
      "Iteration 27, loss = 0.94828825\n",
      "Iteration 28, loss = 0.93642900\n",
      "Iteration 29, loss = 0.92482846\n",
      "Iteration 30, loss = 0.91342053\n",
      "Iteration 31, loss = 0.90224842\n",
      "Iteration 32, loss = 0.89133621\n",
      "Iteration 33, loss = 0.88069594\n",
      "Iteration 34, loss = 0.87038962\n",
      "Iteration 35, loss = 0.86043220\n",
      "Iteration 36, loss = 0.85077349\n",
      "Iteration 37, loss = 0.84146314\n",
      "Iteration 38, loss = 0.83242061\n",
      "Iteration 39, loss = 0.82357661\n",
      "Iteration 40, loss = 0.81486482\n",
      "Iteration 41, loss = 0.80635064\n",
      "Iteration 42, loss = 0.79799501\n",
      "Iteration 43, loss = 0.78994108\n",
      "Iteration 44, loss = 0.78214620\n",
      "Iteration 45, loss = 0.77465344\n",
      "Iteration 46, loss = 0.76740757\n",
      "Iteration 47, loss = 0.76043088\n",
      "Iteration 48, loss = 0.75370128\n",
      "Iteration 49, loss = 0.74720726\n",
      "Iteration 50, loss = 0.74098738\n",
      "Iteration 51, loss = 0.73497066\n",
      "Iteration 52, loss = 0.72913536\n",
      "Iteration 53, loss = 0.72348707\n",
      "Iteration 54, loss = 0.71797754\n",
      "Iteration 55, loss = 0.71260491\n",
      "Iteration 56, loss = 0.70736614\n",
      "Iteration 57, loss = 0.70225014\n",
      "Iteration 58, loss = 0.69724916\n",
      "Iteration 59, loss = 0.69235838\n",
      "Iteration 60, loss = 0.68757301\n",
      "Iteration 61, loss = 0.68289728\n",
      "Iteration 62, loss = 0.67832683\n",
      "Iteration 63, loss = 0.67385433\n",
      "Iteration 64, loss = 0.66947483\n",
      "Iteration 65, loss = 0.66518102\n",
      "Iteration 66, loss = 0.66096980\n",
      "Iteration 67, loss = 0.65684024\n",
      "Iteration 68, loss = 0.65278706\n",
      "Iteration 69, loss = 0.64881102\n",
      "Iteration 70, loss = 0.64491185\n",
      "Iteration 71, loss = 0.64109127\n",
      "Iteration 72, loss = 0.63735051\n",
      "Iteration 73, loss = 0.63367991\n",
      "Iteration 74, loss = 0.63007907\n",
      "Iteration 75, loss = 0.62654530\n",
      "Iteration 76, loss = 0.62307456\n",
      "Iteration 77, loss = 0.61965988\n",
      "Iteration 78, loss = 0.61630148\n",
      "Iteration 79, loss = 0.61299251\n",
      "Iteration 80, loss = 0.60973302\n",
      "Iteration 81, loss = 0.60652640\n",
      "Iteration 82, loss = 0.60336998\n",
      "Iteration 83, loss = 0.60026396\n",
      "Iteration 84, loss = 0.59721660\n",
      "Iteration 85, loss = 0.59421551\n",
      "Iteration 86, loss = 0.59126090\n",
      "Iteration 87, loss = 0.58835359\n",
      "Iteration 88, loss = 0.58549000\n",
      "Iteration 89, loss = 0.58266912\n",
      "Iteration 90, loss = 0.57988720\n",
      "Iteration 91, loss = 0.57714559\n",
      "Iteration 92, loss = 0.57443931\n",
      "Iteration 93, loss = 0.57177569\n",
      "Iteration 94, loss = 0.56915523\n",
      "Iteration 95, loss = 0.56656953\n",
      "Iteration 96, loss = 0.56402858\n",
      "Iteration 97, loss = 0.56153649\n",
      "Iteration 98, loss = 0.55908271\n",
      "Iteration 99, loss = 0.55666270\n",
      "Iteration 100, loss = 0.55427624\n",
      "Iteration 101, loss = 0.55192523\n",
      "Iteration 102, loss = 0.54960995\n",
      "Iteration 103, loss = 0.54732944\n",
      "Iteration 104, loss = 0.54508254\n",
      "Iteration 105, loss = 0.54286797\n",
      "Iteration 106, loss = 0.54068922\n",
      "Iteration 107, loss = 0.53855628\n",
      "Iteration 108, loss = 0.53646766\n",
      "Iteration 109, loss = 0.53441623\n",
      "Iteration 110, loss = 0.53239751\n",
      "Iteration 111, loss = 0.53040769\n",
      "Iteration 112, loss = 0.52844550\n",
      "Iteration 113, loss = 0.52651541\n",
      "Iteration 114, loss = 0.52461000\n",
      "Iteration 115, loss = 0.52272882\n",
      "Iteration 116, loss = 0.52086938\n",
      "Iteration 117, loss = 0.51903036\n",
      "Iteration 118, loss = 0.51721098\n",
      "Iteration 119, loss = 0.51541147\n",
      "Iteration 120, loss = 0.51363299\n",
      "Iteration 121, loss = 0.51187550\n",
      "Iteration 122, loss = 0.51013846\n",
      "Iteration 123, loss = 0.50842100\n",
      "Iteration 124, loss = 0.50672506\n",
      "Iteration 125, loss = 0.50505010\n",
      "Iteration 126, loss = 0.50339646\n",
      "Iteration 127, loss = 0.50176202\n",
      "Iteration 128, loss = 0.50014636\n",
      "Iteration 129, loss = 0.49854916\n",
      "Iteration 130, loss = 0.49696955\n",
      "Iteration 131, loss = 0.49540720\n",
      "Iteration 132, loss = 0.49386174\n",
      "Iteration 133, loss = 0.49233441\n",
      "Iteration 134, loss = 0.49082329\n",
      "Iteration 135, loss = 0.48932821\n",
      "Iteration 136, loss = 0.48784939\n",
      "Iteration 137, loss = 0.48638595\n",
      "Iteration 138, loss = 0.48493764\n",
      "Iteration 139, loss = 0.48350434\n",
      "Iteration 140, loss = 0.48208587\n",
      "Iteration 141, loss = 0.48068158\n",
      "Iteration 142, loss = 0.47929174\n",
      "Iteration 143, loss = 0.47791574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 144, loss = 0.47655308\n",
      "Iteration 145, loss = 0.47520341\n",
      "Iteration 146, loss = 0.47386677\n",
      "Iteration 147, loss = 0.47254255\n",
      "Iteration 148, loss = 0.47123071\n",
      "Iteration 149, loss = 0.46993138\n",
      "Iteration 150, loss = 0.46864404\n",
      "Iteration 151, loss = 0.46736858\n",
      "Iteration 152, loss = 0.46610482\n",
      "Iteration 153, loss = 0.46485239\n",
      "Iteration 154, loss = 0.46361152\n",
      "Iteration 155, loss = 0.46238166\n",
      "Iteration 156, loss = 0.46116259\n",
      "Iteration 157, loss = 0.45995406\n",
      "Iteration 158, loss = 0.45875595\n",
      "Iteration 159, loss = 0.45756794\n",
      "Iteration 160, loss = 0.45638965\n",
      "Iteration 161, loss = 0.45522098\n",
      "Iteration 162, loss = 0.45406190\n",
      "Iteration 163, loss = 0.45291235\n",
      "Iteration 164, loss = 0.45177217\n",
      "Iteration 165, loss = 0.45064136\n",
      "Iteration 166, loss = 0.44951949\n",
      "Iteration 167, loss = 0.44840659\n",
      "Iteration 168, loss = 0.44730218\n",
      "Iteration 169, loss = 0.44620621\n",
      "Iteration 170, loss = 0.44511856\n",
      "Iteration 171, loss = 0.44403919\n",
      "Iteration 172, loss = 0.44296787\n",
      "Iteration 173, loss = 0.44190499\n",
      "Iteration 174, loss = 0.44084985\n",
      "Iteration 175, loss = 0.43980341\n",
      "Iteration 176, loss = 0.43876466\n",
      "Iteration 177, loss = 0.43773367\n",
      "Iteration 178, loss = 0.43671014\n",
      "Iteration 179, loss = 0.43569396\n",
      "Iteration 180, loss = 0.43468503\n",
      "Iteration 181, loss = 0.43368341\n",
      "Iteration 182, loss = 0.43268904\n",
      "Iteration 183, loss = 0.43170166\n",
      "Iteration 184, loss = 0.43072103\n",
      "Iteration 185, loss = 0.42974719\n",
      "Iteration 186, loss = 0.42878005\n",
      "Iteration 187, loss = 0.42781940\n",
      "Iteration 188, loss = 0.42686541\n",
      "Iteration 189, loss = 0.42591744\n",
      "Iteration 190, loss = 0.42497668\n",
      "Iteration 191, loss = 0.42404212\n",
      "Iteration 192, loss = 0.42311410\n",
      "Iteration 193, loss = 0.42219128\n",
      "Iteration 194, loss = 0.42127485\n",
      "Iteration 195, loss = 0.42036421\n",
      "Iteration 196, loss = 0.41945937\n",
      "Iteration 197, loss = 0.41856017\n",
      "Iteration 198, loss = 0.41766663\n",
      "Iteration 199, loss = 0.41677856\n",
      "Iteration 200, loss = 0.41589589\n",
      "Iteration 201, loss = 0.41501869\n",
      "Iteration 202, loss = 0.41414663\n",
      "Iteration 203, loss = 0.41327991\n",
      "Iteration 204, loss = 0.41241833\n",
      "Iteration 205, loss = 0.41156178\n",
      "Iteration 206, loss = 0.41071031\n",
      "Iteration 207, loss = 0.40986370\n",
      "Iteration 208, loss = 0.40902193\n",
      "Iteration 209, loss = 0.40818500\n",
      "Iteration 210, loss = 0.40735276\n",
      "Iteration 211, loss = 0.40652523\n",
      "Iteration 212, loss = 0.40570217\n",
      "Iteration 213, loss = 0.40488385\n",
      "Iteration 214, loss = 0.40406984\n",
      "Iteration 215, loss = 0.40326030\n",
      "Iteration 216, loss = 0.40245526\n",
      "Iteration 217, loss = 0.40165449\n",
      "Iteration 218, loss = 0.40085794\n",
      "Iteration 219, loss = 0.40006568\n",
      "Iteration 220, loss = 0.39927764\n",
      "Iteration 221, loss = 0.39849381\n",
      "Iteration 222, loss = 0.39771409\n",
      "Iteration 223, loss = 0.39693821\n",
      "Iteration 224, loss = 0.39616634\n",
      "Iteration 225, loss = 0.39539846\n",
      "Iteration 226, loss = 0.39463476\n",
      "Iteration 227, loss = 0.39387514\n",
      "Iteration 228, loss = 0.39311948\n",
      "Iteration 229, loss = 0.39236809\n",
      "Iteration 230, loss = 0.39162052\n",
      "Iteration 231, loss = 0.39087707\n",
      "Iteration 232, loss = 0.39013675\n",
      "Iteration 233, loss = 0.38940041\n",
      "Iteration 234, loss = 0.38866727\n",
      "Iteration 235, loss = 0.38793783\n",
      "Iteration 236, loss = 0.38721171\n",
      "Iteration 237, loss = 0.38648914\n",
      "Iteration 238, loss = 0.38577006\n",
      "Iteration 239, loss = 0.38505430\n",
      "Iteration 240, loss = 0.38434190\n",
      "Iteration 241, loss = 0.38363282\n",
      "Iteration 242, loss = 0.38292691\n",
      "Iteration 243, loss = 0.38222425\n",
      "Iteration 244, loss = 0.38152485\n",
      "Iteration 245, loss = 0.38082854\n",
      "Iteration 246, loss = 0.38013535\n",
      "Iteration 247, loss = 0.37944530\n",
      "Iteration 248, loss = 0.37875840\n",
      "Iteration 249, loss = 0.37807442\n",
      "Iteration 250, loss = 0.37739338\n",
      "Iteration 251, loss = 0.37671547\n",
      "Iteration 252, loss = 0.37604023\n",
      "Iteration 253, loss = 0.37536793\n",
      "Iteration 254, loss = 0.37469786\n",
      "Iteration 255, loss = 0.37403058\n",
      "Iteration 256, loss = 0.37336610\n",
      "Iteration 257, loss = 0.37270426\n",
      "Iteration 258, loss = 0.37204521\n",
      "Iteration 259, loss = 0.37138876\n",
      "Iteration 260, loss = 0.37073511\n",
      "Iteration 261, loss = 0.37008396\n",
      "Iteration 262, loss = 0.36943558\n",
      "Iteration 263, loss = 0.36878970\n",
      "Iteration 264, loss = 0.36814636\n",
      "Iteration 265, loss = 0.36750558\n",
      "Iteration 266, loss = 0.36686755\n",
      "Iteration 267, loss = 0.36623185\n",
      "Iteration 268, loss = 0.36559869\n",
      "Iteration 269, loss = 0.36496739\n",
      "Iteration 270, loss = 0.36433817\n",
      "Iteration 271, loss = 0.36371136\n",
      "Iteration 272, loss = 0.36308682\n",
      "Iteration 273, loss = 0.36246460\n",
      "Iteration 274, loss = 0.36184473\n",
      "Iteration 275, loss = 0.36122715\n",
      "Iteration 276, loss = 0.36061184\n",
      "Iteration 277, loss = 0.35999884\n",
      "Iteration 278, loss = 0.35938810\n",
      "Iteration 279, loss = 0.35877952\n",
      "Iteration 280, loss = 0.35817327\n",
      "Iteration 281, loss = 0.35756905\n",
      "Iteration 282, loss = 0.35696593\n",
      "Iteration 283, loss = 0.35636492\n",
      "Iteration 284, loss = 0.35576567\n",
      "Iteration 285, loss = 0.35516763\n",
      "Iteration 286, loss = 0.35457155\n",
      "Iteration 287, loss = 0.35397734\n",
      "Iteration 288, loss = 0.35338523\n",
      "Iteration 289, loss = 0.35279505\n",
      "Iteration 290, loss = 0.35220679\n",
      "Iteration 291, loss = 0.35162053\n",
      "Iteration 292, loss = 0.35103620\n",
      "Iteration 293, loss = 0.35045388\n",
      "Iteration 294, loss = 0.34987350\n",
      "Iteration 295, loss = 0.34929508\n",
      "Iteration 296, loss = 0.34871854\n",
      "Iteration 297, loss = 0.34814349\n",
      "Iteration 298, loss = 0.34756948\n",
      "Iteration 299, loss = 0.34699734\n",
      "Iteration 300, loss = 0.34642638\n",
      "Iteration 301, loss = 0.34585564\n",
      "Iteration 302, loss = 0.34528561\n",
      "Iteration 303, loss = 0.34471708\n",
      "Iteration 304, loss = 0.34415017\n",
      "Iteration 305, loss = 0.34358482\n",
      "Iteration 306, loss = 0.34302112\n",
      "Iteration 307, loss = 0.34245909\n",
      "Iteration 308, loss = 0.34189870\n",
      "Iteration 309, loss = 0.34134003\n",
      "Iteration 310, loss = 0.34078300\n",
      "Iteration 311, loss = 0.34022770\n",
      "Iteration 312, loss = 0.33967410\n",
      "Iteration 313, loss = 0.33912216\n",
      "Iteration 314, loss = 0.33857200\n",
      "Iteration 315, loss = 0.33802279\n",
      "Iteration 316, loss = 0.33747479\n",
      "Iteration 317, loss = 0.33692844\n",
      "Iteration 318, loss = 0.33638370\n",
      "Iteration 319, loss = 0.33584064\n",
      "Iteration 320, loss = 0.33529914\n",
      "Iteration 321, loss = 0.33475873\n",
      "Iteration 322, loss = 0.33421977\n",
      "Iteration 323, loss = 0.33368211\n",
      "Iteration 324, loss = 0.33314241\n",
      "Iteration 325, loss = 0.33259992\n",
      "Iteration 326, loss = 0.33205526\n",
      "Iteration 327, loss = 0.33150809\n",
      "Iteration 328, loss = 0.33096019\n",
      "Iteration 329, loss = 0.33041130\n",
      "Iteration 330, loss = 0.32986263\n",
      "Iteration 331, loss = 0.32931267\n",
      "Iteration 332, loss = 0.32876348\n",
      "Iteration 333, loss = 0.32821873\n",
      "Iteration 334, loss = 0.32767341\n",
      "Iteration 335, loss = 0.32712856\n",
      "Iteration 336, loss = 0.32658245\n",
      "Iteration 337, loss = 0.32603315\n",
      "Iteration 338, loss = 0.32548230\n",
      "Iteration 339, loss = 0.32493641\n",
      "Iteration 340, loss = 0.32439632\n",
      "Iteration 341, loss = 0.32385512\n",
      "Iteration 342, loss = 0.32331341\n",
      "Iteration 343, loss = 0.32277636\n",
      "Iteration 344, loss = 0.32224182\n",
      "Iteration 345, loss = 0.32170903\n",
      "Iteration 346, loss = 0.32117793\n",
      "Iteration 347, loss = 0.32064850\n",
      "Iteration 348, loss = 0.32012051\n",
      "Iteration 349, loss = 0.31959077\n",
      "Iteration 350, loss = 0.31906533\n",
      "Iteration 351, loss = 0.31853872\n",
      "Iteration 352, loss = 0.31801184\n",
      "Iteration 353, loss = 0.31748525\n",
      "Iteration 354, loss = 0.31695783\n",
      "Iteration 355, loss = 0.31642468\n",
      "Iteration 356, loss = 0.31588661\n",
      "Iteration 357, loss = 0.31535200\n",
      "Iteration 358, loss = 0.31482292\n",
      "Iteration 359, loss = 0.31430604\n",
      "Iteration 360, loss = 0.31379253\n",
      "Iteration 361, loss = 0.31328235\n",
      "Iteration 362, loss = 0.31277491\n",
      "Iteration 363, loss = 0.31227340\n",
      "Iteration 364, loss = 0.31178640\n",
      "Iteration 365, loss = 0.31130283\n",
      "Iteration 366, loss = 0.31081990\n",
      "Iteration 367, loss = 0.31034223\n",
      "Iteration 368, loss = 0.30986738\n",
      "Iteration 369, loss = 0.30939713\n",
      "Iteration 370, loss = 0.30893036\n",
      "Iteration 371, loss = 0.30846762\n",
      "Iteration 372, loss = 0.30800711\n",
      "Iteration 373, loss = 0.30755084\n",
      "Iteration 374, loss = 0.30710103\n",
      "Iteration 375, loss = 0.30665238\n",
      "Iteration 376, loss = 0.30620518\n",
      "Iteration 377, loss = 0.30575932\n",
      "Iteration 378, loss = 0.30531483\n",
      "Iteration 379, loss = 0.30487208\n",
      "Iteration 380, loss = 0.30443123\n",
      "Iteration 381, loss = 0.30399164\n",
      "Iteration 382, loss = 0.30355331\n",
      "Iteration 383, loss = 0.30311615\n",
      "Iteration 384, loss = 0.30268030\n",
      "Iteration 385, loss = 0.30224561\n",
      "Iteration 386, loss = 0.30181249\n",
      "Iteration 387, loss = 0.30138052\n",
      "Iteration 388, loss = 0.30095002\n",
      "Iteration 389, loss = 0.30052087\n",
      "Iteration 390, loss = 0.30009278\n",
      "Iteration 391, loss = 0.29966590\n",
      "Iteration 392, loss = 0.29924001\n",
      "Iteration 393, loss = 0.29881531\n",
      "Iteration 394, loss = 0.29839164\n",
      "Iteration 395, loss = 0.29796907\n",
      "Iteration 396, loss = 0.29754757\n",
      "Iteration 397, loss = 0.29712718\n",
      "Iteration 398, loss = 0.29670780\n",
      "Iteration 399, loss = 0.29628950\n",
      "Iteration 400, loss = 0.29587224\n",
      "Iteration 401, loss = 0.29545603\n",
      "Iteration 402, loss = 0.29504087\n",
      "Iteration 403, loss = 0.29462670\n",
      "Iteration 404, loss = 0.29421359\n",
      "Iteration 405, loss = 0.29380148\n",
      "Iteration 406, loss = 0.29339042\n",
      "Iteration 407, loss = 0.29298032\n",
      "Iteration 408, loss = 0.29257161\n",
      "Iteration 409, loss = 0.29216377\n",
      "Iteration 410, loss = 0.29175686\n",
      "Iteration 411, loss = 0.29135093\n",
      "Iteration 412, loss = 0.29094588\n",
      "Iteration 413, loss = 0.29054181\n",
      "Iteration 414, loss = 0.29013888\n",
      "Iteration 415, loss = 0.28973697\n",
      "Iteration 416, loss = 0.28933595\n",
      "Iteration 417, loss = 0.28893591\n",
      "Iteration 418, loss = 0.28853682\n",
      "Iteration 419, loss = 0.28813863\n",
      "Iteration 420, loss = 0.28774154\n",
      "Iteration 421, loss = 0.28734533\n",
      "Iteration 422, loss = 0.28695008\n",
      "Iteration 423, loss = 0.28655580\n",
      "Iteration 424, loss = 0.28616243\n",
      "Iteration 425, loss = 0.28577002\n",
      "Iteration 426, loss = 0.28537855\n",
      "Iteration 427, loss = 0.28498798\n",
      "Iteration 428, loss = 0.28459838\n",
      "Iteration 429, loss = 0.28420964\n",
      "Iteration 430, loss = 0.28382190\n",
      "Iteration 431, loss = 0.28343501\n",
      "Iteration 432, loss = 0.28304903\n",
      "Iteration 433, loss = 0.28266397\n",
      "Iteration 434, loss = 0.28227985\n",
      "Iteration 435, loss = 0.28189660\n",
      "Iteration 436, loss = 0.28151423\n",
      "Iteration 437, loss = 0.28113281\n",
      "Iteration 438, loss = 0.28075222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 439, loss = 0.28037258\n",
      "Iteration 440, loss = 0.27999374\n",
      "Iteration 441, loss = 0.27961587\n",
      "Iteration 442, loss = 0.27923881\n",
      "Iteration 443, loss = 0.27886271\n",
      "Iteration 444, loss = 0.27848741\n",
      "Iteration 445, loss = 0.27811295\n",
      "Iteration 446, loss = 0.27773939\n",
      "Iteration 447, loss = 0.27736673\n",
      "Iteration 448, loss = 0.27699488\n",
      "Iteration 449, loss = 0.27662394\n",
      "Iteration 450, loss = 0.27625380\n",
      "Iteration 451, loss = 0.27588455\n",
      "Iteration 452, loss = 0.27551613\n",
      "Iteration 453, loss = 0.27514855\n",
      "Iteration 454, loss = 0.27478183\n",
      "Iteration 455, loss = 0.27441593\n",
      "Iteration 456, loss = 0.27405088\n",
      "Iteration 457, loss = 0.27368665\n",
      "Iteration 458, loss = 0.27332330\n",
      "Iteration 459, loss = 0.27296066\n",
      "Iteration 460, loss = 0.27259899\n",
      "Iteration 461, loss = 0.27223799\n",
      "Iteration 462, loss = 0.27187797\n",
      "Iteration 463, loss = 0.27151863\n",
      "Iteration 464, loss = 0.27116016\n",
      "Iteration 465, loss = 0.27080247\n",
      "Iteration 466, loss = 0.27044562\n",
      "Iteration 467, loss = 0.27008956\n",
      "Iteration 468, loss = 0.26973433\n",
      "Iteration 469, loss = 0.26937986\n",
      "Iteration 470, loss = 0.26902619\n",
      "Iteration 471, loss = 0.26867339\n",
      "Iteration 472, loss = 0.26832127\n",
      "Iteration 473, loss = 0.26797005\n",
      "Iteration 474, loss = 0.26761951\n",
      "Iteration 475, loss = 0.26726985\n",
      "Iteration 476, loss = 0.26692088\n",
      "Iteration 477, loss = 0.26657278\n",
      "Iteration 478, loss = 0.26622540\n",
      "Iteration 479, loss = 0.26587879\n",
      "Iteration 480, loss = 0.26553303\n",
      "Iteration 481, loss = 0.26518802\n",
      "Iteration 482, loss = 0.26484395\n",
      "Iteration 483, loss = 0.26450069\n",
      "Iteration 484, loss = 0.26415825\n",
      "Iteration 485, loss = 0.26381652\n",
      "Iteration 486, loss = 0.26347558\n",
      "Iteration 487, loss = 0.26313546\n",
      "Iteration 488, loss = 0.26279600\n",
      "Iteration 489, loss = 0.26245736\n",
      "Iteration 490, loss = 0.26211947\n",
      "Iteration 491, loss = 0.26178226\n",
      "Iteration 492, loss = 0.26144594\n",
      "Iteration 493, loss = 0.26111041\n",
      "Iteration 494, loss = 0.26077579\n",
      "Iteration 495, loss = 0.26044185\n",
      "Iteration 496, loss = 0.26010884\n",
      "Iteration 497, loss = 0.25977648\n",
      "Iteration 498, loss = 0.25944491\n",
      "Iteration 499, loss = 0.25911414\n",
      "Iteration 500, loss = 0.25878403\n",
      "Iteration 501, loss = 0.25845476\n",
      "Iteration 502, loss = 0.25812612\n",
      "Iteration 503, loss = 0.25779828\n",
      "Iteration 504, loss = 0.25747119\n",
      "Iteration 505, loss = 0.25714495\n",
      "Iteration 506, loss = 0.25681958\n",
      "Iteration 507, loss = 0.25649494\n",
      "Iteration 508, loss = 0.25617105\n",
      "Iteration 509, loss = 0.25584794\n",
      "Iteration 510, loss = 0.25552562\n",
      "Iteration 511, loss = 0.25520395\n",
      "Iteration 512, loss = 0.25488308\n",
      "Iteration 513, loss = 0.25456287\n",
      "Iteration 514, loss = 0.25424348\n",
      "Iteration 515, loss = 0.25392471\n",
      "Iteration 516, loss = 0.25360662\n",
      "Iteration 517, loss = 0.25328935\n",
      "Iteration 518, loss = 0.25297268\n",
      "Iteration 519, loss = 0.25265673\n",
      "Iteration 520, loss = 0.25234148\n",
      "Iteration 521, loss = 0.25202696\n",
      "Iteration 522, loss = 0.25171306\n",
      "Iteration 523, loss = 0.25139997\n",
      "Iteration 524, loss = 0.25108758\n",
      "Iteration 525, loss = 0.25077588\n",
      "Iteration 526, loss = 0.25046494\n",
      "Iteration 527, loss = 0.25015469\n",
      "Iteration 528, loss = 0.24984515\n",
      "Iteration 529, loss = 0.24953632\n",
      "Iteration 530, loss = 0.24922814\n",
      "Iteration 531, loss = 0.24892062\n",
      "Iteration 532, loss = 0.24861378\n",
      "Iteration 533, loss = 0.24830762\n",
      "Iteration 534, loss = 0.24800213\n",
      "Iteration 535, loss = 0.24769728\n",
      "Iteration 536, loss = 0.24739323\n",
      "Iteration 537, loss = 0.24708977\n",
      "Iteration 538, loss = 0.24678700\n",
      "Iteration 539, loss = 0.24648488\n",
      "Iteration 540, loss = 0.24618347\n",
      "Iteration 541, loss = 0.24588262\n",
      "Iteration 542, loss = 0.24558251\n",
      "Iteration 543, loss = 0.24528300\n",
      "Iteration 544, loss = 0.24498432\n",
      "Iteration 545, loss = 0.24468645\n",
      "Iteration 546, loss = 0.24438929\n",
      "Iteration 547, loss = 0.24409268\n",
      "Iteration 548, loss = 0.24379675\n",
      "Iteration 549, loss = 0.24350141\n",
      "Iteration 550, loss = 0.24320679\n",
      "Iteration 551, loss = 0.24291271\n",
      "Iteration 552, loss = 0.24261929\n",
      "Iteration 553, loss = 0.24232650\n",
      "Iteration 554, loss = 0.24203438\n",
      "Iteration 555, loss = 0.24174280\n",
      "Iteration 556, loss = 0.24145193\n",
      "Iteration 557, loss = 0.24116162\n",
      "Iteration 558, loss = 0.24087198\n",
      "Iteration 559, loss = 0.24058299\n",
      "Iteration 560, loss = 0.24029467\n",
      "Iteration 561, loss = 0.24000690\n",
      "Iteration 562, loss = 0.23971980\n",
      "Iteration 563, loss = 0.23943329\n",
      "Iteration 564, loss = 0.23914745\n",
      "Iteration 565, loss = 0.23886218\n",
      "Iteration 566, loss = 0.23857753\n",
      "Iteration 567, loss = 0.23829352\n",
      "Iteration 568, loss = 0.23801003\n",
      "Iteration 569, loss = 0.23772725\n",
      "Iteration 570, loss = 0.23744501\n",
      "Iteration 571, loss = 0.23716339\n",
      "Iteration 572, loss = 0.23688234\n",
      "Iteration 573, loss = 0.23660195\n",
      "Iteration 574, loss = 0.23632210\n",
      "Iteration 575, loss = 0.23604283\n",
      "Iteration 576, loss = 0.23576422\n",
      "Iteration 577, loss = 0.23548617\n",
      "Iteration 578, loss = 0.23520879\n",
      "Iteration 579, loss = 0.23493193\n",
      "Iteration 580, loss = 0.23465574\n",
      "Iteration 581, loss = 0.23438008\n",
      "Iteration 582, loss = 0.23410505\n",
      "Iteration 583, loss = 0.23383057\n",
      "Iteration 584, loss = 0.23355670\n",
      "Iteration 585, loss = 0.23328337\n",
      "Iteration 586, loss = 0.23301067\n",
      "Iteration 587, loss = 0.23273854\n",
      "Iteration 588, loss = 0.23246697\n",
      "Iteration 589, loss = 0.23219610\n",
      "Iteration 590, loss = 0.23192587\n",
      "Iteration 591, loss = 0.23165622\n",
      "Iteration 592, loss = 0.23138713\n",
      "Iteration 593, loss = 0.23111865\n",
      "Iteration 594, loss = 0.23085072\n",
      "Iteration 595, loss = 0.23058336\n",
      "Iteration 596, loss = 0.23031664\n",
      "Iteration 597, loss = 0.23005044\n",
      "Iteration 598, loss = 0.22978488\n",
      "Iteration 599, loss = 0.22951982\n",
      "Iteration 600, loss = 0.22925537\n",
      "Iteration 601, loss = 0.22899144\n",
      "Iteration 602, loss = 0.22872811\n",
      "Iteration 603, loss = 0.22846535\n",
      "Iteration 604, loss = 0.22820309\n",
      "Iteration 605, loss = 0.22794140\n",
      "Iteration 606, loss = 0.22768030\n",
      "Iteration 607, loss = 0.22741970\n",
      "Iteration 608, loss = 0.22715971\n",
      "Iteration 609, loss = 0.22690019\n",
      "Iteration 610, loss = 0.22664132\n",
      "Iteration 611, loss = 0.22638290\n",
      "Iteration 612, loss = 0.22612503\n",
      "Iteration 613, loss = 0.22586775\n",
      "Iteration 614, loss = 0.22561100\n",
      "Iteration 615, loss = 0.22535483\n",
      "Iteration 616, loss = 0.22509923\n",
      "Iteration 617, loss = 0.22484413\n",
      "Iteration 618, loss = 0.22458966\n",
      "Iteration 619, loss = 0.22433563\n",
      "Iteration 620, loss = 0.22408220\n",
      "Iteration 621, loss = 0.22382930\n",
      "Iteration 622, loss = 0.22357693\n",
      "Iteration 623, loss = 0.22332506\n",
      "Iteration 624, loss = 0.22307379\n",
      "Iteration 625, loss = 0.22282297\n",
      "Iteration 626, loss = 0.22257274\n",
      "Iteration 627, loss = 0.22232298\n",
      "Iteration 628, loss = 0.22207380\n",
      "Iteration 629, loss = 0.22182519\n",
      "Iteration 630, loss = 0.22157706\n",
      "Iteration 631, loss = 0.22132946\n",
      "Iteration 632, loss = 0.22108242\n",
      "Iteration 633, loss = 0.22083582\n",
      "Iteration 634, loss = 0.22058978\n",
      "Iteration 635, loss = 0.22034424\n",
      "Iteration 636, loss = 0.22009922\n",
      "Iteration 637, loss = 0.21985473\n",
      "Iteration 638, loss = 0.21961072\n",
      "Iteration 639, loss = 0.21936721\n",
      "Iteration 640, loss = 0.21912429\n",
      "Iteration 641, loss = 0.21888179\n",
      "Iteration 642, loss = 0.21863985\n",
      "Iteration 643, loss = 0.21839841\n",
      "Iteration 644, loss = 0.21815740\n",
      "Iteration 645, loss = 0.21791701\n",
      "Iteration 646, loss = 0.21767704\n",
      "Iteration 647, loss = 0.21743758\n",
      "Iteration 648, loss = 0.21719859\n",
      "Iteration 649, loss = 0.21696016\n",
      "Iteration 650, loss = 0.21672218\n",
      "Iteration 651, loss = 0.21648478\n",
      "Iteration 652, loss = 0.21624785\n",
      "Iteration 653, loss = 0.21601140\n",
      "Iteration 654, loss = 0.21577549\n",
      "Iteration 655, loss = 0.21554001\n",
      "Iteration 656, loss = 0.21530506\n",
      "Iteration 657, loss = 0.21507056\n",
      "Iteration 658, loss = 0.21483657\n",
      "Iteration 659, loss = 0.21460308\n",
      "Iteration 660, loss = 0.21437007\n",
      "Iteration 661, loss = 0.21413749\n",
      "Iteration 662, loss = 0.21390547\n",
      "Iteration 663, loss = 0.21367386\n",
      "Iteration 664, loss = 0.21344281\n",
      "Iteration 665, loss = 0.21321225\n",
      "Iteration 666, loss = 0.21298210\n",
      "Iteration 667, loss = 0.21275247\n",
      "Iteration 668, loss = 0.21252331\n",
      "Iteration 669, loss = 0.21229465\n",
      "Iteration 670, loss = 0.21206642\n",
      "Iteration 671, loss = 0.21183865\n",
      "Iteration 672, loss = 0.21161142\n",
      "Iteration 673, loss = 0.21138457\n",
      "Iteration 674, loss = 0.21115825\n",
      "Iteration 675, loss = 0.21093240\n",
      "Iteration 676, loss = 0.21070702\n",
      "Iteration 677, loss = 0.21048206\n",
      "Iteration 678, loss = 0.21025762\n",
      "Iteration 679, loss = 0.21003366\n",
      "Iteration 680, loss = 0.20981014\n",
      "Iteration 681, loss = 0.20958706\n",
      "Iteration 682, loss = 0.20936449\n",
      "Iteration 683, loss = 0.20914233\n",
      "Iteration 684, loss = 0.20892067\n",
      "Iteration 685, loss = 0.20869944\n",
      "Iteration 686, loss = 0.20847863\n",
      "Iteration 687, loss = 0.20825835\n",
      "Iteration 688, loss = 0.20803843\n",
      "Iteration 689, loss = 0.20781903\n",
      "Iteration 690, loss = 0.20760008\n",
      "Iteration 691, loss = 0.20738155\n",
      "Iteration 692, loss = 0.20716345\n",
      "Iteration 693, loss = 0.20694584\n",
      "Iteration 694, loss = 0.20672865\n",
      "Iteration 695, loss = 0.20651191\n",
      "Iteration 696, loss = 0.20629565\n",
      "Iteration 697, loss = 0.20607977\n",
      "Iteration 698, loss = 0.20586435\n",
      "Iteration 699, loss = 0.20564938\n",
      "Iteration 700, loss = 0.20543483\n",
      "Iteration 701, loss = 0.20522071\n",
      "Iteration 702, loss = 0.20500703\n",
      "Iteration 703, loss = 0.20479380\n",
      "Iteration 704, loss = 0.20458098\n",
      "Iteration 705, loss = 0.20436859\n",
      "Iteration 706, loss = 0.20415666\n",
      "Iteration 707, loss = 0.20394515\n",
      "Iteration 708, loss = 0.20373408\n",
      "Iteration 709, loss = 0.20352339\n",
      "Iteration 710, loss = 0.20331319\n",
      "Iteration 711, loss = 0.20310337\n",
      "Iteration 712, loss = 0.20289406\n",
      "Iteration 713, loss = 0.20268507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 714, loss = 0.20247665\n",
      "Iteration 715, loss = 0.20226856\n",
      "Iteration 716, loss = 0.20206096\n",
      "Iteration 717, loss = 0.20185376\n",
      "Iteration 718, loss = 0.20164704\n",
      "Iteration 719, loss = 0.20144067\n",
      "Iteration 720, loss = 0.20123477\n",
      "Iteration 721, loss = 0.20102925\n",
      "Iteration 722, loss = 0.20082420\n",
      "Iteration 723, loss = 0.20061953\n",
      "Iteration 724, loss = 0.20041530\n",
      "Iteration 725, loss = 0.20021146\n",
      "Iteration 726, loss = 0.20000806\n",
      "Iteration 727, loss = 0.19980504\n",
      "Iteration 728, loss = 0.19960246\n",
      "Iteration 729, loss = 0.19940028\n",
      "Iteration 730, loss = 0.19919853\n",
      "Iteration 731, loss = 0.19899718\n",
      "Iteration 732, loss = 0.19879622\n",
      "Iteration 733, loss = 0.19859569\n",
      "Iteration 734, loss = 0.19839558\n",
      "Iteration 735, loss = 0.19819590\n",
      "Iteration 736, loss = 0.19799659\n",
      "Iteration 737, loss = 0.19779774\n",
      "Iteration 738, loss = 0.19759922\n",
      "Iteration 739, loss = 0.19740116\n",
      "Iteration 740, loss = 0.19720347\n",
      "Iteration 741, loss = 0.19700622\n",
      "Iteration 742, loss = 0.19680933\n",
      "Iteration 743, loss = 0.19661289\n",
      "Iteration 744, loss = 0.19641687\n",
      "Iteration 745, loss = 0.19622122\n",
      "Iteration 746, loss = 0.19602598\n",
      "Iteration 747, loss = 0.19583114\n",
      "Iteration 748, loss = 0.19563671\n",
      "Iteration 749, loss = 0.19544263\n",
      "Iteration 750, loss = 0.19524899\n",
      "Iteration 751, loss = 0.19505574\n",
      "Iteration 752, loss = 0.19486283\n",
      "Iteration 753, loss = 0.19467034\n",
      "Iteration 754, loss = 0.19447823\n",
      "Iteration 755, loss = 0.19428651\n",
      "Iteration 756, loss = 0.19409518\n",
      "Iteration 757, loss = 0.19390416\n",
      "Iteration 758, loss = 0.19371353\n",
      "Iteration 759, loss = 0.19352330\n",
      "Iteration 760, loss = 0.19333342\n",
      "Iteration 761, loss = 0.19314393\n",
      "Iteration 762, loss = 0.19295481\n",
      "Iteration 763, loss = 0.19276607\n",
      "Iteration 764, loss = 0.19257771\n",
      "Iteration 765, loss = 0.19238973\n",
      "Iteration 766, loss = 0.19220210\n",
      "Iteration 767, loss = 0.19201487\n",
      "Iteration 768, loss = 0.19182799\n",
      "Iteration 769, loss = 0.19164149\n",
      "Iteration 770, loss = 0.19145539\n",
      "Iteration 771, loss = 0.19126961\n",
      "Iteration 772, loss = 0.19108423\n",
      "Iteration 773, loss = 0.19089920\n",
      "Iteration 774, loss = 0.19071454\n",
      "Iteration 775, loss = 0.19053029\n",
      "Iteration 776, loss = 0.19034634\n",
      "Iteration 777, loss = 0.19016283\n",
      "Iteration 778, loss = 0.18997964\n",
      "Iteration 779, loss = 0.18979685\n",
      "Iteration 780, loss = 0.18961441\n",
      "Iteration 781, loss = 0.18943234\n",
      "Iteration 782, loss = 0.18925063\n",
      "Iteration 783, loss = 0.18906926\n",
      "Iteration 784, loss = 0.18888829\n",
      "Iteration 785, loss = 0.18870764\n",
      "Iteration 786, loss = 0.18852736\n",
      "Iteration 787, loss = 0.18834743\n",
      "Iteration 788, loss = 0.18816786\n",
      "Iteration 789, loss = 0.18798867\n",
      "Iteration 790, loss = 0.18780981\n",
      "Iteration 791, loss = 0.18763131\n",
      "Iteration 792, loss = 0.18745318\n",
      "Iteration 793, loss = 0.18727543\n",
      "Iteration 794, loss = 0.18709803\n",
      "Iteration 795, loss = 0.18692104\n",
      "Iteration 796, loss = 0.18674435\n",
      "Iteration 797, loss = 0.18656802\n",
      "Iteration 798, loss = 0.18639204\n",
      "Iteration 799, loss = 0.18621643\n",
      "Iteration 800, loss = 0.18604115\n",
      "Iteration 801, loss = 0.18586623\n",
      "Iteration 802, loss = 0.18569164\n",
      "Iteration 803, loss = 0.18551740\n",
      "Iteration 804, loss = 0.18534350\n",
      "Iteration 805, loss = 0.18516995\n",
      "Iteration 806, loss = 0.18499672\n",
      "Iteration 807, loss = 0.18482389\n",
      "Iteration 808, loss = 0.18465135\n",
      "Iteration 809, loss = 0.18447915\n",
      "Iteration 810, loss = 0.18430731\n",
      "Iteration 811, loss = 0.18413579\n",
      "Iteration 812, loss = 0.18396464\n",
      "Iteration 813, loss = 0.18379378\n",
      "Iteration 814, loss = 0.18362329\n",
      "Iteration 815, loss = 0.18345311\n",
      "Iteration 816, loss = 0.18328330\n",
      "Iteration 817, loss = 0.18311379\n",
      "Iteration 818, loss = 0.18294464\n",
      "Iteration 819, loss = 0.18277583\n",
      "Iteration 820, loss = 0.18260735\n",
      "Iteration 821, loss = 0.18243920\n",
      "Iteration 822, loss = 0.18227137\n",
      "Iteration 823, loss = 0.18210388\n",
      "Iteration 824, loss = 0.18193674\n",
      "Iteration 825, loss = 0.18176989\n",
      "Iteration 826, loss = 0.18160339\n",
      "Iteration 827, loss = 0.18143721\n",
      "Iteration 828, loss = 0.18127136\n",
      "Iteration 829, loss = 0.18110582\n",
      "Iteration 830, loss = 0.18094062\n",
      "Iteration 831, loss = 0.18077572\n",
      "Iteration 832, loss = 0.18061116\n",
      "Iteration 833, loss = 0.18044695\n",
      "Iteration 834, loss = 0.18028300\n",
      "Iteration 835, loss = 0.18011934\n",
      "Iteration 836, loss = 0.17995599\n",
      "Iteration 837, loss = 0.17979295\n",
      "Iteration 838, loss = 0.17963024\n",
      "Iteration 839, loss = 0.17946782\n",
      "Iteration 840, loss = 0.17930570\n",
      "Iteration 841, loss = 0.17914392\n",
      "Iteration 842, loss = 0.17898243\n",
      "Iteration 843, loss = 0.17882126\n",
      "Iteration 844, loss = 0.17866038\n",
      "Iteration 845, loss = 0.17849981\n",
      "Iteration 846, loss = 0.17833953\n",
      "Iteration 847, loss = 0.17817956\n",
      "Iteration 848, loss = 0.17801993\n",
      "Iteration 849, loss = 0.17786058\n",
      "Iteration 850, loss = 0.17770155\n",
      "Iteration 851, loss = 0.17754284\n",
      "Iteration 852, loss = 0.17738444\n",
      "Iteration 853, loss = 0.17722632\n",
      "Iteration 854, loss = 0.17706851\n",
      "Iteration 855, loss = 0.17691101\n",
      "Iteration 856, loss = 0.17675381\n",
      "Iteration 857, loss = 0.17659695\n",
      "Iteration 858, loss = 0.17644036\n",
      "Iteration 859, loss = 0.17628406\n",
      "Iteration 860, loss = 0.17612809\n",
      "Iteration 861, loss = 0.17597243\n",
      "Iteration 862, loss = 0.17581705\n",
      "Iteration 863, loss = 0.17566196\n",
      "Iteration 864, loss = 0.17550717\n",
      "Iteration 865, loss = 0.17535268\n",
      "Iteration 866, loss = 0.17519850\n",
      "Iteration 867, loss = 0.17504461\n",
      "Iteration 868, loss = 0.17489100\n",
      "Iteration 869, loss = 0.17473767\n",
      "Iteration 870, loss = 0.17458465\n",
      "Iteration 871, loss = 0.17443192\n",
      "Iteration 872, loss = 0.17427948\n",
      "Iteration 873, loss = 0.17412733\n",
      "Iteration 874, loss = 0.17397548\n",
      "Iteration 875, loss = 0.17382391\n",
      "Iteration 876, loss = 0.17367263\n",
      "Iteration 877, loss = 0.17352163\n",
      "Iteration 878, loss = 0.17337094\n",
      "Iteration 879, loss = 0.17322052\n",
      "Iteration 880, loss = 0.17307041\n",
      "Iteration 881, loss = 0.17292055\n",
      "Iteration 882, loss = 0.17277100\n",
      "Iteration 883, loss = 0.17262172\n",
      "Iteration 884, loss = 0.17247273\n",
      "Iteration 885, loss = 0.17232403\n",
      "Iteration 886, loss = 0.17217560\n",
      "Iteration 887, loss = 0.17202746\n",
      "Iteration 888, loss = 0.17187961\n",
      "Iteration 889, loss = 0.17173202\n",
      "Iteration 890, loss = 0.17158474\n",
      "Iteration 891, loss = 0.17143771\n",
      "Iteration 892, loss = 0.17129098\n",
      "Iteration 893, loss = 0.17114451\n",
      "Iteration 894, loss = 0.17099833\n",
      "Iteration 895, loss = 0.17085242\n",
      "Iteration 896, loss = 0.17070678\n",
      "Iteration 897, loss = 0.17056143\n",
      "Iteration 898, loss = 0.17041635\n",
      "Iteration 899, loss = 0.17027154\n",
      "Iteration 900, loss = 0.17012700\n",
      "Iteration 901, loss = 0.16998275\n",
      "Iteration 902, loss = 0.16983875\n",
      "Iteration 903, loss = 0.16969504\n",
      "Iteration 904, loss = 0.16955159\n",
      "Iteration 905, loss = 0.16940840\n",
      "Iteration 906, loss = 0.16926542\n",
      "Iteration 907, loss = 0.16912269\n",
      "Iteration 908, loss = 0.16898024\n",
      "Iteration 909, loss = 0.16883804\n",
      "Iteration 910, loss = 0.16869611\n",
      "Iteration 911, loss = 0.16855444\n",
      "Iteration 912, loss = 0.16841304\n",
      "Iteration 913, loss = 0.16827191\n",
      "Iteration 914, loss = 0.16813115\n",
      "Iteration 915, loss = 0.16799077\n",
      "Iteration 916, loss = 0.16785064\n",
      "Iteration 917, loss = 0.16771076\n",
      "Iteration 918, loss = 0.16757114\n",
      "Iteration 919, loss = 0.16743177\n",
      "Iteration 920, loss = 0.16729266\n",
      "Iteration 921, loss = 0.16715380\n",
      "Iteration 922, loss = 0.16701519\n",
      "Iteration 923, loss = 0.16687685\n",
      "Iteration 924, loss = 0.16673876\n",
      "Iteration 925, loss = 0.16660092\n",
      "Iteration 926, loss = 0.16646333\n",
      "Iteration 927, loss = 0.16632600\n",
      "Iteration 928, loss = 0.16618893\n",
      "Iteration 929, loss = 0.16605210\n",
      "Iteration 930, loss = 0.16591547\n",
      "Iteration 931, loss = 0.16577908\n",
      "Iteration 932, loss = 0.16564295\n",
      "Iteration 933, loss = 0.16550706\n",
      "Iteration 934, loss = 0.16537143\n",
      "Iteration 935, loss = 0.16523606\n",
      "Iteration 936, loss = 0.16510096\n",
      "Iteration 937, loss = 0.16496609\n",
      "Iteration 938, loss = 0.16483147\n",
      "Iteration 939, loss = 0.16469710\n",
      "Iteration 940, loss = 0.16456297\n",
      "Iteration 941, loss = 0.16442909\n",
      "Iteration 942, loss = 0.16429545\n",
      "Iteration 943, loss = 0.16416206\n",
      "Iteration 944, loss = 0.16402891\n",
      "Iteration 945, loss = 0.16389602\n",
      "Iteration 946, loss = 0.16376338\n",
      "Iteration 947, loss = 0.16363098\n",
      "Iteration 948, loss = 0.16349885\n",
      "Iteration 949, loss = 0.16336697\n",
      "Iteration 950, loss = 0.16323527\n",
      "Iteration 951, loss = 0.16310380\n",
      "Iteration 952, loss = 0.16297261\n",
      "Iteration 953, loss = 0.16284171\n",
      "Iteration 954, loss = 0.16271105\n",
      "Iteration 955, loss = 0.16258064\n",
      "Iteration 956, loss = 0.16245046\n",
      "Iteration 957, loss = 0.16232052\n",
      "Iteration 958, loss = 0.16219082\n",
      "Iteration 959, loss = 0.16206136\n",
      "Iteration 960, loss = 0.16193215\n",
      "Iteration 961, loss = 0.16180316\n",
      "Iteration 962, loss = 0.16167445\n",
      "Iteration 963, loss = 0.16154593\n",
      "Iteration 964, loss = 0.16141766\n",
      "Iteration 965, loss = 0.16128963\n",
      "Iteration 966, loss = 0.16116184\n",
      "Iteration 967, loss = 0.16103427\n",
      "Iteration 968, loss = 0.16090695\n",
      "Iteration 969, loss = 0.16077985\n",
      "Iteration 970, loss = 0.16065299\n",
      "Iteration 971, loss = 0.16052636\n",
      "Iteration 972, loss = 0.16039991\n",
      "Iteration 973, loss = 0.16027371\n",
      "Iteration 974, loss = 0.16014772\n",
      "Iteration 975, loss = 0.16002197\n",
      "Iteration 976, loss = 0.15989644\n",
      "Iteration 977, loss = 0.15977116\n",
      "Iteration 978, loss = 0.15964613\n",
      "Iteration 979, loss = 0.15952133\n",
      "Iteration 980, loss = 0.15939675\n",
      "Iteration 981, loss = 0.15927239\n",
      "Iteration 982, loss = 0.15914827\n",
      "Iteration 983, loss = 0.15902437\n",
      "Iteration 984, loss = 0.15890069\n",
      "Iteration 985, loss = 0.15877724\n",
      "Iteration 986, loss = 0.15865402\n",
      "Iteration 987, loss = 0.15853102\n",
      "Iteration 988, loss = 0.15840823\n",
      "Iteration 989, loss = 0.15828567\n",
      "Iteration 990, loss = 0.15816334\n",
      "Iteration 991, loss = 0.15804123\n",
      "Iteration 992, loss = 0.15791933\n",
      "Iteration 993, loss = 0.15779766\n",
      "Iteration 994, loss = 0.15767621\n",
      "Iteration 995, loss = 0.15755497\n",
      "Iteration 996, loss = 0.15743401\n",
      "Iteration 997, loss = 0.15731336\n",
      "Iteration 998, loss = 0.15719293\n",
      "Iteration 999, loss = 0.15707273\n",
      "Iteration 1000, loss = 0.15695274\n",
      "Iteration 1, loss = 1.49174295\n",
      "Iteration 2, loss = 1.47766840\n",
      "Iteration 3, loss = 1.45789899\n",
      "Iteration 4, loss = 1.43337458\n",
      "Iteration 5, loss = 1.40509817\n",
      "Iteration 6, loss = 1.37396278\n",
      "Iteration 7, loss = 1.34081595\n",
      "Iteration 8, loss = 1.30669020\n",
      "Iteration 9, loss = 1.27261121\n",
      "Iteration 10, loss = 1.23970099\n",
      "Iteration 11, loss = 1.20826669\n",
      "Iteration 12, loss = 1.17879780\n",
      "Iteration 13, loss = 1.15178669\n",
      "Iteration 14, loss = 1.12737491\n",
      "Iteration 15, loss = 1.10589709\n",
      "Iteration 16, loss = 1.08709143\n",
      "Iteration 17, loss = 1.07037567\n",
      "Iteration 18, loss = 1.05546375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19, loss = 1.04202444\n",
      "Iteration 20, loss = 1.02948197\n",
      "Iteration 21, loss = 1.01756254\n",
      "Iteration 22, loss = 1.00574447\n",
      "Iteration 23, loss = 0.99377160\n",
      "Iteration 24, loss = 0.98156624\n",
      "Iteration 25, loss = 0.96915537\n",
      "Iteration 26, loss = 0.95667874\n",
      "Iteration 27, loss = 0.94426787\n",
      "Iteration 28, loss = 0.93196854\n",
      "Iteration 29, loss = 0.91980693\n",
      "Iteration 30, loss = 0.90794477\n",
      "Iteration 31, loss = 0.89634742\n",
      "Iteration 32, loss = 0.88506706\n",
      "Iteration 33, loss = 0.87421834\n",
      "Iteration 34, loss = 0.86389466\n",
      "Iteration 35, loss = 0.85408038\n",
      "Iteration 36, loss = 0.84462527\n",
      "Iteration 37, loss = 0.83551381\n",
      "Iteration 38, loss = 0.82668806\n",
      "Iteration 39, loss = 0.81810280\n",
      "Iteration 40, loss = 0.80969354\n",
      "Iteration 41, loss = 0.80148592\n",
      "Iteration 42, loss = 0.79345166\n",
      "Iteration 43, loss = 0.78566282\n",
      "Iteration 44, loss = 0.77811779\n",
      "Iteration 45, loss = 0.77085084\n",
      "Iteration 46, loss = 0.76383040\n",
      "Iteration 47, loss = 0.75697145\n",
      "Iteration 48, loss = 0.75026431\n",
      "Iteration 49, loss = 0.74374743\n",
      "Iteration 50, loss = 0.73741269\n",
      "Iteration 51, loss = 0.73124710\n",
      "Iteration 52, loss = 0.72528505\n",
      "Iteration 53, loss = 0.71950349\n",
      "Iteration 54, loss = 0.71386369\n",
      "Iteration 55, loss = 0.70835719\n",
      "Iteration 56, loss = 0.70299171\n",
      "Iteration 57, loss = 0.69775428\n",
      "Iteration 58, loss = 0.69267672\n",
      "Iteration 59, loss = 0.68772964\n",
      "Iteration 60, loss = 0.68291348\n",
      "Iteration 61, loss = 0.67821083\n",
      "Iteration 62, loss = 0.67360310\n",
      "Iteration 63, loss = 0.66908205\n",
      "Iteration 64, loss = 0.66465562\n",
      "Iteration 65, loss = 0.66031956\n",
      "Iteration 66, loss = 0.65607929\n",
      "Iteration 67, loss = 0.65195774\n",
      "Iteration 68, loss = 0.64795262\n",
      "Iteration 69, loss = 0.64406770\n",
      "Iteration 70, loss = 0.64028155\n",
      "Iteration 71, loss = 0.63659040\n",
      "Iteration 72, loss = 0.63298840\n",
      "Iteration 73, loss = 0.62945302\n",
      "Iteration 74, loss = 0.62598297\n",
      "Iteration 75, loss = 0.62257299\n",
      "Iteration 76, loss = 0.61921957\n",
      "Iteration 77, loss = 0.61592188\n",
      "Iteration 78, loss = 0.61267877\n",
      "Iteration 79, loss = 0.60948861\n",
      "Iteration 80, loss = 0.60635168\n",
      "Iteration 81, loss = 0.60326715\n",
      "Iteration 82, loss = 0.60023759\n",
      "Iteration 83, loss = 0.59726360\n",
      "Iteration 84, loss = 0.59433974\n",
      "Iteration 85, loss = 0.59146659\n",
      "Iteration 86, loss = 0.58863930\n",
      "Iteration 87, loss = 0.58585975\n",
      "Iteration 88, loss = 0.58313278\n",
      "Iteration 89, loss = 0.58045331\n",
      "Iteration 90, loss = 0.57782290\n",
      "Iteration 91, loss = 0.57523801\n",
      "Iteration 92, loss = 0.57269820\n",
      "Iteration 93, loss = 0.57019934\n",
      "Iteration 94, loss = 0.56773925\n",
      "Iteration 95, loss = 0.56531738\n",
      "Iteration 96, loss = 0.56293285\n",
      "Iteration 97, loss = 0.56058502\n",
      "Iteration 98, loss = 0.55827343\n",
      "Iteration 99, loss = 0.55599780\n",
      "Iteration 100, loss = 0.55375847\n",
      "Iteration 101, loss = 0.55155471\n",
      "Iteration 102, loss = 0.54938273\n",
      "Iteration 103, loss = 0.54724366\n",
      "Iteration 104, loss = 0.54513628\n",
      "Iteration 105, loss = 0.54306027\n",
      "Iteration 106, loss = 0.54101601\n",
      "Iteration 107, loss = 0.53900138\n",
      "Iteration 108, loss = 0.53701595\n",
      "Iteration 109, loss = 0.53505805\n",
      "Iteration 110, loss = 0.53312718\n",
      "Iteration 111, loss = 0.53122293\n",
      "Iteration 112, loss = 0.52934452\n",
      "Iteration 113, loss = 0.52749263\n",
      "Iteration 114, loss = 0.52566513\n",
      "Iteration 115, loss = 0.52386183\n",
      "Iteration 116, loss = 0.52208185\n",
      "Iteration 117, loss = 0.52032441\n",
      "Iteration 118, loss = 0.51858821\n",
      "Iteration 119, loss = 0.51687362\n",
      "Iteration 120, loss = 0.51518074\n",
      "Iteration 121, loss = 0.51350865\n",
      "Iteration 122, loss = 0.51185676\n",
      "Iteration 123, loss = 0.51022463\n",
      "Iteration 124, loss = 0.50861303\n",
      "Iteration 125, loss = 0.50702191\n",
      "Iteration 126, loss = 0.50544990\n",
      "Iteration 127, loss = 0.50389394\n",
      "Iteration 128, loss = 0.50235445\n",
      "Iteration 129, loss = 0.50083274\n",
      "Iteration 130, loss = 0.49932832\n",
      "Iteration 131, loss = 0.49784080\n",
      "Iteration 132, loss = 0.49636813\n",
      "Iteration 133, loss = 0.49490884\n",
      "Iteration 134, loss = 0.49346179\n",
      "Iteration 135, loss = 0.49202411\n",
      "Iteration 136, loss = 0.49059263\n",
      "Iteration 137, loss = 0.48916867\n",
      "Iteration 138, loss = 0.48774238\n",
      "Iteration 139, loss = 0.48631283\n",
      "Iteration 140, loss = 0.48489317\n",
      "Iteration 141, loss = 0.48348728\n",
      "Iteration 142, loss = 0.48209389\n",
      "Iteration 143, loss = 0.48071099\n",
      "Iteration 144, loss = 0.47933245\n",
      "Iteration 145, loss = 0.47797277\n",
      "Iteration 146, loss = 0.47661889\n",
      "Iteration 147, loss = 0.47527852\n",
      "Iteration 148, loss = 0.47393405\n",
      "Iteration 149, loss = 0.47260773\n",
      "Iteration 150, loss = 0.47131175\n",
      "Iteration 151, loss = 0.47004580\n",
      "Iteration 152, loss = 0.46877876\n",
      "Iteration 153, loss = 0.46752051\n",
      "Iteration 154, loss = 0.46628424\n",
      "Iteration 155, loss = 0.46505932\n",
      "Iteration 156, loss = 0.46386758\n",
      "Iteration 157, loss = 0.46270429\n",
      "Iteration 158, loss = 0.46156234\n",
      "Iteration 159, loss = 0.46043355\n",
      "Iteration 160, loss = 0.45931122\n",
      "Iteration 161, loss = 0.45819650\n",
      "Iteration 162, loss = 0.45709348\n",
      "Iteration 163, loss = 0.45600108\n",
      "Iteration 164, loss = 0.45492184\n",
      "Iteration 165, loss = 0.45384935\n",
      "Iteration 166, loss = 0.45278493\n",
      "Iteration 167, loss = 0.45172567\n",
      "Iteration 168, loss = 0.45067261\n",
      "Iteration 169, loss = 0.44962578\n",
      "Iteration 170, loss = 0.44858553\n",
      "Iteration 171, loss = 0.44755350\n",
      "Iteration 172, loss = 0.44652908\n",
      "Iteration 173, loss = 0.44551267\n",
      "Iteration 174, loss = 0.44450410\n",
      "Iteration 175, loss = 0.44350349\n",
      "Iteration 176, loss = 0.44251352\n",
      "Iteration 177, loss = 0.44153100\n",
      "Iteration 178, loss = 0.44055595\n",
      "Iteration 179, loss = 0.43958810\n",
      "Iteration 180, loss = 0.43862751\n",
      "Iteration 181, loss = 0.43767403\n",
      "Iteration 182, loss = 0.43672776\n",
      "Iteration 183, loss = 0.43578805\n",
      "Iteration 184, loss = 0.43485501\n",
      "Iteration 185, loss = 0.43392840\n",
      "Iteration 186, loss = 0.43300863\n",
      "Iteration 187, loss = 0.43209514\n",
      "Iteration 188, loss = 0.43118814\n",
      "Iteration 189, loss = 0.43028718\n",
      "Iteration 190, loss = 0.42939221\n",
      "Iteration 191, loss = 0.42850330\n",
      "Iteration 192, loss = 0.42762010\n",
      "Iteration 193, loss = 0.42674280\n",
      "Iteration 194, loss = 0.42587180\n",
      "Iteration 195, loss = 0.42500661\n",
      "Iteration 196, loss = 0.42414689\n",
      "Iteration 197, loss = 0.42329264\n",
      "Iteration 198, loss = 0.42244396\n",
      "Iteration 199, loss = 0.42160052\n",
      "Iteration 200, loss = 0.42076244\n",
      "Iteration 201, loss = 0.41992959\n",
      "Iteration 202, loss = 0.41910187\n",
      "Iteration 203, loss = 0.41827939\n",
      "Iteration 204, loss = 0.41746190\n",
      "Iteration 205, loss = 0.41664935\n",
      "Iteration 206, loss = 0.41584167\n",
      "Iteration 207, loss = 0.41503884\n",
      "Iteration 208, loss = 0.41424075\n",
      "Iteration 209, loss = 0.41344732\n",
      "Iteration 210, loss = 0.41265848\n",
      "Iteration 211, loss = 0.41187419\n",
      "Iteration 212, loss = 0.41109438\n",
      "Iteration 213, loss = 0.41031899\n",
      "Iteration 214, loss = 0.40954797\n",
      "Iteration 215, loss = 0.40878129\n",
      "Iteration 216, loss = 0.40801875\n",
      "Iteration 217, loss = 0.40726044\n",
      "Iteration 218, loss = 0.40650623\n",
      "Iteration 219, loss = 0.40575609\n",
      "Iteration 220, loss = 0.40500995\n",
      "Iteration 221, loss = 0.40426785\n",
      "Iteration 222, loss = 0.40352995\n",
      "Iteration 223, loss = 0.40279624\n",
      "Iteration 224, loss = 0.40206634\n",
      "Iteration 225, loss = 0.40134043\n",
      "Iteration 226, loss = 0.40061844\n",
      "Iteration 227, loss = 0.39990008\n",
      "Iteration 228, loss = 0.39918539\n",
      "Iteration 229, loss = 0.39847432\n",
      "Iteration 230, loss = 0.39776764\n",
      "Iteration 231, loss = 0.39706476\n",
      "Iteration 232, loss = 0.39636539\n",
      "Iteration 233, loss = 0.39566946\n",
      "Iteration 234, loss = 0.39497657\n",
      "Iteration 235, loss = 0.39428674\n",
      "Iteration 236, loss = 0.39360039\n",
      "Iteration 237, loss = 0.39291759\n",
      "Iteration 238, loss = 0.39223805\n",
      "Iteration 239, loss = 0.39156182\n",
      "Iteration 240, loss = 0.39088898\n",
      "Iteration 241, loss = 0.39021961\n",
      "Iteration 242, loss = 0.38955333\n",
      "Iteration 243, loss = 0.38889012\n",
      "Iteration 244, loss = 0.38822993\n",
      "Iteration 245, loss = 0.38757289\n",
      "Iteration 246, loss = 0.38691841\n",
      "Iteration 247, loss = 0.38626618\n",
      "Iteration 248, loss = 0.38561692\n",
      "Iteration 249, loss = 0.38497056\n",
      "Iteration 250, loss = 0.38432710\n",
      "Iteration 251, loss = 0.38368635\n",
      "Iteration 252, loss = 0.38304838\n",
      "Iteration 253, loss = 0.38241322\n",
      "Iteration 254, loss = 0.38178079\n",
      "Iteration 255, loss = 0.38115107\n",
      "Iteration 256, loss = 0.38052399\n",
      "Iteration 257, loss = 0.37989842\n",
      "Iteration 258, loss = 0.37927526\n",
      "Iteration 259, loss = 0.37865382\n",
      "Iteration 260, loss = 0.37803455\n",
      "Iteration 261, loss = 0.37741765\n",
      "Iteration 262, loss = 0.37680318\n",
      "Iteration 263, loss = 0.37619103\n",
      "Iteration 264, loss = 0.37558125\n",
      "Iteration 265, loss = 0.37497382\n",
      "Iteration 266, loss = 0.37436879\n",
      "Iteration 267, loss = 0.37376615\n",
      "Iteration 268, loss = 0.37316584\n",
      "Iteration 269, loss = 0.37256788\n",
      "Iteration 270, loss = 0.37197120\n",
      "Iteration 271, loss = 0.37137646\n",
      "Iteration 272, loss = 0.37078293\n",
      "Iteration 273, loss = 0.37019108\n",
      "Iteration 274, loss = 0.36960001\n",
      "Iteration 275, loss = 0.36901053\n",
      "Iteration 276, loss = 0.36842309\n",
      "Iteration 277, loss = 0.36783757\n",
      "Iteration 278, loss = 0.36725404\n",
      "Iteration 279, loss = 0.36667250\n",
      "Iteration 280, loss = 0.36609293\n",
      "Iteration 281, loss = 0.36551537\n",
      "Iteration 282, loss = 0.36493981\n",
      "Iteration 283, loss = 0.36436625\n",
      "Iteration 284, loss = 0.36379466\n",
      "Iteration 285, loss = 0.36322380\n",
      "Iteration 286, loss = 0.36265484\n",
      "Iteration 287, loss = 0.36208780\n",
      "Iteration 288, loss = 0.36152430\n",
      "Iteration 289, loss = 0.36096489\n",
      "Iteration 290, loss = 0.36040514\n",
      "Iteration 291, loss = 0.35984738\n",
      "Iteration 292, loss = 0.35929204\n",
      "Iteration 293, loss = 0.35873790\n",
      "Iteration 294, loss = 0.35818399\n",
      "Iteration 295, loss = 0.35762864\n",
      "Iteration 296, loss = 0.35706807\n",
      "Iteration 297, loss = 0.35650646\n",
      "Iteration 298, loss = 0.35594453\n",
      "Iteration 299, loss = 0.35538189\n",
      "Iteration 300, loss = 0.35481820\n",
      "Iteration 301, loss = 0.35425339\n",
      "Iteration 302, loss = 0.35368663\n",
      "Iteration 303, loss = 0.35311963\n",
      "Iteration 304, loss = 0.35255320\n",
      "Iteration 305, loss = 0.35198761\n",
      "Iteration 306, loss = 0.35141962\n",
      "Iteration 307, loss = 0.35084941\n",
      "Iteration 308, loss = 0.35027977\n",
      "Iteration 309, loss = 0.34971438\n",
      "Iteration 310, loss = 0.34915305\n",
      "Iteration 311, loss = 0.34859134\n",
      "Iteration 312, loss = 0.34802860\n",
      "Iteration 313, loss = 0.34747008\n",
      "Iteration 314, loss = 0.34691784\n",
      "Iteration 315, loss = 0.34636734\n",
      "Iteration 316, loss = 0.34581751\n",
      "Iteration 317, loss = 0.34526964\n",
      "Iteration 318, loss = 0.34472201\n",
      "Iteration 319, loss = 0.34417402\n",
      "Iteration 320, loss = 0.34362482\n",
      "Iteration 321, loss = 0.34307909\n",
      "Iteration 322, loss = 0.34253484\n",
      "Iteration 323, loss = 0.34198791\n",
      "Iteration 324, loss = 0.34143886\n",
      "Iteration 325, loss = 0.34089212\n",
      "Iteration 326, loss = 0.34035022\n",
      "Iteration 327, loss = 0.33981232\n",
      "Iteration 328, loss = 0.33927198\n",
      "Iteration 329, loss = 0.33873553\n",
      "Iteration 330, loss = 0.33820685\n",
      "Iteration 331, loss = 0.33768523\n",
      "Iteration 332, loss = 0.33717286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 333, loss = 0.33666573\n",
      "Iteration 334, loss = 0.33616339\n",
      "Iteration 335, loss = 0.33566354\n",
      "Iteration 336, loss = 0.33516773\n",
      "Iteration 337, loss = 0.33467788\n",
      "Iteration 338, loss = 0.33419513\n",
      "Iteration 339, loss = 0.33371935\n",
      "Iteration 340, loss = 0.33325191\n",
      "Iteration 341, loss = 0.33278868\n",
      "Iteration 342, loss = 0.33232761\n",
      "Iteration 343, loss = 0.33186972\n",
      "Iteration 344, loss = 0.33141371\n",
      "Iteration 345, loss = 0.33095927\n",
      "Iteration 346, loss = 0.33050645\n",
      "Iteration 347, loss = 0.33005514\n",
      "Iteration 348, loss = 0.32960562\n",
      "Iteration 349, loss = 0.32915880\n",
      "Iteration 350, loss = 0.32871370\n",
      "Iteration 351, loss = 0.32827097\n",
      "Iteration 352, loss = 0.32782966\n",
      "Iteration 353, loss = 0.32738958\n",
      "Iteration 354, loss = 0.32695071\n",
      "Iteration 355, loss = 0.32651313\n",
      "Iteration 356, loss = 0.32607680\n",
      "Iteration 357, loss = 0.32564199\n",
      "Iteration 358, loss = 0.32520835\n",
      "Iteration 359, loss = 0.32477593\n",
      "Iteration 360, loss = 0.32434454\n",
      "Iteration 361, loss = 0.32391441\n",
      "Iteration 362, loss = 0.32348546\n",
      "Iteration 363, loss = 0.32305767\n",
      "Iteration 364, loss = 0.32263112\n",
      "Iteration 365, loss = 0.32220567\n",
      "Iteration 366, loss = 0.32178139\n",
      "Iteration 367, loss = 0.32135827\n",
      "Iteration 368, loss = 0.32093632\n",
      "Iteration 369, loss = 0.32051554\n",
      "Iteration 370, loss = 0.32009597\n",
      "Iteration 371, loss = 0.31967792\n",
      "Iteration 372, loss = 0.31926107\n",
      "Iteration 373, loss = 0.31884534\n",
      "Iteration 374, loss = 0.31843042\n",
      "Iteration 375, loss = 0.31801667\n",
      "Iteration 376, loss = 0.31760401\n",
      "Iteration 377, loss = 0.31719240\n",
      "Iteration 378, loss = 0.31678187\n",
      "Iteration 379, loss = 0.31637247\n",
      "Iteration 380, loss = 0.31596415\n",
      "Iteration 381, loss = 0.31555698\n",
      "Iteration 382, loss = 0.31515098\n",
      "Iteration 383, loss = 0.31474611\n",
      "Iteration 384, loss = 0.31434217\n",
      "Iteration 385, loss = 0.31393933\n",
      "Iteration 386, loss = 0.31353755\n",
      "Iteration 387, loss = 0.31313678\n",
      "Iteration 388, loss = 0.31273708\n",
      "Iteration 389, loss = 0.31233848\n",
      "Iteration 390, loss = 0.31194085\n",
      "Iteration 391, loss = 0.31154421\n",
      "Iteration 392, loss = 0.31114858\n",
      "Iteration 393, loss = 0.31075399\n",
      "Iteration 394, loss = 0.31036042\n",
      "Iteration 395, loss = 0.30996791\n",
      "Iteration 396, loss = 0.30957635\n",
      "Iteration 397, loss = 0.30918576\n",
      "Iteration 398, loss = 0.30879621\n",
      "Iteration 399, loss = 0.30840755\n",
      "Iteration 400, loss = 0.30801985\n",
      "Iteration 401, loss = 0.30763318\n",
      "Iteration 402, loss = 0.30724741\n",
      "Iteration 403, loss = 0.30686265\n",
      "Iteration 404, loss = 0.30647878\n",
      "Iteration 405, loss = 0.30609587\n",
      "Iteration 406, loss = 0.30571391\n",
      "Iteration 407, loss = 0.30533302\n",
      "Iteration 408, loss = 0.30495308\n",
      "Iteration 409, loss = 0.30457407\n",
      "Iteration 410, loss = 0.30419614\n",
      "Iteration 411, loss = 0.30381919\n",
      "Iteration 412, loss = 0.30344338\n",
      "Iteration 413, loss = 0.30306862\n",
      "Iteration 414, loss = 0.30269487\n",
      "Iteration 415, loss = 0.30232262\n",
      "Iteration 416, loss = 0.30195123\n",
      "Iteration 417, loss = 0.30158071\n",
      "Iteration 418, loss = 0.30121104\n",
      "Iteration 419, loss = 0.30084221\n",
      "Iteration 420, loss = 0.30047427\n",
      "Iteration 421, loss = 0.30010720\n",
      "Iteration 422, loss = 0.29974103\n",
      "Iteration 423, loss = 0.29937570\n",
      "Iteration 424, loss = 0.29901127\n",
      "Iteration 425, loss = 0.29864771\n",
      "Iteration 426, loss = 0.29828573\n",
      "Iteration 427, loss = 0.29792420\n",
      "Iteration 428, loss = 0.29756338\n",
      "Iteration 429, loss = 0.29720368\n",
      "Iteration 430, loss = 0.29684487\n",
      "Iteration 431, loss = 0.29648693\n",
      "Iteration 432, loss = 0.29612986\n",
      "Iteration 433, loss = 0.29577365\n",
      "Iteration 434, loss = 0.29541830\n",
      "Iteration 435, loss = 0.29506378\n",
      "Iteration 436, loss = 0.29471011\n",
      "Iteration 437, loss = 0.29435730\n",
      "Iteration 438, loss = 0.29400529\n",
      "Iteration 439, loss = 0.29365411\n",
      "Iteration 440, loss = 0.29330378\n",
      "Iteration 441, loss = 0.29295427\n",
      "Iteration 442, loss = 0.29260556\n",
      "Iteration 443, loss = 0.29225766\n",
      "Iteration 444, loss = 0.29191056\n",
      "Iteration 445, loss = 0.29156428\n",
      "Iteration 446, loss = 0.29121879\n",
      "Iteration 447, loss = 0.29087412\n",
      "Iteration 448, loss = 0.29053024\n",
      "Iteration 449, loss = 0.29018713\n",
      "Iteration 450, loss = 0.28984482\n",
      "Iteration 451, loss = 0.28950329\n",
      "Iteration 452, loss = 0.28916256\n",
      "Iteration 453, loss = 0.28882259\n",
      "Iteration 454, loss = 0.28848340\n",
      "Iteration 455, loss = 0.28814499\n",
      "Iteration 456, loss = 0.28780736\n",
      "Iteration 457, loss = 0.28747047\n",
      "Iteration 458, loss = 0.28713435\n",
      "Iteration 459, loss = 0.28679900\n",
      "Iteration 460, loss = 0.28646439\n",
      "Iteration 461, loss = 0.28613056\n",
      "Iteration 462, loss = 0.28579745\n",
      "Iteration 463, loss = 0.28546511\n",
      "Iteration 464, loss = 0.28513351\n",
      "Iteration 465, loss = 0.28480266\n",
      "Iteration 466, loss = 0.28447255\n",
      "Iteration 467, loss = 0.28414318\n",
      "Iteration 468, loss = 0.28381453\n",
      "Iteration 469, loss = 0.28348664\n",
      "Iteration 470, loss = 0.28315946\n",
      "Iteration 471, loss = 0.28283303\n",
      "Iteration 472, loss = 0.28250734\n",
      "Iteration 473, loss = 0.28218242\n",
      "Iteration 474, loss = 0.28185821\n",
      "Iteration 475, loss = 0.28153469\n",
      "Iteration 476, loss = 0.28121188\n",
      "Iteration 477, loss = 0.28088981\n",
      "Iteration 478, loss = 0.28056851\n",
      "Iteration 479, loss = 0.28024785\n",
      "Iteration 480, loss = 0.27992793\n",
      "Iteration 481, loss = 0.27960871\n",
      "Iteration 482, loss = 0.27929022\n",
      "Iteration 483, loss = 0.27897241\n",
      "Iteration 484, loss = 0.27865532\n",
      "Iteration 485, loss = 0.27833892\n",
      "Iteration 486, loss = 0.27802324\n",
      "Iteration 487, loss = 0.27770822\n",
      "Iteration 488, loss = 0.27739395\n",
      "Iteration 489, loss = 0.27708031\n",
      "Iteration 490, loss = 0.27676739\n",
      "Iteration 491, loss = 0.27645517\n",
      "Iteration 492, loss = 0.27614363\n",
      "Iteration 493, loss = 0.27583281\n",
      "Iteration 494, loss = 0.27552262\n",
      "Iteration 495, loss = 0.27521311\n",
      "Iteration 496, loss = 0.27490430\n",
      "Iteration 497, loss = 0.27459617\n",
      "Iteration 498, loss = 0.27428874\n",
      "Iteration 499, loss = 0.27398195\n",
      "Iteration 500, loss = 0.27367586\n",
      "Iteration 501, loss = 0.27337043\n",
      "Iteration 502, loss = 0.27306572\n",
      "Iteration 503, loss = 0.27276167\n",
      "Iteration 504, loss = 0.27245832\n",
      "Iteration 505, loss = 0.27215560\n",
      "Iteration 506, loss = 0.27185357\n",
      "Iteration 507, loss = 0.27155219\n",
      "Iteration 508, loss = 0.27125147\n",
      "Iteration 509, loss = 0.27095140\n",
      "Iteration 510, loss = 0.27065199\n",
      "Iteration 511, loss = 0.27035323\n",
      "Iteration 512, loss = 0.27005513\n",
      "Iteration 513, loss = 0.26975764\n",
      "Iteration 514, loss = 0.26946083\n",
      "Iteration 515, loss = 0.26916466\n",
      "Iteration 516, loss = 0.26886910\n",
      "Iteration 517, loss = 0.26857421\n",
      "Iteration 518, loss = 0.26827996\n",
      "Iteration 519, loss = 0.26798631\n",
      "Iteration 520, loss = 0.26769332\n",
      "Iteration 521, loss = 0.26740097\n",
      "Iteration 522, loss = 0.26710923\n",
      "Iteration 523, loss = 0.26681813\n",
      "Iteration 524, loss = 0.26652786\n",
      "Iteration 525, loss = 0.26623818\n",
      "Iteration 526, loss = 0.26594914\n",
      "Iteration 527, loss = 0.26566075\n",
      "Iteration 528, loss = 0.26537303\n",
      "Iteration 529, loss = 0.26508590\n",
      "Iteration 530, loss = 0.26479945\n",
      "Iteration 531, loss = 0.26451359\n",
      "Iteration 532, loss = 0.26422863\n",
      "Iteration 533, loss = 0.26394407\n",
      "Iteration 534, loss = 0.26366025\n",
      "Iteration 535, loss = 0.26337699\n",
      "Iteration 536, loss = 0.26309443\n",
      "Iteration 537, loss = 0.26281243\n",
      "Iteration 538, loss = 0.26253107\n",
      "Iteration 539, loss = 0.26225032\n",
      "Iteration 540, loss = 0.26197013\n",
      "Iteration 541, loss = 0.26169077\n",
      "Iteration 542, loss = 0.26141191\n",
      "Iteration 543, loss = 0.26113370\n",
      "Iteration 544, loss = 0.26085608\n",
      "Iteration 545, loss = 0.26057906\n",
      "Iteration 546, loss = 0.26030266\n",
      "Iteration 547, loss = 0.26002680\n",
      "Iteration 548, loss = 0.25975163\n",
      "Iteration 549, loss = 0.25947698\n",
      "Iteration 550, loss = 0.25920320\n",
      "Iteration 551, loss = 0.25892991\n",
      "Iteration 552, loss = 0.25865721\n",
      "Iteration 553, loss = 0.25838522\n",
      "Iteration 554, loss = 0.25811367\n",
      "Iteration 555, loss = 0.25784286\n",
      "Iteration 556, loss = 0.25757256\n",
      "Iteration 557, loss = 0.25730287\n",
      "Iteration 558, loss = 0.25703399\n",
      "Iteration 559, loss = 0.25676559\n",
      "Iteration 560, loss = 0.25649798\n",
      "Iteration 561, loss = 0.25623082\n",
      "Iteration 562, loss = 0.25596423\n",
      "Iteration 563, loss = 0.25569829\n",
      "Iteration 564, loss = 0.25543283\n",
      "Iteration 565, loss = 0.25516805\n",
      "Iteration 566, loss = 0.25490378\n",
      "Iteration 567, loss = 0.25464006\n",
      "Iteration 568, loss = 0.25437702\n",
      "Iteration 569, loss = 0.25411439\n",
      "Iteration 570, loss = 0.25385243\n",
      "Iteration 571, loss = 0.25359100\n",
      "Iteration 572, loss = 0.25333010\n",
      "Iteration 573, loss = 0.25306980\n",
      "Iteration 574, loss = 0.25281004\n",
      "Iteration 575, loss = 0.25255081\n",
      "Iteration 576, loss = 0.25229218\n",
      "Iteration 577, loss = 0.25203404\n",
      "Iteration 578, loss = 0.25177647\n",
      "Iteration 579, loss = 0.25151958\n",
      "Iteration 580, loss = 0.25126309\n",
      "Iteration 581, loss = 0.25100723\n",
      "Iteration 582, loss = 0.25075194\n",
      "Iteration 583, loss = 0.25049713\n",
      "Iteration 584, loss = 0.25024288\n",
      "Iteration 585, loss = 0.24998931\n",
      "Iteration 586, loss = 0.24973614\n",
      "Iteration 587, loss = 0.24948360\n",
      "Iteration 588, loss = 0.24923160\n",
      "Iteration 589, loss = 0.24898011\n",
      "Iteration 590, loss = 0.24872911\n",
      "Iteration 591, loss = 0.24847876\n",
      "Iteration 592, loss = 0.24822883\n",
      "Iteration 593, loss = 0.24797953\n",
      "Iteration 594, loss = 0.24773082\n",
      "Iteration 595, loss = 0.24748256\n",
      "Iteration 596, loss = 0.24723482\n",
      "Iteration 597, loss = 0.24698764\n",
      "Iteration 598, loss = 0.24674098\n",
      "Iteration 599, loss = 0.24649483\n",
      "Iteration 600, loss = 0.24624926\n",
      "Iteration 601, loss = 0.24600419\n",
      "Iteration 602, loss = 0.24575958\n",
      "Iteration 603, loss = 0.24551553\n",
      "Iteration 604, loss = 0.24527206\n",
      "Iteration 605, loss = 0.24502901\n",
      "Iteration 606, loss = 0.24478647\n",
      "Iteration 607, loss = 0.24454453\n",
      "Iteration 608, loss = 0.24430297\n",
      "Iteration 609, loss = 0.24406199\n",
      "Iteration 610, loss = 0.24382169\n",
      "Iteration 611, loss = 0.24358184\n",
      "Iteration 612, loss = 0.24334249\n",
      "Iteration 613, loss = 0.24310365\n",
      "Iteration 614, loss = 0.24286546\n",
      "Iteration 615, loss = 0.24262752\n",
      "Iteration 616, loss = 0.24239032\n",
      "Iteration 617, loss = 0.24215351\n",
      "Iteration 618, loss = 0.24191721\n",
      "Iteration 619, loss = 0.24168138\n",
      "Iteration 620, loss = 0.24144602\n",
      "Iteration 621, loss = 0.24121130\n",
      "Iteration 622, loss = 0.24097689\n",
      "Iteration 623, loss = 0.24074310\n",
      "Iteration 624, loss = 0.24050979\n",
      "Iteration 625, loss = 0.24027697\n",
      "Iteration 626, loss = 0.24004458\n",
      "Iteration 627, loss = 0.23981271\n",
      "Iteration 628, loss = 0.23958138\n",
      "Iteration 629, loss = 0.23935048\n",
      "Iteration 630, loss = 0.23912006\n",
      "Iteration 631, loss = 0.23889017\n",
      "Iteration 632, loss = 0.23866074\n",
      "Iteration 633, loss = 0.23843175\n",
      "Iteration 634, loss = 0.23820326\n",
      "Iteration 635, loss = 0.23797536\n",
      "Iteration 636, loss = 0.23774774\n",
      "Iteration 637, loss = 0.23752068\n",
      "Iteration 638, loss = 0.23729413\n",
      "Iteration 639, loss = 0.23706804\n",
      "Iteration 640, loss = 0.23684240\n",
      "Iteration 641, loss = 0.23661722\n",
      "Iteration 642, loss = 0.23639248\n",
      "Iteration 643, loss = 0.23616834\n",
      "Iteration 644, loss = 0.23594449\n",
      "Iteration 645, loss = 0.23572118\n",
      "Iteration 646, loss = 0.23549836\n",
      "Iteration 647, loss = 0.23527599\n",
      "Iteration 648, loss = 0.23505411\n",
      "Iteration 649, loss = 0.23483262\n",
      "Iteration 650, loss = 0.23461160\n",
      "Iteration 651, loss = 0.23439108\n",
      "Iteration 652, loss = 0.23417101\n",
      "Iteration 653, loss = 0.23395137\n",
      "Iteration 654, loss = 0.23373221\n",
      "Iteration 655, loss = 0.23351359\n",
      "Iteration 656, loss = 0.23329541\n",
      "Iteration 657, loss = 0.23307773\n",
      "Iteration 658, loss = 0.23286049\n",
      "Iteration 659, loss = 0.23264371\n",
      "Iteration 660, loss = 0.23242745\n",
      "Iteration 661, loss = 0.23221158\n",
      "Iteration 662, loss = 0.23199621\n",
      "Iteration 663, loss = 0.23178124\n",
      "Iteration 664, loss = 0.23156680\n",
      "Iteration 665, loss = 0.23135287\n",
      "Iteration 666, loss = 0.23113951\n",
      "Iteration 667, loss = 0.23092656\n",
      "Iteration 668, loss = 0.23071411\n",
      "Iteration 669, loss = 0.23050211\n",
      "Iteration 670, loss = 0.23029051\n",
      "Iteration 671, loss = 0.23007938\n",
      "Iteration 672, loss = 0.22986863\n",
      "Iteration 673, loss = 0.22965846\n",
      "Iteration 674, loss = 0.22944859\n",
      "Iteration 675, loss = 0.22923919\n",
      "Iteration 676, loss = 0.22903023\n",
      "Iteration 677, loss = 0.22882180\n",
      "Iteration 678, loss = 0.22861367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 679, loss = 0.22840603\n",
      "Iteration 680, loss = 0.22819878\n",
      "Iteration 681, loss = 0.22799203\n",
      "Iteration 682, loss = 0.22778567\n",
      "Iteration 683, loss = 0.22757975\n",
      "Iteration 684, loss = 0.22737426\n",
      "Iteration 685, loss = 0.22716915\n",
      "Iteration 686, loss = 0.22696452\n",
      "Iteration 687, loss = 0.22676029\n",
      "Iteration 688, loss = 0.22655650\n",
      "Iteration 689, loss = 0.22635308\n",
      "Iteration 690, loss = 0.22615011\n",
      "Iteration 691, loss = 0.22594762\n",
      "Iteration 692, loss = 0.22574546\n",
      "Iteration 693, loss = 0.22554375\n",
      "Iteration 694, loss = 0.22534246\n",
      "Iteration 695, loss = 0.22514160\n",
      "Iteration 696, loss = 0.22494126\n",
      "Iteration 697, loss = 0.22474121\n",
      "Iteration 698, loss = 0.22454163\n",
      "Iteration 699, loss = 0.22434245\n",
      "Iteration 700, loss = 0.22414365\n",
      "Iteration 701, loss = 0.22394531\n",
      "Iteration 702, loss = 0.22374734\n",
      "Iteration 703, loss = 0.22354983\n",
      "Iteration 704, loss = 0.22335267\n",
      "Iteration 705, loss = 0.22315591\n",
      "Iteration 706, loss = 0.22295966\n",
      "Iteration 707, loss = 0.22276370\n",
      "Iteration 708, loss = 0.22256818\n",
      "Iteration 709, loss = 0.22237306\n",
      "Iteration 710, loss = 0.22217833\n",
      "Iteration 711, loss = 0.22198400\n",
      "Iteration 712, loss = 0.22179012\n",
      "Iteration 713, loss = 0.22159659\n",
      "Iteration 714, loss = 0.22140349\n",
      "Iteration 715, loss = 0.22121080\n",
      "Iteration 716, loss = 0.22101845\n",
      "Iteration 717, loss = 0.22082657\n",
      "Iteration 718, loss = 0.22063506\n",
      "Iteration 719, loss = 0.22044390\n",
      "Iteration 720, loss = 0.22025317\n",
      "Iteration 721, loss = 0.22006279\n",
      "Iteration 722, loss = 0.21987281\n",
      "Iteration 723, loss = 0.21968330\n",
      "Iteration 724, loss = 0.21949409\n",
      "Iteration 725, loss = 0.21930527\n",
      "Iteration 726, loss = 0.21911686\n",
      "Iteration 727, loss = 0.21892879\n",
      "Iteration 728, loss = 0.21874110\n",
      "Iteration 729, loss = 0.21855395\n",
      "Iteration 730, loss = 0.21836696\n",
      "Iteration 731, loss = 0.21818037\n",
      "Iteration 732, loss = 0.21799411\n",
      "Iteration 733, loss = 0.21780823\n",
      "Iteration 734, loss = 0.21762278\n",
      "Iteration 735, loss = 0.21743758\n",
      "Iteration 736, loss = 0.21725284\n",
      "Iteration 737, loss = 0.21706839\n",
      "Iteration 738, loss = 0.21688434\n",
      "Iteration 739, loss = 0.21670064\n",
      "Iteration 740, loss = 0.21651730\n",
      "Iteration 741, loss = 0.21633445\n",
      "Iteration 742, loss = 0.21615178\n",
      "Iteration 743, loss = 0.21596956\n",
      "Iteration 744, loss = 0.21578768\n",
      "Iteration 745, loss = 0.21560618\n",
      "Iteration 746, loss = 0.21542504\n",
      "Iteration 747, loss = 0.21524434\n",
      "Iteration 748, loss = 0.21506387\n",
      "Iteration 749, loss = 0.21488382\n",
      "Iteration 750, loss = 0.21470413\n",
      "Iteration 751, loss = 0.21452481\n",
      "Iteration 752, loss = 0.21434600\n",
      "Iteration 753, loss = 0.21416756\n",
      "Iteration 754, loss = 0.21398960\n",
      "Iteration 755, loss = 0.21381182\n",
      "Iteration 756, loss = 0.21363449\n",
      "Iteration 757, loss = 0.21345748\n",
      "Iteration 758, loss = 0.21328082\n",
      "Iteration 759, loss = 0.21310451\n",
      "Iteration 760, loss = 0.21292856\n",
      "Iteration 761, loss = 0.21275301\n",
      "Iteration 762, loss = 0.21257778\n",
      "Iteration 763, loss = 0.21240291\n",
      "Iteration 764, loss = 0.21222836\n",
      "Iteration 765, loss = 0.21205420\n",
      "Iteration 766, loss = 0.21188035\n",
      "Iteration 767, loss = 0.21170688\n",
      "Iteration 768, loss = 0.21153378\n",
      "Iteration 769, loss = 0.21136105\n",
      "Iteration 770, loss = 0.21118867\n",
      "Iteration 771, loss = 0.21101663\n",
      "Iteration 772, loss = 0.21084492\n",
      "Iteration 773, loss = 0.21067358\n",
      "Iteration 774, loss = 0.21050256\n",
      "Iteration 775, loss = 0.21033196\n",
      "Iteration 776, loss = 0.21016163\n",
      "Iteration 777, loss = 0.20999168\n",
      "Iteration 778, loss = 0.20982206\n",
      "Iteration 779, loss = 0.20965275\n",
      "Iteration 780, loss = 0.20948374\n",
      "Iteration 781, loss = 0.20931509\n",
      "Iteration 782, loss = 0.20914677\n",
      "Iteration 783, loss = 0.20897884\n",
      "Iteration 784, loss = 0.20881121\n",
      "Iteration 785, loss = 0.20864391\n",
      "Iteration 786, loss = 0.20847693\n",
      "Iteration 787, loss = 0.20831031\n",
      "Iteration 788, loss = 0.20814406\n",
      "Iteration 789, loss = 0.20797814\n",
      "Iteration 790, loss = 0.20781258\n",
      "Iteration 791, loss = 0.20764732\n",
      "Iteration 792, loss = 0.20748240\n",
      "Iteration 793, loss = 0.20731781\n",
      "Iteration 794, loss = 0.20715355\n",
      "Iteration 795, loss = 0.20698961\n",
      "Iteration 796, loss = 0.20682600\n",
      "Iteration 797, loss = 0.20666277\n",
      "Iteration 798, loss = 0.20649981\n",
      "Iteration 799, loss = 0.20633723\n",
      "Iteration 800, loss = 0.20617495\n",
      "Iteration 801, loss = 0.20601301\n",
      "Iteration 802, loss = 0.20585138\n",
      "Iteration 803, loss = 0.20569007\n",
      "Iteration 804, loss = 0.20552910\n",
      "Iteration 805, loss = 0.20536844\n",
      "Iteration 806, loss = 0.20520810\n",
      "Iteration 807, loss = 0.20504809\n",
      "Iteration 808, loss = 0.20488838\n",
      "Iteration 809, loss = 0.20472900\n",
      "Iteration 810, loss = 0.20456993\n",
      "Iteration 811, loss = 0.20441118\n",
      "Iteration 812, loss = 0.20425275\n",
      "Iteration 813, loss = 0.20409463\n",
      "Iteration 814, loss = 0.20393682\n",
      "Iteration 815, loss = 0.20377933\n",
      "Iteration 816, loss = 0.20362215\n",
      "Iteration 817, loss = 0.20346528\n",
      "Iteration 818, loss = 0.20330872\n",
      "Iteration 819, loss = 0.20315247\n",
      "Iteration 820, loss = 0.20299654\n",
      "Iteration 821, loss = 0.20284090\n",
      "Iteration 822, loss = 0.20268558\n",
      "Iteration 823, loss = 0.20253058\n",
      "Iteration 824, loss = 0.20237587\n",
      "Iteration 825, loss = 0.20222146\n",
      "Iteration 826, loss = 0.20206738\n",
      "Iteration 827, loss = 0.20191360\n",
      "Iteration 828, loss = 0.20176012\n",
      "Iteration 829, loss = 0.20160693\n",
      "Iteration 830, loss = 0.20145405\n",
      "Iteration 831, loss = 0.20130147\n",
      "Iteration 832, loss = 0.20114920\n",
      "Iteration 833, loss = 0.20099723\n",
      "Iteration 834, loss = 0.20084555\n",
      "Iteration 835, loss = 0.20069417\n",
      "Iteration 836, loss = 0.20054310\n",
      "Iteration 837, loss = 0.20039231\n",
      "Iteration 838, loss = 0.20024183\n",
      "Iteration 839, loss = 0.20009164\n",
      "Iteration 840, loss = 0.19994175\n",
      "Iteration 841, loss = 0.19979215\n",
      "Iteration 842, loss = 0.19964285\n",
      "Iteration 843, loss = 0.19949383\n",
      "Iteration 844, loss = 0.19934512\n",
      "Iteration 845, loss = 0.19919670\n",
      "Iteration 846, loss = 0.19904856\n",
      "Iteration 847, loss = 0.19890072\n",
      "Iteration 848, loss = 0.19875324\n",
      "Iteration 849, loss = 0.19860604\n",
      "Iteration 850, loss = 0.19845913\n",
      "Iteration 851, loss = 0.19831251\n",
      "Iteration 852, loss = 0.19816619\n",
      "Iteration 853, loss = 0.19802015\n",
      "Iteration 854, loss = 0.19787440\n",
      "Iteration 855, loss = 0.19772893\n",
      "Iteration 856, loss = 0.19758376\n",
      "Iteration 857, loss = 0.19743886\n",
      "Iteration 858, loss = 0.19729426\n",
      "Iteration 859, loss = 0.19714993\n",
      "Iteration 860, loss = 0.19700589\n",
      "Iteration 861, loss = 0.19686214\n",
      "Iteration 862, loss = 0.19671866\n",
      "Iteration 863, loss = 0.19657546\n",
      "Iteration 864, loss = 0.19643254\n",
      "Iteration 865, loss = 0.19628990\n",
      "Iteration 866, loss = 0.19614755\n",
      "Iteration 867, loss = 0.19600546\n",
      "Iteration 868, loss = 0.19586366\n",
      "Iteration 869, loss = 0.19572213\n",
      "Iteration 870, loss = 0.19558088\n",
      "Iteration 871, loss = 0.19543990\n",
      "Iteration 872, loss = 0.19529920\n",
      "Iteration 873, loss = 0.19515877\n",
      "Iteration 874, loss = 0.19501861\n",
      "Iteration 875, loss = 0.19487873\n",
      "Iteration 876, loss = 0.19473912\n",
      "Iteration 877, loss = 0.19459978\n",
      "Iteration 878, loss = 0.19446071\n",
      "Iteration 879, loss = 0.19432191\n",
      "Iteration 880, loss = 0.19418338\n",
      "Iteration 881, loss = 0.19404512\n",
      "Iteration 882, loss = 0.19390713\n",
      "Iteration 883, loss = 0.19376941\n",
      "Iteration 884, loss = 0.19363195\n",
      "Iteration 885, loss = 0.19349476\n",
      "Iteration 886, loss = 0.19335783\n",
      "Iteration 887, loss = 0.19322117\n",
      "Iteration 888, loss = 0.19308478\n",
      "Iteration 889, loss = 0.19294864\n",
      "Iteration 890, loss = 0.19281277\n",
      "Iteration 891, loss = 0.19267716\n",
      "Iteration 892, loss = 0.19254182\n",
      "Iteration 893, loss = 0.19240674\n",
      "Iteration 894, loss = 0.19227192\n",
      "Iteration 895, loss = 0.19213737\n",
      "Iteration 896, loss = 0.19200308\n",
      "Iteration 897, loss = 0.19186904\n",
      "Iteration 898, loss = 0.19173526\n",
      "Iteration 899, loss = 0.19160175\n",
      "Iteration 900, loss = 0.19146836\n",
      "Iteration 901, loss = 0.19133511\n",
      "Iteration 902, loss = 0.19120208\n",
      "Iteration 903, loss = 0.19106929\n",
      "Iteration 904, loss = 0.19093673\n",
      "Iteration 905, loss = 0.19080441\n",
      "Iteration 906, loss = 0.19067226\n",
      "Iteration 907, loss = 0.19054032\n",
      "Iteration 908, loss = 0.19040860\n",
      "Iteration 909, loss = 0.19027713\n",
      "Iteration 910, loss = 0.19014589\n",
      "Iteration 911, loss = 0.19001487\n",
      "Iteration 912, loss = 0.18988410\n",
      "Iteration 913, loss = 0.18975358\n",
      "Iteration 914, loss = 0.18962384\n",
      "Iteration 915, loss = 0.18949434\n",
      "Iteration 916, loss = 0.18936510\n",
      "Iteration 917, loss = 0.18923610\n",
      "Iteration 918, loss = 0.18910736\n",
      "Iteration 919, loss = 0.18897887\n",
      "Iteration 920, loss = 0.18885065\n",
      "Iteration 921, loss = 0.18872265\n",
      "Iteration 922, loss = 0.18859491\n",
      "Iteration 923, loss = 0.18846741\n",
      "Iteration 924, loss = 0.18834016\n",
      "Iteration 925, loss = 0.18821314\n",
      "Iteration 926, loss = 0.18808637\n",
      "Iteration 927, loss = 0.18795985\n",
      "Iteration 928, loss = 0.18783356\n",
      "Iteration 929, loss = 0.18770752\n",
      "Iteration 930, loss = 0.18758172\n",
      "Iteration 931, loss = 0.18745616\n",
      "Iteration 932, loss = 0.18733085\n",
      "Iteration 933, loss = 0.18720576\n",
      "Iteration 934, loss = 0.18708092\n",
      "Iteration 935, loss = 0.18695632\n",
      "Iteration 936, loss = 0.18683194\n",
      "Iteration 937, loss = 0.18670781\n",
      "Iteration 938, loss = 0.18658391\n",
      "Iteration 939, loss = 0.18646025\n",
      "Iteration 940, loss = 0.18633682\n",
      "Iteration 941, loss = 0.18621362\n",
      "Iteration 942, loss = 0.18609066\n",
      "Iteration 943, loss = 0.18596793\n",
      "Iteration 944, loss = 0.18584543\n",
      "Iteration 945, loss = 0.18572316\n",
      "Iteration 946, loss = 0.18560112\n",
      "Iteration 947, loss = 0.18547931\n",
      "Iteration 948, loss = 0.18535773\n",
      "Iteration 949, loss = 0.18523638\n",
      "Iteration 950, loss = 0.18511526\n",
      "Iteration 951, loss = 0.18499437\n",
      "Iteration 952, loss = 0.18487370\n",
      "Iteration 953, loss = 0.18475326\n",
      "Iteration 954, loss = 0.18463304\n",
      "Iteration 955, loss = 0.18451305\n",
      "Iteration 956, loss = 0.18439329\n",
      "Iteration 957, loss = 0.18427375\n",
      "Iteration 958, loss = 0.18415443\n",
      "Iteration 959, loss = 0.18403534\n",
      "Iteration 960, loss = 0.18391648\n",
      "Iteration 961, loss = 0.18379783\n",
      "Iteration 962, loss = 0.18367940\n",
      "Iteration 963, loss = 0.18356120\n",
      "Iteration 964, loss = 0.18344322\n",
      "Iteration 965, loss = 0.18332545\n",
      "Iteration 966, loss = 0.18320791\n",
      "Iteration 967, loss = 0.18309059\n",
      "Iteration 968, loss = 0.18297350\n",
      "Iteration 969, loss = 0.18285661\n",
      "Iteration 970, loss = 0.18273995\n",
      "Iteration 971, loss = 0.18262350\n",
      "Iteration 972, loss = 0.18250727\n",
      "Iteration 973, loss = 0.18239126\n",
      "Iteration 974, loss = 0.18227546\n",
      "Iteration 975, loss = 0.18215988\n",
      "Iteration 976, loss = 0.18204451\n",
      "Iteration 977, loss = 0.18192936\n",
      "Iteration 978, loss = 0.18181442\n",
      "Iteration 979, loss = 0.18169969\n",
      "Iteration 980, loss = 0.18158518\n",
      "Iteration 981, loss = 0.18147088\n",
      "Iteration 982, loss = 0.18135679\n",
      "Iteration 983, loss = 0.18124291\n",
      "Iteration 984, loss = 0.18112924\n",
      "Iteration 985, loss = 0.18101579\n",
      "Iteration 986, loss = 0.18090254\n",
      "Iteration 987, loss = 0.18078950\n",
      "Iteration 988, loss = 0.18067667\n",
      "Iteration 989, loss = 0.18056405\n",
      "Iteration 990, loss = 0.18045164\n",
      "Iteration 991, loss = 0.18033943\n",
      "Iteration 992, loss = 0.18022744\n",
      "Iteration 993, loss = 0.18011564\n",
      "Iteration 994, loss = 0.18000405\n",
      "Iteration 995, loss = 0.17989267\n",
      "Iteration 996, loss = 0.17978150\n",
      "Iteration 997, loss = 0.17967052\n",
      "Iteration 998, loss = 0.17955976\n",
      "Iteration 999, loss = 0.17944919\n",
      "Iteration 1000, loss = 0.17933883\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 7.61823747\n",
      "Iteration 3, loss = 6.56289214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 2.34739817\n",
      "Iteration 5, loss = 2.17906063\n",
      "Iteration 6, loss = 1.68909108\n",
      "Iteration 7, loss = 0.82257109\n",
      "Iteration 8, loss = 0.75929698\n",
      "Iteration 9, loss = 0.87045474\n",
      "Iteration 10, loss = 0.81510445\n",
      "Iteration 11, loss = 0.74455888\n",
      "Iteration 12, loss = 0.69390257\n",
      "Iteration 13, loss = 0.63812403\n",
      "Iteration 14, loss = 0.59012755\n",
      "Iteration 15, loss = 0.53752963\n",
      "Iteration 16, loss = 0.49826417\n",
      "Iteration 17, loss = 0.46988952\n",
      "Iteration 18, loss = 0.45238378\n",
      "Iteration 19, loss = 0.41061576\n",
      "Iteration 20, loss = 0.38847258\n",
      "Iteration 21, loss = 0.37746835\n",
      "Iteration 22, loss = 0.36182499\n",
      "Iteration 23, loss = 0.33427811\n",
      "Iteration 24, loss = 0.31688273\n",
      "Iteration 25, loss = 0.30509151\n",
      "Iteration 26, loss = 0.28746695\n",
      "Iteration 27, loss = 0.27457806\n",
      "Iteration 28, loss = 0.26184453\n",
      "Iteration 29, loss = 0.24606778\n",
      "Iteration 30, loss = 0.23658529\n",
      "Iteration 31, loss = 0.22284147\n",
      "Iteration 32, loss = 0.21338166\n",
      "Iteration 33, loss = 0.20326170\n",
      "Iteration 34, loss = 0.19352864\n",
      "Iteration 35, loss = 0.18633083\n",
      "Iteration 36, loss = 0.17746272\n",
      "Iteration 37, loss = 0.17247137\n",
      "Iteration 38, loss = 0.16506320\n",
      "Iteration 39, loss = 0.16128003\n",
      "Iteration 40, loss = 0.15559196\n",
      "Iteration 41, loss = 0.15266166\n",
      "Iteration 42, loss = 0.14831912\n",
      "Iteration 43, loss = 0.14586926\n",
      "Iteration 44, loss = 0.14266760\n",
      "Iteration 45, loss = 0.14059433\n",
      "Iteration 46, loss = 0.13818003\n",
      "Iteration 47, loss = 0.13639530\n",
      "Iteration 48, loss = 0.13457303\n",
      "Iteration 49, loss = 0.13307754\n",
      "Iteration 50, loss = 0.13163096\n",
      "Iteration 51, loss = 0.13036171\n",
      "Iteration 52, loss = 0.12915874\n",
      "Iteration 53, loss = 0.12806400\n",
      "Iteration 54, loss = 0.12701483\n",
      "Iteration 55, loss = 0.12609087\n",
      "Iteration 56, loss = 0.12512485\n",
      "Iteration 57, loss = 0.12433489\n",
      "Iteration 58, loss = 0.12341850\n",
      "Iteration 59, loss = 0.12275162\n",
      "Iteration 60, loss = 0.12187826\n",
      "Iteration 61, loss = 0.12129708\n",
      "Iteration 62, loss = 0.12046787\n",
      "Iteration 63, loss = 0.11994759\n",
      "Iteration 64, loss = 0.11918483\n",
      "Iteration 65, loss = 0.11869225\n",
      "Iteration 66, loss = 0.11801311\n",
      "Iteration 67, loss = 0.11750870\n",
      "Iteration 68, loss = 0.11692493\n",
      "Iteration 69, loss = 0.11639527\n",
      "Iteration 70, loss = 0.11590272\n",
      "Iteration 71, loss = 0.11535743\n",
      "Iteration 72, loss = 0.11492279\n",
      "Iteration 73, loss = 0.11439077\n",
      "Iteration 74, loss = 0.11396959\n",
      "Iteration 75, loss = 0.11348588\n",
      "Iteration 76, loss = 0.11304816\n",
      "Iteration 77, loss = 0.11262270\n",
      "Iteration 78, loss = 0.11216986\n",
      "Iteration 79, loss = 0.11177914\n",
      "Iteration 80, loss = 0.11134015\n",
      "Iteration 81, loss = 0.11094774\n",
      "Iteration 82, loss = 0.11054716\n",
      "Iteration 83, loss = 0.11014182\n",
      "Iteration 84, loss = 0.10977121\n",
      "Iteration 85, loss = 0.10937381\n",
      "Iteration 86, loss = 0.10900592\n",
      "Iteration 87, loss = 0.10863698\n",
      "Iteration 88, loss = 0.10826287\n",
      "Iteration 89, loss = 0.10791447\n",
      "Iteration 90, loss = 0.10755236\n",
      "Iteration 91, loss = 0.10720374\n",
      "Iteration 92, loss = 0.10686457\n",
      "Iteration 93, loss = 0.10651738\n",
      "Iteration 94, loss = 0.10618765\n",
      "Iteration 95, loss = 0.10585728\n",
      "Iteration 96, loss = 0.10552763\n",
      "Iteration 97, loss = 0.10521124\n",
      "Iteration 98, loss = 0.10489149\n",
      "Iteration 99, loss = 0.10457737\n",
      "Iteration 100, loss = 0.10427173\n",
      "Iteration 101, loss = 0.10396377\n",
      "Iteration 102, loss = 0.10366292\n",
      "Iteration 103, loss = 0.10336746\n",
      "Iteration 104, loss = 0.10307137\n",
      "Iteration 105, loss = 0.10278203\n",
      "Iteration 106, loss = 0.10249680\n",
      "Iteration 107, loss = 0.10221206\n",
      "Iteration 108, loss = 0.10193323\n",
      "Iteration 109, loss = 0.10165815\n",
      "Iteration 110, loss = 0.10138413\n",
      "Iteration 111, loss = 0.10111519\n",
      "Iteration 112, loss = 0.10084987\n",
      "Iteration 113, loss = 0.10058599\n",
      "Iteration 114, loss = 0.10032630\n",
      "Iteration 115, loss = 0.10007036\n",
      "Iteration 116, loss = 0.09981609\n",
      "Iteration 117, loss = 0.09956514\n",
      "Iteration 118, loss = 0.09931798\n",
      "Iteration 119, loss = 0.09907288\n",
      "Iteration 120, loss = 0.09883045\n",
      "Iteration 121, loss = 0.09859151\n",
      "Iteration 122, loss = 0.09835500\n",
      "Iteration 123, loss = 0.09812071\n",
      "Iteration 124, loss = 0.09788944\n",
      "Iteration 125, loss = 0.09766098\n",
      "Iteration 126, loss = 0.09743460\n",
      "Iteration 127, loss = 0.09721068\n",
      "Iteration 128, loss = 0.09698951\n",
      "Iteration 129, loss = 0.09677061\n",
      "Iteration 130, loss = 0.09655377\n",
      "Iteration 131, loss = 0.09633934\n",
      "Iteration 132, loss = 0.09612728\n",
      "Iteration 133, loss = 0.09591730\n",
      "Iteration 134, loss = 0.09570935\n",
      "Iteration 135, loss = 0.09550364\n",
      "Iteration 136, loss = 0.09530007\n",
      "Iteration 137, loss = 0.09509842\n",
      "Iteration 138, loss = 0.09489871\n",
      "Iteration 139, loss = 0.09470103\n",
      "Iteration 140, loss = 0.09450530\n",
      "Iteration 141, loss = 0.09431141\n",
      "Iteration 142, loss = 0.09411930\n",
      "Iteration 143, loss = 0.09392907\n",
      "Iteration 144, loss = 0.09374068\n",
      "Iteration 145, loss = 0.09355402\n",
      "Iteration 146, loss = 0.09336907\n",
      "Iteration 147, loss = 0.09318579\n",
      "Iteration 148, loss = 0.09300434\n",
      "Iteration 149, loss = 0.09282452\n",
      "Iteration 150, loss = 0.09264629\n",
      "Iteration 151, loss = 0.09246961\n",
      "Iteration 152, loss = 0.09229446\n",
      "Iteration 153, loss = 0.09212082\n",
      "Iteration 154, loss = 0.09194866\n",
      "Iteration 155, loss = 0.09177796\n",
      "Iteration 156, loss = 0.09160869\n",
      "Iteration 157, loss = 0.09144084\n",
      "Iteration 158, loss = 0.09127439\n",
      "Iteration 159, loss = 0.09110930\n",
      "Iteration 160, loss = 0.09094558\n",
      "Iteration 161, loss = 0.09078317\n",
      "Iteration 162, loss = 0.09062208\n",
      "Iteration 163, loss = 0.09046227\n",
      "Iteration 164, loss = 0.09030373\n",
      "Iteration 165, loss = 0.09014644\n",
      "Iteration 166, loss = 0.08999040\n",
      "Iteration 167, loss = 0.08983558\n",
      "Iteration 168, loss = 0.08968195\n",
      "Iteration 169, loss = 0.08952950\n",
      "Iteration 170, loss = 0.08937821\n",
      "Iteration 171, loss = 0.08922805\n",
      "Iteration 172, loss = 0.08907903\n",
      "Iteration 173, loss = 0.08893110\n",
      "Iteration 174, loss = 0.08878427\n",
      "Iteration 175, loss = 0.08863852\n",
      "Iteration 176, loss = 0.08849383\n",
      "Iteration 177, loss = 0.08835020\n",
      "Iteration 178, loss = 0.08820759\n",
      "Iteration 179, loss = 0.08806601\n",
      "Iteration 180, loss = 0.08792543\n",
      "Iteration 181, loss = 0.08778585\n",
      "Iteration 182, loss = 0.08764726\n",
      "Iteration 183, loss = 0.08750965\n",
      "Iteration 184, loss = 0.08737301\n",
      "Iteration 185, loss = 0.08723732\n",
      "Iteration 186, loss = 0.08710259\n",
      "Iteration 187, loss = 0.08696880\n",
      "Iteration 188, loss = 0.08683597\n",
      "Iteration 189, loss = 0.08670412\n",
      "Iteration 190, loss = 0.08657332\n",
      "Iteration 191, loss = 0.08644373\n",
      "Iteration 192, loss = 0.08631580\n",
      "Iteration 193, loss = 0.08619060\n",
      "Iteration 194, loss = 0.08607083\n",
      "Iteration 195, loss = 0.08596370\n",
      "Iteration 196, loss = 0.08588707\n",
      "Iteration 197, loss = 0.08589532\n",
      "Iteration 198, loss = 0.08610139\n",
      "Iteration 199, loss = 0.08696304\n",
      "Iteration 200, loss = 0.08904321\n",
      "Iteration 201, loss = 0.09625394\n",
      "Iteration 202, loss = 0.10394344\n",
      "Iteration 203, loss = 0.12948167\n",
      "Iteration 204, loss = 0.10196014\n",
      "Iteration 205, loss = 0.08772481\n",
      "Iteration 206, loss = 0.08596064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 7.69527616\n",
      "Iteration 3, loss = 6.63738584\n",
      "Iteration 4, loss = 2.38485357\n",
      "Iteration 5, loss = 2.22182610\n",
      "Iteration 6, loss = 1.69673610\n",
      "Iteration 7, loss = 0.84644767\n",
      "Iteration 8, loss = 0.76424588\n",
      "Iteration 9, loss = 0.87397031\n",
      "Iteration 10, loss = 0.82540106\n",
      "Iteration 11, loss = 0.76061042\n",
      "Iteration 12, loss = 0.71239507\n",
      "Iteration 13, loss = 0.65390601\n",
      "Iteration 14, loss = 0.59972076\n",
      "Iteration 15, loss = 0.54786885\n",
      "Iteration 16, loss = 0.50515938\n",
      "Iteration 17, loss = 0.46655106\n",
      "Iteration 18, loss = 0.41957583\n",
      "Iteration 19, loss = 0.39396694\n",
      "Iteration 20, loss = 0.38274169\n",
      "Iteration 21, loss = 0.36962837\n",
      "Iteration 22, loss = 0.35191176\n",
      "Iteration 23, loss = 0.32454473\n",
      "Iteration 24, loss = 0.30741753\n",
      "Iteration 25, loss = 0.29374272\n",
      "Iteration 26, loss = 0.27656518\n",
      "Iteration 27, loss = 0.26117464\n",
      "Iteration 28, loss = 0.24767418\n",
      "Iteration 29, loss = 0.23246572\n",
      "Iteration 30, loss = 0.22166778\n",
      "Iteration 31, loss = 0.20940399\n",
      "Iteration 32, loss = 0.19899503\n",
      "Iteration 33, loss = 0.18794877\n",
      "Iteration 34, loss = 0.17857071\n",
      "Iteration 35, loss = 0.17518395\n",
      "Iteration 36, loss = 0.16652180\n",
      "Iteration 37, loss = 0.15681693\n",
      "Iteration 38, loss = 0.15369595\n",
      "Iteration 39, loss = 0.14641432\n",
      "Iteration 40, loss = 0.14414217\n",
      "Iteration 41, loss = 0.13861665\n",
      "Iteration 42, loss = 0.13697781\n",
      "Iteration 43, loss = 0.13262766\n",
      "Iteration 44, loss = 0.13133226\n",
      "Iteration 45, loss = 0.12836507\n",
      "Iteration 46, loss = 0.12688311\n",
      "Iteration 47, loss = 0.12455679\n",
      "Iteration 48, loss = 0.12346702\n",
      "Iteration 49, loss = 0.12188418\n",
      "Iteration 50, loss = 0.12074079\n",
      "Iteration 51, loss = 0.11975117\n",
      "Iteration 52, loss = 0.11836260\n",
      "Iteration 53, loss = 0.11780659\n",
      "Iteration 54, loss = 0.11639954\n",
      "Iteration 55, loss = 0.11580692\n",
      "Iteration 56, loss = 0.11492984\n",
      "Iteration 57, loss = 0.11391388\n",
      "Iteration 58, loss = 0.11348850\n",
      "Iteration 59, loss = 0.11261211\n",
      "Iteration 60, loss = 0.11182777\n",
      "Iteration 61, loss = 0.11144112\n",
      "Iteration 62, loss = 0.11079719\n",
      "Iteration 63, loss = 0.10998843\n",
      "Iteration 64, loss = 0.10943221\n",
      "Iteration 65, loss = 0.10909688\n",
      "Iteration 66, loss = 0.10881676\n",
      "Iteration 67, loss = 0.10859086\n",
      "Iteration 68, loss = 0.10866667\n",
      "Iteration 69, loss = 0.10849670\n",
      "Iteration 70, loss = 0.10810146\n",
      "Iteration 71, loss = 0.10669940\n",
      "Iteration 72, loss = 0.10553410\n",
      "Iteration 73, loss = 0.10503849\n",
      "Iteration 74, loss = 0.10513515\n",
      "Iteration 75, loss = 0.10584314\n",
      "Iteration 76, loss = 0.10630275\n",
      "Iteration 77, loss = 0.10674281\n",
      "Iteration 78, loss = 0.10462117\n",
      "Iteration 79, loss = 0.10283676\n",
      "Iteration 80, loss = 0.10238082\n",
      "Iteration 81, loss = 0.10311154\n",
      "Iteration 82, loss = 0.10416973\n",
      "Iteration 83, loss = 0.10344260\n",
      "Iteration 84, loss = 0.10220952\n",
      "Iteration 85, loss = 0.10069102\n",
      "Iteration 86, loss = 0.10029828\n",
      "Iteration 87, loss = 0.10085338\n",
      "Iteration 88, loss = 0.10146493\n",
      "Iteration 89, loss = 0.10226481\n",
      "Iteration 90, loss = 0.10124011\n",
      "Iteration 91, loss = 0.10012511\n",
      "Iteration 92, loss = 0.09866731\n",
      "Iteration 93, loss = 0.09805131\n",
      "Iteration 94, loss = 0.09818758\n",
      "Iteration 95, loss = 0.09872401\n",
      "Iteration 96, loss = 0.09983441\n",
      "Iteration 97, loss = 0.10011337\n",
      "Iteration 98, loss = 0.10076848\n",
      "Iteration 99, loss = 0.09891839\n",
      "Iteration 100, loss = 0.09733915\n",
      "Iteration 101, loss = 0.09604445\n",
      "Iteration 102, loss = 0.09585701\n",
      "Iteration 103, loss = 0.09650760\n",
      "Iteration 104, loss = 0.09724112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 105, loss = 0.09825496\n",
      "Iteration 106, loss = 0.09753737\n",
      "Iteration 107, loss = 0.09685847\n",
      "Iteration 108, loss = 0.09522936\n",
      "Iteration 109, loss = 0.09422955\n",
      "Iteration 110, loss = 0.09392609\n",
      "Iteration 111, loss = 0.09420888\n",
      "Iteration 112, loss = 0.09489124\n",
      "Iteration 113, loss = 0.09537804\n",
      "Iteration 114, loss = 0.09632030\n",
      "Iteration 115, loss = 0.09586043\n",
      "Iteration 116, loss = 0.09576614\n",
      "Iteration 117, loss = 0.09420793\n",
      "Iteration 118, loss = 0.09307726\n",
      "Iteration 119, loss = 0.09218223\n",
      "Iteration 120, loss = 0.09190765\n",
      "Iteration 121, loss = 0.09212828\n",
      "Iteration 122, loss = 0.09257726\n",
      "Iteration 123, loss = 0.09339596\n",
      "Iteration 124, loss = 0.09378249\n",
      "Iteration 125, loss = 0.09484269\n",
      "Iteration 126, loss = 0.09423454\n",
      "Iteration 127, loss = 0.09414007\n",
      "Iteration 128, loss = 0.09240234\n",
      "Iteration 129, loss = 0.09116203\n",
      "Iteration 130, loss = 0.09023405\n",
      "Iteration 131, loss = 0.09001382\n",
      "Iteration 132, loss = 0.09034066\n",
      "Iteration 133, loss = 0.09085503\n",
      "Iteration 134, loss = 0.09164276\n",
      "Iteration 135, loss = 0.09176434\n",
      "Iteration 136, loss = 0.09224065\n",
      "Iteration 137, loss = 0.09141585\n",
      "Iteration 138, loss = 0.09091252\n",
      "Iteration 139, loss = 0.08974618\n",
      "Iteration 140, loss = 0.08895558\n",
      "Iteration 141, loss = 0.08844810\n",
      "Iteration 142, loss = 0.08829560\n",
      "Iteration 143, loss = 0.08841199\n",
      "Iteration 144, loss = 0.08867432\n",
      "Iteration 145, loss = 0.08917869\n",
      "Iteration 146, loss = 0.08953548\n",
      "Iteration 147, loss = 0.09049434\n",
      "Iteration 148, loss = 0.09065530\n",
      "Iteration 149, loss = 0.09174478\n",
      "Iteration 150, loss = 0.09094011\n",
      "Iteration 151, loss = 0.09088923\n",
      "Iteration 152, loss = 0.08909460\n",
      "Iteration 153, loss = 0.08789364\n",
      "Iteration 154, loss = 0.08684257\n",
      "Iteration 155, loss = 0.08647923\n",
      "Iteration 156, loss = 0.08668313\n",
      "Iteration 157, loss = 0.08714673\n",
      "Iteration 158, loss = 0.08780707\n",
      "Iteration 159, loss = 0.08789764\n",
      "Iteration 160, loss = 0.08813169\n",
      "Iteration 161, loss = 0.08745947\n",
      "Iteration 162, loss = 0.08698282\n",
      "Iteration 163, loss = 0.08619046\n",
      "Iteration 164, loss = 0.08564587\n",
      "Iteration 165, loss = 0.08527512\n",
      "Iteration 166, loss = 0.08511576\n",
      "Iteration 167, loss = 0.08511684\n",
      "Iteration 168, loss = 0.08521795\n",
      "Iteration 169, loss = 0.08544115\n",
      "Iteration 170, loss = 0.08566351\n",
      "Iteration 171, loss = 0.08619097\n",
      "Iteration 172, loss = 0.08653306\n",
      "Iteration 173, loss = 0.08762056\n",
      "Iteration 174, loss = 0.08788832\n",
      "Iteration 175, loss = 0.08944695\n",
      "Iteration 176, loss = 0.08872354\n",
      "Iteration 177, loss = 0.08916776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 7.32435576\n",
      "Iteration 3, loss = 6.69208868\n",
      "Iteration 4, loss = 2.50091930\n",
      "Iteration 5, loss = 2.07430790\n",
      "Iteration 6, loss = 1.54701161\n",
      "Iteration 7, loss = 0.74416627\n",
      "Iteration 8, loss = 0.74502472\n",
      "Iteration 9, loss = 0.82895549\n",
      "Iteration 10, loss = 0.76290585\n",
      "Iteration 11, loss = 0.69034417\n",
      "Iteration 12, loss = 0.63090005\n",
      "Iteration 13, loss = 0.57886358\n",
      "Iteration 14, loss = 0.54758168\n",
      "Iteration 15, loss = 0.51211913\n",
      "Iteration 16, loss = 0.45870759\n",
      "Iteration 17, loss = 0.41493497\n",
      "Iteration 18, loss = 0.40670565\n",
      "Iteration 19, loss = 0.40397949\n",
      "Iteration 20, loss = 0.38518178\n",
      "Iteration 21, loss = 0.35717318\n",
      "Iteration 22, loss = 0.34523452\n",
      "Iteration 23, loss = 0.34614400\n",
      "Iteration 24, loss = 0.32600111\n",
      "Iteration 25, loss = 0.29380276\n",
      "Iteration 26, loss = 0.28621888\n",
      "Iteration 27, loss = 0.27985280\n",
      "Iteration 28, loss = 0.25172942\n",
      "Iteration 29, loss = 0.23894074\n",
      "Iteration 30, loss = 0.23711898\n",
      "Iteration 31, loss = 0.21289427\n",
      "Iteration 32, loss = 0.20539235\n",
      "Iteration 33, loss = 0.20121321\n",
      "Iteration 34, loss = 0.18201839\n",
      "Iteration 35, loss = 0.17971893\n",
      "Iteration 36, loss = 0.17213758\n",
      "Iteration 37, loss = 0.15919142\n",
      "Iteration 38, loss = 0.15301495\n",
      "Iteration 39, loss = 0.14066838\n",
      "Iteration 40, loss = 0.13500738\n",
      "Iteration 41, loss = 0.13285089\n",
      "Iteration 42, loss = 0.12766962\n",
      "Iteration 43, loss = 0.12495405\n",
      "Iteration 44, loss = 0.12046300\n",
      "Iteration 45, loss = 0.11915477\n",
      "Iteration 46, loss = 0.11541843\n",
      "Iteration 47, loss = 0.11407132\n",
      "Iteration 48, loss = 0.11059528\n",
      "Iteration 49, loss = 0.10997638\n",
      "Iteration 50, loss = 0.10706793\n",
      "Iteration 51, loss = 0.10649512\n",
      "Iteration 52, loss = 0.10372874\n",
      "Iteration 53, loss = 0.10357020\n",
      "Iteration 54, loss = 0.10122159\n",
      "Iteration 55, loss = 0.10113858\n",
      "Iteration 56, loss = 0.09886988\n",
      "Iteration 57, loss = 0.09894553\n",
      "Iteration 58, loss = 0.09698360\n",
      "Iteration 59, loss = 0.09700939\n",
      "Iteration 60, loss = 0.09555460\n",
      "Iteration 61, loss = 0.09447623\n",
      "Iteration 62, loss = 0.09352986\n",
      "Iteration 63, loss = 0.09302603\n",
      "Iteration 64, loss = 0.09214830\n",
      "Iteration 65, loss = 0.09115363\n",
      "Iteration 66, loss = 0.09059162\n",
      "Iteration 67, loss = 0.09111487\n",
      "Iteration 68, loss = 0.08998397\n",
      "Iteration 69, loss = 0.08980316\n",
      "Iteration 70, loss = 0.08900175\n",
      "Iteration 71, loss = 0.08858524\n",
      "Iteration 72, loss = 0.08812968\n",
      "Iteration 73, loss = 0.08748460\n",
      "Iteration 74, loss = 0.08724837\n",
      "Iteration 75, loss = 0.08649429\n",
      "Iteration 76, loss = 0.08631245\n",
      "Iteration 77, loss = 0.08565224\n",
      "Iteration 78, loss = 0.08537075\n",
      "Iteration 79, loss = 0.08491517\n",
      "Iteration 80, loss = 0.08446383\n",
      "Iteration 81, loss = 0.08419077\n",
      "Iteration 82, loss = 0.08366030\n",
      "Iteration 83, loss = 0.08342463\n",
      "Iteration 84, loss = 0.08298321\n",
      "Iteration 85, loss = 0.08265760\n",
      "Iteration 86, loss = 0.08236772\n",
      "Iteration 87, loss = 0.08196262\n",
      "Iteration 88, loss = 0.08173455\n",
      "Iteration 89, loss = 0.08137439\n",
      "Iteration 90, loss = 0.08108365\n",
      "Iteration 91, loss = 0.08083416\n",
      "Iteration 92, loss = 0.08049692\n",
      "Iteration 93, loss = 0.08026885\n",
      "Iteration 94, loss = 0.07999127\n",
      "Iteration 95, loss = 0.07970936\n",
      "Iteration 96, loss = 0.07949253\n",
      "Iteration 97, loss = 0.07921487\n",
      "Iteration 98, loss = 0.07897364\n",
      "Iteration 99, loss = 0.07875264\n",
      "Iteration 100, loss = 0.07848988\n",
      "Iteration 101, loss = 0.07826909\n",
      "Iteration 102, loss = 0.07804631\n",
      "Iteration 103, loss = 0.07779951\n",
      "Iteration 104, loss = 0.07758756\n",
      "Iteration 105, loss = 0.07736808\n",
      "Iteration 106, loss = 0.07713387\n",
      "Iteration 107, loss = 0.07692651\n",
      "Iteration 108, loss = 0.07671371\n",
      "Iteration 109, loss = 0.07648945\n",
      "Iteration 110, loss = 0.07628501\n",
      "Iteration 111, loss = 0.07607962\n",
      "Iteration 112, loss = 0.07586386\n",
      "Iteration 113, loss = 0.07566148\n",
      "Iteration 114, loss = 0.07546344\n",
      "Iteration 115, loss = 0.07525617\n",
      "Iteration 116, loss = 0.07505532\n",
      "Iteration 117, loss = 0.07486254\n",
      "Iteration 118, loss = 0.07466453\n",
      "Iteration 119, loss = 0.07446657\n",
      "Iteration 120, loss = 0.07427672\n",
      "Iteration 121, loss = 0.07408764\n",
      "Iteration 122, loss = 0.07389572\n",
      "Iteration 123, loss = 0.07370780\n",
      "Iteration 124, loss = 0.07352493\n",
      "Iteration 125, loss = 0.07334155\n",
      "Iteration 126, loss = 0.07315765\n",
      "Iteration 127, loss = 0.07297786\n",
      "Iteration 128, loss = 0.07280144\n",
      "Iteration 129, loss = 0.07262493\n",
      "Iteration 130, loss = 0.07244885\n",
      "Iteration 131, loss = 0.07227583\n",
      "Iteration 132, loss = 0.07210559\n",
      "Iteration 133, loss = 0.07193604\n",
      "Iteration 134, loss = 0.07176689\n",
      "Iteration 135, loss = 0.07159975\n",
      "Iteration 136, loss = 0.07143520\n",
      "Iteration 137, loss = 0.07127234\n",
      "Iteration 138, loss = 0.07111032\n",
      "Iteration 139, loss = 0.07094947\n",
      "Iteration 140, loss = 0.07079062\n",
      "Iteration 141, loss = 0.07063377\n",
      "Iteration 142, loss = 0.07047845\n",
      "Iteration 143, loss = 0.07032428\n",
      "Iteration 144, loss = 0.07017123\n",
      "Iteration 145, loss = 0.07001957\n",
      "Iteration 146, loss = 0.06986980\n",
      "Iteration 147, loss = 0.06972147\n",
      "Iteration 148, loss = 0.06957436\n",
      "Iteration 149, loss = 0.06942833\n",
      "Iteration 150, loss = 0.06928335\n",
      "Iteration 151, loss = 0.06913941\n",
      "Iteration 152, loss = 0.06899691\n",
      "Iteration 153, loss = 0.06885578\n",
      "Iteration 154, loss = 0.06871606\n",
      "Iteration 155, loss = 0.06857760\n",
      "Iteration 156, loss = 0.06844036\n",
      "Iteration 157, loss = 0.06830432\n",
      "Iteration 158, loss = 0.06816953\n",
      "Iteration 159, loss = 0.06803630\n",
      "Iteration 160, loss = 0.06790414\n",
      "Iteration 161, loss = 0.06777312\n",
      "Iteration 162, loss = 0.06764332\n",
      "Iteration 163, loss = 0.06751504\n",
      "Iteration 164, loss = 0.06738884\n",
      "Iteration 165, loss = 0.06726624\n",
      "Iteration 166, loss = 0.06715032\n",
      "Iteration 167, loss = 0.06705017\n",
      "Iteration 168, loss = 0.06698355\n",
      "Iteration 169, loss = 0.06701047\n",
      "Iteration 170, loss = 0.06723590\n",
      "Iteration 171, loss = 0.06809379\n",
      "Iteration 172, loss = 0.06980173\n",
      "Iteration 173, loss = 0.07426671\n",
      "Iteration 174, loss = 0.07501960\n",
      "Iteration 175, loss = 0.07306064\n",
      "Iteration 176, loss = 0.06602078\n",
      "Iteration 177, loss = 0.07074673\n",
      "Iteration 178, loss = 0.07404760\n",
      "Iteration 179, loss = 0.06583408\n",
      "Iteration 180, loss = 0.07102998\n",
      "Iteration 181, loss = 0.07268468\n",
      "Iteration 182, loss = 0.06548055\n",
      "Iteration 183, loss = 0.07297410\n",
      "Iteration 184, loss = 0.06960350\n",
      "Iteration 185, loss = 0.06708722\n",
      "Iteration 186, loss = 0.07167462\n",
      "Iteration 187, loss = 0.06521598\n",
      "Iteration 188, loss = 0.06875483\n",
      "Iteration 189, loss = 0.06607078\n",
      "Iteration 190, loss = 0.06622810\n",
      "Iteration 191, loss = 0.06711321\n",
      "Iteration 192, loss = 0.06486368\n",
      "Iteration 193, loss = 0.06723163\n",
      "Iteration 194, loss = 0.06427088\n",
      "Iteration 195, loss = 0.06615060\n",
      "Iteration 196, loss = 0.06434123\n",
      "Iteration 197, loss = 0.06520453\n",
      "Iteration 198, loss = 0.06471690\n",
      "Iteration 199, loss = 0.06433973\n",
      "Iteration 200, loss = 0.06477617\n",
      "Iteration 201, loss = 0.06366570\n",
      "Iteration 202, loss = 0.06452238\n",
      "Iteration 203, loss = 0.06340645\n",
      "Iteration 204, loss = 0.06418666\n",
      "Iteration 205, loss = 0.06335322\n",
      "Iteration 206, loss = 0.06366431\n",
      "Iteration 207, loss = 0.06332103\n",
      "Iteration 208, loss = 0.06318731\n",
      "Iteration 209, loss = 0.06327418\n",
      "Iteration 210, loss = 0.06284396\n",
      "Iteration 211, loss = 0.06312970\n",
      "Iteration 212, loss = 0.06261241\n",
      "Iteration 213, loss = 0.06285299\n",
      "Iteration 214, loss = 0.06248507\n",
      "Iteration 215, loss = 0.06254869\n",
      "Iteration 216, loss = 0.06240395\n",
      "Iteration 217, loss = 0.06225540\n",
      "Iteration 218, loss = 0.06228701\n",
      "Iteration 219, loss = 0.06201958\n",
      "Iteration 220, loss = 0.06210245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 221, loss = 0.06186006\n",
      "Iteration 222, loss = 0.06187898\n",
      "Iteration 223, loss = 0.06174246\n",
      "Iteration 224, loss = 0.06164596\n",
      "Iteration 225, loss = 0.06161483\n",
      "Iteration 226, loss = 0.06144614\n",
      "Iteration 227, loss = 0.06144841\n",
      "Iteration 228, loss = 0.06129326\n",
      "Iteration 229, loss = 0.06125552\n",
      "Iteration 230, loss = 0.06116395\n",
      "Iteration 231, loss = 0.06106295\n",
      "Iteration 232, loss = 0.06102387\n",
      "Iteration 233, loss = 0.06089709\n",
      "Iteration 234, loss = 0.06085787\n",
      "Iteration 235, loss = 0.06075740\n",
      "Iteration 236, loss = 0.06068255\n",
      "Iteration 237, loss = 0.06062130\n",
      "Iteration 238, loss = 0.06052035\n",
      "Iteration 239, loss = 0.06047091\n",
      "Iteration 240, loss = 0.06037827\n",
      "Iteration 241, loss = 0.06030949\n",
      "Iteration 242, loss = 0.06024253\n",
      "Iteration 243, loss = 0.06015506\n",
      "Iteration 244, loss = 0.06009790\n",
      "Iteration 245, loss = 0.06001512\n",
      "Iteration 246, loss = 0.05994650\n",
      "Iteration 247, loss = 0.05988021\n",
      "Iteration 248, loss = 0.05980052\n",
      "Iteration 249, loss = 0.05973958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 7.44561173\n",
      "Iteration 3, loss = 6.60302349\n",
      "Iteration 4, loss = 2.42171784\n",
      "Iteration 5, loss = 2.15833823\n",
      "Iteration 6, loss = 1.58817094\n",
      "Iteration 7, loss = 0.78183370\n",
      "Iteration 8, loss = 0.74613860\n",
      "Iteration 9, loss = 0.82670259\n",
      "Iteration 10, loss = 0.77831940\n",
      "Iteration 11, loss = 0.72501511\n",
      "Iteration 12, loss = 0.68025195\n",
      "Iteration 13, loss = 0.62077108\n",
      "Iteration 14, loss = 0.56885095\n",
      "Iteration 15, loss = 0.52534397\n",
      "Iteration 16, loss = 0.48718941\n",
      "Iteration 17, loss = 0.45650453\n",
      "Iteration 18, loss = 0.42851970\n",
      "Iteration 19, loss = 0.40594183\n",
      "Iteration 20, loss = 0.38457345\n",
      "Iteration 21, loss = 0.36222759\n",
      "Iteration 22, loss = 0.35398198\n",
      "Iteration 23, loss = 0.34643384\n",
      "Iteration 24, loss = 0.31986749\n",
      "Iteration 25, loss = 0.29605096\n",
      "Iteration 26, loss = 0.28979426\n",
      "Iteration 27, loss = 0.27281698\n",
      "Iteration 28, loss = 0.24949214\n",
      "Iteration 29, loss = 0.24477881\n",
      "Iteration 30, loss = 0.23005966\n",
      "Iteration 31, loss = 0.21246217\n",
      "Iteration 32, loss = 0.20925363\n",
      "Iteration 33, loss = 0.19339232\n",
      "Iteration 34, loss = 0.18365549\n",
      "Iteration 35, loss = 0.17890184\n",
      "Iteration 36, loss = 0.16449764\n",
      "Iteration 37, loss = 0.16551704\n",
      "Iteration 38, loss = 0.15067087\n",
      "Iteration 39, loss = 0.14712583\n",
      "Iteration 40, loss = 0.14131846\n",
      "Iteration 41, loss = 0.13472149\n",
      "Iteration 42, loss = 0.13284642\n",
      "Iteration 43, loss = 0.12666326\n",
      "Iteration 44, loss = 0.12417445\n",
      "Iteration 45, loss = 0.12086453\n",
      "Iteration 46, loss = 0.11461758\n",
      "Iteration 47, loss = 0.12417812\n",
      "Iteration 48, loss = 0.10811363\n",
      "Iteration 49, loss = 0.11823947\n",
      "Iteration 50, loss = 0.10470867\n",
      "Iteration 51, loss = 0.11192348\n",
      "Iteration 52, loss = 0.10153905\n",
      "Iteration 53, loss = 0.10806830\n",
      "Iteration 54, loss = 0.09874019\n",
      "Iteration 55, loss = 0.10313502\n",
      "Iteration 56, loss = 0.09711164\n",
      "Iteration 57, loss = 0.09939423\n",
      "Iteration 58, loss = 0.09490615\n",
      "Iteration 59, loss = 0.09601952\n",
      "Iteration 60, loss = 0.09370092\n",
      "Iteration 61, loss = 0.09311818\n",
      "Iteration 62, loss = 0.09190417\n",
      "Iteration 63, loss = 0.09103265\n",
      "Iteration 64, loss = 0.09045596\n",
      "Iteration 65, loss = 0.08894791\n",
      "Iteration 66, loss = 0.08875266\n",
      "Iteration 67, loss = 0.08742329\n",
      "Iteration 68, loss = 0.08717502\n",
      "Iteration 69, loss = 0.08584117\n",
      "Iteration 70, loss = 0.08556954\n",
      "Iteration 71, loss = 0.08462047\n",
      "Iteration 72, loss = 0.08410679\n",
      "Iteration 73, loss = 0.08340282\n",
      "Iteration 74, loss = 0.08263754\n",
      "Iteration 75, loss = 0.08236136\n",
      "Iteration 76, loss = 0.08134389\n",
      "Iteration 77, loss = 0.08133536\n",
      "Iteration 78, loss = 0.08010093\n",
      "Iteration 79, loss = 0.08020250\n",
      "Iteration 80, loss = 0.07908380\n",
      "Iteration 81, loss = 0.07901254\n",
      "Iteration 82, loss = 0.07829950\n",
      "Iteration 83, loss = 0.07774167\n",
      "Iteration 84, loss = 0.07753972\n",
      "Iteration 85, loss = 0.07666407\n",
      "Iteration 86, loss = 0.07651832\n",
      "Iteration 87, loss = 0.07594501\n",
      "Iteration 88, loss = 0.07538174\n",
      "Iteration 89, loss = 0.07520377\n",
      "Iteration 90, loss = 0.07457189\n",
      "Iteration 91, loss = 0.07417130\n",
      "Iteration 92, loss = 0.07393012\n",
      "Iteration 93, loss = 0.07335508\n",
      "Iteration 94, loss = 0.07296473\n",
      "Iteration 95, loss = 0.07272505\n",
      "Iteration 96, loss = 0.07224829\n",
      "Iteration 97, loss = 0.07180135\n",
      "Iteration 98, loss = 0.07153817\n",
      "Iteration 99, loss = 0.07121560\n",
      "Iteration 100, loss = 0.07078116\n",
      "Iteration 101, loss = 0.07039052\n",
      "Iteration 102, loss = 0.07010926\n",
      "Iteration 103, loss = 0.06983624\n",
      "Iteration 104, loss = 0.06948311\n",
      "Iteration 105, loss = 0.06909982\n",
      "Iteration 106, loss = 0.06874124\n",
      "Iteration 107, loss = 0.06843386\n",
      "Iteration 108, loss = 0.06816397\n",
      "Iteration 109, loss = 0.06790878\n",
      "Iteration 110, loss = 0.06767315\n",
      "Iteration 111, loss = 0.06744803\n",
      "Iteration 112, loss = 0.06730121\n",
      "Iteration 113, loss = 0.06722196\n",
      "Iteration 114, loss = 0.06745618\n",
      "Iteration 115, loss = 0.06783631\n",
      "Iteration 116, loss = 0.06908942\n",
      "Iteration 117, loss = 0.06906127\n",
      "Iteration 118, loss = 0.06902555\n",
      "Iteration 119, loss = 0.06628364\n",
      "Iteration 120, loss = 0.06483252\n",
      "Iteration 121, loss = 0.06526907\n",
      "Iteration 122, loss = 0.06620237\n",
      "Iteration 123, loss = 0.06657109\n",
      "Iteration 124, loss = 0.06481212\n",
      "Iteration 125, loss = 0.06370737\n",
      "Iteration 126, loss = 0.06394684\n",
      "Iteration 127, loss = 0.06452157\n",
      "Iteration 128, loss = 0.06457804\n",
      "Iteration 129, loss = 0.06339487\n",
      "Iteration 130, loss = 0.06265368\n",
      "Iteration 131, loss = 0.06276700\n",
      "Iteration 132, loss = 0.06309084\n",
      "Iteration 133, loss = 0.06308630\n",
      "Iteration 134, loss = 0.06231343\n",
      "Iteration 135, loss = 0.06170509\n",
      "Iteration 136, loss = 0.06157739\n",
      "Iteration 137, loss = 0.06173672\n",
      "Iteration 138, loss = 0.06183032\n",
      "Iteration 139, loss = 0.06144753\n",
      "Iteration 140, loss = 0.06096947\n",
      "Iteration 141, loss = 0.06058101\n",
      "Iteration 142, loss = 0.06045007\n",
      "Iteration 143, loss = 0.06048283\n",
      "Iteration 144, loss = 0.06045824\n",
      "Iteration 145, loss = 0.06034938\n",
      "Iteration 146, loss = 0.06003247\n",
      "Iteration 147, loss = 0.05970771\n",
      "Iteration 148, loss = 0.05940935\n",
      "Iteration 149, loss = 0.05920590\n",
      "Iteration 150, loss = 0.05908676\n",
      "Iteration 151, loss = 0.05901493\n",
      "Iteration 152, loss = 0.05897122\n",
      "Iteration 153, loss = 0.05889653\n",
      "Iteration 154, loss = 0.05884945\n",
      "Iteration 155, loss = 0.05872624\n",
      "Iteration 156, loss = 0.05865974\n",
      "Iteration 157, loss = 0.05849621\n",
      "Iteration 158, loss = 0.05841226\n",
      "Iteration 159, loss = 0.05822281\n",
      "Iteration 160, loss = 0.05812219\n",
      "Iteration 161, loss = 0.05791705\n",
      "Iteration 162, loss = 0.05779978\n",
      "Iteration 163, loss = 0.05758843\n",
      "Iteration 164, loss = 0.05745827\n",
      "Iteration 165, loss = 0.05725094\n",
      "Iteration 166, loss = 0.05711686\n",
      "Iteration 167, loss = 0.05692372\n",
      "Iteration 168, loss = 0.05679678\n",
      "Iteration 169, loss = 0.05662442\n",
      "Iteration 170, loss = 0.05651586\n",
      "Iteration 171, loss = 0.05636784\n",
      "Iteration 172, loss = 0.05628984\n",
      "Iteration 173, loss = 0.05616983\n",
      "Iteration 174, loss = 0.05613927\n",
      "Iteration 175, loss = 0.05604881\n",
      "Iteration 176, loss = 0.05608372\n",
      "Iteration 177, loss = 0.05600683\n",
      "Iteration 178, loss = 0.05609751\n",
      "Iteration 179, loss = 0.05596705\n",
      "Iteration 180, loss = 0.05601364\n",
      "Iteration 181, loss = 0.05572215\n",
      "Iteration 182, loss = 0.05556097\n",
      "Iteration 183, loss = 0.05512678\n",
      "Iteration 184, loss = 0.05478992\n",
      "Iteration 185, loss = 0.05442502\n",
      "Iteration 186, loss = 0.05416503\n",
      "Iteration 187, loss = 0.05399578\n",
      "Iteration 188, loss = 0.05390815\n",
      "Iteration 189, loss = 0.05387793\n",
      "Iteration 190, loss = 0.05386872\n",
      "Iteration 191, loss = 0.05389184\n",
      "Iteration 192, loss = 0.05386745\n",
      "Iteration 193, loss = 0.05389169\n",
      "Iteration 194, loss = 0.05380392\n",
      "Iteration 195, loss = 0.05378487\n",
      "Iteration 196, loss = 0.05361329\n",
      "Iteration 197, loss = 0.05351103\n",
      "Iteration 198, loss = 0.05327332\n",
      "Iteration 199, loss = 0.05309273\n",
      "Iteration 200, loss = 0.05284810\n",
      "Iteration 201, loss = 0.05264910\n",
      "Iteration 202, loss = 0.05245182\n",
      "Iteration 203, loss = 0.05228924\n",
      "Iteration 204, loss = 0.05214944\n",
      "Iteration 205, loss = 0.05203189\n",
      "Iteration 206, loss = 0.05193096\n",
      "Iteration 207, loss = 0.05184249\n",
      "Iteration 208, loss = 0.05176471\n",
      "Iteration 209, loss = 0.05169551\n",
      "Iteration 210, loss = 0.05164324\n",
      "Iteration 211, loss = 0.05160167\n",
      "Iteration 212, loss = 0.05160498\n",
      "Iteration 213, loss = 0.05162448\n",
      "Iteration 214, loss = 0.05176599\n",
      "Iteration 215, loss = 0.05190769\n",
      "Iteration 216, loss = 0.05235279\n",
      "Iteration 217, loss = 0.05260646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 7.97739132\n",
      "Iteration 3, loss = 6.20100727\n",
      "Iteration 4, loss = 2.24248670\n",
      "Iteration 5, loss = 2.27998341\n",
      "Iteration 6, loss = 1.57582497\n",
      "Iteration 7, loss = 0.75469039\n",
      "Iteration 8, loss = 0.75675959\n",
      "Iteration 9, loss = 0.82015300\n",
      "Iteration 10, loss = 0.76912637\n",
      "Iteration 11, loss = 0.71305526\n",
      "Iteration 12, loss = 0.66432156\n",
      "Iteration 13, loss = 0.60936543\n",
      "Iteration 14, loss = 0.57590997\n",
      "Iteration 15, loss = 0.54143403\n",
      "Iteration 16, loss = 0.51180415\n",
      "Iteration 17, loss = 0.48152578\n",
      "Iteration 18, loss = 0.46104736\n",
      "Iteration 19, loss = 0.43711812\n",
      "Iteration 20, loss = 0.39285299\n",
      "Iteration 21, loss = 0.37324390\n",
      "Iteration 22, loss = 0.36146369\n",
      "Iteration 23, loss = 0.34898609\n",
      "Iteration 24, loss = 0.33187872\n",
      "Iteration 25, loss = 0.31522134\n",
      "Iteration 26, loss = 0.30284360\n",
      "Iteration 27, loss = 0.28822433\n",
      "Iteration 28, loss = 0.27315708\n",
      "Iteration 29, loss = 0.26271600\n",
      "Iteration 30, loss = 0.24917144\n",
      "Iteration 31, loss = 0.23733552\n",
      "Iteration 32, loss = 0.22815074\n",
      "Iteration 33, loss = 0.21640652\n",
      "Iteration 34, loss = 0.20888151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.19943386\n",
      "Iteration 36, loss = 0.19184790\n",
      "Iteration 37, loss = 0.18519724\n",
      "Iteration 38, loss = 0.17805588\n",
      "Iteration 39, loss = 0.17349336\n",
      "Iteration 40, loss = 0.16747455\n",
      "Iteration 41, loss = 0.16402053\n",
      "Iteration 42, loss = 0.15928056\n",
      "Iteration 43, loss = 0.15630196\n",
      "Iteration 44, loss = 0.15268398\n",
      "Iteration 45, loss = 0.14999027\n",
      "Iteration 46, loss = 0.14732574\n",
      "Iteration 47, loss = 0.14484117\n",
      "Iteration 48, loss = 0.14281746\n",
      "Iteration 49, loss = 0.14056067\n",
      "Iteration 50, loss = 0.13895299\n",
      "Iteration 51, loss = 0.13693949\n",
      "Iteration 52, loss = 0.13564041\n",
      "Iteration 53, loss = 0.13389977\n",
      "Iteration 54, loss = 0.13281552\n",
      "Iteration 55, loss = 0.13131359\n",
      "Iteration 56, loss = 0.13037448\n",
      "Iteration 57, loss = 0.12906799\n",
      "Iteration 58, loss = 0.12822415\n",
      "Iteration 59, loss = 0.12707715\n",
      "Iteration 60, loss = 0.12630787\n",
      "Iteration 61, loss = 0.12528561\n",
      "Iteration 62, loss = 0.12457281\n",
      "Iteration 63, loss = 0.12365303\n",
      "Iteration 64, loss = 0.12298688\n",
      "Iteration 65, loss = 0.12214846\n",
      "Iteration 66, loss = 0.12151913\n",
      "Iteration 67, loss = 0.12075255\n",
      "Iteration 68, loss = 0.12015601\n",
      "Iteration 69, loss = 0.11945284\n",
      "Iteration 70, loss = 0.11888208\n",
      "Iteration 71, loss = 0.11823704\n",
      "Iteration 72, loss = 0.11768776\n",
      "Iteration 73, loss = 0.11709542\n",
      "Iteration 74, loss = 0.11656244\n",
      "Iteration 75, loss = 0.11601785\n",
      "Iteration 76, loss = 0.11549998\n",
      "Iteration 77, loss = 0.11499785\n",
      "Iteration 78, loss = 0.11449456\n",
      "Iteration 79, loss = 0.11402646\n",
      "Iteration 80, loss = 0.11353945\n",
      "Iteration 81, loss = 0.11309748\n",
      "Iteration 82, loss = 0.11263084\n",
      "Iteration 83, loss = 0.11220735\n",
      "Iteration 84, loss = 0.11176411\n",
      "Iteration 85, loss = 0.11135259\n",
      "Iteration 86, loss = 0.11093433\n",
      "Iteration 87, loss = 0.11053131\n",
      "Iteration 88, loss = 0.11013630\n",
      "Iteration 89, loss = 0.10974212\n",
      "Iteration 90, loss = 0.10936612\n",
      "Iteration 91, loss = 0.10898355\n",
      "Iteration 92, loss = 0.10862090\n",
      "Iteration 93, loss = 0.10825319\n",
      "Iteration 94, loss = 0.10789948\n",
      "Iteration 95, loss = 0.10754796\n",
      "Iteration 96, loss = 0.10720168\n",
      "Iteration 97, loss = 0.10686480\n",
      "Iteration 98, loss = 0.10652729\n",
      "Iteration 99, loss = 0.10620276\n",
      "Iteration 100, loss = 0.10587775\n",
      "Iteration 101, loss = 0.10556122\n",
      "Iteration 102, loss = 0.10524833\n",
      "Iteration 103, loss = 0.10493883\n",
      "Iteration 104, loss = 0.10463664\n",
      "Iteration 105, loss = 0.10433529\n",
      "Iteration 106, loss = 0.10404210\n",
      "Iteration 107, loss = 0.10375195\n",
      "Iteration 108, loss = 0.10346613\n",
      "Iteration 109, loss = 0.10318529\n",
      "Iteration 110, loss = 0.10290640\n",
      "Iteration 111, loss = 0.10263311\n",
      "Iteration 112, loss = 0.10236209\n",
      "Iteration 113, loss = 0.10209495\n",
      "Iteration 114, loss = 0.10183171\n",
      "Iteration 115, loss = 0.10157073\n",
      "Iteration 116, loss = 0.10131413\n",
      "Iteration 117, loss = 0.10106011\n",
      "Iteration 118, loss = 0.10080948\n",
      "Iteration 119, loss = 0.10056241\n",
      "Iteration 120, loss = 0.10031770\n",
      "Iteration 121, loss = 0.10007659\n",
      "Iteration 122, loss = 0.09983810\n",
      "Iteration 123, loss = 0.09960225\n",
      "Iteration 124, loss = 0.09936958\n",
      "Iteration 125, loss = 0.09913923\n",
      "Iteration 126, loss = 0.09891173\n",
      "Iteration 127, loss = 0.09868677\n",
      "Iteration 128, loss = 0.09846414\n",
      "Iteration 129, loss = 0.09824431\n",
      "Iteration 130, loss = 0.09802672\n",
      "Iteration 131, loss = 0.09781158\n",
      "Iteration 132, loss = 0.09759891\n",
      "Iteration 133, loss = 0.09738826\n",
      "Iteration 134, loss = 0.09717989\n",
      "Iteration 135, loss = 0.09697368\n",
      "Iteration 136, loss = 0.09676945\n",
      "Iteration 137, loss = 0.09656744\n",
      "Iteration 138, loss = 0.09636743\n",
      "Iteration 139, loss = 0.09616936\n",
      "Iteration 140, loss = 0.09597331\n",
      "Iteration 141, loss = 0.09577932\n",
      "Iteration 142, loss = 0.09558731\n",
      "Iteration 143, loss = 0.09539737\n",
      "Iteration 144, loss = 0.09520920\n",
      "Iteration 145, loss = 0.09502280\n",
      "Iteration 146, loss = 0.09483820\n",
      "Iteration 147, loss = 0.09465528\n",
      "Iteration 148, loss = 0.09447404\n",
      "Iteration 149, loss = 0.09429447\n",
      "Iteration 150, loss = 0.09411649\n",
      "Iteration 151, loss = 0.09394010\n",
      "Iteration 152, loss = 0.09376526\n",
      "Iteration 153, loss = 0.09359193\n",
      "Iteration 154, loss = 0.09342011\n",
      "Iteration 155, loss = 0.09324978\n",
      "Iteration 156, loss = 0.09308087\n",
      "Iteration 157, loss = 0.09291339\n",
      "Iteration 158, loss = 0.09274732\n",
      "Iteration 159, loss = 0.09258261\n",
      "Iteration 160, loss = 0.09241926\n",
      "Iteration 161, loss = 0.09225724\n",
      "Iteration 162, loss = 0.09209652\n",
      "Iteration 163, loss = 0.09193709\n",
      "Iteration 164, loss = 0.09177899\n",
      "Iteration 165, loss = 0.09162213\n",
      "Iteration 166, loss = 0.09146652\n",
      "Iteration 167, loss = 0.09131212\n",
      "Iteration 168, loss = 0.09115893\n",
      "Iteration 169, loss = 0.09100691\n",
      "Iteration 170, loss = 0.09085606\n",
      "Iteration 171, loss = 0.09070636\n",
      "Iteration 172, loss = 0.09055779\n",
      "Iteration 173, loss = 0.09041034\n",
      "Iteration 174, loss = 0.09026399\n",
      "Iteration 175, loss = 0.09011873\n",
      "Iteration 176, loss = 0.08997454\n",
      "Iteration 177, loss = 0.08983142\n",
      "Iteration 178, loss = 0.08968933\n",
      "Iteration 179, loss = 0.08954828\n",
      "Iteration 180, loss = 0.08940824\n",
      "Iteration 181, loss = 0.08926922\n",
      "Iteration 182, loss = 0.08913118\n",
      "Iteration 183, loss = 0.08899414\n",
      "Iteration 184, loss = 0.08885809\n",
      "Iteration 185, loss = 0.08872299\n",
      "Iteration 186, loss = 0.08858888\n",
      "Iteration 187, loss = 0.08845577\n",
      "Iteration 188, loss = 0.08832360\n",
      "Iteration 189, loss = 0.08819235\n",
      "Iteration 190, loss = 0.08806202\n",
      "Iteration 191, loss = 0.08793259\n",
      "Iteration 192, loss = 0.08780404\n",
      "Iteration 193, loss = 0.08767638\n",
      "Iteration 194, loss = 0.08754965\n",
      "Iteration 195, loss = 0.08742382\n",
      "Iteration 196, loss = 0.08729885\n",
      "Iteration 197, loss = 0.08717473\n",
      "Iteration 198, loss = 0.08705145\n",
      "Iteration 199, loss = 0.08692901\n",
      "Iteration 200, loss = 0.08680739\n",
      "Iteration 201, loss = 0.08668659\n",
      "Iteration 202, loss = 0.08656659\n",
      "Iteration 203, loss = 0.08644738\n",
      "Iteration 204, loss = 0.08632897\n",
      "Iteration 205, loss = 0.08621133\n",
      "Iteration 206, loss = 0.08609447\n",
      "Iteration 207, loss = 0.08597839\n",
      "Iteration 208, loss = 0.08586309\n",
      "Iteration 209, loss = 0.08574856\n",
      "Iteration 210, loss = 0.08563483\n",
      "Iteration 211, loss = 0.08552186\n",
      "Iteration 212, loss = 0.08540963\n",
      "Iteration 213, loss = 0.08529813\n",
      "Iteration 214, loss = 0.08518736\n",
      "Iteration 215, loss = 0.08507730\n",
      "Iteration 216, loss = 0.08496796\n",
      "Iteration 217, loss = 0.08485932\n",
      "Iteration 218, loss = 0.08475137\n",
      "Iteration 219, loss = 0.08464412\n",
      "Iteration 220, loss = 0.08453756\n",
      "Iteration 221, loss = 0.08443167\n",
      "Iteration 222, loss = 0.08432646\n",
      "Iteration 223, loss = 0.08422192\n",
      "Iteration 224, loss = 0.08411804\n",
      "Iteration 225, loss = 0.08401482\n",
      "Iteration 226, loss = 0.08391226\n",
      "Iteration 227, loss = 0.08381033\n",
      "Iteration 228, loss = 0.08370905\n",
      "Iteration 229, loss = 0.08360842\n",
      "Iteration 230, loss = 0.08350841\n",
      "Iteration 231, loss = 0.08340903\n",
      "Iteration 232, loss = 0.08331027\n",
      "Iteration 233, loss = 0.08321213\n",
      "Iteration 234, loss = 0.08311462\n",
      "Iteration 235, loss = 0.08301773\n",
      "Iteration 236, loss = 0.08292145\n",
      "Iteration 237, loss = 0.08282577\n",
      "Iteration 238, loss = 0.08273069\n",
      "Iteration 239, loss = 0.08263619\n",
      "Iteration 240, loss = 0.08254229\n",
      "Iteration 241, loss = 0.08244897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 2.76085304\n",
      "Iteration 3, loss = 3.19401424\n",
      "Iteration 4, loss = 3.00778365\n",
      "Iteration 5, loss = 0.89242969\n",
      "Iteration 6, loss = 0.72850401\n",
      "Iteration 7, loss = 0.62577121\n",
      "Iteration 8, loss = 0.54927698\n",
      "Iteration 9, loss = 0.50060616\n",
      "Iteration 10, loss = 0.46037817\n",
      "Iteration 11, loss = 0.43090335\n",
      "Iteration 12, loss = 0.40558278\n",
      "Iteration 13, loss = 0.38188335\n",
      "Iteration 14, loss = 0.35917481\n",
      "Iteration 15, loss = 0.34384353\n",
      "Iteration 16, loss = 0.41910515\n",
      "Iteration 17, loss = 1.24346525\n",
      "Iteration 18, loss = 1.73424111\n",
      "Iteration 19, loss = 0.38703239\n",
      "Iteration 20, loss = 0.50141891\n",
      "Iteration 21, loss = 0.41747281\n",
      "Iteration 22, loss = 0.41020830\n",
      "Iteration 23, loss = 0.40739404\n",
      "Iteration 24, loss = 0.40054895\n",
      "Iteration 25, loss = 0.39243526\n",
      "Iteration 26, loss = 0.38034941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 2.70849357\n",
      "Iteration 3, loss = 3.03597528\n",
      "Iteration 4, loss = 3.38332993\n",
      "Iteration 5, loss = 0.88357796\n",
      "Iteration 6, loss = 0.72684871\n",
      "Iteration 7, loss = 0.62322723\n",
      "Iteration 8, loss = 0.54981391\n",
      "Iteration 9, loss = 0.49732222\n",
      "Iteration 10, loss = 0.45787654\n",
      "Iteration 11, loss = 0.42655841\n",
      "Iteration 12, loss = 0.39931911\n",
      "Iteration 13, loss = 0.37375521\n",
      "Iteration 14, loss = 0.35022768\n",
      "Iteration 15, loss = 0.34472202\n",
      "Iteration 16, loss = 0.58920668\n",
      "Iteration 17, loss = 2.13622445\n",
      "Iteration 18, loss = 0.51835401\n",
      "Iteration 19, loss = 0.65497843\n",
      "Iteration 20, loss = 0.46248870\n",
      "Iteration 21, loss = 0.42065107\n",
      "Iteration 22, loss = 0.38788997\n",
      "Iteration 23, loss = 0.34633008\n",
      "Iteration 24, loss = 0.32018732\n",
      "Iteration 25, loss = 0.30138156\n",
      "Iteration 26, loss = 0.28219480\n",
      "Iteration 27, loss = 0.26349605\n",
      "Iteration 28, loss = 0.24920538\n",
      "Iteration 29, loss = 0.25984173\n",
      "Iteration 30, loss = 0.52574736\n",
      "Iteration 31, loss = 2.26687888\n",
      "Iteration 32, loss = 0.32741197\n",
      "Iteration 33, loss = 0.27288396\n",
      "Iteration 34, loss = 0.29265658\n",
      "Iteration 35, loss = 0.24361152\n",
      "Iteration 36, loss = 0.26516441\n",
      "Iteration 37, loss = 0.32450264\n",
      "Iteration 38, loss = 0.48447150\n",
      "Iteration 39, loss = 0.96214106\n",
      "Iteration 40, loss = 0.44418844\n",
      "Iteration 41, loss = 0.33973327\n",
      "Iteration 42, loss = 0.30231707\n",
      "Iteration 43, loss = 0.28028769\n",
      "Iteration 44, loss = 0.26628438\n",
      "Iteration 45, loss = 0.25412146\n",
      "Iteration 46, loss = 0.24242009\n",
      "Iteration 47, loss = 0.23005287\n",
      "Iteration 48, loss = 0.21750571\n",
      "Iteration 49, loss = 0.20531656\n",
      "Iteration 50, loss = 0.19419386\n",
      "Iteration 51, loss = 0.18419163\n",
      "Iteration 52, loss = 0.17594518\n",
      "Iteration 53, loss = 0.16968450\n",
      "Iteration 54, loss = 0.16940610\n",
      "Iteration 55, loss = 0.19279955\n",
      "Iteration 56, loss = 0.33473794\n",
      "Iteration 57, loss = 1.44055379\n",
      "Iteration 58, loss = 2.56099209\n",
      "Iteration 59, loss = 0.48892473\n",
      "Iteration 60, loss = 0.34098686\n",
      "Iteration 61, loss = 0.31931374\n",
      "Iteration 62, loss = 0.31809047\n",
      "Iteration 63, loss = 0.31416732\n",
      "Iteration 64, loss = 0.30777787\n",
      "Iteration 65, loss = 0.29963183\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 2.69065295\n",
      "Iteration 3, loss = 3.03817580\n",
      "Iteration 4, loss = 3.12169338\n",
      "Iteration 5, loss = 0.88887158\n",
      "Iteration 6, loss = 0.70264947\n",
      "Iteration 7, loss = 0.60028508\n",
      "Iteration 8, loss = 0.53042574\n",
      "Iteration 9, loss = 0.48119820\n",
      "Iteration 10, loss = 0.44162021\n",
      "Iteration 11, loss = 0.41013785\n",
      "Iteration 12, loss = 0.38212856\n",
      "Iteration 13, loss = 0.35616378\n",
      "Iteration 14, loss = 0.33745132\n",
      "Iteration 15, loss = 0.40197384\n",
      "Iteration 16, loss = 1.20001565\n",
      "Iteration 17, loss = 1.46203657\n",
      "Iteration 18, loss = 0.81417745\n",
      "Iteration 19, loss = 0.40605503\n",
      "Iteration 20, loss = 0.44175659\n",
      "Iteration 21, loss = 0.42392569\n",
      "Iteration 22, loss = 0.38425748\n",
      "Iteration 23, loss = 0.36677618\n",
      "Iteration 24, loss = 0.35227700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.33560046\n",
      "Iteration 26, loss = 0.31651348\n",
      "Iteration 27, loss = 0.29566065\n",
      "Iteration 28, loss = 0.27408746\n",
      "Iteration 29, loss = 0.25313645\n",
      "Iteration 30, loss = 0.23350813\n",
      "Iteration 31, loss = 0.21627324\n",
      "Iteration 32, loss = 0.20384906\n",
      "Iteration 33, loss = 0.23086248\n",
      "Iteration 34, loss = 0.74474167\n",
      "Iteration 35, loss = 3.16224669\n",
      "Iteration 36, loss = 0.55160002\n",
      "Iteration 37, loss = 0.34112307\n",
      "Iteration 38, loss = 0.37196535\n",
      "Iteration 39, loss = 0.36280072\n",
      "Iteration 40, loss = 0.35797595\n",
      "Iteration 41, loss = 0.35009751\n",
      "Iteration 42, loss = 0.34372464\n",
      "Iteration 43, loss = 0.33550143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 2.72268738\n",
      "Iteration 3, loss = 3.06907461\n",
      "Iteration 4, loss = 3.31933919\n",
      "Iteration 5, loss = 0.88486873\n",
      "Iteration 6, loss = 0.72252447\n",
      "Iteration 7, loss = 0.61590794\n",
      "Iteration 8, loss = 0.54360482\n",
      "Iteration 9, loss = 0.49359772\n",
      "Iteration 10, loss = 0.45209434\n",
      "Iteration 11, loss = 0.41942604\n",
      "Iteration 12, loss = 0.39042411\n",
      "Iteration 13, loss = 0.36350847\n",
      "Iteration 14, loss = 0.34338805\n",
      "Iteration 15, loss = 0.40380069\n",
      "Iteration 16, loss = 1.19807560\n",
      "Iteration 17, loss = 1.60358494\n",
      "Iteration 18, loss = 0.80325025\n",
      "Iteration 19, loss = 0.41720401\n",
      "Iteration 20, loss = 0.43406634\n",
      "Iteration 21, loss = 0.43089704\n",
      "Iteration 22, loss = 0.39921417\n",
      "Iteration 23, loss = 0.37895133\n",
      "Iteration 24, loss = 0.36482929\n",
      "Iteration 25, loss = 0.34683023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 2.76103851\n",
      "Iteration 3, loss = 3.12852427\n",
      "Iteration 4, loss = 3.35596419\n",
      "Iteration 5, loss = 0.88785387\n",
      "Iteration 6, loss = 0.71096946\n",
      "Iteration 7, loss = 0.60840827\n",
      "Iteration 8, loss = 0.53892371\n",
      "Iteration 9, loss = 0.48862591\n",
      "Iteration 10, loss = 0.45076680\n",
      "Iteration 11, loss = 0.42023281\n",
      "Iteration 12, loss = 0.39315939\n",
      "Iteration 13, loss = 0.36743301\n",
      "Iteration 14, loss = 0.34341966\n",
      "Iteration 15, loss = 0.33436467\n",
      "Iteration 16, loss = 0.55331141\n",
      "Iteration 17, loss = 2.18626637\n",
      "Iteration 18, loss = 0.62600714\n",
      "Iteration 19, loss = 0.81549213\n",
      "Iteration 20, loss = 0.46531167\n",
      "Iteration 21, loss = 0.46263951\n",
      "Iteration 22, loss = 0.44231605\n",
      "Iteration 23, loss = 0.40346441\n",
      "Iteration 24, loss = 0.37863922\n",
      "Iteration 25, loss = 0.36379977\n",
      "Iteration 26, loss = 0.34709040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.05372679\n",
      "Iteration 3, loss = 0.87031905\n",
      "Iteration 4, loss = 0.84480330\n",
      "Iteration 5, loss = 0.82648473\n",
      "Iteration 6, loss = 0.74203285\n",
      "Iteration 7, loss = 0.64689349\n",
      "Iteration 8, loss = 0.58709581\n",
      "Iteration 9, loss = 0.57098602\n",
      "Iteration 10, loss = 0.55534502\n",
      "Iteration 11, loss = 0.52471833\n",
      "Iteration 12, loss = 0.49889731\n",
      "Iteration 13, loss = 0.48128055\n",
      "Iteration 14, loss = 0.45569041\n",
      "Iteration 15, loss = 0.43095494\n",
      "Iteration 16, loss = 0.41964573\n",
      "Iteration 17, loss = 0.41059976\n",
      "Iteration 18, loss = 0.39566578\n",
      "Iteration 19, loss = 0.38263848\n",
      "Iteration 20, loss = 0.37287298\n",
      "Iteration 21, loss = 0.35914589\n",
      "Iteration 22, loss = 0.34494750\n",
      "Iteration 23, loss = 0.33738802\n",
      "Iteration 24, loss = 0.32966440\n",
      "Iteration 25, loss = 0.31779004\n",
      "Iteration 26, loss = 0.30911717\n",
      "Iteration 27, loss = 0.30217460\n",
      "Iteration 28, loss = 0.29186776\n",
      "Iteration 29, loss = 0.28340356\n",
      "Iteration 30, loss = 0.27606975\n",
      "Iteration 31, loss = 0.26604653\n",
      "Iteration 32, loss = 0.25787712\n",
      "Iteration 33, loss = 0.24966312\n",
      "Iteration 34, loss = 0.24027725\n",
      "Iteration 35, loss = 0.23283163\n",
      "Iteration 36, loss = 0.22377305\n",
      "Iteration 37, loss = 0.21602623\n",
      "Iteration 38, loss = 0.20812938\n",
      "Iteration 39, loss = 0.20012756\n",
      "Iteration 40, loss = 0.19311504\n",
      "Iteration 41, loss = 0.18535100\n",
      "Iteration 42, loss = 0.17890596\n",
      "Iteration 43, loss = 0.17165991\n",
      "Iteration 44, loss = 0.16570984\n",
      "Iteration 45, loss = 0.15905901\n",
      "Iteration 46, loss = 0.15356377\n",
      "Iteration 47, loss = 0.14764530\n",
      "Iteration 48, loss = 0.14257607\n",
      "Iteration 49, loss = 0.13759809\n",
      "Iteration 50, loss = 0.13273431\n",
      "Iteration 51, loss = 0.12871722\n",
      "Iteration 52, loss = 0.12444265\n",
      "Iteration 53, loss = 0.12075650\n",
      "Iteration 54, loss = 0.11750275\n",
      "Iteration 55, loss = 0.11407515\n",
      "Iteration 56, loss = 0.11113229\n",
      "Iteration 57, loss = 0.10863706\n",
      "Iteration 58, loss = 0.10604640\n",
      "Iteration 59, loss = 0.10359067\n",
      "Iteration 60, loss = 0.10156664\n",
      "Iteration 61, loss = 0.09978412\n",
      "Iteration 62, loss = 0.09801592\n",
      "Iteration 63, loss = 0.09624097\n",
      "Iteration 64, loss = 0.09469623\n",
      "Iteration 65, loss = 0.09342461\n",
      "Iteration 66, loss = 0.09231195\n",
      "Iteration 67, loss = 0.09126296\n",
      "Iteration 68, loss = 0.09015008\n",
      "Iteration 69, loss = 0.08905308\n",
      "Iteration 70, loss = 0.08808610\n",
      "Iteration 71, loss = 0.08731285\n",
      "Iteration 72, loss = 0.08668447\n",
      "Iteration 73, loss = 0.08612913\n",
      "Iteration 74, loss = 0.08559578\n",
      "Iteration 75, loss = 0.08494563\n",
      "Iteration 76, loss = 0.08425072\n",
      "Iteration 77, loss = 0.08362216\n",
      "Iteration 78, loss = 0.08316906\n",
      "Iteration 79, loss = 0.08285447\n",
      "Iteration 80, loss = 0.08256340\n",
      "Iteration 81, loss = 0.08223583\n",
      "Iteration 82, loss = 0.08177869\n",
      "Iteration 83, loss = 0.08130384\n",
      "Iteration 84, loss = 0.08091398\n",
      "Iteration 85, loss = 0.08065695\n",
      "Iteration 86, loss = 0.08048266\n",
      "Iteration 87, loss = 0.08028282\n",
      "Iteration 88, loss = 0.08004698\n",
      "Iteration 89, loss = 0.07972128\n",
      "Iteration 90, loss = 0.07938852\n",
      "Iteration 91, loss = 0.07910877\n",
      "Iteration 92, loss = 0.07891001\n",
      "Iteration 93, loss = 0.07876827\n",
      "Iteration 94, loss = 0.07863841\n",
      "Iteration 95, loss = 0.07849487\n",
      "Iteration 96, loss = 0.07829639\n",
      "Iteration 97, loss = 0.07807306\n",
      "Iteration 98, loss = 0.07783132\n",
      "Iteration 99, loss = 0.07761535\n",
      "Iteration 100, loss = 0.07743994\n",
      "Iteration 101, loss = 0.07730187\n",
      "Iteration 102, loss = 0.07718966\n",
      "Iteration 103, loss = 0.07708932\n",
      "Iteration 104, loss = 0.07699854\n",
      "Iteration 105, loss = 0.07689382\n",
      "Iteration 106, loss = 0.07678347\n",
      "Iteration 107, loss = 0.07662682\n",
      "Iteration 108, loss = 0.07645605\n",
      "Iteration 109, loss = 0.07626344\n",
      "Iteration 110, loss = 0.07608696\n",
      "Iteration 111, loss = 0.07594007\n",
      "Iteration 112, loss = 0.07582441\n",
      "Iteration 113, loss = 0.07573137\n",
      "Iteration 114, loss = 0.07565534\n",
      "Iteration 115, loss = 0.07561088\n",
      "Iteration 116, loss = 0.07558553\n",
      "Iteration 117, loss = 0.07560707\n",
      "Iteration 118, loss = 0.07559700\n",
      "Iteration 119, loss = 0.07557226\n",
      "Iteration 120, loss = 0.07534494\n",
      "Iteration 121, loss = 0.07504999\n",
      "Iteration 122, loss = 0.07480575\n",
      "Iteration 123, loss = 0.07472448\n",
      "Iteration 124, loss = 0.07476002\n",
      "Iteration 125, loss = 0.07477519\n",
      "Iteration 126, loss = 0.07469822\n",
      "Iteration 127, loss = 0.07449096\n",
      "Iteration 128, loss = 0.07430784\n",
      "Iteration 129, loss = 0.07421178\n",
      "Iteration 130, loss = 0.07419800\n",
      "Iteration 131, loss = 0.07420144\n",
      "Iteration 132, loss = 0.07414763\n",
      "Iteration 133, loss = 0.07403527\n",
      "Iteration 134, loss = 0.07388387\n",
      "Iteration 135, loss = 0.07375839\n",
      "Iteration 136, loss = 0.07369109\n",
      "Iteration 137, loss = 0.07366336\n",
      "Iteration 138, loss = 0.07364175\n",
      "Iteration 139, loss = 0.07359900\n",
      "Iteration 140, loss = 0.07354017\n",
      "Iteration 141, loss = 0.07346341\n",
      "Iteration 142, loss = 0.07338351\n",
      "Iteration 143, loss = 0.07328990\n",
      "Iteration 144, loss = 0.07320309\n",
      "Iteration 145, loss = 0.07311604\n",
      "Iteration 146, loss = 0.07303377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.05869727\n",
      "Iteration 3, loss = 0.87399416\n",
      "Iteration 4, loss = 0.84882175\n",
      "Iteration 5, loss = 0.83083761\n",
      "Iteration 6, loss = 0.74569409\n",
      "Iteration 7, loss = 0.64954930\n",
      "Iteration 8, loss = 0.59058945\n",
      "Iteration 9, loss = 0.57576136\n",
      "Iteration 10, loss = 0.55838285\n",
      "Iteration 11, loss = 0.52583160\n",
      "Iteration 12, loss = 0.49973865\n",
      "Iteration 13, loss = 0.48214355\n",
      "Iteration 14, loss = 0.45463624\n",
      "Iteration 15, loss = 0.42765176\n",
      "Iteration 16, loss = 0.41460327\n",
      "Iteration 17, loss = 0.40423309\n",
      "Iteration 18, loss = 0.38857005\n",
      "Iteration 19, loss = 0.37628401\n",
      "Iteration 20, loss = 0.36945126\n",
      "Iteration 21, loss = 0.35829875\n",
      "Iteration 22, loss = 0.34400048\n",
      "Iteration 23, loss = 0.33522126\n",
      "Iteration 24, loss = 0.32727237\n",
      "Iteration 25, loss = 0.31472195\n",
      "Iteration 26, loss = 0.30455261\n",
      "Iteration 27, loss = 0.29722943\n",
      "Iteration 28, loss = 0.28672077\n",
      "Iteration 29, loss = 0.27722563\n",
      "Iteration 30, loss = 0.26973033\n",
      "Iteration 31, loss = 0.25956563\n",
      "Iteration 32, loss = 0.25074264\n",
      "Iteration 33, loss = 0.24272757\n",
      "Iteration 34, loss = 0.23298377\n",
      "Iteration 35, loss = 0.22531430\n",
      "Iteration 36, loss = 0.21676279\n",
      "Iteration 37, loss = 0.20863310\n",
      "Iteration 38, loss = 0.20146314\n",
      "Iteration 39, loss = 0.19320739\n",
      "Iteration 40, loss = 0.18640157\n",
      "Iteration 41, loss = 0.17860129\n",
      "Iteration 42, loss = 0.17191511\n",
      "Iteration 43, loss = 0.16490486\n",
      "Iteration 44, loss = 0.15858314\n",
      "Iteration 45, loss = 0.15237338\n",
      "Iteration 46, loss = 0.14660722\n",
      "Iteration 47, loss = 0.14105172\n",
      "Iteration 48, loss = 0.13598872\n",
      "Iteration 49, loss = 0.13091108\n",
      "Iteration 50, loss = 0.12653285\n",
      "Iteration 51, loss = 0.12194908\n",
      "Iteration 52, loss = 0.11817643\n",
      "Iteration 53, loss = 0.11426585\n",
      "Iteration 54, loss = 0.11081285\n",
      "Iteration 55, loss = 0.10774653\n",
      "Iteration 56, loss = 0.10455488\n",
      "Iteration 57, loss = 0.10198300\n",
      "Iteration 58, loss = 0.09953323\n",
      "Iteration 59, loss = 0.09706633\n",
      "Iteration 60, loss = 0.09509494\n",
      "Iteration 61, loss = 0.09327270\n",
      "Iteration 62, loss = 0.09139601\n",
      "Iteration 63, loss = 0.08983799\n",
      "Iteration 64, loss = 0.08853737\n",
      "Iteration 65, loss = 0.08718820\n",
      "Iteration 66, loss = 0.08590553\n",
      "Iteration 67, loss = 0.08487073\n",
      "Iteration 68, loss = 0.08397926\n",
      "Iteration 69, loss = 0.08307917\n",
      "Iteration 70, loss = 0.08218217\n",
      "Iteration 71, loss = 0.08142637\n",
      "Iteration 72, loss = 0.08081666\n",
      "Iteration 73, loss = 0.08025902\n",
      "Iteration 74, loss = 0.07969475\n",
      "Iteration 75, loss = 0.07912302\n",
      "Iteration 76, loss = 0.07861366\n",
      "Iteration 77, loss = 0.07819286\n",
      "Iteration 78, loss = 0.07784109\n",
      "Iteration 79, loss = 0.07752633\n",
      "Iteration 80, loss = 0.07721833\n",
      "Iteration 81, loss = 0.07690684\n",
      "Iteration 82, loss = 0.07658307\n",
      "Iteration 83, loss = 0.07627558\n",
      "Iteration 84, loss = 0.07600329\n",
      "Iteration 85, loss = 0.07577345\n",
      "Iteration 86, loss = 0.07557867\n",
      "Iteration 87, loss = 0.07541141\n",
      "Iteration 88, loss = 0.07527423\n",
      "Iteration 89, loss = 0.07515521\n",
      "Iteration 90, loss = 0.07505657\n",
      "Iteration 91, loss = 0.07490178\n",
      "Iteration 92, loss = 0.07469593\n",
      "Iteration 93, loss = 0.07443854\n",
      "Iteration 94, loss = 0.07423720\n",
      "Iteration 95, loss = 0.07413026\n",
      "Iteration 96, loss = 0.07407725\n",
      "Iteration 97, loss = 0.07401694\n",
      "Iteration 98, loss = 0.07388863\n",
      "Iteration 99, loss = 0.07371646\n",
      "Iteration 100, loss = 0.07354507\n",
      "Iteration 101, loss = 0.07342785\n",
      "Iteration 102, loss = 0.07336170\n",
      "Iteration 103, loss = 0.07330893\n",
      "Iteration 104, loss = 0.07323776\n",
      "Iteration 105, loss = 0.07312455\n",
      "Iteration 106, loss = 0.07299318\n",
      "Iteration 107, loss = 0.07286758\n",
      "Iteration 108, loss = 0.07277056\n",
      "Iteration 109, loss = 0.07270098\n",
      "Iteration 110, loss = 0.07264408\n",
      "Iteration 111, loss = 0.07258778\n",
      "Iteration 112, loss = 0.07251661\n",
      "Iteration 113, loss = 0.07243382\n",
      "Iteration 114, loss = 0.07233594\n",
      "Iteration 115, loss = 0.07223681\n",
      "Iteration 116, loss = 0.07214168\n",
      "Iteration 117, loss = 0.07205513\n",
      "Iteration 118, loss = 0.07197592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.05517284\n",
      "Iteration 3, loss = 0.86604109\n",
      "Iteration 4, loss = 0.83525191\n",
      "Iteration 5, loss = 0.81810118\n",
      "Iteration 6, loss = 0.73589478\n",
      "Iteration 7, loss = 0.63971791\n",
      "Iteration 8, loss = 0.57791441\n",
      "Iteration 9, loss = 0.56097129\n",
      "Iteration 10, loss = 0.54542034\n",
      "Iteration 11, loss = 0.51394833\n",
      "Iteration 12, loss = 0.48628164\n",
      "Iteration 13, loss = 0.46873894\n",
      "Iteration 14, loss = 0.44356733\n",
      "Iteration 15, loss = 0.41594276\n",
      "Iteration 16, loss = 0.40074118\n",
      "Iteration 17, loss = 0.39091835\n",
      "Iteration 18, loss = 0.37594826\n",
      "Iteration 19, loss = 0.36181398\n",
      "Iteration 20, loss = 0.35376878\n",
      "Iteration 21, loss = 0.34368021\n",
      "Iteration 22, loss = 0.32923097\n",
      "Iteration 23, loss = 0.31898694\n",
      "Iteration 24, loss = 0.31098925\n",
      "Iteration 25, loss = 0.29881152\n",
      "Iteration 26, loss = 0.28736681\n",
      "Iteration 27, loss = 0.27936071\n",
      "Iteration 28, loss = 0.26907003\n",
      "Iteration 29, loss = 0.25850523\n",
      "Iteration 30, loss = 0.25043508\n",
      "Iteration 31, loss = 0.24038834\n",
      "Iteration 32, loss = 0.23049035\n",
      "Iteration 33, loss = 0.22214264\n",
      "Iteration 34, loss = 0.21208110\n",
      "Iteration 35, loss = 0.20369512\n",
      "Iteration 36, loss = 0.19476671\n",
      "Iteration 37, loss = 0.18594656\n",
      "Iteration 38, loss = 0.17812661\n",
      "Iteration 39, loss = 0.16941123\n",
      "Iteration 40, loss = 0.16200971\n",
      "Iteration 41, loss = 0.15383159\n",
      "Iteration 42, loss = 0.14673224\n",
      "Iteration 43, loss = 0.13933566\n",
      "Iteration 44, loss = 0.13288533\n",
      "Iteration 45, loss = 0.12632500\n",
      "Iteration 46, loss = 0.12061137\n",
      "Iteration 47, loss = 0.11479874\n",
      "Iteration 48, loss = 0.10988850\n",
      "Iteration 49, loss = 0.10477257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.10049777\n",
      "Iteration 51, loss = 0.09623435\n",
      "Iteration 52, loss = 0.09239592\n",
      "Iteration 53, loss = 0.08901409\n",
      "Iteration 54, loss = 0.08562828\n",
      "Iteration 55, loss = 0.08283754\n",
      "Iteration 56, loss = 0.08015591\n",
      "Iteration 57, loss = 0.07764267\n",
      "Iteration 58, loss = 0.07557685\n",
      "Iteration 59, loss = 0.07352829\n",
      "Iteration 60, loss = 0.07168162\n",
      "Iteration 61, loss = 0.07015484\n",
      "Iteration 62, loss = 0.06865043\n",
      "Iteration 63, loss = 0.06727572\n",
      "Iteration 64, loss = 0.06614899\n",
      "Iteration 65, loss = 0.06508382\n",
      "Iteration 66, loss = 0.06406329\n",
      "Iteration 67, loss = 0.06318634\n",
      "Iteration 68, loss = 0.06243747\n",
      "Iteration 69, loss = 0.06172524\n",
      "Iteration 70, loss = 0.06105099\n",
      "Iteration 71, loss = 0.06047272\n",
      "Iteration 72, loss = 0.05997541\n",
      "Iteration 73, loss = 0.05950682\n",
      "Iteration 74, loss = 0.05906036\n",
      "Iteration 75, loss = 0.05865142\n",
      "Iteration 76, loss = 0.05830031\n",
      "Iteration 77, loss = 0.05799619\n",
      "Iteration 78, loss = 0.05771472\n",
      "Iteration 79, loss = 0.05744419\n",
      "Iteration 80, loss = 0.05718869\n",
      "Iteration 81, loss = 0.05695649\n",
      "Iteration 82, loss = 0.05675264\n",
      "Iteration 83, loss = 0.05657390\n",
      "Iteration 84, loss = 0.05641220\n",
      "Iteration 85, loss = 0.05626261\n",
      "Iteration 86, loss = 0.05611714\n",
      "Iteration 87, loss = 0.05597391\n",
      "Iteration 88, loss = 0.05583335\n",
      "Iteration 89, loss = 0.05570125\n",
      "Iteration 90, loss = 0.05558219\n",
      "Iteration 91, loss = 0.05547616\n",
      "Iteration 92, loss = 0.05538057\n",
      "Iteration 93, loss = 0.05529100\n",
      "Iteration 94, loss = 0.05520333\n",
      "Iteration 95, loss = 0.05511565\n",
      "Iteration 96, loss = 0.05502334\n",
      "Iteration 97, loss = 0.05492846\n",
      "Iteration 98, loss = 0.05483247\n",
      "Iteration 99, loss = 0.05474110\n",
      "Iteration 100, loss = 0.05465673\n",
      "Iteration 101, loss = 0.05457909\n",
      "Iteration 102, loss = 0.05450625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.05079277\n",
      "Iteration 3, loss = 0.86503767\n",
      "Iteration 4, loss = 0.83824751\n",
      "Iteration 5, loss = 0.81894764\n",
      "Iteration 6, loss = 0.73494737\n",
      "Iteration 7, loss = 0.63905805\n",
      "Iteration 8, loss = 0.57888922\n",
      "Iteration 9, loss = 0.56313018\n",
      "Iteration 10, loss = 0.54690158\n",
      "Iteration 11, loss = 0.51474619\n",
      "Iteration 12, loss = 0.48762701\n",
      "Iteration 13, loss = 0.46982539\n",
      "Iteration 14, loss = 0.44275450\n",
      "Iteration 15, loss = 0.41490641\n",
      "Iteration 16, loss = 0.40115915\n",
      "Iteration 17, loss = 0.39158627\n",
      "Iteration 18, loss = 0.37648929\n",
      "Iteration 19, loss = 0.36312075\n",
      "Iteration 20, loss = 0.35576427\n",
      "Iteration 21, loss = 0.34481089\n",
      "Iteration 22, loss = 0.32976221\n",
      "Iteration 23, loss = 0.31967298\n",
      "Iteration 24, loss = 0.31166934\n",
      "Iteration 25, loss = 0.29907362\n",
      "Iteration 26, loss = 0.28783582\n",
      "Iteration 27, loss = 0.28011518\n",
      "Iteration 28, loss = 0.26985925\n",
      "Iteration 29, loss = 0.25955992\n",
      "Iteration 30, loss = 0.25189998\n",
      "Iteration 31, loss = 0.24204462\n",
      "Iteration 32, loss = 0.23236539\n",
      "Iteration 33, loss = 0.22450684\n",
      "Iteration 34, loss = 0.21482144\n",
      "Iteration 35, loss = 0.20678732\n",
      "Iteration 36, loss = 0.19829886\n",
      "Iteration 37, loss = 0.18946663\n",
      "Iteration 38, loss = 0.18194266\n",
      "Iteration 39, loss = 0.17323420\n",
      "Iteration 40, loss = 0.16591522\n",
      "Iteration 41, loss = 0.15804032\n",
      "Iteration 42, loss = 0.15092947\n",
      "Iteration 43, loss = 0.14381461\n",
      "Iteration 44, loss = 0.13716169\n",
      "Iteration 45, loss = 0.13074645\n",
      "Iteration 46, loss = 0.12460321\n",
      "Iteration 47, loss = 0.11877466\n",
      "Iteration 48, loss = 0.11340883\n",
      "Iteration 49, loss = 0.10803702\n",
      "Iteration 50, loss = 0.10338949\n",
      "Iteration 51, loss = 0.09861511\n",
      "Iteration 52, loss = 0.09451905\n",
      "Iteration 53, loss = 0.09054043\n",
      "Iteration 54, loss = 0.08678564\n",
      "Iteration 55, loss = 0.08358365\n",
      "Iteration 56, loss = 0.08035554\n",
      "Iteration 57, loss = 0.07744398\n",
      "Iteration 58, loss = 0.07491654\n",
      "Iteration 59, loss = 0.07240187\n",
      "Iteration 60, loss = 0.07009178\n",
      "Iteration 61, loss = 0.06811850\n",
      "Iteration 62, loss = 0.06625530\n",
      "Iteration 63, loss = 0.06443952\n",
      "Iteration 64, loss = 0.06279962\n",
      "Iteration 65, loss = 0.06138510\n",
      "Iteration 66, loss = 0.06012595\n",
      "Iteration 67, loss = 0.05890369\n",
      "Iteration 68, loss = 0.05770019\n",
      "Iteration 69, loss = 0.05660203\n",
      "Iteration 70, loss = 0.05567749\n",
      "Iteration 71, loss = 0.05487834\n",
      "Iteration 72, loss = 0.05412250\n",
      "Iteration 73, loss = 0.05337317\n",
      "Iteration 74, loss = 0.05261577\n",
      "Iteration 75, loss = 0.05192353\n",
      "Iteration 76, loss = 0.05133123\n",
      "Iteration 77, loss = 0.05082844\n",
      "Iteration 78, loss = 0.05038950\n",
      "Iteration 79, loss = 0.04997835\n",
      "Iteration 80, loss = 0.04957684\n",
      "Iteration 81, loss = 0.04912665\n",
      "Iteration 82, loss = 0.04866695\n",
      "Iteration 83, loss = 0.04824522\n",
      "Iteration 84, loss = 0.04790695\n",
      "Iteration 85, loss = 0.04763883\n",
      "Iteration 86, loss = 0.04740468\n",
      "Iteration 87, loss = 0.04717698\n",
      "Iteration 88, loss = 0.04690714\n",
      "Iteration 89, loss = 0.04660795\n",
      "Iteration 90, loss = 0.04629562\n",
      "Iteration 91, loss = 0.04602747\n",
      "Iteration 92, loss = 0.04581815\n",
      "Iteration 93, loss = 0.04565143\n",
      "Iteration 94, loss = 0.04550549\n",
      "Iteration 95, loss = 0.04535270\n",
      "Iteration 96, loss = 0.04518689\n",
      "Iteration 97, loss = 0.04497840\n",
      "Iteration 98, loss = 0.04475552\n",
      "Iteration 99, loss = 0.04453979\n",
      "Iteration 100, loss = 0.04436208\n",
      "Iteration 101, loss = 0.04422336\n",
      "Iteration 102, loss = 0.04411061\n",
      "Iteration 103, loss = 0.04401246\n",
      "Iteration 104, loss = 0.04391105\n",
      "Iteration 105, loss = 0.04380735\n",
      "Iteration 106, loss = 0.04366740\n",
      "Iteration 107, loss = 0.04350884\n",
      "Iteration 108, loss = 0.04332922\n",
      "Iteration 109, loss = 0.04316690\n",
      "Iteration 110, loss = 0.04303855\n",
      "Iteration 111, loss = 0.04293901\n",
      "Iteration 112, loss = 0.04285949\n",
      "Iteration 113, loss = 0.04278950\n",
      "Iteration 114, loss = 0.04272625\n",
      "Iteration 115, loss = 0.04265469\n",
      "Iteration 116, loss = 0.04258187\n",
      "Iteration 117, loss = 0.04247769\n",
      "Iteration 118, loss = 0.04236756\n",
      "Iteration 119, loss = 0.04223232\n",
      "Iteration 120, loss = 0.04210120\n",
      "Iteration 121, loss = 0.04198239\n",
      "Iteration 122, loss = 0.04188514\n",
      "Iteration 123, loss = 0.04181092\n",
      "Iteration 124, loss = 0.04175258\n",
      "Iteration 125, loss = 0.04171052\n",
      "Iteration 126, loss = 0.04169156\n",
      "Iteration 127, loss = 0.04170998\n",
      "Iteration 128, loss = 0.04173193\n",
      "Iteration 129, loss = 0.04177816\n",
      "Iteration 130, loss = 0.04166597\n",
      "Iteration 131, loss = 0.04146898\n",
      "Iteration 132, loss = 0.04121545\n",
      "Iteration 133, loss = 0.04109901\n",
      "Iteration 134, loss = 0.04112658\n",
      "Iteration 135, loss = 0.04117108\n",
      "Iteration 136, loss = 0.04113661\n",
      "Iteration 137, loss = 0.04097100\n",
      "Iteration 138, loss = 0.04081634\n",
      "Iteration 139, loss = 0.04075705\n",
      "Iteration 140, loss = 0.04077442\n",
      "Iteration 141, loss = 0.04078132\n",
      "Iteration 142, loss = 0.04070068\n",
      "Iteration 143, loss = 0.04058044\n",
      "Iteration 144, loss = 0.04048237\n",
      "Iteration 145, loss = 0.04044619\n",
      "Iteration 146, loss = 0.04044367\n",
      "Iteration 147, loss = 0.04041991\n",
      "Iteration 148, loss = 0.04035905\n",
      "Iteration 149, loss = 0.04026861\n",
      "Iteration 150, loss = 0.04019072\n",
      "Iteration 151, loss = 0.04014255\n",
      "Iteration 152, loss = 0.04011765\n",
      "Iteration 153, loss = 0.04009743\n",
      "Iteration 154, loss = 0.04006235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.03285657\n",
      "Iteration 3, loss = 0.85880740\n",
      "Iteration 4, loss = 0.83845101\n",
      "Iteration 5, loss = 0.81071523\n",
      "Iteration 6, loss = 0.72122095\n",
      "Iteration 7, loss = 0.62847839\n",
      "Iteration 8, loss = 0.57406577\n",
      "Iteration 9, loss = 0.55981003\n",
      "Iteration 10, loss = 0.54096305\n",
      "Iteration 11, loss = 0.50984703\n",
      "Iteration 12, loss = 0.48668091\n",
      "Iteration 13, loss = 0.46822171\n",
      "Iteration 14, loss = 0.43990996\n",
      "Iteration 15, loss = 0.41603423\n",
      "Iteration 16, loss = 0.40488225\n",
      "Iteration 17, loss = 0.39297945\n",
      "Iteration 18, loss = 0.37804573\n",
      "Iteration 19, loss = 0.36909027\n",
      "Iteration 20, loss = 0.36190164\n",
      "Iteration 21, loss = 0.34884386\n",
      "Iteration 22, loss = 0.33724840\n",
      "Iteration 23, loss = 0.32988736\n",
      "Iteration 24, loss = 0.31977424\n",
      "Iteration 25, loss = 0.30809648\n",
      "Iteration 26, loss = 0.30045155\n",
      "Iteration 27, loss = 0.29177561\n",
      "Iteration 28, loss = 0.28144296\n",
      "Iteration 29, loss = 0.27413206\n",
      "Iteration 30, loss = 0.26576533\n",
      "Iteration 31, loss = 0.25613664\n",
      "Iteration 32, loss = 0.24894293\n",
      "Iteration 33, loss = 0.24000710\n",
      "Iteration 34, loss = 0.23184797\n",
      "Iteration 35, loss = 0.22428433\n",
      "Iteration 36, loss = 0.21587276\n",
      "Iteration 37, loss = 0.20911729\n",
      "Iteration 38, loss = 0.20130061\n",
      "Iteration 39, loss = 0.19447112\n",
      "Iteration 40, loss = 0.18739992\n",
      "Iteration 41, loss = 0.18051773\n",
      "Iteration 42, loss = 0.17411008\n",
      "Iteration 43, loss = 0.16750400\n",
      "Iteration 44, loss = 0.16170100\n",
      "Iteration 45, loss = 0.15563265\n",
      "Iteration 46, loss = 0.15030125\n",
      "Iteration 47, loss = 0.14499103\n",
      "Iteration 48, loss = 0.14009559\n",
      "Iteration 49, loss = 0.13554531\n",
      "Iteration 50, loss = 0.13097840\n",
      "Iteration 51, loss = 0.12707998\n",
      "Iteration 52, loss = 0.12312209\n",
      "Iteration 53, loss = 0.11948094\n",
      "Iteration 54, loss = 0.11632675\n",
      "Iteration 55, loss = 0.11312790\n",
      "Iteration 56, loss = 0.11018145\n",
      "Iteration 57, loss = 0.10765622\n",
      "Iteration 58, loss = 0.10523527\n",
      "Iteration 59, loss = 0.10286533\n",
      "Iteration 60, loss = 0.10072714\n",
      "Iteration 61, loss = 0.09887523\n",
      "Iteration 62, loss = 0.09724097\n",
      "Iteration 63, loss = 0.09574424\n",
      "Iteration 64, loss = 0.09430411\n",
      "Iteration 65, loss = 0.09284576\n",
      "Iteration 66, loss = 0.09150006\n",
      "Iteration 67, loss = 0.09036796\n",
      "Iteration 68, loss = 0.08943455\n",
      "Iteration 69, loss = 0.08865601\n",
      "Iteration 70, loss = 0.08794827\n",
      "Iteration 71, loss = 0.08717298\n",
      "Iteration 72, loss = 0.08621945\n",
      "Iteration 73, loss = 0.08536918\n",
      "Iteration 74, loss = 0.08481600\n",
      "Iteration 75, loss = 0.08445169\n",
      "Iteration 76, loss = 0.08405413\n",
      "Iteration 77, loss = 0.08344361\n",
      "Iteration 78, loss = 0.08279646\n",
      "Iteration 79, loss = 0.08234359\n",
      "Iteration 80, loss = 0.08209170\n",
      "Iteration 81, loss = 0.08186667\n",
      "Iteration 82, loss = 0.08149240\n",
      "Iteration 83, loss = 0.08102918\n",
      "Iteration 84, loss = 0.08063400\n",
      "Iteration 85, loss = 0.08039859\n",
      "Iteration 86, loss = 0.08024793\n",
      "Iteration 87, loss = 0.08005318\n",
      "Iteration 88, loss = 0.07977449\n",
      "Iteration 89, loss = 0.07943880\n",
      "Iteration 90, loss = 0.07915977\n",
      "Iteration 91, loss = 0.07897599\n",
      "Iteration 92, loss = 0.07885115\n",
      "Iteration 93, loss = 0.07873025\n",
      "Iteration 94, loss = 0.07856158\n",
      "Iteration 95, loss = 0.07834890\n",
      "Iteration 96, loss = 0.07810997\n",
      "Iteration 97, loss = 0.07790028\n",
      "Iteration 98, loss = 0.07773975\n",
      "Iteration 99, loss = 0.07762036\n",
      "Iteration 100, loss = 0.07752352\n",
      "Iteration 101, loss = 0.07742837\n",
      "Iteration 102, loss = 0.07732828\n",
      "Iteration 103, loss = 0.07719860\n",
      "Iteration 104, loss = 0.07705217\n",
      "Iteration 105, loss = 0.07688107\n",
      "Iteration 106, loss = 0.07671407\n",
      "Iteration 107, loss = 0.07656280\n",
      "Iteration 108, loss = 0.07643601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109, loss = 0.07633030\n",
      "Iteration 110, loss = 0.07624083\n",
      "Iteration 111, loss = 0.07616486\n",
      "Iteration 112, loss = 0.07610250\n",
      "Iteration 113, loss = 0.07606823\n",
      "Iteration 114, loss = 0.07605430\n",
      "Iteration 115, loss = 0.07608862\n",
      "Iteration 116, loss = 0.07606829\n",
      "Iteration 117, loss = 0.07598864\n",
      "Iteration 118, loss = 0.07570984\n",
      "Iteration 119, loss = 0.07542385\n",
      "Iteration 120, loss = 0.07526509\n",
      "Iteration 121, loss = 0.07526992\n",
      "Iteration 122, loss = 0.07532476\n",
      "Iteration 123, loss = 0.07526982\n",
      "Iteration 124, loss = 0.07510690\n",
      "Iteration 125, loss = 0.07490548\n",
      "Iteration 126, loss = 0.07479641\n",
      "Iteration 127, loss = 0.07478633\n",
      "Iteration 128, loss = 0.07478885\n",
      "Iteration 129, loss = 0.07473774\n",
      "Iteration 130, loss = 0.07460483\n",
      "Iteration 131, loss = 0.07446241\n",
      "Iteration 132, loss = 0.07436249\n",
      "Iteration 133, loss = 0.07431799\n",
      "Iteration 134, loss = 0.07430093\n",
      "Iteration 135, loss = 0.07426974\n",
      "Iteration 136, loss = 0.07421279\n",
      "Iteration 137, loss = 0.07411844\n",
      "Iteration 138, loss = 0.07401602\n",
      "Iteration 139, loss = 0.07391663\n",
      "Iteration 140, loss = 0.07383496\n",
      "Iteration 141, loss = 0.07377128\n",
      "Iteration 142, loss = 0.07372214\n",
      "Iteration 143, loss = 0.07368587\n",
      "Iteration 144, loss = 0.07366150\n",
      "Iteration 145, loss = 0.07365990\n",
      "Iteration 146, loss = 0.07367610\n",
      "Iteration 147, loss = 0.07373763\n",
      "Iteration 148, loss = 0.07376782\n",
      "Iteration 149, loss = 0.07377911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.15359881\n",
      "Iteration 3, loss = 0.96938958\n",
      "Iteration 4, loss = 0.86592362\n",
      "Iteration 5, loss = 0.85570323\n",
      "Iteration 6, loss = 0.82596799\n",
      "Iteration 7, loss = 0.75964938\n",
      "Iteration 8, loss = 0.69899234\n",
      "Iteration 9, loss = 0.65633874\n",
      "Iteration 10, loss = 0.62428740\n",
      "Iteration 11, loss = 0.59781428\n",
      "Iteration 12, loss = 0.57570120\n",
      "Iteration 13, loss = 0.55677272\n",
      "Iteration 14, loss = 0.53832107\n",
      "Iteration 15, loss = 0.52084449\n",
      "Iteration 16, loss = 0.50446117\n",
      "Iteration 17, loss = 0.48905808\n",
      "Iteration 18, loss = 0.47472471\n",
      "Iteration 19, loss = 0.46240436\n",
      "Iteration 20, loss = 0.45174934\n",
      "Iteration 21, loss = 0.44199774\n",
      "Iteration 22, loss = 0.43284743\n",
      "Iteration 23, loss = 0.42417531\n",
      "Iteration 24, loss = 0.41582237\n",
      "Iteration 25, loss = 0.40777768\n",
      "Iteration 26, loss = 0.39989025\n",
      "Iteration 27, loss = 0.39231911\n",
      "Iteration 28, loss = 0.38539149\n",
      "Iteration 29, loss = 0.37912740\n",
      "Iteration 30, loss = 0.37333388\n",
      "Iteration 31, loss = 0.36786652\n",
      "Iteration 32, loss = 0.36263762\n",
      "Iteration 33, loss = 0.35763580\n",
      "Iteration 34, loss = 0.35282792\n",
      "Iteration 35, loss = 0.34817471\n",
      "Iteration 36, loss = 0.34364061\n",
      "Iteration 37, loss = 0.33921850\n",
      "Iteration 38, loss = 0.33491231\n",
      "Iteration 39, loss = 0.33071418\n",
      "Iteration 40, loss = 0.32661427\n",
      "Iteration 41, loss = 0.32261624\n",
      "Iteration 42, loss = 0.31871896\n",
      "Iteration 43, loss = 0.31491756\n",
      "Iteration 44, loss = 0.31119490\n",
      "Iteration 45, loss = 0.30754981\n",
      "Iteration 46, loss = 0.30398102\n",
      "Iteration 47, loss = 0.30048504\n",
      "Iteration 48, loss = 0.29705936\n",
      "Iteration 49, loss = 0.29370226\n",
      "Iteration 50, loss = 0.29041182\n",
      "Iteration 51, loss = 0.28718932\n",
      "Iteration 52, loss = 0.28403176\n",
      "Iteration 53, loss = 0.28093760\n",
      "Iteration 54, loss = 0.27790620\n",
      "Iteration 55, loss = 0.27493609\n",
      "Iteration 56, loss = 0.27203231\n",
      "Iteration 57, loss = 0.26918464\n",
      "Iteration 58, loss = 0.26639418\n",
      "Iteration 59, loss = 0.26365821\n",
      "Iteration 60, loss = 0.26097613\n",
      "Iteration 61, loss = 0.25834671\n",
      "Iteration 62, loss = 0.25576916\n",
      "Iteration 63, loss = 0.25324203\n",
      "Iteration 64, loss = 0.25076816\n",
      "Iteration 65, loss = 0.24834308\n",
      "Iteration 66, loss = 0.24596629\n",
      "Iteration 67, loss = 0.24363636\n",
      "Iteration 68, loss = 0.24135302\n",
      "Iteration 69, loss = 0.23911568\n",
      "Iteration 70, loss = 0.23692364\n",
      "Iteration 71, loss = 0.23477564\n",
      "Iteration 72, loss = 0.23266979\n",
      "Iteration 73, loss = 0.23060675\n",
      "Iteration 74, loss = 0.22858471\n",
      "Iteration 75, loss = 0.22660281\n",
      "Iteration 76, loss = 0.22466104\n",
      "Iteration 77, loss = 0.22275837\n",
      "Iteration 78, loss = 0.22089333\n",
      "Iteration 79, loss = 0.21906523\n",
      "Iteration 80, loss = 0.21727339\n",
      "Iteration 81, loss = 0.21551759\n",
      "Iteration 82, loss = 0.21379546\n",
      "Iteration 83, loss = 0.21210749\n",
      "Iteration 84, loss = 0.21045313\n",
      "Iteration 85, loss = 0.20883101\n",
      "Iteration 86, loss = 0.20723999\n",
      "Iteration 87, loss = 0.20568119\n",
      "Iteration 88, loss = 0.20415341\n",
      "Iteration 89, loss = 0.20265487\n",
      "Iteration 90, loss = 0.20118509\n",
      "Iteration 91, loss = 0.19974443\n",
      "Iteration 92, loss = 0.19833064\n",
      "Iteration 93, loss = 0.19694441\n",
      "Iteration 94, loss = 0.19558499\n",
      "Iteration 95, loss = 0.19425139\n",
      "Iteration 96, loss = 0.19294431\n",
      "Iteration 97, loss = 0.19166089\n",
      "Iteration 98, loss = 0.19040270\n",
      "Iteration 99, loss = 0.18916705\n",
      "Iteration 100, loss = 0.18795541\n",
      "Iteration 101, loss = 0.18676655\n",
      "Iteration 102, loss = 0.18560055\n",
      "Iteration 103, loss = 0.18445711\n",
      "Iteration 104, loss = 0.18333407\n",
      "Iteration 105, loss = 0.18223208\n",
      "Iteration 106, loss = 0.18115051\n",
      "Iteration 107, loss = 0.18008852\n",
      "Iteration 108, loss = 0.17904587\n",
      "Iteration 109, loss = 0.17802278\n",
      "Iteration 110, loss = 0.17701721\n",
      "Iteration 111, loss = 0.17602975\n",
      "Iteration 112, loss = 0.17506095\n",
      "Iteration 113, loss = 0.17410799\n",
      "Iteration 114, loss = 0.17317230\n",
      "Iteration 115, loss = 0.17225407\n",
      "Iteration 116, loss = 0.17135082\n",
      "Iteration 117, loss = 0.17046341\n",
      "Iteration 118, loss = 0.16959221\n",
      "Iteration 119, loss = 0.16873540\n",
      "Iteration 120, loss = 0.16789335\n",
      "Iteration 121, loss = 0.16706605\n",
      "Iteration 122, loss = 0.16625314\n",
      "Iteration 123, loss = 0.16545353\n",
      "Iteration 124, loss = 0.16466762\n",
      "Iteration 125, loss = 0.16389531\n",
      "Iteration 126, loss = 0.16313535\n",
      "Iteration 127, loss = 0.16238831\n",
      "Iteration 128, loss = 0.16165345\n",
      "Iteration 129, loss = 0.16093028\n",
      "Iteration 130, loss = 0.16021984\n",
      "Iteration 131, loss = 0.15951961\n",
      "Iteration 132, loss = 0.15883165\n",
      "Iteration 133, loss = 0.15815442\n",
      "Iteration 134, loss = 0.15748765\n",
      "Iteration 135, loss = 0.15683186\n",
      "Iteration 136, loss = 0.15618635\n",
      "Iteration 137, loss = 0.15555121\n",
      "Iteration 138, loss = 0.15492579\n",
      "Iteration 139, loss = 0.15430993\n",
      "Iteration 140, loss = 0.15370354\n",
      "Iteration 141, loss = 0.15310639\n",
      "Iteration 142, loss = 0.15251825\n",
      "Iteration 143, loss = 0.15193928\n",
      "Iteration 144, loss = 0.15136885\n",
      "Iteration 145, loss = 0.15080698\n",
      "Iteration 146, loss = 0.15025342\n",
      "Iteration 147, loss = 0.14970803\n",
      "Iteration 148, loss = 0.14917100\n",
      "Iteration 149, loss = 0.14864143\n",
      "Iteration 150, loss = 0.14811952\n",
      "Iteration 151, loss = 0.14760531\n",
      "Iteration 152, loss = 0.14709853\n",
      "Iteration 153, loss = 0.14659915\n",
      "Iteration 154, loss = 0.14610693\n",
      "Iteration 155, loss = 0.14562146\n",
      "Iteration 156, loss = 0.14514284\n",
      "Iteration 157, loss = 0.14467091\n",
      "Iteration 158, loss = 0.14420570\n",
      "Iteration 159, loss = 0.14374706\n",
      "Iteration 160, loss = 0.14329470\n",
      "Iteration 161, loss = 0.14284852\n",
      "Iteration 162, loss = 0.14240838\n",
      "Iteration 163, loss = 0.14197422\n",
      "Iteration 164, loss = 0.14154585\n",
      "Iteration 165, loss = 0.14112328\n",
      "Iteration 166, loss = 0.14070629\n",
      "Iteration 167, loss = 0.14029482\n",
      "Iteration 168, loss = 0.13988876\n",
      "Iteration 169, loss = 0.13948861\n",
      "Iteration 170, loss = 0.13909277\n",
      "Iteration 171, loss = 0.13870250\n",
      "Iteration 172, loss = 0.13831741\n",
      "Iteration 173, loss = 0.13793724\n",
      "Iteration 174, loss = 0.13756185\n",
      "Iteration 175, loss = 0.13719120\n",
      "Iteration 176, loss = 0.13682527\n",
      "Iteration 177, loss = 0.13646421\n",
      "Iteration 178, loss = 0.13610749\n",
      "Iteration 179, loss = 0.13575522\n",
      "Iteration 180, loss = 0.13540734\n",
      "Iteration 181, loss = 0.13506381\n",
      "Iteration 182, loss = 0.13472448\n",
      "Iteration 183, loss = 0.13438929\n",
      "Iteration 184, loss = 0.13405815\n",
      "Iteration 185, loss = 0.13373100\n",
      "Iteration 186, loss = 0.13340778\n",
      "Iteration 187, loss = 0.13308840\n",
      "Iteration 188, loss = 0.13277280\n",
      "Iteration 189, loss = 0.13246100\n",
      "Iteration 190, loss = 0.13215276\n",
      "Iteration 191, loss = 0.13184823\n",
      "Iteration 192, loss = 0.13154725\n",
      "Iteration 193, loss = 0.13124976\n",
      "Iteration 194, loss = 0.13095572\n",
      "Iteration 195, loss = 0.13066502\n",
      "Iteration 196, loss = 0.13037763\n",
      "Iteration 197, loss = 0.13009351\n",
      "Iteration 198, loss = 0.12981259\n",
      "Iteration 199, loss = 0.12953488\n",
      "Iteration 200, loss = 0.12926022\n",
      "Iteration 201, loss = 0.12898863\n",
      "Iteration 202, loss = 0.12872006\n",
      "Iteration 203, loss = 0.12845445\n",
      "Iteration 204, loss = 0.12819175\n",
      "Iteration 205, loss = 0.12793192\n",
      "Iteration 206, loss = 0.12767489\n",
      "Iteration 207, loss = 0.12742064\n",
      "Iteration 208, loss = 0.12716916\n",
      "Iteration 209, loss = 0.12692031\n",
      "Iteration 210, loss = 0.12667414\n",
      "Iteration 211, loss = 0.12643059\n",
      "Iteration 212, loss = 0.12618958\n",
      "Iteration 213, loss = 0.12595112\n",
      "Iteration 214, loss = 0.12571517\n",
      "Iteration 215, loss = 0.12548170\n",
      "Iteration 216, loss = 0.12525057\n",
      "Iteration 217, loss = 0.12502185\n",
      "Iteration 218, loss = 0.12479546\n",
      "Iteration 219, loss = 0.12457142\n",
      "Iteration 220, loss = 0.12434962\n",
      "Iteration 221, loss = 0.12413006\n",
      "Iteration 222, loss = 0.12391274\n",
      "Iteration 223, loss = 0.12369752\n",
      "Iteration 224, loss = 0.12348448\n",
      "Iteration 225, loss = 0.12327352\n",
      "Iteration 226, loss = 0.12306467\n",
      "Iteration 227, loss = 0.12285786\n",
      "Iteration 228, loss = 0.12265308\n",
      "Iteration 229, loss = 0.12245027\n",
      "Iteration 230, loss = 0.12224948\n",
      "Iteration 231, loss = 0.12205056\n",
      "Iteration 232, loss = 0.12185361\n",
      "Iteration 233, loss = 0.12165851\n",
      "Iteration 234, loss = 0.12146525\n",
      "Iteration 235, loss = 0.12127387\n",
      "Iteration 236, loss = 0.12108426\n",
      "Iteration 237, loss = 0.12089643\n",
      "Iteration 238, loss = 0.12071037\n",
      "Iteration 239, loss = 0.12052602\n",
      "Iteration 240, loss = 0.12034339\n",
      "Iteration 241, loss = 0.12016240\n",
      "Iteration 242, loss = 0.11998304\n",
      "Iteration 243, loss = 0.11980529\n",
      "Iteration 244, loss = 0.11962920\n",
      "Iteration 245, loss = 0.11945465\n",
      "Iteration 246, loss = 0.11928169\n",
      "Iteration 247, loss = 0.11911026\n",
      "Iteration 248, loss = 0.11894037\n",
      "Iteration 249, loss = 0.11877198\n",
      "Iteration 250, loss = 0.11860506\n",
      "Iteration 251, loss = 0.11843945\n",
      "Iteration 252, loss = 0.11827534\n",
      "Iteration 253, loss = 0.11811258\n",
      "Iteration 254, loss = 0.11795120\n",
      "Iteration 255, loss = 0.11779123\n",
      "Iteration 256, loss = 0.11763260\n",
      "Iteration 257, loss = 0.11747529\n",
      "Iteration 258, loss = 0.11731932\n",
      "Iteration 259, loss = 0.11716464\n",
      "Iteration 260, loss = 0.11701126\n",
      "Iteration 261, loss = 0.11685916\n",
      "Iteration 262, loss = 0.11670838\n",
      "Iteration 263, loss = 0.11655879\n",
      "Iteration 264, loss = 0.11641037\n",
      "Iteration 265, loss = 0.11626315\n",
      "Iteration 266, loss = 0.11611715\n",
      "Iteration 267, loss = 0.11597228\n",
      "Iteration 268, loss = 0.11582857\n",
      "Iteration 269, loss = 0.11568603\n",
      "Iteration 270, loss = 0.11554464\n",
      "Iteration 271, loss = 0.11540431\n",
      "Iteration 272, loss = 0.11526509\n",
      "Iteration 273, loss = 0.11512694\n",
      "Iteration 274, loss = 0.11498987\n",
      "Iteration 275, loss = 0.11485392\n",
      "Iteration 276, loss = 0.11471899\n",
      "Iteration 277, loss = 0.11458511\n",
      "Iteration 278, loss = 0.11445234\n",
      "Iteration 279, loss = 0.11432055\n",
      "Iteration 280, loss = 0.11418983\n",
      "Iteration 281, loss = 0.11406013\n",
      "Iteration 282, loss = 0.11393140\n",
      "Iteration 283, loss = 0.11380369\n",
      "Iteration 284, loss = 0.11367696\n",
      "Iteration 285, loss = 0.11355119\n",
      "Iteration 286, loss = 0.11342640\n",
      "Iteration 287, loss = 0.11330254\n",
      "Iteration 288, loss = 0.11317966\n",
      "Iteration 289, loss = 0.11305763\n",
      "Iteration 290, loss = 0.11293657\n",
      "Iteration 291, loss = 0.11281633\n",
      "Iteration 292, loss = 0.11269702\n",
      "Iteration 293, loss = 0.11257857\n",
      "Iteration 294, loss = 0.11246097\n",
      "Iteration 295, loss = 0.11234425\n",
      "Iteration 296, loss = 0.11222837\n",
      "Iteration 297, loss = 0.11211336\n",
      "Iteration 298, loss = 0.11199914\n",
      "Iteration 299, loss = 0.11188573\n",
      "Iteration 300, loss = 0.11177307\n",
      "Iteration 301, loss = 0.11166123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 302, loss = 0.11155014\n",
      "Iteration 303, loss = 0.11143986\n",
      "Iteration 304, loss = 0.11133031\n",
      "Iteration 305, loss = 0.11122156\n",
      "Iteration 306, loss = 0.11111356\n",
      "Iteration 307, loss = 0.11100635\n",
      "Iteration 308, loss = 0.11089986\n",
      "Iteration 309, loss = 0.11079415\n",
      "Iteration 310, loss = 0.11068915\n",
      "Iteration 311, loss = 0.11058491\n",
      "Iteration 312, loss = 0.11048134\n",
      "Iteration 313, loss = 0.11037850\n",
      "Iteration 314, loss = 0.11027636\n",
      "Iteration 315, loss = 0.11017493\n",
      "Iteration 316, loss = 0.11007419\n",
      "Iteration 317, loss = 0.10997413\n",
      "Iteration 318, loss = 0.10987478\n",
      "Iteration 319, loss = 0.10977605\n",
      "Iteration 320, loss = 0.10967795\n",
      "Iteration 321, loss = 0.10958037\n",
      "Iteration 322, loss = 0.10948347\n",
      "Iteration 323, loss = 0.10938718\n",
      "Iteration 324, loss = 0.10929154\n",
      "Iteration 325, loss = 0.10919653\n",
      "Iteration 326, loss = 0.10910214\n",
      "Iteration 327, loss = 0.10900839\n",
      "Iteration 328, loss = 0.10891522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.15655690\n",
      "Iteration 3, loss = 0.97627098\n",
      "Iteration 4, loss = 0.87219722\n",
      "Iteration 5, loss = 0.86143075\n",
      "Iteration 6, loss = 0.83223910\n",
      "Iteration 7, loss = 0.76711648\n",
      "Iteration 8, loss = 0.70532645\n",
      "Iteration 9, loss = 0.66197002\n",
      "Iteration 10, loss = 0.63010781\n",
      "Iteration 11, loss = 0.60382781\n",
      "Iteration 12, loss = 0.57918631\n",
      "Iteration 13, loss = 0.55757352\n",
      "Iteration 14, loss = 0.53929630\n",
      "Iteration 15, loss = 0.52173765\n",
      "Iteration 16, loss = 0.50515604\n",
      "Iteration 17, loss = 0.48932977\n",
      "Iteration 18, loss = 0.47423858\n",
      "Iteration 19, loss = 0.46096905\n",
      "Iteration 20, loss = 0.44956616\n",
      "Iteration 21, loss = 0.43937722\n",
      "Iteration 22, loss = 0.42979352\n",
      "Iteration 23, loss = 0.42075869\n",
      "Iteration 24, loss = 0.41218760\n",
      "Iteration 25, loss = 0.40392347\n",
      "Iteration 26, loss = 0.39586624\n",
      "Iteration 27, loss = 0.38792707\n",
      "Iteration 28, loss = 0.38035228\n",
      "Iteration 29, loss = 0.37339438\n",
      "Iteration 30, loss = 0.36703894\n",
      "Iteration 31, loss = 0.36126376\n",
      "Iteration 32, loss = 0.35582006\n",
      "Iteration 33, loss = 0.35055664\n",
      "Iteration 34, loss = 0.34542601\n",
      "Iteration 35, loss = 0.34045278\n",
      "Iteration 36, loss = 0.33564406\n",
      "Iteration 37, loss = 0.33096959\n",
      "Iteration 38, loss = 0.32643153\n",
      "Iteration 39, loss = 0.32200101\n",
      "Iteration 40, loss = 0.31767735\n",
      "Iteration 41, loss = 0.31345358\n",
      "Iteration 42, loss = 0.30932655\n",
      "Iteration 43, loss = 0.30530025\n",
      "Iteration 44, loss = 0.30137176\n",
      "Iteration 45, loss = 0.29753197\n",
      "Iteration 46, loss = 0.29377729\n",
      "Iteration 47, loss = 0.29010444\n",
      "Iteration 48, loss = 0.28650970\n",
      "Iteration 49, loss = 0.28299226\n",
      "Iteration 50, loss = 0.27955163\n",
      "Iteration 51, loss = 0.27618448\n",
      "Iteration 52, loss = 0.27288891\n",
      "Iteration 53, loss = 0.26966324\n",
      "Iteration 54, loss = 0.26650737\n",
      "Iteration 55, loss = 0.26342006\n",
      "Iteration 56, loss = 0.26039891\n",
      "Iteration 57, loss = 0.25744252\n",
      "Iteration 58, loss = 0.25454960\n",
      "Iteration 59, loss = 0.25171919\n",
      "Iteration 60, loss = 0.24894894\n",
      "Iteration 61, loss = 0.24623876\n",
      "Iteration 62, loss = 0.24358801\n",
      "Iteration 63, loss = 0.24099399\n",
      "Iteration 64, loss = 0.23845528\n",
      "Iteration 65, loss = 0.23597112\n",
      "Iteration 66, loss = 0.23354047\n",
      "Iteration 67, loss = 0.23116183\n",
      "Iteration 68, loss = 0.22883649\n",
      "Iteration 69, loss = 0.22656105\n",
      "Iteration 70, loss = 0.22433451\n",
      "Iteration 71, loss = 0.22215580\n",
      "Iteration 72, loss = 0.22002403\n",
      "Iteration 73, loss = 0.21793811\n",
      "Iteration 74, loss = 0.21589752\n",
      "Iteration 75, loss = 0.21390085\n",
      "Iteration 76, loss = 0.21194632\n",
      "Iteration 77, loss = 0.21003359\n",
      "Iteration 78, loss = 0.20816169\n",
      "Iteration 79, loss = 0.20632984\n",
      "Iteration 80, loss = 0.20453704\n",
      "Iteration 81, loss = 0.20278232\n",
      "Iteration 82, loss = 0.20106467\n",
      "Iteration 83, loss = 0.19938318\n",
      "Iteration 84, loss = 0.19773725\n",
      "Iteration 85, loss = 0.19612628\n",
      "Iteration 86, loss = 0.19454912\n",
      "Iteration 87, loss = 0.19300496\n",
      "Iteration 88, loss = 0.19149259\n",
      "Iteration 89, loss = 0.19001149\n",
      "Iteration 90, loss = 0.18856149\n",
      "Iteration 91, loss = 0.18714187\n",
      "Iteration 92, loss = 0.18575174\n",
      "Iteration 93, loss = 0.18439041\n",
      "Iteration 94, loss = 0.18305624\n",
      "Iteration 95, loss = 0.18175036\n",
      "Iteration 96, loss = 0.18046921\n",
      "Iteration 97, loss = 0.17921537\n",
      "Iteration 98, loss = 0.17798572\n",
      "Iteration 99, loss = 0.17678051\n",
      "Iteration 100, loss = 0.17560015\n",
      "Iteration 101, loss = 0.17444284\n",
      "Iteration 102, loss = 0.17330845\n",
      "Iteration 103, loss = 0.17219642\n",
      "Iteration 104, loss = 0.17110606\n",
      "Iteration 105, loss = 0.17003633\n",
      "Iteration 106, loss = 0.16898751\n",
      "Iteration 107, loss = 0.16795885\n",
      "Iteration 108, loss = 0.16694934\n",
      "Iteration 109, loss = 0.16595916\n",
      "Iteration 110, loss = 0.16498818\n",
      "Iteration 111, loss = 0.16403512\n",
      "Iteration 112, loss = 0.16309979\n",
      "Iteration 113, loss = 0.16218273\n",
      "Iteration 114, loss = 0.16128169\n",
      "Iteration 115, loss = 0.16039699\n",
      "Iteration 116, loss = 0.15952971\n",
      "Iteration 117, loss = 0.15867666\n",
      "Iteration 118, loss = 0.15784016\n",
      "Iteration 119, loss = 0.15701861\n",
      "Iteration 120, loss = 0.15621157\n",
      "Iteration 121, loss = 0.15541874\n",
      "Iteration 122, loss = 0.15464140\n",
      "Iteration 123, loss = 0.15387569\n",
      "Iteration 124, loss = 0.15312500\n",
      "Iteration 125, loss = 0.15238633\n",
      "Iteration 126, loss = 0.15166059\n",
      "Iteration 127, loss = 0.15094774\n",
      "Iteration 128, loss = 0.15024673\n",
      "Iteration 129, loss = 0.14955781\n",
      "Iteration 130, loss = 0.14888044\n",
      "Iteration 131, loss = 0.14821456\n",
      "Iteration 132, loss = 0.14755940\n",
      "Iteration 133, loss = 0.14691560\n",
      "Iteration 134, loss = 0.14628168\n",
      "Iteration 135, loss = 0.14565934\n",
      "Iteration 136, loss = 0.14504601\n",
      "Iteration 137, loss = 0.14444340\n",
      "Iteration 138, loss = 0.14385029\n",
      "Iteration 139, loss = 0.14326640\n",
      "Iteration 140, loss = 0.14269234\n",
      "Iteration 141, loss = 0.14212696\n",
      "Iteration 142, loss = 0.14157029\n",
      "Iteration 143, loss = 0.14102317\n",
      "Iteration 144, loss = 0.14048356\n",
      "Iteration 145, loss = 0.13995271\n",
      "Iteration 146, loss = 0.13943023\n",
      "Iteration 147, loss = 0.13891526\n",
      "Iteration 148, loss = 0.13840819\n",
      "Iteration 149, loss = 0.13790923\n",
      "Iteration 150, loss = 0.13741723\n",
      "Iteration 151, loss = 0.13693310\n",
      "Iteration 152, loss = 0.13645587\n",
      "Iteration 153, loss = 0.13598574\n",
      "Iteration 154, loss = 0.13552234\n",
      "Iteration 155, loss = 0.13506574\n",
      "Iteration 156, loss = 0.13461637\n",
      "Iteration 157, loss = 0.13417256\n",
      "Iteration 158, loss = 0.13373549\n",
      "Iteration 159, loss = 0.13330465\n",
      "Iteration 160, loss = 0.13288037\n",
      "Iteration 161, loss = 0.13246170\n",
      "Iteration 162, loss = 0.13204887\n",
      "Iteration 163, loss = 0.13164174\n",
      "Iteration 164, loss = 0.13124038\n",
      "Iteration 165, loss = 0.13084474\n",
      "Iteration 166, loss = 0.13045428\n",
      "Iteration 167, loss = 0.13006906\n",
      "Iteration 168, loss = 0.12968935\n",
      "Iteration 169, loss = 0.12931472\n",
      "Iteration 170, loss = 0.12894500\n",
      "Iteration 171, loss = 0.12858012\n",
      "Iteration 172, loss = 0.12822026\n",
      "Iteration 173, loss = 0.12786547\n",
      "Iteration 174, loss = 0.12751486\n",
      "Iteration 175, loss = 0.12716901\n",
      "Iteration 176, loss = 0.12682763\n",
      "Iteration 177, loss = 0.12649067\n",
      "Iteration 178, loss = 0.12615802\n",
      "Iteration 179, loss = 0.12582966\n",
      "Iteration 180, loss = 0.12550597\n",
      "Iteration 181, loss = 0.12518565\n",
      "Iteration 182, loss = 0.12486962\n",
      "Iteration 183, loss = 0.12455751\n",
      "Iteration 184, loss = 0.12424930\n",
      "Iteration 185, loss = 0.12394515\n",
      "Iteration 186, loss = 0.12364445\n",
      "Iteration 187, loss = 0.12334758\n",
      "Iteration 188, loss = 0.12305425\n",
      "Iteration 189, loss = 0.12276447\n",
      "Iteration 190, loss = 0.12247814\n",
      "Iteration 191, loss = 0.12219567\n",
      "Iteration 192, loss = 0.12191609\n",
      "Iteration 193, loss = 0.12163996\n",
      "Iteration 194, loss = 0.12136707\n",
      "Iteration 195, loss = 0.12109735\n",
      "Iteration 196, loss = 0.12083127\n",
      "Iteration 197, loss = 0.12056764\n",
      "Iteration 198, loss = 0.12030725\n",
      "Iteration 199, loss = 0.12004986\n",
      "Iteration 200, loss = 0.11979554\n",
      "Iteration 201, loss = 0.11954418\n",
      "Iteration 202, loss = 0.11929549\n",
      "Iteration 203, loss = 0.11904962\n",
      "Iteration 204, loss = 0.11880647\n",
      "Iteration 205, loss = 0.11856602\n",
      "Iteration 206, loss = 0.11832876\n",
      "Iteration 207, loss = 0.11809338\n",
      "Iteration 208, loss = 0.11786082\n",
      "Iteration 209, loss = 0.11763104\n",
      "Iteration 210, loss = 0.11740363\n",
      "Iteration 211, loss = 0.11717867\n",
      "Iteration 212, loss = 0.11695609\n",
      "Iteration 213, loss = 0.11673588\n",
      "Iteration 214, loss = 0.11651809\n",
      "Iteration 215, loss = 0.11630303\n",
      "Iteration 216, loss = 0.11608969\n",
      "Iteration 217, loss = 0.11587875\n",
      "Iteration 218, loss = 0.11567005\n",
      "Iteration 219, loss = 0.11546354\n",
      "Iteration 220, loss = 0.11525916\n",
      "Iteration 221, loss = 0.11505690\n",
      "Iteration 222, loss = 0.11485703\n",
      "Iteration 223, loss = 0.11465885\n",
      "Iteration 224, loss = 0.11446277\n",
      "Iteration 225, loss = 0.11426871\n",
      "Iteration 226, loss = 0.11407659\n",
      "Iteration 227, loss = 0.11388648\n",
      "Iteration 228, loss = 0.11369830\n",
      "Iteration 229, loss = 0.11351201\n",
      "Iteration 230, loss = 0.11332755\n",
      "Iteration 231, loss = 0.11314494\n",
      "Iteration 232, loss = 0.11296410\n",
      "Iteration 233, loss = 0.11278507\n",
      "Iteration 234, loss = 0.11260780\n",
      "Iteration 235, loss = 0.11243221\n",
      "Iteration 236, loss = 0.11225834\n",
      "Iteration 237, loss = 0.11208615\n",
      "Iteration 238, loss = 0.11191558\n",
      "Iteration 239, loss = 0.11174669\n",
      "Iteration 240, loss = 0.11157936\n",
      "Iteration 241, loss = 0.11141363\n",
      "Iteration 242, loss = 0.11124948\n",
      "Iteration 243, loss = 0.11108684\n",
      "Iteration 244, loss = 0.11092576\n",
      "Iteration 245, loss = 0.11076614\n",
      "Iteration 246, loss = 0.11060802\n",
      "Iteration 247, loss = 0.11045135\n",
      "Iteration 248, loss = 0.11029608\n",
      "Iteration 249, loss = 0.11014223\n",
      "Iteration 250, loss = 0.10998975\n",
      "Iteration 251, loss = 0.10983866\n",
      "Iteration 252, loss = 0.10968893\n",
      "Iteration 253, loss = 0.10954057\n",
      "Iteration 254, loss = 0.10939350\n",
      "Iteration 255, loss = 0.10924774\n",
      "Iteration 256, loss = 0.10910329\n",
      "Iteration 257, loss = 0.10896011\n",
      "Iteration 258, loss = 0.10881819\n",
      "Iteration 259, loss = 0.10867750\n",
      "Iteration 260, loss = 0.10853804\n",
      "Iteration 261, loss = 0.10839979\n",
      "Iteration 262, loss = 0.10826274\n",
      "Iteration 263, loss = 0.10812686\n",
      "Iteration 264, loss = 0.10799215\n",
      "Iteration 265, loss = 0.10785860\n",
      "Iteration 266, loss = 0.10772618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 267, loss = 0.10759487\n",
      "Iteration 268, loss = 0.10746468\n",
      "Iteration 269, loss = 0.10733558\n",
      "Iteration 270, loss = 0.10720756\n",
      "Iteration 271, loss = 0.10708061\n",
      "Iteration 272, loss = 0.10695468\n",
      "Iteration 273, loss = 0.10682976\n",
      "Iteration 274, loss = 0.10670587\n",
      "Iteration 275, loss = 0.10658299\n",
      "Iteration 276, loss = 0.10646111\n",
      "Iteration 277, loss = 0.10634022\n",
      "Iteration 278, loss = 0.10622030\n",
      "Iteration 279, loss = 0.10610135\n",
      "Iteration 280, loss = 0.10598336\n",
      "Iteration 281, loss = 0.10586632\n",
      "Iteration 282, loss = 0.10575023\n",
      "Iteration 283, loss = 0.10563504\n",
      "Iteration 284, loss = 0.10552077\n",
      "Iteration 285, loss = 0.10540737\n",
      "Iteration 286, loss = 0.10529487\n",
      "Iteration 287, loss = 0.10518323\n",
      "Iteration 288, loss = 0.10507246\n",
      "Iteration 289, loss = 0.10496242\n",
      "Iteration 290, loss = 0.10485321\n",
      "Iteration 291, loss = 0.10474481\n",
      "Iteration 292, loss = 0.10463722\n",
      "Iteration 293, loss = 0.10453045\n",
      "Iteration 294, loss = 0.10442448\n",
      "Iteration 295, loss = 0.10431931\n",
      "Iteration 296, loss = 0.10421492\n",
      "Iteration 297, loss = 0.10411131\n",
      "Iteration 298, loss = 0.10400843\n",
      "Iteration 299, loss = 0.10390629\n",
      "Iteration 300, loss = 0.10380486\n",
      "Iteration 301, loss = 0.10370416\n",
      "Iteration 302, loss = 0.10360420\n",
      "Iteration 303, loss = 0.10350495\n",
      "Iteration 304, loss = 0.10340643\n",
      "Iteration 305, loss = 0.10330861\n",
      "Iteration 306, loss = 0.10321149\n",
      "Iteration 307, loss = 0.10311506\n",
      "Iteration 308, loss = 0.10301928\n",
      "Iteration 309, loss = 0.10292417\n",
      "Iteration 310, loss = 0.10282975\n",
      "Iteration 311, loss = 0.10273598\n",
      "Iteration 312, loss = 0.10264287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.14783885\n",
      "Iteration 3, loss = 0.96469443\n",
      "Iteration 4, loss = 0.85994437\n",
      "Iteration 5, loss = 0.84849121\n",
      "Iteration 6, loss = 0.81825647\n",
      "Iteration 7, loss = 0.75227772\n",
      "Iteration 8, loss = 0.68897845\n",
      "Iteration 9, loss = 0.64467770\n",
      "Iteration 10, loss = 0.61293099\n",
      "Iteration 11, loss = 0.58707044\n",
      "Iteration 12, loss = 0.56544887\n",
      "Iteration 13, loss = 0.54605341\n",
      "Iteration 14, loss = 0.52698733\n",
      "Iteration 15, loss = 0.50874141\n",
      "Iteration 16, loss = 0.49163611\n",
      "Iteration 17, loss = 0.47561660\n",
      "Iteration 18, loss = 0.46098126\n",
      "Iteration 19, loss = 0.44834183\n",
      "Iteration 20, loss = 0.43732302\n",
      "Iteration 21, loss = 0.42711458\n",
      "Iteration 22, loss = 0.41754050\n",
      "Iteration 23, loss = 0.40846450\n",
      "Iteration 24, loss = 0.39980112\n",
      "Iteration 25, loss = 0.39143594\n",
      "Iteration 26, loss = 0.38330919\n",
      "Iteration 27, loss = 0.37529332\n",
      "Iteration 28, loss = 0.36752709\n",
      "Iteration 29, loss = 0.36029053\n",
      "Iteration 30, loss = 0.35373075\n",
      "Iteration 31, loss = 0.34770291\n",
      "Iteration 32, loss = 0.34201565\n",
      "Iteration 33, loss = 0.33656185\n",
      "Iteration 34, loss = 0.33125724\n",
      "Iteration 35, loss = 0.32609074\n",
      "Iteration 36, loss = 0.32106224\n",
      "Iteration 37, loss = 0.31620302\n",
      "Iteration 38, loss = 0.31146713\n",
      "Iteration 39, loss = 0.30684665\n",
      "Iteration 40, loss = 0.30233540\n",
      "Iteration 41, loss = 0.29793256\n",
      "Iteration 42, loss = 0.29363247\n",
      "Iteration 43, loss = 0.28942634\n",
      "Iteration 44, loss = 0.28531830\n",
      "Iteration 45, loss = 0.28130291\n",
      "Iteration 46, loss = 0.27737617\n",
      "Iteration 47, loss = 0.27353770\n",
      "Iteration 48, loss = 0.26978165\n",
      "Iteration 49, loss = 0.26610675\n",
      "Iteration 50, loss = 0.26251193\n",
      "Iteration 51, loss = 0.25899435\n",
      "Iteration 52, loss = 0.25555307\n",
      "Iteration 53, loss = 0.25218719\n",
      "Iteration 54, loss = 0.24889695\n",
      "Iteration 55, loss = 0.24567950\n",
      "Iteration 56, loss = 0.24253274\n",
      "Iteration 57, loss = 0.23945556\n",
      "Iteration 58, loss = 0.23644617\n",
      "Iteration 59, loss = 0.23350341\n",
      "Iteration 60, loss = 0.23062629\n",
      "Iteration 61, loss = 0.22781504\n",
      "Iteration 62, loss = 0.22506440\n",
      "Iteration 63, loss = 0.22237705\n",
      "Iteration 64, loss = 0.21974941\n",
      "Iteration 65, loss = 0.21718159\n",
      "Iteration 66, loss = 0.21467171\n",
      "Iteration 67, loss = 0.21221840\n",
      "Iteration 68, loss = 0.20981921\n",
      "Iteration 69, loss = 0.20747549\n",
      "Iteration 70, loss = 0.20518271\n",
      "Iteration 71, loss = 0.20294172\n",
      "Iteration 72, loss = 0.20075103\n",
      "Iteration 73, loss = 0.19860972\n",
      "Iteration 74, loss = 0.19651531\n",
      "Iteration 75, loss = 0.19446851\n",
      "Iteration 76, loss = 0.19246658\n",
      "Iteration 77, loss = 0.19050975\n",
      "Iteration 78, loss = 0.18859600\n",
      "Iteration 79, loss = 0.18672441\n",
      "Iteration 80, loss = 0.18489499\n",
      "Iteration 81, loss = 0.18310477\n",
      "Iteration 82, loss = 0.18135463\n",
      "Iteration 83, loss = 0.17964319\n",
      "Iteration 84, loss = 0.17796847\n",
      "Iteration 85, loss = 0.17633014\n",
      "Iteration 86, loss = 0.17472784\n",
      "Iteration 87, loss = 0.17315974\n",
      "Iteration 88, loss = 0.17162515\n",
      "Iteration 89, loss = 0.17012498\n",
      "Iteration 90, loss = 0.16865621\n",
      "Iteration 91, loss = 0.16721842\n",
      "Iteration 92, loss = 0.16581208\n",
      "Iteration 93, loss = 0.16443409\n",
      "Iteration 94, loss = 0.16308577\n",
      "Iteration 95, loss = 0.16176558\n",
      "Iteration 96, loss = 0.16047317\n",
      "Iteration 97, loss = 0.15920777\n",
      "Iteration 98, loss = 0.15796858\n",
      "Iteration 99, loss = 0.15675536\n",
      "Iteration 100, loss = 0.15556658\n",
      "Iteration 101, loss = 0.15440238\n",
      "Iteration 102, loss = 0.15326100\n",
      "Iteration 103, loss = 0.15214292\n",
      "Iteration 104, loss = 0.15104656\n",
      "Iteration 105, loss = 0.14997281\n",
      "Iteration 106, loss = 0.14891972\n",
      "Iteration 107, loss = 0.14788768\n",
      "Iteration 108, loss = 0.14687506\n",
      "Iteration 109, loss = 0.14588264\n",
      "Iteration 110, loss = 0.14490939\n",
      "Iteration 111, loss = 0.14395452\n",
      "Iteration 112, loss = 0.14301834\n",
      "Iteration 113, loss = 0.14209918\n",
      "Iteration 114, loss = 0.14119838\n",
      "Iteration 115, loss = 0.14031326\n",
      "Iteration 116, loss = 0.13944568\n",
      "Iteration 117, loss = 0.13859327\n",
      "Iteration 118, loss = 0.13775704\n",
      "Iteration 119, loss = 0.13693583\n",
      "Iteration 120, loss = 0.13613000\n",
      "Iteration 121, loss = 0.13533888\n",
      "Iteration 122, loss = 0.13456220\n",
      "Iteration 123, loss = 0.13379936\n",
      "Iteration 124, loss = 0.13305042\n",
      "Iteration 125, loss = 0.13231447\n",
      "Iteration 126, loss = 0.13159143\n",
      "Iteration 127, loss = 0.13088126\n",
      "Iteration 128, loss = 0.13018340\n",
      "Iteration 129, loss = 0.12949758\n",
      "Iteration 130, loss = 0.12882330\n",
      "Iteration 131, loss = 0.12816060\n",
      "Iteration 132, loss = 0.12750913\n",
      "Iteration 133, loss = 0.12686920\n",
      "Iteration 134, loss = 0.12623940\n",
      "Iteration 135, loss = 0.12562034\n",
      "Iteration 136, loss = 0.12501159\n",
      "Iteration 137, loss = 0.12441274\n",
      "Iteration 138, loss = 0.12382380\n",
      "Iteration 139, loss = 0.12324433\n",
      "Iteration 140, loss = 0.12267420\n",
      "Iteration 141, loss = 0.12211316\n",
      "Iteration 142, loss = 0.12156117\n",
      "Iteration 143, loss = 0.12101779\n",
      "Iteration 144, loss = 0.12048297\n",
      "Iteration 145, loss = 0.11995661\n",
      "Iteration 146, loss = 0.11943831\n",
      "Iteration 147, loss = 0.11892817\n",
      "Iteration 148, loss = 0.11842579\n",
      "Iteration 149, loss = 0.11793112\n",
      "Iteration 150, loss = 0.11744393\n",
      "Iteration 151, loss = 0.11696416\n",
      "Iteration 152, loss = 0.11649158\n",
      "Iteration 153, loss = 0.11602604\n",
      "Iteration 154, loss = 0.11556749\n",
      "Iteration 155, loss = 0.11511565\n",
      "Iteration 156, loss = 0.11467040\n",
      "Iteration 157, loss = 0.11423162\n",
      "Iteration 158, loss = 0.11379919\n",
      "Iteration 159, loss = 0.11337302\n",
      "Iteration 160, loss = 0.11295288\n",
      "Iteration 161, loss = 0.11253890\n",
      "Iteration 162, loss = 0.11213062\n",
      "Iteration 163, loss = 0.11172808\n",
      "Iteration 164, loss = 0.11133119\n",
      "Iteration 165, loss = 0.11093977\n",
      "Iteration 166, loss = 0.11055379\n",
      "Iteration 167, loss = 0.11017310\n",
      "Iteration 168, loss = 0.10979755\n",
      "Iteration 169, loss = 0.10942712\n",
      "Iteration 170, loss = 0.10906165\n",
      "Iteration 171, loss = 0.10870109\n",
      "Iteration 172, loss = 0.10834530\n",
      "Iteration 173, loss = 0.10799423\n",
      "Iteration 174, loss = 0.10764784\n",
      "Iteration 175, loss = 0.10730608\n",
      "Iteration 176, loss = 0.10696863\n",
      "Iteration 177, loss = 0.10663556\n",
      "Iteration 178, loss = 0.10630680\n",
      "Iteration 179, loss = 0.10598222\n",
      "Iteration 180, loss = 0.10566177\n",
      "Iteration 181, loss = 0.10534564\n",
      "Iteration 182, loss = 0.10503316\n",
      "Iteration 183, loss = 0.10472468\n",
      "Iteration 184, loss = 0.10442005\n",
      "Iteration 185, loss = 0.10411916\n",
      "Iteration 186, loss = 0.10382202\n",
      "Iteration 187, loss = 0.10352867\n",
      "Iteration 188, loss = 0.10323876\n",
      "Iteration 189, loss = 0.10295236\n",
      "Iteration 190, loss = 0.10266937\n",
      "Iteration 191, loss = 0.10238978\n",
      "Iteration 192, loss = 0.10211352\n",
      "Iteration 193, loss = 0.10184073\n",
      "Iteration 194, loss = 0.10157100\n",
      "Iteration 195, loss = 0.10130442\n",
      "Iteration 196, loss = 0.10104093\n",
      "Iteration 197, loss = 0.10078075\n",
      "Iteration 198, loss = 0.10052328\n",
      "Iteration 199, loss = 0.10026887\n",
      "Iteration 200, loss = 0.10001734\n",
      "Iteration 201, loss = 0.09976889\n",
      "Iteration 202, loss = 0.09952302\n",
      "Iteration 203, loss = 0.09927993\n",
      "Iteration 204, loss = 0.09903960\n",
      "Iteration 205, loss = 0.09880204\n",
      "Iteration 206, loss = 0.09856706\n",
      "Iteration 207, loss = 0.09833461\n",
      "Iteration 208, loss = 0.09810470\n",
      "Iteration 209, loss = 0.09787741\n",
      "Iteration 210, loss = 0.09765258\n",
      "Iteration 211, loss = 0.09743009\n",
      "Iteration 212, loss = 0.09720998\n",
      "Iteration 213, loss = 0.09699237\n",
      "Iteration 214, loss = 0.09677696\n",
      "Iteration 215, loss = 0.09656382\n",
      "Iteration 216, loss = 0.09635291\n",
      "Iteration 217, loss = 0.09614445\n",
      "Iteration 218, loss = 0.09593798\n",
      "Iteration 219, loss = 0.09573356\n",
      "Iteration 220, loss = 0.09553133\n",
      "Iteration 221, loss = 0.09533125\n",
      "Iteration 222, loss = 0.09513313\n",
      "Iteration 223, loss = 0.09493687\n",
      "Iteration 224, loss = 0.09474288\n",
      "Iteration 225, loss = 0.09455052\n",
      "Iteration 226, loss = 0.09436016\n",
      "Iteration 227, loss = 0.09417195\n",
      "Iteration 228, loss = 0.09398534\n",
      "Iteration 229, loss = 0.09380063\n",
      "Iteration 230, loss = 0.09361770\n",
      "Iteration 231, loss = 0.09343672\n",
      "Iteration 232, loss = 0.09325734\n",
      "Iteration 233, loss = 0.09307970\n",
      "Iteration 234, loss = 0.09290397\n",
      "Iteration 235, loss = 0.09272979\n",
      "Iteration 236, loss = 0.09255720\n",
      "Iteration 237, loss = 0.09238662\n",
      "Iteration 238, loss = 0.09221717\n",
      "Iteration 239, loss = 0.09204968\n",
      "Iteration 240, loss = 0.09188361\n",
      "Iteration 241, loss = 0.09171909\n",
      "Iteration 242, loss = 0.09155623\n",
      "Iteration 243, loss = 0.09139466\n",
      "Iteration 244, loss = 0.09123462\n",
      "Iteration 245, loss = 0.09107625\n",
      "Iteration 246, loss = 0.09091910\n",
      "Iteration 247, loss = 0.09076341\n",
      "Iteration 248, loss = 0.09060935\n",
      "Iteration 249, loss = 0.09045640\n",
      "Iteration 250, loss = 0.09030497\n",
      "Iteration 251, loss = 0.09015497\n",
      "Iteration 252, loss = 0.09000616\n",
      "Iteration 253, loss = 0.08985849\n",
      "Iteration 254, loss = 0.08971232\n",
      "Iteration 255, loss = 0.08956724\n",
      "Iteration 256, loss = 0.08942347\n",
      "Iteration 257, loss = 0.08928109\n",
      "Iteration 258, loss = 0.08913975\n",
      "Iteration 259, loss = 0.08899959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.08886067\n",
      "Iteration 261, loss = 0.08872295\n",
      "Iteration 262, loss = 0.08858638\n",
      "Iteration 263, loss = 0.08845095\n",
      "Iteration 264, loss = 0.08831668\n",
      "Iteration 265, loss = 0.08818350\n",
      "Iteration 266, loss = 0.08805136\n",
      "Iteration 267, loss = 0.08792029\n",
      "Iteration 268, loss = 0.08779024\n",
      "Iteration 269, loss = 0.08766122\n",
      "Iteration 270, loss = 0.08753325\n",
      "Iteration 271, loss = 0.08740625\n",
      "Iteration 272, loss = 0.08728026\n",
      "Iteration 273, loss = 0.08715530\n",
      "Iteration 274, loss = 0.08703134\n",
      "Iteration 275, loss = 0.08690833\n",
      "Iteration 276, loss = 0.08678631\n",
      "Iteration 277, loss = 0.08666526\n",
      "Iteration 278, loss = 0.08654514\n",
      "Iteration 279, loss = 0.08642597\n",
      "Iteration 280, loss = 0.08630773\n",
      "Iteration 281, loss = 0.08619044\n",
      "Iteration 282, loss = 0.08607405\n",
      "Iteration 283, loss = 0.08595857\n",
      "Iteration 284, loss = 0.08584401\n",
      "Iteration 285, loss = 0.08573031\n",
      "Iteration 286, loss = 0.08561751\n",
      "Iteration 287, loss = 0.08550558\n",
      "Iteration 288, loss = 0.08539450\n",
      "Iteration 289, loss = 0.08528428\n",
      "Iteration 290, loss = 0.08517489\n",
      "Iteration 291, loss = 0.08506633\n",
      "Iteration 292, loss = 0.08495859\n",
      "Iteration 293, loss = 0.08485167\n",
      "Iteration 294, loss = 0.08474556\n",
      "Iteration 295, loss = 0.08464023\n",
      "Iteration 296, loss = 0.08453570\n",
      "Iteration 297, loss = 0.08443195\n",
      "Iteration 298, loss = 0.08432898\n",
      "Iteration 299, loss = 0.08422675\n",
      "Iteration 300, loss = 0.08412526\n",
      "Iteration 301, loss = 0.08402452\n",
      "Iteration 302, loss = 0.08392452\n",
      "Iteration 303, loss = 0.08382524\n",
      "Iteration 304, loss = 0.08372667\n",
      "Iteration 305, loss = 0.08362878\n",
      "Iteration 306, loss = 0.08353160\n",
      "Iteration 307, loss = 0.08343513\n",
      "Iteration 308, loss = 0.08333934\n",
      "Iteration 309, loss = 0.08324425\n",
      "Iteration 310, loss = 0.08314983\n",
      "Iteration 311, loss = 0.08305606\n",
      "Iteration 312, loss = 0.08296293\n",
      "Iteration 313, loss = 0.08287045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.14979834\n",
      "Iteration 3, loss = 0.96590973\n",
      "Iteration 4, loss = 0.86239058\n",
      "Iteration 5, loss = 0.85184405\n",
      "Iteration 6, loss = 0.82149425\n",
      "Iteration 7, loss = 0.75554220\n",
      "Iteration 8, loss = 0.69299591\n",
      "Iteration 9, loss = 0.64921708\n",
      "Iteration 10, loss = 0.61726045\n",
      "Iteration 11, loss = 0.59112404\n",
      "Iteration 12, loss = 0.56817051\n",
      "Iteration 13, loss = 0.54856232\n",
      "Iteration 14, loss = 0.52952543\n",
      "Iteration 15, loss = 0.51121000\n",
      "Iteration 16, loss = 0.49383799\n",
      "Iteration 17, loss = 0.47754655\n",
      "Iteration 18, loss = 0.46331670\n",
      "Iteration 19, loss = 0.45082347\n",
      "Iteration 20, loss = 0.43955188\n",
      "Iteration 21, loss = 0.42909054\n",
      "Iteration 22, loss = 0.41926267\n",
      "Iteration 23, loss = 0.40998814\n",
      "Iteration 24, loss = 0.40114931\n",
      "Iteration 25, loss = 0.39263774\n",
      "Iteration 26, loss = 0.38439759\n",
      "Iteration 27, loss = 0.37627557\n",
      "Iteration 28, loss = 0.36840914\n",
      "Iteration 29, loss = 0.36109710\n",
      "Iteration 30, loss = 0.35448541\n",
      "Iteration 31, loss = 0.34839151\n",
      "Iteration 32, loss = 0.34261117\n",
      "Iteration 33, loss = 0.33705652\n",
      "Iteration 34, loss = 0.33166403\n",
      "Iteration 35, loss = 0.32643127\n",
      "Iteration 36, loss = 0.32135274\n",
      "Iteration 37, loss = 0.31642184\n",
      "Iteration 38, loss = 0.31162752\n",
      "Iteration 39, loss = 0.30696137\n",
      "Iteration 40, loss = 0.30239744\n",
      "Iteration 41, loss = 0.29793696\n",
      "Iteration 42, loss = 0.29358367\n",
      "Iteration 43, loss = 0.28932920\n",
      "Iteration 44, loss = 0.28517347\n",
      "Iteration 45, loss = 0.28111297\n",
      "Iteration 46, loss = 0.27714275\n",
      "Iteration 47, loss = 0.27326305\n",
      "Iteration 48, loss = 0.26946867\n",
      "Iteration 49, loss = 0.26575781\n",
      "Iteration 50, loss = 0.26212992\n",
      "Iteration 51, loss = 0.25858091\n",
      "Iteration 52, loss = 0.25511124\n",
      "Iteration 53, loss = 0.25171861\n",
      "Iteration 54, loss = 0.24839901\n",
      "Iteration 55, loss = 0.24515283\n",
      "Iteration 56, loss = 0.24197840\n",
      "Iteration 57, loss = 0.23887319\n",
      "Iteration 58, loss = 0.23583695\n",
      "Iteration 59, loss = 0.23286684\n",
      "Iteration 60, loss = 0.22996229\n",
      "Iteration 61, loss = 0.22712201\n",
      "Iteration 62, loss = 0.22434586\n",
      "Iteration 63, loss = 0.22162906\n",
      "Iteration 64, loss = 0.21897344\n",
      "Iteration 65, loss = 0.21637601\n",
      "Iteration 66, loss = 0.21383629\n",
      "Iteration 67, loss = 0.21135294\n",
      "Iteration 68, loss = 0.20892478\n",
      "Iteration 69, loss = 0.20654961\n",
      "Iteration 70, loss = 0.20422615\n",
      "Iteration 71, loss = 0.20195339\n",
      "Iteration 72, loss = 0.19973033\n",
      "Iteration 73, loss = 0.19755645\n",
      "Iteration 74, loss = 0.19542951\n",
      "Iteration 75, loss = 0.19334828\n",
      "Iteration 76, loss = 0.19131202\n",
      "Iteration 77, loss = 0.18931976\n",
      "Iteration 78, loss = 0.18737016\n",
      "Iteration 79, loss = 0.18546302\n",
      "Iteration 80, loss = 0.18359650\n",
      "Iteration 81, loss = 0.18177041\n",
      "Iteration 82, loss = 0.17998317\n",
      "Iteration 83, loss = 0.17823387\n",
      "Iteration 84, loss = 0.17652176\n",
      "Iteration 85, loss = 0.17484536\n",
      "Iteration 86, loss = 0.17320396\n",
      "Iteration 87, loss = 0.17159709\n",
      "Iteration 88, loss = 0.17002442\n",
      "Iteration 89, loss = 0.16848419\n",
      "Iteration 90, loss = 0.16697554\n",
      "Iteration 91, loss = 0.16549769\n",
      "Iteration 92, loss = 0.16404973\n",
      "Iteration 93, loss = 0.16263088\n",
      "Iteration 94, loss = 0.16124052\n",
      "Iteration 95, loss = 0.15987793\n",
      "Iteration 96, loss = 0.15854269\n",
      "Iteration 97, loss = 0.15723414\n",
      "Iteration 98, loss = 0.15595146\n",
      "Iteration 99, loss = 0.15469408\n",
      "Iteration 100, loss = 0.15346150\n",
      "Iteration 101, loss = 0.15225262\n",
      "Iteration 102, loss = 0.15106695\n",
      "Iteration 103, loss = 0.14990391\n",
      "Iteration 104, loss = 0.14876298\n",
      "Iteration 105, loss = 0.14764375\n",
      "Iteration 106, loss = 0.14654556\n",
      "Iteration 107, loss = 0.14546844\n",
      "Iteration 108, loss = 0.14441163\n",
      "Iteration 109, loss = 0.14337462\n",
      "Iteration 110, loss = 0.14235677\n",
      "Iteration 111, loss = 0.14135721\n",
      "Iteration 112, loss = 0.14037575\n",
      "Iteration 113, loss = 0.13941226\n",
      "Iteration 114, loss = 0.13846609\n",
      "Iteration 115, loss = 0.13753712\n",
      "Iteration 116, loss = 0.13662471\n",
      "Iteration 117, loss = 0.13572842\n",
      "Iteration 118, loss = 0.13484784\n",
      "Iteration 119, loss = 0.13398273\n",
      "Iteration 120, loss = 0.13313269\n",
      "Iteration 121, loss = 0.13229737\n",
      "Iteration 122, loss = 0.13147634\n",
      "Iteration 123, loss = 0.13066926\n",
      "Iteration 124, loss = 0.12987582\n",
      "Iteration 125, loss = 0.12909571\n",
      "Iteration 126, loss = 0.12832868\n",
      "Iteration 127, loss = 0.12757429\n",
      "Iteration 128, loss = 0.12683239\n",
      "Iteration 129, loss = 0.12610265\n",
      "Iteration 130, loss = 0.12538487\n",
      "Iteration 131, loss = 0.12467867\n",
      "Iteration 132, loss = 0.12398384\n",
      "Iteration 133, loss = 0.12330008\n",
      "Iteration 134, loss = 0.12262716\n",
      "Iteration 135, loss = 0.12196485\n",
      "Iteration 136, loss = 0.12131289\n",
      "Iteration 137, loss = 0.12067107\n",
      "Iteration 138, loss = 0.12003927\n",
      "Iteration 139, loss = 0.11941719\n",
      "Iteration 140, loss = 0.11880474\n",
      "Iteration 141, loss = 0.11820153\n",
      "Iteration 142, loss = 0.11760746\n",
      "Iteration 143, loss = 0.11702225\n",
      "Iteration 144, loss = 0.11644578\n",
      "Iteration 145, loss = 0.11587788\n",
      "Iteration 146, loss = 0.11531833\n",
      "Iteration 147, loss = 0.11476690\n",
      "Iteration 148, loss = 0.11422358\n",
      "Iteration 149, loss = 0.11368805\n",
      "Iteration 150, loss = 0.11316021\n",
      "Iteration 151, loss = 0.11263991\n",
      "Iteration 152, loss = 0.11212696\n",
      "Iteration 153, loss = 0.11162125\n",
      "Iteration 154, loss = 0.11112263\n",
      "Iteration 155, loss = 0.11063089\n",
      "Iteration 156, loss = 0.11014604\n",
      "Iteration 157, loss = 0.10966795\n",
      "Iteration 158, loss = 0.10919638\n",
      "Iteration 159, loss = 0.10873120\n",
      "Iteration 160, loss = 0.10827235\n",
      "Iteration 161, loss = 0.10781962\n",
      "Iteration 162, loss = 0.10737295\n",
      "Iteration 163, loss = 0.10693220\n",
      "Iteration 164, loss = 0.10649727\n",
      "Iteration 165, loss = 0.10606805\n",
      "Iteration 166, loss = 0.10564444\n",
      "Iteration 167, loss = 0.10522632\n",
      "Iteration 168, loss = 0.10481359\n",
      "Iteration 169, loss = 0.10440613\n",
      "Iteration 170, loss = 0.10400386\n",
      "Iteration 171, loss = 0.10360669\n",
      "Iteration 172, loss = 0.10321453\n",
      "Iteration 173, loss = 0.10282728\n",
      "Iteration 174, loss = 0.10244496\n",
      "Iteration 175, loss = 0.10206741\n",
      "Iteration 176, loss = 0.10169445\n",
      "Iteration 177, loss = 0.10132604\n",
      "Iteration 178, loss = 0.10096213\n",
      "Iteration 179, loss = 0.10060264\n",
      "Iteration 180, loss = 0.10024752\n",
      "Iteration 181, loss = 0.09989662\n",
      "Iteration 182, loss = 0.09954994\n",
      "Iteration 183, loss = 0.09920735\n",
      "Iteration 184, loss = 0.09886883\n",
      "Iteration 185, loss = 0.09853426\n",
      "Iteration 186, loss = 0.09820360\n",
      "Iteration 187, loss = 0.09787681\n",
      "Iteration 188, loss = 0.09755377\n",
      "Iteration 189, loss = 0.09723448\n",
      "Iteration 190, loss = 0.09691893\n",
      "Iteration 191, loss = 0.09660702\n",
      "Iteration 192, loss = 0.09629879\n",
      "Iteration 193, loss = 0.09599401\n",
      "Iteration 194, loss = 0.09569263\n",
      "Iteration 195, loss = 0.09539461\n",
      "Iteration 196, loss = 0.09509982\n",
      "Iteration 197, loss = 0.09480827\n",
      "Iteration 198, loss = 0.09451991\n",
      "Iteration 199, loss = 0.09423469\n",
      "Iteration 200, loss = 0.09395256\n",
      "Iteration 201, loss = 0.09367347\n",
      "Iteration 202, loss = 0.09339738\n",
      "Iteration 203, loss = 0.09312425\n",
      "Iteration 204, loss = 0.09285403\n",
      "Iteration 205, loss = 0.09258670\n",
      "Iteration 206, loss = 0.09232217\n",
      "Iteration 207, loss = 0.09206040\n",
      "Iteration 208, loss = 0.09180137\n",
      "Iteration 209, loss = 0.09154506\n",
      "Iteration 210, loss = 0.09129139\n",
      "Iteration 211, loss = 0.09104033\n",
      "Iteration 212, loss = 0.09079183\n",
      "Iteration 213, loss = 0.09054586\n",
      "Iteration 214, loss = 0.09030230\n",
      "Iteration 215, loss = 0.09006114\n",
      "Iteration 216, loss = 0.08982219\n",
      "Iteration 217, loss = 0.08958517\n",
      "Iteration 218, loss = 0.08935030\n",
      "Iteration 219, loss = 0.08911755\n",
      "Iteration 220, loss = 0.08888694\n",
      "Iteration 221, loss = 0.08865842\n",
      "Iteration 222, loss = 0.08843195\n",
      "Iteration 223, loss = 0.08820738\n",
      "Iteration 224, loss = 0.08798473\n",
      "Iteration 225, loss = 0.08776408\n",
      "Iteration 226, loss = 0.08754598\n",
      "Iteration 227, loss = 0.08733086\n",
      "Iteration 228, loss = 0.08711774\n",
      "Iteration 229, loss = 0.08690662\n",
      "Iteration 230, loss = 0.08669748\n",
      "Iteration 231, loss = 0.08649083\n",
      "Iteration 232, loss = 0.08628610\n",
      "Iteration 233, loss = 0.08608329\n",
      "Iteration 234, loss = 0.08588208\n",
      "Iteration 235, loss = 0.08568247\n",
      "Iteration 236, loss = 0.08548450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 237, loss = 0.08528828\n",
      "Iteration 238, loss = 0.08509368\n",
      "Iteration 239, loss = 0.08490075\n",
      "Iteration 240, loss = 0.08470946\n",
      "Iteration 241, loss = 0.08451982\n",
      "Iteration 242, loss = 0.08433177\n",
      "Iteration 243, loss = 0.08414528\n",
      "Iteration 244, loss = 0.08396059\n",
      "Iteration 245, loss = 0.08377764\n",
      "Iteration 246, loss = 0.08359624\n",
      "Iteration 247, loss = 0.08341641\n",
      "Iteration 248, loss = 0.08323815\n",
      "Iteration 249, loss = 0.08306135\n",
      "Iteration 250, loss = 0.08288604\n",
      "Iteration 251, loss = 0.08271228\n",
      "Iteration 252, loss = 0.08254000\n",
      "Iteration 253, loss = 0.08236916\n",
      "Iteration 254, loss = 0.08219976\n",
      "Iteration 255, loss = 0.08203179\n",
      "Iteration 256, loss = 0.08186523\n",
      "Iteration 257, loss = 0.08170002\n",
      "Iteration 258, loss = 0.08153621\n",
      "Iteration 259, loss = 0.08137374\n",
      "Iteration 260, loss = 0.08121260\n",
      "Iteration 261, loss = 0.08105280\n",
      "Iteration 262, loss = 0.08089429\n",
      "Iteration 263, loss = 0.08073706\n",
      "Iteration 264, loss = 0.08058106\n",
      "Iteration 265, loss = 0.08042626\n",
      "Iteration 266, loss = 0.08027266\n",
      "Iteration 267, loss = 0.08012024\n",
      "Iteration 268, loss = 0.07996903\n",
      "Iteration 269, loss = 0.07981903\n",
      "Iteration 270, loss = 0.07967025\n",
      "Iteration 271, loss = 0.07952261\n",
      "Iteration 272, loss = 0.07937613\n",
      "Iteration 273, loss = 0.07923080\n",
      "Iteration 274, loss = 0.07908659\n",
      "Iteration 275, loss = 0.07894349\n",
      "Iteration 276, loss = 0.07880141\n",
      "Iteration 277, loss = 0.07866042\n",
      "Iteration 278, loss = 0.07852057\n",
      "Iteration 279, loss = 0.07838179\n",
      "Iteration 280, loss = 0.07824404\n",
      "Iteration 281, loss = 0.07810739\n",
      "Iteration 282, loss = 0.07797170\n",
      "Iteration 283, loss = 0.07783707\n",
      "Iteration 284, loss = 0.07770342\n",
      "Iteration 285, loss = 0.07757077\n",
      "Iteration 286, loss = 0.07743915\n",
      "Iteration 287, loss = 0.07730848\n",
      "Iteration 288, loss = 0.07717880\n",
      "Iteration 289, loss = 0.07705006\n",
      "Iteration 290, loss = 0.07692210\n",
      "Iteration 291, loss = 0.07679506\n",
      "Iteration 292, loss = 0.07666898\n",
      "Iteration 293, loss = 0.07654378\n",
      "Iteration 294, loss = 0.07641954\n",
      "Iteration 295, loss = 0.07629614\n",
      "Iteration 296, loss = 0.07617366\n",
      "Iteration 297, loss = 0.07605204\n",
      "Iteration 298, loss = 0.07593136\n",
      "Iteration 299, loss = 0.07581160\n",
      "Iteration 300, loss = 0.07569270\n",
      "Iteration 301, loss = 0.07557466\n",
      "Iteration 302, loss = 0.07545746\n",
      "Iteration 303, loss = 0.07534111\n",
      "Iteration 304, loss = 0.07522556\n",
      "Iteration 305, loss = 0.07511084\n",
      "Iteration 306, loss = 0.07499693\n",
      "Iteration 307, loss = 0.07488382\n",
      "Iteration 308, loss = 0.07477148\n",
      "Iteration 309, loss = 0.07465995\n",
      "Iteration 310, loss = 0.07454917\n",
      "Iteration 311, loss = 0.07443930\n",
      "Iteration 312, loss = 0.07433017\n",
      "Iteration 313, loss = 0.07422180\n",
      "Iteration 314, loss = 0.07411418\n",
      "Iteration 315, loss = 0.07400728\n",
      "Iteration 316, loss = 0.07390118\n",
      "Iteration 317, loss = 0.07379581\n",
      "Iteration 318, loss = 0.07369118\n",
      "Iteration 319, loss = 0.07358724\n",
      "Iteration 320, loss = 0.07348401\n",
      "Iteration 321, loss = 0.07338153\n",
      "Iteration 322, loss = 0.07327968\n",
      "Iteration 323, loss = 0.07317853\n",
      "Iteration 324, loss = 0.07307807\n",
      "Iteration 325, loss = 0.07297824\n",
      "Iteration 326, loss = 0.07287907\n",
      "Iteration 327, loss = 0.07278053\n",
      "Iteration 328, loss = 0.07268267\n",
      "Iteration 329, loss = 0.07258543\n",
      "Iteration 330, loss = 0.07248884\n",
      "Iteration 331, loss = 0.07239291\n",
      "Iteration 332, loss = 0.07229761\n",
      "Iteration 333, loss = 0.07220297\n",
      "Iteration 334, loss = 0.07210919\n",
      "Iteration 335, loss = 0.07201600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.14784789\n",
      "Iteration 3, loss = 0.96133715\n",
      "Iteration 4, loss = 0.85692983\n",
      "Iteration 5, loss = 0.84686874\n",
      "Iteration 6, loss = 0.81547565\n",
      "Iteration 7, loss = 0.74781547\n",
      "Iteration 8, loss = 0.68582014\n",
      "Iteration 9, loss = 0.64322893\n",
      "Iteration 10, loss = 0.61236716\n",
      "Iteration 11, loss = 0.58664077\n",
      "Iteration 12, loss = 0.56437157\n",
      "Iteration 13, loss = 0.54526318\n",
      "Iteration 14, loss = 0.52676800\n",
      "Iteration 15, loss = 0.50913678\n",
      "Iteration 16, loss = 0.49267735\n",
      "Iteration 17, loss = 0.47703043\n",
      "Iteration 18, loss = 0.46302808\n",
      "Iteration 19, loss = 0.45078720\n",
      "Iteration 20, loss = 0.44012280\n",
      "Iteration 21, loss = 0.43041020\n",
      "Iteration 22, loss = 0.42130710\n",
      "Iteration 23, loss = 0.41275601\n",
      "Iteration 24, loss = 0.40462355\n",
      "Iteration 25, loss = 0.39676398\n",
      "Iteration 26, loss = 0.38915847\n",
      "Iteration 27, loss = 0.38164108\n",
      "Iteration 28, loss = 0.37435490\n",
      "Iteration 29, loss = 0.36755400\n",
      "Iteration 30, loss = 0.36136637\n",
      "Iteration 31, loss = 0.35567233\n",
      "Iteration 32, loss = 0.35038118\n",
      "Iteration 33, loss = 0.34531872\n",
      "Iteration 34, loss = 0.34041378\n",
      "Iteration 35, loss = 0.33563795\n",
      "Iteration 36, loss = 0.33099751\n",
      "Iteration 37, loss = 0.32648310\n",
      "Iteration 38, loss = 0.32210381\n",
      "Iteration 39, loss = 0.31783844\n",
      "Iteration 40, loss = 0.31369318\n",
      "Iteration 41, loss = 0.30965425\n",
      "Iteration 42, loss = 0.30571208\n",
      "Iteration 43, loss = 0.30186529\n",
      "Iteration 44, loss = 0.29810544\n",
      "Iteration 45, loss = 0.29442979\n",
      "Iteration 46, loss = 0.29083485\n",
      "Iteration 47, loss = 0.28732203\n",
      "Iteration 48, loss = 0.28388522\n",
      "Iteration 49, loss = 0.28052286\n",
      "Iteration 50, loss = 0.27723369\n",
      "Iteration 51, loss = 0.27401866\n",
      "Iteration 52, loss = 0.27087461\n",
      "Iteration 53, loss = 0.26779801\n",
      "Iteration 54, loss = 0.26478855\n",
      "Iteration 55, loss = 0.26184489\n",
      "Iteration 56, loss = 0.25896614\n",
      "Iteration 57, loss = 0.25615124\n",
      "Iteration 58, loss = 0.25340209\n",
      "Iteration 59, loss = 0.25071208\n",
      "Iteration 60, loss = 0.24808069\n",
      "Iteration 61, loss = 0.24550679\n",
      "Iteration 62, loss = 0.24298955\n",
      "Iteration 63, loss = 0.24052671\n",
      "Iteration 64, loss = 0.23811982\n",
      "Iteration 65, loss = 0.23576428\n",
      "Iteration 66, loss = 0.23346006\n",
      "Iteration 67, loss = 0.23120511\n",
      "Iteration 68, loss = 0.22900092\n",
      "Iteration 69, loss = 0.22684496\n",
      "Iteration 70, loss = 0.22473637\n",
      "Iteration 71, loss = 0.22267418\n",
      "Iteration 72, loss = 0.22065594\n",
      "Iteration 73, loss = 0.21868236\n",
      "Iteration 74, loss = 0.21675045\n",
      "Iteration 75, loss = 0.21486099\n",
      "Iteration 76, loss = 0.21301231\n",
      "Iteration 77, loss = 0.21120315\n",
      "Iteration 78, loss = 0.20943350\n",
      "Iteration 79, loss = 0.20770228\n",
      "Iteration 80, loss = 0.20600800\n",
      "Iteration 81, loss = 0.20434939\n",
      "Iteration 82, loss = 0.20272580\n",
      "Iteration 83, loss = 0.20113630\n",
      "Iteration 84, loss = 0.19958083\n",
      "Iteration 85, loss = 0.19805746\n",
      "Iteration 86, loss = 0.19656562\n",
      "Iteration 87, loss = 0.19510628\n",
      "Iteration 88, loss = 0.19367573\n",
      "Iteration 89, loss = 0.19227634\n",
      "Iteration 90, loss = 0.19090523\n",
      "Iteration 91, loss = 0.18956187\n",
      "Iteration 92, loss = 0.18824669\n",
      "Iteration 93, loss = 0.18695900\n",
      "Iteration 94, loss = 0.18569733\n",
      "Iteration 95, loss = 0.18446209\n",
      "Iteration 96, loss = 0.18325043\n",
      "Iteration 97, loss = 0.18206429\n",
      "Iteration 98, loss = 0.18090110\n",
      "Iteration 99, loss = 0.17976069\n",
      "Iteration 100, loss = 0.17864411\n",
      "Iteration 101, loss = 0.17754763\n",
      "Iteration 102, loss = 0.17647427\n",
      "Iteration 103, loss = 0.17542083\n",
      "Iteration 104, loss = 0.17438776\n",
      "Iteration 105, loss = 0.17337534\n",
      "Iteration 106, loss = 0.17238155\n",
      "Iteration 107, loss = 0.17140796\n",
      "Iteration 108, loss = 0.17045208\n",
      "Iteration 109, loss = 0.16951511\n",
      "Iteration 110, loss = 0.16859540\n",
      "Iteration 111, loss = 0.16769331\n",
      "Iteration 112, loss = 0.16680768\n",
      "Iteration 113, loss = 0.16593863\n",
      "Iteration 114, loss = 0.16508544\n",
      "Iteration 115, loss = 0.16424797\n",
      "Iteration 116, loss = 0.16342555\n",
      "Iteration 117, loss = 0.16261790\n",
      "Iteration 118, loss = 0.16182496\n",
      "Iteration 119, loss = 0.16104583\n",
      "Iteration 120, loss = 0.16028102\n",
      "Iteration 121, loss = 0.15952906\n",
      "Iteration 122, loss = 0.15879098\n",
      "Iteration 123, loss = 0.15806496\n",
      "Iteration 124, loss = 0.15735231\n",
      "Iteration 125, loss = 0.15665131\n",
      "Iteration 126, loss = 0.15596282\n",
      "Iteration 127, loss = 0.15528546\n",
      "Iteration 128, loss = 0.15462005\n",
      "Iteration 129, loss = 0.15396551\n",
      "Iteration 130, loss = 0.15332192\n",
      "Iteration 131, loss = 0.15268936\n",
      "Iteration 132, loss = 0.15206673\n",
      "Iteration 133, loss = 0.15145567\n",
      "Iteration 134, loss = 0.15085386\n",
      "Iteration 135, loss = 0.15026233\n",
      "Iteration 136, loss = 0.14967987\n",
      "Iteration 137, loss = 0.14910688\n",
      "Iteration 138, loss = 0.14854342\n",
      "Iteration 139, loss = 0.14798848\n",
      "Iteration 140, loss = 0.14744287\n",
      "Iteration 141, loss = 0.14690515\n",
      "Iteration 142, loss = 0.14637667\n",
      "Iteration 143, loss = 0.14585576\n",
      "Iteration 144, loss = 0.14534347\n",
      "Iteration 145, loss = 0.14483860\n",
      "Iteration 146, loss = 0.14434215\n",
      "Iteration 147, loss = 0.14385248\n",
      "Iteration 148, loss = 0.14337083\n",
      "Iteration 149, loss = 0.14289590\n",
      "Iteration 150, loss = 0.14242824\n",
      "Iteration 151, loss = 0.14196755\n",
      "Iteration 152, loss = 0.14151366\n",
      "Iteration 153, loss = 0.14106661\n",
      "Iteration 154, loss = 0.14062581\n",
      "Iteration 155, loss = 0.14019201\n",
      "Iteration 156, loss = 0.13976378\n",
      "Iteration 157, loss = 0.13934212\n",
      "Iteration 158, loss = 0.13892644\n",
      "Iteration 159, loss = 0.13851644\n",
      "Iteration 160, loss = 0.13811235\n",
      "Iteration 161, loss = 0.13771402\n",
      "Iteration 162, loss = 0.13732101\n",
      "Iteration 163, loss = 0.13693367\n",
      "Iteration 164, loss = 0.13655149\n",
      "Iteration 165, loss = 0.13617464\n",
      "Iteration 166, loss = 0.13580320\n",
      "Iteration 167, loss = 0.13543648\n",
      "Iteration 168, loss = 0.13507472\n",
      "Iteration 169, loss = 0.13471787\n",
      "Iteration 170, loss = 0.13436599\n",
      "Iteration 171, loss = 0.13401856\n",
      "Iteration 172, loss = 0.13367569\n",
      "Iteration 173, loss = 0.13333730\n",
      "Iteration 174, loss = 0.13300344\n",
      "Iteration 175, loss = 0.13267395\n",
      "Iteration 176, loss = 0.13234860\n",
      "Iteration 177, loss = 0.13202742\n",
      "Iteration 178, loss = 0.13171028\n",
      "Iteration 179, loss = 0.13139770\n",
      "Iteration 180, loss = 0.13108832\n",
      "Iteration 181, loss = 0.13078312\n",
      "Iteration 182, loss = 0.13048180\n",
      "Iteration 183, loss = 0.13018419\n",
      "Iteration 184, loss = 0.12989026\n",
      "Iteration 185, loss = 0.12960014\n",
      "Iteration 186, loss = 0.12931330\n",
      "Iteration 187, loss = 0.12902998\n",
      "Iteration 188, loss = 0.12875009\n",
      "Iteration 189, loss = 0.12847357\n",
      "Iteration 190, loss = 0.12820033\n",
      "Iteration 191, loss = 0.12793042\n",
      "Iteration 192, loss = 0.12766366\n",
      "Iteration 193, loss = 0.12740008\n",
      "Iteration 194, loss = 0.12713964\n",
      "Iteration 195, loss = 0.12688214\n",
      "Iteration 196, loss = 0.12662766\n",
      "Iteration 197, loss = 0.12637607\n",
      "Iteration 198, loss = 0.12612740\n",
      "Iteration 199, loss = 0.12588153\n",
      "Iteration 200, loss = 0.12563847\n",
      "Iteration 201, loss = 0.12539811\n",
      "Iteration 202, loss = 0.12516051\n",
      "Iteration 203, loss = 0.12492553\n",
      "Iteration 204, loss = 0.12469317\n",
      "Iteration 205, loss = 0.12446338\n",
      "Iteration 206, loss = 0.12423609\n",
      "Iteration 207, loss = 0.12401130\n",
      "Iteration 208, loss = 0.12378896\n",
      "Iteration 209, loss = 0.12356911\n",
      "Iteration 210, loss = 0.12335153\n",
      "Iteration 211, loss = 0.12313631\n",
      "Iteration 212, loss = 0.12292336\n",
      "Iteration 213, loss = 0.12271266\n",
      "Iteration 214, loss = 0.12250426\n",
      "Iteration 215, loss = 0.12229813\n",
      "Iteration 216, loss = 0.12209405\n",
      "Iteration 217, loss = 0.12189207\n",
      "Iteration 218, loss = 0.12169218\n",
      "Iteration 219, loss = 0.12149434\n",
      "Iteration 220, loss = 0.12129853\n",
      "Iteration 221, loss = 0.12110481\n",
      "Iteration 222, loss = 0.12091302\n",
      "Iteration 223, loss = 0.12072314\n",
      "Iteration 224, loss = 0.12053517\n",
      "Iteration 225, loss = 0.12034907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 226, loss = 0.12016480\n",
      "Iteration 227, loss = 0.11998256\n",
      "Iteration 228, loss = 0.11980189\n",
      "Iteration 229, loss = 0.11962304\n",
      "Iteration 230, loss = 0.11944594\n",
      "Iteration 231, loss = 0.11927059\n",
      "Iteration 232, loss = 0.11909701\n",
      "Iteration 233, loss = 0.11892501\n",
      "Iteration 234, loss = 0.11875465\n",
      "Iteration 235, loss = 0.11858590\n",
      "Iteration 236, loss = 0.11841873\n",
      "Iteration 237, loss = 0.11825333\n",
      "Iteration 238, loss = 0.11808925\n",
      "Iteration 239, loss = 0.11792676\n",
      "Iteration 240, loss = 0.11776582\n",
      "Iteration 241, loss = 0.11760644\n",
      "Iteration 242, loss = 0.11744845\n",
      "Iteration 243, loss = 0.11729190\n",
      "Iteration 244, loss = 0.11713679\n",
      "Iteration 245, loss = 0.11698311\n",
      "Iteration 246, loss = 0.11683092\n",
      "Iteration 247, loss = 0.11667998\n",
      "Iteration 248, loss = 0.11653037\n",
      "Iteration 249, loss = 0.11638228\n",
      "Iteration 250, loss = 0.11623533\n",
      "Iteration 251, loss = 0.11608970\n",
      "Iteration 252, loss = 0.11594545\n",
      "Iteration 253, loss = 0.11580245\n",
      "Iteration 254, loss = 0.11566065\n",
      "Iteration 255, loss = 0.11552005\n",
      "Iteration 256, loss = 0.11538072\n",
      "Iteration 257, loss = 0.11524265\n",
      "Iteration 258, loss = 0.11510567\n",
      "Iteration 259, loss = 0.11496984\n",
      "Iteration 260, loss = 0.11483536\n",
      "Iteration 261, loss = 0.11470178\n",
      "Iteration 262, loss = 0.11456938\n",
      "Iteration 263, loss = 0.11443828\n",
      "Iteration 264, loss = 0.11430806\n",
      "Iteration 265, loss = 0.11417894\n",
      "Iteration 266, loss = 0.11405108\n",
      "Iteration 267, loss = 0.11392413\n",
      "Iteration 268, loss = 0.11379824\n",
      "Iteration 269, loss = 0.11367337\n",
      "Iteration 270, loss = 0.11354965\n",
      "Iteration 271, loss = 0.11342682\n",
      "Iteration 272, loss = 0.11330498\n",
      "Iteration 273, loss = 0.11318412\n",
      "Iteration 274, loss = 0.11306429\n",
      "Iteration 275, loss = 0.11294541\n",
      "Iteration 276, loss = 0.11282744\n",
      "Iteration 277, loss = 0.11271040\n",
      "Iteration 278, loss = 0.11259426\n",
      "Iteration 279, loss = 0.11247923\n",
      "Iteration 280, loss = 0.11236489\n",
      "Iteration 281, loss = 0.11225149\n",
      "Iteration 282, loss = 0.11213908\n",
      "Iteration 283, loss = 0.11202744\n",
      "Iteration 284, loss = 0.11191666\n",
      "Iteration 285, loss = 0.11180672\n",
      "Iteration 286, loss = 0.11169769\n",
      "Iteration 287, loss = 0.11158944\n",
      "Iteration 288, loss = 0.11148199\n",
      "Iteration 289, loss = 0.11137534\n",
      "Iteration 290, loss = 0.11126963\n",
      "Iteration 291, loss = 0.11116454\n",
      "Iteration 292, loss = 0.11106028\n",
      "Iteration 293, loss = 0.11095679\n",
      "Iteration 294, loss = 0.11085412\n",
      "Iteration 295, loss = 0.11075219\n",
      "Iteration 296, loss = 0.11065100\n",
      "Iteration 297, loss = 0.11055054\n",
      "Iteration 298, loss = 0.11045080\n",
      "Iteration 299, loss = 0.11035177\n",
      "Iteration 300, loss = 0.11025343\n",
      "Iteration 301, loss = 0.11015591\n",
      "Iteration 302, loss = 0.11005891\n",
      "Iteration 303, loss = 0.10996266\n",
      "Iteration 304, loss = 0.10986708\n",
      "Iteration 305, loss = 0.10977227\n",
      "Iteration 306, loss = 0.10967804\n",
      "Iteration 307, loss = 0.10958448\n",
      "Iteration 308, loss = 0.10949156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.31012045\n",
      "Iteration 3, loss = 1.26643198\n",
      "Iteration 4, loss = 1.22553533\n",
      "Iteration 5, loss = 1.18712214\n",
      "Iteration 6, loss = 1.15097675\n",
      "Iteration 7, loss = 1.11679489\n",
      "Iteration 8, loss = 1.08433352\n",
      "Iteration 9, loss = 1.05344777\n",
      "Iteration 10, loss = 1.02427548\n",
      "Iteration 11, loss = 0.99681347\n",
      "Iteration 12, loss = 0.97117390\n",
      "Iteration 13, loss = 0.94751401\n",
      "Iteration 14, loss = 0.92586927\n",
      "Iteration 15, loss = 0.90623024\n",
      "Iteration 16, loss = 0.88885227\n",
      "Iteration 17, loss = 0.87349786\n",
      "Iteration 18, loss = 0.86004620\n",
      "Iteration 19, loss = 0.84839160\n",
      "Iteration 20, loss = 0.83822994\n",
      "Iteration 21, loss = 0.82905945\n",
      "Iteration 22, loss = 0.82057335\n",
      "Iteration 23, loss = 0.81245782\n",
      "Iteration 24, loss = 0.80446602\n",
      "Iteration 25, loss = 0.79638196\n",
      "Iteration 26, loss = 0.78806801\n",
      "Iteration 27, loss = 0.77941877\n",
      "Iteration 28, loss = 0.77041333\n",
      "Iteration 29, loss = 0.76109467\n",
      "Iteration 30, loss = 0.75160524\n",
      "Iteration 31, loss = 0.74202020\n",
      "Iteration 32, loss = 0.73244855\n",
      "Iteration 33, loss = 0.72292116\n",
      "Iteration 34, loss = 0.71350466\n",
      "Iteration 35, loss = 0.70421640\n",
      "Iteration 36, loss = 0.69513376\n",
      "Iteration 37, loss = 0.68641513\n",
      "Iteration 38, loss = 0.67806956\n",
      "Iteration 39, loss = 0.67014411\n",
      "Iteration 40, loss = 0.66265601\n",
      "Iteration 41, loss = 0.65557571\n",
      "Iteration 42, loss = 0.64883384\n",
      "Iteration 43, loss = 0.64243012\n",
      "Iteration 44, loss = 0.63627714\n",
      "Iteration 45, loss = 0.63038459\n",
      "Iteration 46, loss = 0.62469962\n",
      "Iteration 47, loss = 0.61925227\n",
      "Iteration 48, loss = 0.61395477\n",
      "Iteration 49, loss = 0.60888300\n",
      "Iteration 50, loss = 0.60397344\n",
      "Iteration 51, loss = 0.59919672\n",
      "Iteration 52, loss = 0.59452269\n",
      "Iteration 53, loss = 0.58993470\n",
      "Iteration 54, loss = 0.58542794\n",
      "Iteration 55, loss = 0.58100203\n",
      "Iteration 56, loss = 0.57662405\n",
      "Iteration 57, loss = 0.57228637\n",
      "Iteration 58, loss = 0.56799256\n",
      "Iteration 59, loss = 0.56374494\n",
      "Iteration 60, loss = 0.55954763\n",
      "Iteration 61, loss = 0.55540745\n",
      "Iteration 62, loss = 0.55132692\n",
      "Iteration 63, loss = 0.54731366\n",
      "Iteration 64, loss = 0.54338542\n",
      "Iteration 65, loss = 0.53951319\n",
      "Iteration 66, loss = 0.53569350\n",
      "Iteration 67, loss = 0.53193592\n",
      "Iteration 68, loss = 0.52822215\n",
      "Iteration 69, loss = 0.52452936\n",
      "Iteration 70, loss = 0.52087693\n",
      "Iteration 71, loss = 0.51726644\n",
      "Iteration 72, loss = 0.51369911\n",
      "Iteration 73, loss = 0.51018173\n",
      "Iteration 74, loss = 0.50670789\n",
      "Iteration 75, loss = 0.50327645\n",
      "Iteration 76, loss = 0.49989361\n",
      "Iteration 77, loss = 0.49658436\n",
      "Iteration 78, loss = 0.49336358\n",
      "Iteration 79, loss = 0.49022221\n",
      "Iteration 80, loss = 0.48716928\n",
      "Iteration 81, loss = 0.48415943\n",
      "Iteration 82, loss = 0.48118915\n",
      "Iteration 83, loss = 0.47825081\n",
      "Iteration 84, loss = 0.47534530\n",
      "Iteration 85, loss = 0.47247786\n",
      "Iteration 86, loss = 0.46964727\n",
      "Iteration 87, loss = 0.46685094\n",
      "Iteration 88, loss = 0.46408853\n",
      "Iteration 89, loss = 0.46136141\n",
      "Iteration 90, loss = 0.45866843\n",
      "Iteration 91, loss = 0.45600899\n",
      "Iteration 92, loss = 0.45338581\n",
      "Iteration 93, loss = 0.45079675\n",
      "Iteration 94, loss = 0.44824126\n",
      "Iteration 95, loss = 0.44571693\n",
      "Iteration 96, loss = 0.44322236\n",
      "Iteration 97, loss = 0.44075719\n",
      "Iteration 98, loss = 0.43832067\n",
      "Iteration 99, loss = 0.43591061\n",
      "Iteration 100, loss = 0.43352615\n",
      "Iteration 101, loss = 0.43116729\n",
      "Iteration 102, loss = 0.42883270\n",
      "Iteration 103, loss = 0.42652205\n",
      "Iteration 104, loss = 0.42423499\n",
      "Iteration 105, loss = 0.42197431\n",
      "Iteration 106, loss = 0.41973690\n",
      "Iteration 107, loss = 0.41752205\n",
      "Iteration 108, loss = 0.41532929\n",
      "Iteration 109, loss = 0.41315813\n",
      "Iteration 110, loss = 0.41100804\n",
      "Iteration 111, loss = 0.40887905\n",
      "Iteration 112, loss = 0.40676944\n",
      "Iteration 113, loss = 0.40467829\n",
      "Iteration 114, loss = 0.40260541\n",
      "Iteration 115, loss = 0.40055197\n",
      "Iteration 116, loss = 0.39851634\n",
      "Iteration 117, loss = 0.39649726\n",
      "Iteration 118, loss = 0.39449460\n",
      "Iteration 119, loss = 0.39250759\n",
      "Iteration 120, loss = 0.39053582\n",
      "Iteration 121, loss = 0.38857903\n",
      "Iteration 122, loss = 0.38663709\n",
      "Iteration 123, loss = 0.38470979\n",
      "Iteration 124, loss = 0.38279691\n",
      "Iteration 125, loss = 0.38089812\n",
      "Iteration 126, loss = 0.37901222\n",
      "Iteration 127, loss = 0.37713895\n",
      "Iteration 128, loss = 0.37527824\n",
      "Iteration 129, loss = 0.37342987\n",
      "Iteration 130, loss = 0.37159342\n",
      "Iteration 131, loss = 0.36976847\n",
      "Iteration 132, loss = 0.36795449\n",
      "Iteration 133, loss = 0.36615113\n",
      "Iteration 134, loss = 0.36435847\n",
      "Iteration 135, loss = 0.36257726\n",
      "Iteration 136, loss = 0.36080707\n",
      "Iteration 137, loss = 0.35904694\n",
      "Iteration 138, loss = 0.35729721\n",
      "Iteration 139, loss = 0.35555713\n",
      "Iteration 140, loss = 0.35382598\n",
      "Iteration 141, loss = 0.35210433\n",
      "Iteration 142, loss = 0.35039215\n",
      "Iteration 143, loss = 0.34868935\n",
      "Iteration 144, loss = 0.34699583\n",
      "Iteration 145, loss = 0.34531101\n",
      "Iteration 146, loss = 0.34363504\n",
      "Iteration 147, loss = 0.34196779\n",
      "Iteration 148, loss = 0.34030940\n",
      "Iteration 149, loss = 0.33865966\n",
      "Iteration 150, loss = 0.33701817\n",
      "Iteration 151, loss = 0.33538488\n",
      "Iteration 152, loss = 0.33375978\n",
      "Iteration 153, loss = 0.33214327\n",
      "Iteration 154, loss = 0.33053529\n",
      "Iteration 155, loss = 0.32893498\n",
      "Iteration 156, loss = 0.32734225\n",
      "Iteration 157, loss = 0.32575758\n",
      "Iteration 158, loss = 0.32418093\n",
      "Iteration 159, loss = 0.32261198\n",
      "Iteration 160, loss = 0.32105064\n",
      "Iteration 161, loss = 0.31949710\n",
      "Iteration 162, loss = 0.31795158\n",
      "Iteration 163, loss = 0.31641353\n",
      "Iteration 164, loss = 0.31488305\n",
      "Iteration 165, loss = 0.31336030\n",
      "Iteration 166, loss = 0.31184520\n",
      "Iteration 167, loss = 0.31033775\n",
      "Iteration 168, loss = 0.30883779\n",
      "Iteration 169, loss = 0.30734540\n",
      "Iteration 170, loss = 0.30586036\n",
      "Iteration 171, loss = 0.30438263\n",
      "Iteration 172, loss = 0.30291228\n",
      "Iteration 173, loss = 0.30144957\n",
      "Iteration 174, loss = 0.29999443\n",
      "Iteration 175, loss = 0.29854757\n",
      "Iteration 176, loss = 0.29710807\n",
      "Iteration 177, loss = 0.29567626\n",
      "Iteration 178, loss = 0.29425182\n",
      "Iteration 179, loss = 0.29283585\n",
      "Iteration 180, loss = 0.29142919\n",
      "Iteration 181, loss = 0.29002989\n",
      "Iteration 182, loss = 0.28863796\n",
      "Iteration 183, loss = 0.28725351\n",
      "Iteration 184, loss = 0.28587623\n",
      "Iteration 185, loss = 0.28450487\n",
      "Iteration 186, loss = 0.28313630\n",
      "Iteration 187, loss = 0.28176789\n",
      "Iteration 188, loss = 0.28039735\n",
      "Iteration 189, loss = 0.27902586\n",
      "Iteration 190, loss = 0.27765478\n",
      "Iteration 191, loss = 0.27630673\n",
      "Iteration 192, loss = 0.27496945\n",
      "Iteration 193, loss = 0.27363989\n",
      "Iteration 194, loss = 0.27231240\n",
      "Iteration 195, loss = 0.27098613\n",
      "Iteration 196, loss = 0.26966186\n",
      "Iteration 197, loss = 0.26833961\n",
      "Iteration 198, loss = 0.26701874\n",
      "Iteration 199, loss = 0.26569863\n",
      "Iteration 200, loss = 0.26437901\n",
      "Iteration 201, loss = 0.26305988\n",
      "Iteration 202, loss = 0.26174165\n",
      "Iteration 203, loss = 0.26042428\n",
      "Iteration 204, loss = 0.25910816\n",
      "Iteration 205, loss = 0.25779349\n",
      "Iteration 206, loss = 0.25647941\n",
      "Iteration 207, loss = 0.25516785\n",
      "Iteration 208, loss = 0.25385757\n",
      "Iteration 209, loss = 0.25254954\n",
      "Iteration 210, loss = 0.25124559\n",
      "Iteration 211, loss = 0.24994423\n",
      "Iteration 212, loss = 0.24864556\n",
      "Iteration 213, loss = 0.24734991\n",
      "Iteration 214, loss = 0.24605743\n",
      "Iteration 215, loss = 0.24476829\n",
      "Iteration 216, loss = 0.24348293\n",
      "Iteration 217, loss = 0.24220111\n",
      "Iteration 218, loss = 0.24092380\n",
      "Iteration 219, loss = 0.23965072\n",
      "Iteration 220, loss = 0.23838194\n",
      "Iteration 221, loss = 0.23711785\n",
      "Iteration 222, loss = 0.23585880\n",
      "Iteration 223, loss = 0.23460546\n",
      "Iteration 224, loss = 0.23335797\n",
      "Iteration 225, loss = 0.23211743\n",
      "Iteration 226, loss = 0.23088326\n",
      "Iteration 227, loss = 0.22965525\n",
      "Iteration 228, loss = 0.22843358\n",
      "Iteration 229, loss = 0.22721860\n",
      "Iteration 230, loss = 0.22601049\n",
      "Iteration 231, loss = 0.22480931\n",
      "Iteration 232, loss = 0.22361516\n",
      "Iteration 233, loss = 0.22242885\n",
      "Iteration 234, loss = 0.22124930\n",
      "Iteration 235, loss = 0.22007758\n",
      "Iteration 236, loss = 0.21891346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 237, loss = 0.21775708\n",
      "Iteration 238, loss = 0.21660898\n",
      "Iteration 239, loss = 0.21546889\n",
      "Iteration 240, loss = 0.21433705\n",
      "Iteration 241, loss = 0.21321359\n",
      "Iteration 242, loss = 0.21209846\n",
      "Iteration 243, loss = 0.21099189\n",
      "Iteration 244, loss = 0.20989445\n",
      "Iteration 245, loss = 0.20880558\n",
      "Iteration 246, loss = 0.20772519\n",
      "Iteration 247, loss = 0.20665338\n",
      "Iteration 248, loss = 0.20559088\n",
      "Iteration 249, loss = 0.20453657\n",
      "Iteration 250, loss = 0.20349060\n",
      "Iteration 251, loss = 0.20245394\n",
      "Iteration 252, loss = 0.20142596\n",
      "Iteration 253, loss = 0.20040670\n",
      "Iteration 254, loss = 0.19939637\n",
      "Iteration 255, loss = 0.19839496\n",
      "Iteration 256, loss = 0.19740249\n",
      "Iteration 257, loss = 0.19641887\n",
      "Iteration 258, loss = 0.19544403\n",
      "Iteration 259, loss = 0.19447831\n",
      "Iteration 260, loss = 0.19352179\n",
      "Iteration 261, loss = 0.19257437\n",
      "Iteration 262, loss = 0.19163569\n",
      "Iteration 263, loss = 0.19070587\n",
      "Iteration 264, loss = 0.18978455\n",
      "Iteration 265, loss = 0.18887172\n",
      "Iteration 266, loss = 0.18796747\n",
      "Iteration 267, loss = 0.18707193\n",
      "Iteration 268, loss = 0.18618497\n",
      "Iteration 269, loss = 0.18530647\n",
      "Iteration 270, loss = 0.18443645\n",
      "Iteration 271, loss = 0.18357472\n",
      "Iteration 272, loss = 0.18272120\n",
      "Iteration 273, loss = 0.18187615\n",
      "Iteration 274, loss = 0.18103882\n",
      "Iteration 275, loss = 0.18021049\n",
      "Iteration 276, loss = 0.17938992\n",
      "Iteration 277, loss = 0.17857846\n",
      "Iteration 278, loss = 0.17777525\n",
      "Iteration 279, loss = 0.17697980\n",
      "Iteration 280, loss = 0.17619207\n",
      "Iteration 281, loss = 0.17541218\n",
      "Iteration 282, loss = 0.17464050\n",
      "Iteration 283, loss = 0.17387595\n",
      "Iteration 284, loss = 0.17311894\n",
      "Iteration 285, loss = 0.17236997\n",
      "Iteration 286, loss = 0.17162850\n",
      "Iteration 287, loss = 0.17089480\n",
      "Iteration 288, loss = 0.17016843\n",
      "Iteration 289, loss = 0.16944917\n",
      "Iteration 290, loss = 0.16873704\n",
      "Iteration 291, loss = 0.16803204\n",
      "Iteration 292, loss = 0.16733403\n",
      "Iteration 293, loss = 0.16664296\n",
      "Iteration 294, loss = 0.16595878\n",
      "Iteration 295, loss = 0.16528155\n",
      "Iteration 296, loss = 0.16461111\n",
      "Iteration 297, loss = 0.16394729\n",
      "Iteration 298, loss = 0.16328996\n",
      "Iteration 299, loss = 0.16263945\n",
      "Iteration 300, loss = 0.16199558\n",
      "Iteration 301, loss = 0.16135815\n",
      "Iteration 302, loss = 0.16072709\n",
      "Iteration 303, loss = 0.16010229\n",
      "Iteration 304, loss = 0.15948374\n",
      "Iteration 305, loss = 0.15887130\n",
      "Iteration 306, loss = 0.15826489\n",
      "Iteration 307, loss = 0.15766431\n",
      "Iteration 308, loss = 0.15706978\n",
      "Iteration 309, loss = 0.15648121\n",
      "Iteration 310, loss = 0.15589871\n",
      "Iteration 311, loss = 0.15532188\n",
      "Iteration 312, loss = 0.15475071\n",
      "Iteration 313, loss = 0.15418513\n",
      "Iteration 314, loss = 0.15362476\n",
      "Iteration 315, loss = 0.15306990\n",
      "Iteration 316, loss = 0.15252050\n",
      "Iteration 317, loss = 0.15197591\n",
      "Iteration 318, loss = 0.15143663\n",
      "Iteration 319, loss = 0.15090126\n",
      "Iteration 320, loss = 0.15037118\n",
      "Iteration 321, loss = 0.14984519\n",
      "Iteration 322, loss = 0.14932330\n",
      "Iteration 323, loss = 0.14880521\n",
      "Iteration 324, loss = 0.14829045\n",
      "Iteration 325, loss = 0.14777882\n",
      "Iteration 326, loss = 0.14726982\n",
      "Iteration 327, loss = 0.14676310\n",
      "Iteration 328, loss = 0.14625886\n",
      "Iteration 329, loss = 0.14575670\n",
      "Iteration 330, loss = 0.14525649\n",
      "Iteration 331, loss = 0.14475828\n",
      "Iteration 332, loss = 0.14426177\n",
      "Iteration 333, loss = 0.14376689\n",
      "Iteration 334, loss = 0.14327313\n",
      "Iteration 335, loss = 0.14277975\n",
      "Iteration 336, loss = 0.14229242\n",
      "Iteration 337, loss = 0.14179939\n",
      "Iteration 338, loss = 0.14131344\n",
      "Iteration 339, loss = 0.14082784\n",
      "Iteration 340, loss = 0.14034342\n",
      "Iteration 341, loss = 0.13986002\n",
      "Iteration 342, loss = 0.13937690\n",
      "Iteration 343, loss = 0.13889487\n",
      "Iteration 344, loss = 0.13841384\n",
      "Iteration 345, loss = 0.13793270\n",
      "Iteration 346, loss = 0.13745015\n",
      "Iteration 347, loss = 0.13696659\n",
      "Iteration 348, loss = 0.13648550\n",
      "Iteration 349, loss = 0.13601715\n",
      "Iteration 350, loss = 0.13553371\n",
      "Iteration 351, loss = 0.13505937\n",
      "Iteration 352, loss = 0.13459005\n",
      "Iteration 353, loss = 0.13411869\n",
      "Iteration 354, loss = 0.13364808\n",
      "Iteration 355, loss = 0.13317489\n",
      "Iteration 356, loss = 0.13270198\n",
      "Iteration 357, loss = 0.13223030\n",
      "Iteration 358, loss = 0.13176493\n",
      "Iteration 359, loss = 0.13129707\n",
      "Iteration 360, loss = 0.13082483\n",
      "Iteration 361, loss = 0.13035895\n",
      "Iteration 362, loss = 0.12989159\n",
      "Iteration 363, loss = 0.12942440\n",
      "Iteration 364, loss = 0.12895503\n",
      "Iteration 365, loss = 0.12848521\n",
      "Iteration 366, loss = 0.12801591\n",
      "Iteration 367, loss = 0.12754771\n",
      "Iteration 368, loss = 0.12708084\n",
      "Iteration 369, loss = 0.12661016\n",
      "Iteration 370, loss = 0.12614117\n",
      "Iteration 371, loss = 0.12567336\n",
      "Iteration 372, loss = 0.12520695\n",
      "Iteration 373, loss = 0.12474012\n",
      "Iteration 374, loss = 0.12427376\n",
      "Iteration 375, loss = 0.12380895\n",
      "Iteration 376, loss = 0.12334569\n",
      "Iteration 377, loss = 0.12288472\n",
      "Iteration 378, loss = 0.12242480\n",
      "Iteration 379, loss = 0.12196747\n",
      "Iteration 380, loss = 0.12151172\n",
      "Iteration 381, loss = 0.12105879\n",
      "Iteration 382, loss = 0.12060824\n",
      "Iteration 383, loss = 0.12016048\n",
      "Iteration 384, loss = 0.11971594\n",
      "Iteration 385, loss = 0.11927410\n",
      "Iteration 386, loss = 0.11883540\n",
      "Iteration 387, loss = 0.11839980\n",
      "Iteration 388, loss = 0.11796752\n",
      "Iteration 389, loss = 0.11753900\n",
      "Iteration 390, loss = 0.11711381\n",
      "Iteration 391, loss = 0.11669217\n",
      "Iteration 392, loss = 0.11627395\n",
      "Iteration 393, loss = 0.11585968\n",
      "Iteration 394, loss = 0.11544888\n",
      "Iteration 395, loss = 0.11504171\n",
      "Iteration 396, loss = 0.11463815\n",
      "Iteration 397, loss = 0.11423831\n",
      "Iteration 398, loss = 0.11384210\n",
      "Iteration 399, loss = 0.11344969\n",
      "Iteration 400, loss = 0.11306103\n",
      "Iteration 401, loss = 0.11267600\n",
      "Iteration 402, loss = 0.11229465\n",
      "Iteration 403, loss = 0.11191708\n",
      "Iteration 404, loss = 0.11154324\n",
      "Iteration 405, loss = 0.11117312\n",
      "Iteration 406, loss = 0.11080658\n",
      "Iteration 407, loss = 0.11044384\n",
      "Iteration 408, loss = 0.11008487\n",
      "Iteration 409, loss = 0.10972970\n",
      "Iteration 410, loss = 0.10937849\n",
      "Iteration 411, loss = 0.10903101\n",
      "Iteration 412, loss = 0.10868734\n",
      "Iteration 413, loss = 0.10834755\n",
      "Iteration 414, loss = 0.10801156\n",
      "Iteration 415, loss = 0.10767955\n",
      "Iteration 416, loss = 0.10735140\n",
      "Iteration 417, loss = 0.10702706\n",
      "Iteration 418, loss = 0.10670658\n",
      "Iteration 419, loss = 0.10639009\n",
      "Iteration 420, loss = 0.10607766\n",
      "Iteration 421, loss = 0.10576895\n",
      "Iteration 422, loss = 0.10546398\n",
      "Iteration 423, loss = 0.10516294\n",
      "Iteration 424, loss = 0.10486578\n",
      "Iteration 425, loss = 0.10457243\n",
      "Iteration 426, loss = 0.10428275\n",
      "Iteration 427, loss = 0.10399690\n",
      "Iteration 428, loss = 0.10371483\n",
      "Iteration 429, loss = 0.10343644\n",
      "Iteration 430, loss = 0.10316165\n",
      "Iteration 431, loss = 0.10289064\n",
      "Iteration 432, loss = 0.10262321\n",
      "Iteration 433, loss = 0.10235929\n",
      "Iteration 434, loss = 0.10209888\n",
      "Iteration 435, loss = 0.10184218\n",
      "Iteration 436, loss = 0.10158863\n",
      "Iteration 437, loss = 0.10133865\n",
      "Iteration 438, loss = 0.10109191\n",
      "Iteration 439, loss = 0.10084857\n",
      "Iteration 440, loss = 0.10060863\n",
      "Iteration 441, loss = 0.10037192\n",
      "Iteration 442, loss = 0.10013872\n",
      "Iteration 443, loss = 0.09990869\n",
      "Iteration 444, loss = 0.09968154\n",
      "Iteration 445, loss = 0.09945778\n",
      "Iteration 446, loss = 0.09923701\n",
      "Iteration 447, loss = 0.09901928\n",
      "Iteration 448, loss = 0.09880451\n",
      "Iteration 449, loss = 0.09859274\n",
      "Iteration 450, loss = 0.09838393\n",
      "Iteration 451, loss = 0.09817812\n",
      "Iteration 452, loss = 0.09797521\n",
      "Iteration 453, loss = 0.09777512\n",
      "Iteration 454, loss = 0.09757765\n",
      "Iteration 455, loss = 0.09738285\n",
      "Iteration 456, loss = 0.09719068\n",
      "Iteration 457, loss = 0.09700106\n",
      "Iteration 458, loss = 0.09681396\n",
      "Iteration 459, loss = 0.09662947\n",
      "Iteration 460, loss = 0.09644752\n",
      "Iteration 461, loss = 0.09626804\n",
      "Iteration 462, loss = 0.09609083\n",
      "Iteration 463, loss = 0.09591591\n",
      "Iteration 464, loss = 0.09574324\n",
      "Iteration 465, loss = 0.09557281\n",
      "Iteration 466, loss = 0.09540450\n",
      "Iteration 467, loss = 0.09523838\n",
      "Iteration 468, loss = 0.09507441\n",
      "Iteration 469, loss = 0.09491242\n",
      "Iteration 470, loss = 0.09475236\n",
      "Iteration 471, loss = 0.09459423\n",
      "Iteration 472, loss = 0.09443801\n",
      "Iteration 473, loss = 0.09428373\n",
      "Iteration 474, loss = 0.09413116\n",
      "Iteration 475, loss = 0.09398031\n",
      "Iteration 476, loss = 0.09383122\n",
      "Iteration 477, loss = 0.09368375\n",
      "Iteration 478, loss = 0.09353786\n",
      "Iteration 479, loss = 0.09339349\n",
      "Iteration 480, loss = 0.09325064\n",
      "Iteration 481, loss = 0.09310926\n",
      "Iteration 482, loss = 0.09296936\n",
      "Iteration 483, loss = 0.09283091\n",
      "Iteration 484, loss = 0.09269390\n",
      "Iteration 485, loss = 0.09255825\n",
      "Iteration 486, loss = 0.09242388\n",
      "Iteration 487, loss = 0.09229083\n",
      "Iteration 488, loss = 0.09215906\n",
      "Iteration 489, loss = 0.09202852\n",
      "Iteration 490, loss = 0.09189919\n",
      "Iteration 491, loss = 0.09177109\n",
      "Iteration 492, loss = 0.09164420\n",
      "Iteration 493, loss = 0.09151847\n",
      "Iteration 494, loss = 0.09139390\n",
      "Iteration 495, loss = 0.09127048\n",
      "Iteration 496, loss = 0.09114824\n",
      "Iteration 497, loss = 0.09102716\n",
      "Iteration 498, loss = 0.09090723\n",
      "Iteration 499, loss = 0.09078846\n",
      "Iteration 500, loss = 0.09067084\n",
      "Iteration 501, loss = 0.09055428\n",
      "Iteration 502, loss = 0.09043884\n",
      "Iteration 503, loss = 0.09032452\n",
      "Iteration 504, loss = 0.09021128\n",
      "Iteration 505, loss = 0.09009914\n",
      "Iteration 506, loss = 0.08998806\n",
      "Iteration 507, loss = 0.08987801\n",
      "Iteration 508, loss = 0.08976899\n",
      "Iteration 509, loss = 0.08966103\n",
      "Iteration 510, loss = 0.08955406\n",
      "Iteration 511, loss = 0.08944810\n",
      "Iteration 512, loss = 0.08934313\n",
      "Iteration 513, loss = 0.08923915\n",
      "Iteration 514, loss = 0.08913615\n",
      "Iteration 515, loss = 0.08903412\n",
      "Iteration 516, loss = 0.08893305\n",
      "Iteration 517, loss = 0.08883296\n",
      "Iteration 518, loss = 0.08873386\n",
      "Iteration 519, loss = 0.08863577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 520, loss = 0.08853865\n",
      "Iteration 521, loss = 0.08844248\n",
      "Iteration 522, loss = 0.08834725\n",
      "Iteration 523, loss = 0.08825301\n",
      "Iteration 524, loss = 0.08815982\n",
      "Iteration 525, loss = 0.08806763\n",
      "Iteration 526, loss = 0.08797646\n",
      "Iteration 527, loss = 0.08788636\n",
      "Iteration 528, loss = 0.08779756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.30676601\n",
      "Iteration 3, loss = 1.26399856\n",
      "Iteration 4, loss = 1.22395589\n",
      "Iteration 5, loss = 1.18632980\n",
      "Iteration 6, loss = 1.15085494\n",
      "Iteration 7, loss = 1.11725067\n",
      "Iteration 8, loss = 1.08524952\n",
      "Iteration 9, loss = 1.05477689\n",
      "Iteration 10, loss = 1.02595185\n",
      "Iteration 11, loss = 0.99872904\n",
      "Iteration 12, loss = 0.97326594\n",
      "Iteration 13, loss = 0.94975436\n",
      "Iteration 14, loss = 0.92826947\n",
      "Iteration 15, loss = 0.90888230\n",
      "Iteration 16, loss = 0.89171768\n",
      "Iteration 17, loss = 0.87656530\n",
      "Iteration 18, loss = 0.86334519\n",
      "Iteration 19, loss = 0.85196471\n",
      "Iteration 20, loss = 0.84206148\n",
      "Iteration 21, loss = 0.83308979\n",
      "Iteration 22, loss = 0.82477083\n",
      "Iteration 23, loss = 0.81675776\n",
      "Iteration 24, loss = 0.80881921\n",
      "Iteration 25, loss = 0.80075368\n",
      "Iteration 26, loss = 0.79241318\n",
      "Iteration 27, loss = 0.78370689\n",
      "Iteration 28, loss = 0.77463711\n",
      "Iteration 29, loss = 0.76522848\n",
      "Iteration 30, loss = 0.75563552\n",
      "Iteration 31, loss = 0.74593607\n",
      "Iteration 32, loss = 0.73617616\n",
      "Iteration 33, loss = 0.72645563\n",
      "Iteration 34, loss = 0.71695382\n",
      "Iteration 35, loss = 0.70756304\n",
      "Iteration 36, loss = 0.69842649\n",
      "Iteration 37, loss = 0.68955280\n",
      "Iteration 38, loss = 0.68105715\n",
      "Iteration 39, loss = 0.67296879\n",
      "Iteration 40, loss = 0.66536285\n",
      "Iteration 41, loss = 0.65820073\n",
      "Iteration 42, loss = 0.65136997\n",
      "Iteration 43, loss = 0.64482230\n",
      "Iteration 44, loss = 0.63856122\n",
      "Iteration 45, loss = 0.63253910\n",
      "Iteration 46, loss = 0.62679878\n",
      "Iteration 47, loss = 0.62126967\n",
      "Iteration 48, loss = 0.61601839\n",
      "Iteration 49, loss = 0.61096702\n",
      "Iteration 50, loss = 0.60606956\n",
      "Iteration 51, loss = 0.60127616\n",
      "Iteration 52, loss = 0.59657961\n",
      "Iteration 53, loss = 0.59196587\n",
      "Iteration 54, loss = 0.58741122\n",
      "Iteration 55, loss = 0.58288890\n",
      "Iteration 56, loss = 0.57840709\n",
      "Iteration 57, loss = 0.57396881\n",
      "Iteration 58, loss = 0.56957022\n",
      "Iteration 59, loss = 0.56521817\n",
      "Iteration 60, loss = 0.56091620\n",
      "Iteration 61, loss = 0.55666491\n",
      "Iteration 62, loss = 0.55248106\n",
      "Iteration 63, loss = 0.54836976\n",
      "Iteration 64, loss = 0.54432886\n",
      "Iteration 65, loss = 0.54036091\n",
      "Iteration 66, loss = 0.53644710\n",
      "Iteration 67, loss = 0.53258763\n",
      "Iteration 68, loss = 0.52878246\n",
      "Iteration 69, loss = 0.52501206\n",
      "Iteration 70, loss = 0.52128254\n",
      "Iteration 71, loss = 0.51759234\n",
      "Iteration 72, loss = 0.51394366\n",
      "Iteration 73, loss = 0.51034316\n",
      "Iteration 74, loss = 0.50678142\n",
      "Iteration 75, loss = 0.50326217\n",
      "Iteration 76, loss = 0.49978700\n",
      "Iteration 77, loss = 0.49639258\n",
      "Iteration 78, loss = 0.49307915\n",
      "Iteration 79, loss = 0.48983905\n",
      "Iteration 80, loss = 0.48667851\n",
      "Iteration 81, loss = 0.48356608\n",
      "Iteration 82, loss = 0.48049941\n",
      "Iteration 83, loss = 0.47747275\n",
      "Iteration 84, loss = 0.47448074\n",
      "Iteration 85, loss = 0.47152705\n",
      "Iteration 86, loss = 0.46861290\n",
      "Iteration 87, loss = 0.46573541\n",
      "Iteration 88, loss = 0.46289379\n",
      "Iteration 89, loss = 0.46008964\n",
      "Iteration 90, loss = 0.45732225\n",
      "Iteration 91, loss = 0.45458981\n",
      "Iteration 92, loss = 0.45189075\n",
      "Iteration 93, loss = 0.44922209\n",
      "Iteration 94, loss = 0.44658671\n",
      "Iteration 95, loss = 0.44398465\n",
      "Iteration 96, loss = 0.44141166\n",
      "Iteration 97, loss = 0.43886792\n",
      "Iteration 98, loss = 0.43635266\n",
      "Iteration 99, loss = 0.43386406\n",
      "Iteration 100, loss = 0.43140195\n",
      "Iteration 101, loss = 0.42896785\n",
      "Iteration 102, loss = 0.42655935\n",
      "Iteration 103, loss = 0.42417533\n",
      "Iteration 104, loss = 0.42181556\n",
      "Iteration 105, loss = 0.41948085\n",
      "Iteration 106, loss = 0.41716959\n",
      "Iteration 107, loss = 0.41488191\n",
      "Iteration 108, loss = 0.41261570\n",
      "Iteration 109, loss = 0.41037075\n",
      "Iteration 110, loss = 0.40814591\n",
      "Iteration 111, loss = 0.40594112\n",
      "Iteration 112, loss = 0.40375554\n",
      "Iteration 113, loss = 0.40158900\n",
      "Iteration 114, loss = 0.39944090\n",
      "Iteration 115, loss = 0.39731114\n",
      "Iteration 116, loss = 0.39519916\n",
      "Iteration 117, loss = 0.39310499\n",
      "Iteration 118, loss = 0.39102899\n",
      "Iteration 119, loss = 0.38896946\n",
      "Iteration 120, loss = 0.38692638\n",
      "Iteration 121, loss = 0.38489872\n",
      "Iteration 122, loss = 0.38288558\n",
      "Iteration 123, loss = 0.38088734\n",
      "Iteration 124, loss = 0.37890350\n",
      "Iteration 125, loss = 0.37693375\n",
      "Iteration 126, loss = 0.37497778\n",
      "Iteration 127, loss = 0.37303398\n",
      "Iteration 128, loss = 0.37110315\n",
      "Iteration 129, loss = 0.36918491\n",
      "Iteration 130, loss = 0.36727929\n",
      "Iteration 131, loss = 0.36538555\n",
      "Iteration 132, loss = 0.36350356\n",
      "Iteration 133, loss = 0.36163312\n",
      "Iteration 134, loss = 0.35977396\n",
      "Iteration 135, loss = 0.35792606\n",
      "Iteration 136, loss = 0.35608886\n",
      "Iteration 137, loss = 0.35426199\n",
      "Iteration 138, loss = 0.35244544\n",
      "Iteration 139, loss = 0.35063901\n",
      "Iteration 140, loss = 0.34884240\n",
      "Iteration 141, loss = 0.34705582\n",
      "Iteration 142, loss = 0.34527869\n",
      "Iteration 143, loss = 0.34351100\n",
      "Iteration 144, loss = 0.34175309\n",
      "Iteration 145, loss = 0.34000463\n",
      "Iteration 146, loss = 0.33826516\n",
      "Iteration 147, loss = 0.33653427\n",
      "Iteration 148, loss = 0.33481184\n",
      "Iteration 149, loss = 0.33309849\n",
      "Iteration 150, loss = 0.33139374\n",
      "Iteration 151, loss = 0.32969711\n",
      "Iteration 152, loss = 0.32800910\n",
      "Iteration 153, loss = 0.32632932\n",
      "Iteration 154, loss = 0.32465787\n",
      "Iteration 155, loss = 0.32299719\n",
      "Iteration 156, loss = 0.32134644\n",
      "Iteration 157, loss = 0.31970400\n",
      "Iteration 158, loss = 0.31807061\n",
      "Iteration 159, loss = 0.31644495\n",
      "Iteration 160, loss = 0.31482747\n",
      "Iteration 161, loss = 0.31321877\n",
      "Iteration 162, loss = 0.31161807\n",
      "Iteration 163, loss = 0.31002572\n",
      "Iteration 164, loss = 0.30844110\n",
      "Iteration 165, loss = 0.30686439\n",
      "Iteration 166, loss = 0.30529656\n",
      "Iteration 167, loss = 0.30373681\n",
      "Iteration 168, loss = 0.30218504\n",
      "Iteration 169, loss = 0.30064107\n",
      "Iteration 170, loss = 0.29910531\n",
      "Iteration 171, loss = 0.29757737\n",
      "Iteration 172, loss = 0.29605739\n",
      "Iteration 173, loss = 0.29454503\n",
      "Iteration 174, loss = 0.29304079\n",
      "Iteration 175, loss = 0.29154440\n",
      "Iteration 176, loss = 0.29005601\n",
      "Iteration 177, loss = 0.28857592\n",
      "Iteration 178, loss = 0.28710621\n",
      "Iteration 179, loss = 0.28564571\n",
      "Iteration 180, loss = 0.28419359\n",
      "Iteration 181, loss = 0.28274959\n",
      "Iteration 182, loss = 0.28131314\n",
      "Iteration 183, loss = 0.27988505\n",
      "Iteration 184, loss = 0.27846823\n",
      "Iteration 185, loss = 0.27705854\n",
      "Iteration 186, loss = 0.27565183\n",
      "Iteration 187, loss = 0.27424659\n",
      "Iteration 188, loss = 0.27284227\n",
      "Iteration 189, loss = 0.27143682\n",
      "Iteration 190, loss = 0.27003290\n",
      "Iteration 191, loss = 0.26865057\n",
      "Iteration 192, loss = 0.26728324\n",
      "Iteration 193, loss = 0.26592002\n",
      "Iteration 194, loss = 0.26456291\n",
      "Iteration 195, loss = 0.26320906\n",
      "Iteration 196, loss = 0.26185738\n",
      "Iteration 197, loss = 0.26050751\n",
      "Iteration 198, loss = 0.25916134\n",
      "Iteration 199, loss = 0.25781587\n",
      "Iteration 200, loss = 0.25647175\n",
      "Iteration 201, loss = 0.25512939\n",
      "Iteration 202, loss = 0.25378779\n",
      "Iteration 203, loss = 0.25244692\n",
      "Iteration 204, loss = 0.25110715\n",
      "Iteration 205, loss = 0.24976955\n",
      "Iteration 206, loss = 0.24843489\n",
      "Iteration 207, loss = 0.24710212\n",
      "Iteration 208, loss = 0.24577151\n",
      "Iteration 209, loss = 0.24444358\n",
      "Iteration 210, loss = 0.24311989\n",
      "Iteration 211, loss = 0.24179894\n",
      "Iteration 212, loss = 0.24048110\n",
      "Iteration 213, loss = 0.23916605\n",
      "Iteration 214, loss = 0.23785426\n",
      "Iteration 215, loss = 0.23654630\n",
      "Iteration 216, loss = 0.23524205\n",
      "Iteration 217, loss = 0.23394170\n",
      "Iteration 218, loss = 0.23264580\n",
      "Iteration 219, loss = 0.23135447\n",
      "Iteration 220, loss = 0.23006796\n",
      "Iteration 221, loss = 0.22878645\n",
      "Iteration 222, loss = 0.22751014\n",
      "Iteration 223, loss = 0.22623925\n",
      "Iteration 224, loss = 0.22497403\n",
      "Iteration 225, loss = 0.22371470\n",
      "Iteration 226, loss = 0.22246176\n",
      "Iteration 227, loss = 0.22121561\n",
      "Iteration 228, loss = 0.21997605\n",
      "Iteration 229, loss = 0.21874313\n",
      "Iteration 230, loss = 0.21751717\n",
      "Iteration 231, loss = 0.21629837\n",
      "Iteration 232, loss = 0.21508695\n",
      "Iteration 233, loss = 0.21388329\n",
      "Iteration 234, loss = 0.21268722\n",
      "Iteration 235, loss = 0.21149885\n",
      "Iteration 236, loss = 0.21031951\n",
      "Iteration 237, loss = 0.20914762\n",
      "Iteration 238, loss = 0.20798342\n",
      "Iteration 239, loss = 0.20682814\n",
      "Iteration 240, loss = 0.20568144\n",
      "Iteration 241, loss = 0.20454330\n",
      "Iteration 242, loss = 0.20341377\n",
      "Iteration 243, loss = 0.20229306\n",
      "Iteration 244, loss = 0.20118129\n",
      "Iteration 245, loss = 0.20007822\n",
      "Iteration 246, loss = 0.19898394\n",
      "Iteration 247, loss = 0.19789859\n",
      "Iteration 248, loss = 0.19682214\n",
      "Iteration 249, loss = 0.19575462\n",
      "Iteration 250, loss = 0.19469715\n",
      "Iteration 251, loss = 0.19364939\n",
      "Iteration 252, loss = 0.19261075\n",
      "Iteration 253, loss = 0.19158123\n",
      "Iteration 254, loss = 0.19056092\n",
      "Iteration 255, loss = 0.18955001\n",
      "Iteration 256, loss = 0.18854829\n",
      "Iteration 257, loss = 0.18755569\n",
      "Iteration 258, loss = 0.18657245\n",
      "Iteration 259, loss = 0.18559907\n",
      "Iteration 260, loss = 0.18463485\n",
      "Iteration 261, loss = 0.18367975\n",
      "Iteration 262, loss = 0.18273356\n",
      "Iteration 263, loss = 0.18179629\n",
      "Iteration 264, loss = 0.18086797\n",
      "Iteration 265, loss = 0.17994859\n",
      "Iteration 266, loss = 0.17903837\n",
      "Iteration 267, loss = 0.17813696\n",
      "Iteration 268, loss = 0.17724456\n",
      "Iteration 269, loss = 0.17636123\n",
      "Iteration 270, loss = 0.17548690\n",
      "Iteration 271, loss = 0.17462122\n",
      "Iteration 272, loss = 0.17376412\n",
      "Iteration 273, loss = 0.17291550\n",
      "Iteration 274, loss = 0.17207535\n",
      "Iteration 275, loss = 0.17124364\n",
      "Iteration 276, loss = 0.17042076\n",
      "Iteration 277, loss = 0.16960640\n",
      "Iteration 278, loss = 0.16880028\n",
      "Iteration 279, loss = 0.16800221\n",
      "Iteration 280, loss = 0.16721215\n",
      "Iteration 281, loss = 0.16643018\n",
      "Iteration 282, loss = 0.16565614\n",
      "Iteration 283, loss = 0.16489003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 284, loss = 0.16413165\n",
      "Iteration 285, loss = 0.16338097\n",
      "Iteration 286, loss = 0.16263793\n",
      "Iteration 287, loss = 0.16190325\n",
      "Iteration 288, loss = 0.16117578\n",
      "Iteration 289, loss = 0.16045562\n",
      "Iteration 290, loss = 0.15974290\n",
      "Iteration 291, loss = 0.15903783\n",
      "Iteration 292, loss = 0.15833985\n",
      "Iteration 293, loss = 0.15764906\n",
      "Iteration 294, loss = 0.15696543\n",
      "Iteration 295, loss = 0.15628893\n",
      "Iteration 296, loss = 0.15561959\n",
      "Iteration 297, loss = 0.15495712\n",
      "Iteration 298, loss = 0.15430140\n",
      "Iteration 299, loss = 0.15365227\n",
      "Iteration 300, loss = 0.15300989\n",
      "Iteration 301, loss = 0.15237426\n",
      "Iteration 302, loss = 0.15174493\n",
      "Iteration 303, loss = 0.15112219\n",
      "Iteration 304, loss = 0.15050580\n",
      "Iteration 305, loss = 0.14989580\n",
      "Iteration 306, loss = 0.14929227\n",
      "Iteration 307, loss = 0.14869494\n",
      "Iteration 308, loss = 0.14810342\n",
      "Iteration 309, loss = 0.14751825\n",
      "Iteration 310, loss = 0.14693901\n",
      "Iteration 311, loss = 0.14636615\n",
      "Iteration 312, loss = 0.14579830\n",
      "Iteration 313, loss = 0.14523671\n",
      "Iteration 314, loss = 0.14468062\n",
      "Iteration 315, loss = 0.14412999\n",
      "Iteration 316, loss = 0.14358446\n",
      "Iteration 317, loss = 0.14304458\n",
      "Iteration 318, loss = 0.14250889\n",
      "Iteration 319, loss = 0.14197816\n",
      "Iteration 320, loss = 0.14145266\n",
      "Iteration 321, loss = 0.14093050\n",
      "Iteration 322, loss = 0.14041213\n",
      "Iteration 323, loss = 0.13989756\n",
      "Iteration 324, loss = 0.13938657\n",
      "Iteration 325, loss = 0.13887843\n",
      "Iteration 326, loss = 0.13837330\n",
      "Iteration 327, loss = 0.13787047\n",
      "Iteration 328, loss = 0.13736980\n",
      "Iteration 329, loss = 0.13687197\n",
      "Iteration 330, loss = 0.13637585\n",
      "Iteration 331, loss = 0.13588162\n",
      "Iteration 332, loss = 0.13538924\n",
      "Iteration 333, loss = 0.13489927\n",
      "Iteration 334, loss = 0.13441102\n",
      "Iteration 335, loss = 0.13392436\n",
      "Iteration 336, loss = 0.13343930\n",
      "Iteration 337, loss = 0.13295455\n",
      "Iteration 338, loss = 0.13247796\n",
      "Iteration 339, loss = 0.13199428\n",
      "Iteration 340, loss = 0.13151906\n",
      "Iteration 341, loss = 0.13104430\n",
      "Iteration 342, loss = 0.13057042\n",
      "Iteration 343, loss = 0.13009688\n",
      "Iteration 344, loss = 0.12962414\n",
      "Iteration 345, loss = 0.12915985\n",
      "Iteration 346, loss = 0.12868962\n",
      "Iteration 347, loss = 0.12822615\n",
      "Iteration 348, loss = 0.12776613\n",
      "Iteration 349, loss = 0.12730615\n",
      "Iteration 350, loss = 0.12684701\n",
      "Iteration 351, loss = 0.12638875\n",
      "Iteration 352, loss = 0.12593277\n",
      "Iteration 353, loss = 0.12548151\n",
      "Iteration 354, loss = 0.12502727\n",
      "Iteration 355, loss = 0.12457684\n",
      "Iteration 356, loss = 0.12412800\n",
      "Iteration 357, loss = 0.12367885\n",
      "Iteration 358, loss = 0.12322928\n",
      "Iteration 359, loss = 0.12278342\n",
      "Iteration 360, loss = 0.12233549\n",
      "Iteration 361, loss = 0.12189081\n",
      "Iteration 362, loss = 0.12144516\n",
      "Iteration 363, loss = 0.12100055\n",
      "Iteration 364, loss = 0.12055617\n",
      "Iteration 365, loss = 0.12011296\n",
      "Iteration 366, loss = 0.11966834\n",
      "Iteration 367, loss = 0.11922561\n",
      "Iteration 368, loss = 0.11878364\n",
      "Iteration 369, loss = 0.11834227\n",
      "Iteration 370, loss = 0.11790037\n",
      "Iteration 371, loss = 0.11746033\n",
      "Iteration 372, loss = 0.11702095\n",
      "Iteration 373, loss = 0.11658405\n",
      "Iteration 374, loss = 0.11614784\n",
      "Iteration 375, loss = 0.11571383\n",
      "Iteration 376, loss = 0.11528036\n",
      "Iteration 377, loss = 0.11484967\n",
      "Iteration 378, loss = 0.11442156\n",
      "Iteration 379, loss = 0.11399508\n",
      "Iteration 380, loss = 0.11357124\n",
      "Iteration 381, loss = 0.11315054\n",
      "Iteration 382, loss = 0.11273270\n",
      "Iteration 383, loss = 0.11231714\n",
      "Iteration 384, loss = 0.11190420\n",
      "Iteration 385, loss = 0.11149470\n",
      "Iteration 386, loss = 0.11108862\n",
      "Iteration 387, loss = 0.11068542\n",
      "Iteration 388, loss = 0.11028580\n",
      "Iteration 389, loss = 0.10989005\n",
      "Iteration 390, loss = 0.10949769\n",
      "Iteration 391, loss = 0.10910921\n",
      "Iteration 392, loss = 0.10872456\n",
      "Iteration 393, loss = 0.10834400\n",
      "Iteration 394, loss = 0.10796771\n",
      "Iteration 395, loss = 0.10759524\n",
      "Iteration 396, loss = 0.10722702\n",
      "Iteration 397, loss = 0.10686297\n",
      "Iteration 398, loss = 0.10650309\n",
      "Iteration 399, loss = 0.10614798\n",
      "Iteration 400, loss = 0.10579710\n",
      "Iteration 401, loss = 0.10545063\n",
      "Iteration 402, loss = 0.10510866\n",
      "Iteration 403, loss = 0.10477130\n",
      "Iteration 404, loss = 0.10443818\n",
      "Iteration 405, loss = 0.10410968\n",
      "Iteration 406, loss = 0.10378590\n",
      "Iteration 407, loss = 0.10346661\n",
      "Iteration 408, loss = 0.10315166\n",
      "Iteration 409, loss = 0.10284154\n",
      "Iteration 410, loss = 0.10253582\n",
      "Iteration 411, loss = 0.10223448\n",
      "Iteration 412, loss = 0.10193729\n",
      "Iteration 413, loss = 0.10164450\n",
      "Iteration 414, loss = 0.10135598\n",
      "Iteration 415, loss = 0.10107153\n",
      "Iteration 416, loss = 0.10079114\n",
      "Iteration 417, loss = 0.10051480\n",
      "Iteration 418, loss = 0.10024230\n",
      "Iteration 419, loss = 0.09997356\n",
      "Iteration 420, loss = 0.09970846\n",
      "Iteration 421, loss = 0.09944691\n",
      "Iteration 422, loss = 0.09918904\n",
      "Iteration 423, loss = 0.09893439\n",
      "Iteration 424, loss = 0.09868293\n",
      "Iteration 425, loss = 0.09843458\n",
      "Iteration 426, loss = 0.09818944\n",
      "Iteration 427, loss = 0.09794727\n",
      "Iteration 428, loss = 0.09770805\n",
      "Iteration 429, loss = 0.09747163\n",
      "Iteration 430, loss = 0.09723794\n",
      "Iteration 431, loss = 0.09700699\n",
      "Iteration 432, loss = 0.09677867\n",
      "Iteration 433, loss = 0.09655299\n",
      "Iteration 434, loss = 0.09632985\n",
      "Iteration 435, loss = 0.09610931\n",
      "Iteration 436, loss = 0.09589127\n",
      "Iteration 437, loss = 0.09567569\n",
      "Iteration 438, loss = 0.09546252\n",
      "Iteration 439, loss = 0.09525175\n",
      "Iteration 440, loss = 0.09504332\n",
      "Iteration 441, loss = 0.09483729\n",
      "Iteration 442, loss = 0.09463367\n",
      "Iteration 443, loss = 0.09443226\n",
      "Iteration 444, loss = 0.09423322\n",
      "Iteration 445, loss = 0.09403641\n",
      "Iteration 446, loss = 0.09384182\n",
      "Iteration 447, loss = 0.09364951\n",
      "Iteration 448, loss = 0.09345942\n",
      "Iteration 449, loss = 0.09327170\n",
      "Iteration 450, loss = 0.09308627\n",
      "Iteration 451, loss = 0.09290295\n",
      "Iteration 452, loss = 0.09272184\n",
      "Iteration 453, loss = 0.09254270\n",
      "Iteration 454, loss = 0.09236573\n",
      "Iteration 455, loss = 0.09219090\n",
      "Iteration 456, loss = 0.09201808\n",
      "Iteration 457, loss = 0.09184727\n",
      "Iteration 458, loss = 0.09167845\n",
      "Iteration 459, loss = 0.09151161\n",
      "Iteration 460, loss = 0.09134677\n",
      "Iteration 461, loss = 0.09118391\n",
      "Iteration 462, loss = 0.09102297\n",
      "Iteration 463, loss = 0.09086392\n",
      "Iteration 464, loss = 0.09070676\n",
      "Iteration 465, loss = 0.09055148\n",
      "Iteration 466, loss = 0.09039816\n",
      "Iteration 467, loss = 0.09024673\n",
      "Iteration 468, loss = 0.09009714\n",
      "Iteration 469, loss = 0.08994923\n",
      "Iteration 470, loss = 0.08980305\n",
      "Iteration 471, loss = 0.08965860\n",
      "Iteration 472, loss = 0.08951587\n",
      "Iteration 473, loss = 0.08937479\n",
      "Iteration 474, loss = 0.08923535\n",
      "Iteration 475, loss = 0.08909747\n",
      "Iteration 476, loss = 0.08896115\n",
      "Iteration 477, loss = 0.08882646\n",
      "Iteration 478, loss = 0.08869337\n",
      "Iteration 479, loss = 0.08856168\n",
      "Iteration 480, loss = 0.08843145\n",
      "Iteration 481, loss = 0.08830263\n",
      "Iteration 482, loss = 0.08817515\n",
      "Iteration 483, loss = 0.08804897\n",
      "Iteration 484, loss = 0.08792408\n",
      "Iteration 485, loss = 0.08780043\n",
      "Iteration 486, loss = 0.08767798\n",
      "Iteration 487, loss = 0.08755670\n",
      "Iteration 488, loss = 0.08743656\n",
      "Iteration 489, loss = 0.08731751\n",
      "Iteration 490, loss = 0.08719955\n",
      "Iteration 491, loss = 0.08708262\n",
      "Iteration 492, loss = 0.08696675\n",
      "Iteration 493, loss = 0.08685190\n",
      "Iteration 494, loss = 0.08673792\n",
      "Iteration 495, loss = 0.08662496\n",
      "Iteration 496, loss = 0.08651296\n",
      "Iteration 497, loss = 0.08640189\n",
      "Iteration 498, loss = 0.08629174\n",
      "Iteration 499, loss = 0.08618252\n",
      "Iteration 500, loss = 0.08607419\n",
      "Iteration 501, loss = 0.08596677\n",
      "Iteration 502, loss = 0.08586024\n",
      "Iteration 503, loss = 0.08575459\n",
      "Iteration 504, loss = 0.08564983\n",
      "Iteration 505, loss = 0.08554594\n",
      "Iteration 506, loss = 0.08544299\n",
      "Iteration 507, loss = 0.08534101\n",
      "Iteration 508, loss = 0.08523991\n",
      "Iteration 509, loss = 0.08513972\n",
      "Iteration 510, loss = 0.08504051\n",
      "Iteration 511, loss = 0.08494231\n",
      "Iteration 512, loss = 0.08484494\n",
      "Iteration 513, loss = 0.08474840\n",
      "Iteration 514, loss = 0.08465265\n",
      "Iteration 515, loss = 0.08455773\n",
      "Iteration 516, loss = 0.08446359\n",
      "Iteration 517, loss = 0.08437036\n",
      "Iteration 518, loss = 0.08427798\n",
      "Iteration 519, loss = 0.08418651\n",
      "Iteration 520, loss = 0.08409593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.30252841\n",
      "Iteration 3, loss = 1.25913051\n",
      "Iteration 4, loss = 1.21849216\n",
      "Iteration 5, loss = 1.18031501\n",
      "Iteration 6, loss = 1.14435427\n",
      "Iteration 7, loss = 1.11032530\n",
      "Iteration 8, loss = 1.07801223\n",
      "Iteration 9, loss = 1.04723159\n",
      "Iteration 10, loss = 1.01806192\n",
      "Iteration 11, loss = 0.99059355\n",
      "Iteration 12, loss = 0.96492362\n",
      "Iteration 13, loss = 0.94122341\n",
      "Iteration 14, loss = 0.91954831\n",
      "Iteration 15, loss = 0.89992001\n",
      "Iteration 16, loss = 0.88248265\n",
      "Iteration 17, loss = 0.86703508\n",
      "Iteration 18, loss = 0.85353556\n",
      "Iteration 19, loss = 0.84184803\n",
      "Iteration 20, loss = 0.83165042\n",
      "Iteration 21, loss = 0.82243328\n",
      "Iteration 22, loss = 0.81388723\n",
      "Iteration 23, loss = 0.80571307\n",
      "Iteration 24, loss = 0.79765053\n",
      "Iteration 25, loss = 0.78949063\n",
      "Iteration 26, loss = 0.78110523\n",
      "Iteration 27, loss = 0.77237712\n",
      "Iteration 28, loss = 0.76332802\n",
      "Iteration 29, loss = 0.75398122\n",
      "Iteration 30, loss = 0.74441825\n",
      "Iteration 31, loss = 0.73468724\n",
      "Iteration 32, loss = 0.72489665\n",
      "Iteration 33, loss = 0.71515208\n",
      "Iteration 34, loss = 0.70556293\n",
      "Iteration 35, loss = 0.69610783\n",
      "Iteration 36, loss = 0.68690381\n",
      "Iteration 37, loss = 0.67802132\n",
      "Iteration 38, loss = 0.66956176\n",
      "Iteration 39, loss = 0.66154306\n",
      "Iteration 40, loss = 0.65392145\n",
      "Iteration 41, loss = 0.64666920\n",
      "Iteration 42, loss = 0.63972107\n",
      "Iteration 43, loss = 0.63312086\n",
      "Iteration 44, loss = 0.62679133\n",
      "Iteration 45, loss = 0.62075550\n",
      "Iteration 46, loss = 0.61495047\n",
      "Iteration 47, loss = 0.60936132\n",
      "Iteration 48, loss = 0.60398673\n",
      "Iteration 49, loss = 0.59883500\n",
      "Iteration 50, loss = 0.59383436\n",
      "Iteration 51, loss = 0.58900345\n",
      "Iteration 52, loss = 0.58427357\n",
      "Iteration 53, loss = 0.57964998\n",
      "Iteration 54, loss = 0.57510713\n",
      "Iteration 55, loss = 0.57061977\n",
      "Iteration 56, loss = 0.56617616\n",
      "Iteration 57, loss = 0.56176999\n",
      "Iteration 58, loss = 0.55740204\n",
      "Iteration 59, loss = 0.55307635\n",
      "Iteration 60, loss = 0.54879832\n",
      "Iteration 61, loss = 0.54457189\n",
      "Iteration 62, loss = 0.54040482\n",
      "Iteration 63, loss = 0.53629528\n",
      "Iteration 64, loss = 0.53225133\n",
      "Iteration 65, loss = 0.52829029\n",
      "Iteration 66, loss = 0.52440361\n",
      "Iteration 67, loss = 0.52059331\n",
      "Iteration 68, loss = 0.51685643\n",
      "Iteration 69, loss = 0.51318801\n",
      "Iteration 70, loss = 0.50957961\n",
      "Iteration 71, loss = 0.50603632\n",
      "Iteration 72, loss = 0.50255533\n",
      "Iteration 73, loss = 0.49912982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74, loss = 0.49575617\n",
      "Iteration 75, loss = 0.49242851\n",
      "Iteration 76, loss = 0.48914468\n",
      "Iteration 77, loss = 0.48589536\n",
      "Iteration 78, loss = 0.48267061\n",
      "Iteration 79, loss = 0.47947105\n",
      "Iteration 80, loss = 0.47627711\n",
      "Iteration 81, loss = 0.47310022\n",
      "Iteration 82, loss = 0.46994526\n",
      "Iteration 83, loss = 0.46681353\n",
      "Iteration 84, loss = 0.46370586\n",
      "Iteration 85, loss = 0.46062365\n",
      "Iteration 86, loss = 0.45757837\n",
      "Iteration 87, loss = 0.45460017\n",
      "Iteration 88, loss = 0.45170182\n",
      "Iteration 89, loss = 0.44887341\n",
      "Iteration 90, loss = 0.44609812\n",
      "Iteration 91, loss = 0.44334870\n",
      "Iteration 92, loss = 0.44063105\n",
      "Iteration 93, loss = 0.43794098\n",
      "Iteration 94, loss = 0.43527349\n",
      "Iteration 95, loss = 0.43263065\n",
      "Iteration 96, loss = 0.43001171\n",
      "Iteration 97, loss = 0.42741664\n",
      "Iteration 98, loss = 0.42485047\n",
      "Iteration 99, loss = 0.42230896\n",
      "Iteration 100, loss = 0.41979173\n",
      "Iteration 101, loss = 0.41729727\n",
      "Iteration 102, loss = 0.41482637\n",
      "Iteration 103, loss = 0.41238066\n",
      "Iteration 104, loss = 0.40995868\n",
      "Iteration 105, loss = 0.40755939\n",
      "Iteration 106, loss = 0.40518175\n",
      "Iteration 107, loss = 0.40282545\n",
      "Iteration 108, loss = 0.40049103\n",
      "Iteration 109, loss = 0.39817713\n",
      "Iteration 110, loss = 0.39588176\n",
      "Iteration 111, loss = 0.39360544\n",
      "Iteration 112, loss = 0.39134692\n",
      "Iteration 113, loss = 0.38910580\n",
      "Iteration 114, loss = 0.38688196\n",
      "Iteration 115, loss = 0.38467496\n",
      "Iteration 116, loss = 0.38248465\n",
      "Iteration 117, loss = 0.38031044\n",
      "Iteration 118, loss = 0.37815262\n",
      "Iteration 119, loss = 0.37601107\n",
      "Iteration 120, loss = 0.37388594\n",
      "Iteration 121, loss = 0.37177607\n",
      "Iteration 122, loss = 0.36967998\n",
      "Iteration 123, loss = 0.36759844\n",
      "Iteration 124, loss = 0.36553001\n",
      "Iteration 125, loss = 0.36347516\n",
      "Iteration 126, loss = 0.36143259\n",
      "Iteration 127, loss = 0.35940326\n",
      "Iteration 128, loss = 0.35738640\n",
      "Iteration 129, loss = 0.35538156\n",
      "Iteration 130, loss = 0.35338853\n",
      "Iteration 131, loss = 0.35140744\n",
      "Iteration 132, loss = 0.34943795\n",
      "Iteration 133, loss = 0.34747928\n",
      "Iteration 134, loss = 0.34553125\n",
      "Iteration 135, loss = 0.34359363\n",
      "Iteration 136, loss = 0.34166656\n",
      "Iteration 137, loss = 0.33974971\n",
      "Iteration 138, loss = 0.33784356\n",
      "Iteration 139, loss = 0.33594754\n",
      "Iteration 140, loss = 0.33406313\n",
      "Iteration 141, loss = 0.33219095\n",
      "Iteration 142, loss = 0.33032846\n",
      "Iteration 143, loss = 0.32847575\n",
      "Iteration 144, loss = 0.32663312\n",
      "Iteration 145, loss = 0.32480020\n",
      "Iteration 146, loss = 0.32297667\n",
      "Iteration 147, loss = 0.32116271\n",
      "Iteration 148, loss = 0.31935832\n",
      "Iteration 149, loss = 0.31756344\n",
      "Iteration 150, loss = 0.31577749\n",
      "Iteration 151, loss = 0.31400165\n",
      "Iteration 152, loss = 0.31223911\n",
      "Iteration 153, loss = 0.31048573\n",
      "Iteration 154, loss = 0.30874161\n",
      "Iteration 155, loss = 0.30700683\n",
      "Iteration 156, loss = 0.30528162\n",
      "Iteration 157, loss = 0.30356546\n",
      "Iteration 158, loss = 0.30185855\n",
      "Iteration 159, loss = 0.30016084\n",
      "Iteration 160, loss = 0.29847255\n",
      "Iteration 161, loss = 0.29679332\n",
      "Iteration 162, loss = 0.29512302\n",
      "Iteration 163, loss = 0.29346259\n",
      "Iteration 164, loss = 0.29181097\n",
      "Iteration 165, loss = 0.29016836\n",
      "Iteration 166, loss = 0.28853437\n",
      "Iteration 167, loss = 0.28690946\n",
      "Iteration 168, loss = 0.28529412\n",
      "Iteration 169, loss = 0.28368786\n",
      "Iteration 170, loss = 0.28209069\n",
      "Iteration 171, loss = 0.28050269\n",
      "Iteration 172, loss = 0.27892365\n",
      "Iteration 173, loss = 0.27735322\n",
      "Iteration 174, loss = 0.27579282\n",
      "Iteration 175, loss = 0.27424365\n",
      "Iteration 176, loss = 0.27270345\n",
      "Iteration 177, loss = 0.27117219\n",
      "Iteration 178, loss = 0.26964996\n",
      "Iteration 179, loss = 0.26813711\n",
      "Iteration 180, loss = 0.26663238\n",
      "Iteration 181, loss = 0.26513651\n",
      "Iteration 182, loss = 0.26364975\n",
      "Iteration 183, loss = 0.26217433\n",
      "Iteration 184, loss = 0.26070846\n",
      "Iteration 185, loss = 0.25924860\n",
      "Iteration 186, loss = 0.25779458\n",
      "Iteration 187, loss = 0.25634546\n",
      "Iteration 188, loss = 0.25489375\n",
      "Iteration 189, loss = 0.25344302\n",
      "Iteration 190, loss = 0.25199674\n",
      "Iteration 191, loss = 0.25057364\n",
      "Iteration 192, loss = 0.24915920\n",
      "Iteration 193, loss = 0.24774816\n",
      "Iteration 194, loss = 0.24633992\n",
      "Iteration 195, loss = 0.24493393\n",
      "Iteration 196, loss = 0.24353166\n",
      "Iteration 197, loss = 0.24213149\n",
      "Iteration 198, loss = 0.24073421\n",
      "Iteration 199, loss = 0.23934002\n",
      "Iteration 200, loss = 0.23794765\n",
      "Iteration 201, loss = 0.23655708\n",
      "Iteration 202, loss = 0.23516835\n",
      "Iteration 203, loss = 0.23378150\n",
      "Iteration 204, loss = 0.23239671\n",
      "Iteration 205, loss = 0.23101430\n",
      "Iteration 206, loss = 0.22963413\n",
      "Iteration 207, loss = 0.22825630\n",
      "Iteration 208, loss = 0.22688115\n",
      "Iteration 209, loss = 0.22550905\n",
      "Iteration 210, loss = 0.22414004\n",
      "Iteration 211, loss = 0.22277441\n",
      "Iteration 212, loss = 0.22141391\n",
      "Iteration 213, loss = 0.22005833\n",
      "Iteration 214, loss = 0.21870730\n",
      "Iteration 215, loss = 0.21736072\n",
      "Iteration 216, loss = 0.21601883\n",
      "Iteration 217, loss = 0.21468227\n",
      "Iteration 218, loss = 0.21335103\n",
      "Iteration 219, loss = 0.21202535\n",
      "Iteration 220, loss = 0.21070544\n",
      "Iteration 221, loss = 0.20939167\n",
      "Iteration 222, loss = 0.20808416\n",
      "Iteration 223, loss = 0.20678324\n",
      "Iteration 224, loss = 0.20548912\n",
      "Iteration 225, loss = 0.20420199\n",
      "Iteration 226, loss = 0.20292223\n",
      "Iteration 227, loss = 0.20164983\n",
      "Iteration 228, loss = 0.20038499\n",
      "Iteration 229, loss = 0.19912852\n",
      "Iteration 230, loss = 0.19788081\n",
      "Iteration 231, loss = 0.19664044\n",
      "Iteration 232, loss = 0.19540779\n",
      "Iteration 233, loss = 0.19418417\n",
      "Iteration 234, loss = 0.19296914\n",
      "Iteration 235, loss = 0.19176286\n",
      "Iteration 236, loss = 0.19056571\n",
      "Iteration 237, loss = 0.18937745\n",
      "Iteration 238, loss = 0.18819817\n",
      "Iteration 239, loss = 0.18702894\n",
      "Iteration 240, loss = 0.18586917\n",
      "Iteration 241, loss = 0.18471883\n",
      "Iteration 242, loss = 0.18357792\n",
      "Iteration 243, loss = 0.18244635\n",
      "Iteration 244, loss = 0.18132436\n",
      "Iteration 245, loss = 0.18021212\n",
      "Iteration 246, loss = 0.17910949\n",
      "Iteration 247, loss = 0.17801632\n",
      "Iteration 248, loss = 0.17693305\n",
      "Iteration 249, loss = 0.17585984\n",
      "Iteration 250, loss = 0.17479725\n",
      "Iteration 251, loss = 0.17374386\n",
      "Iteration 252, loss = 0.17269983\n",
      "Iteration 253, loss = 0.17166538\n",
      "Iteration 254, loss = 0.17064092\n",
      "Iteration 255, loss = 0.16962591\n",
      "Iteration 256, loss = 0.16862093\n",
      "Iteration 257, loss = 0.16762552\n",
      "Iteration 258, loss = 0.16663950\n",
      "Iteration 259, loss = 0.16566322\n",
      "Iteration 260, loss = 0.16469638\n",
      "Iteration 261, loss = 0.16373903\n",
      "Iteration 262, loss = 0.16279094\n",
      "Iteration 263, loss = 0.16185273\n",
      "Iteration 264, loss = 0.16092440\n",
      "Iteration 265, loss = 0.16000496\n",
      "Iteration 266, loss = 0.15909454\n",
      "Iteration 267, loss = 0.15819373\n",
      "Iteration 268, loss = 0.15730195\n",
      "Iteration 269, loss = 0.15641913\n",
      "Iteration 270, loss = 0.15554537\n",
      "Iteration 271, loss = 0.15468070\n",
      "Iteration 272, loss = 0.15382567\n",
      "Iteration 273, loss = 0.15297909\n",
      "Iteration 274, loss = 0.15214107\n",
      "Iteration 275, loss = 0.15131134\n",
      "Iteration 276, loss = 0.15049081\n",
      "Iteration 277, loss = 0.14967869\n",
      "Iteration 278, loss = 0.14887468\n",
      "Iteration 279, loss = 0.14807879\n",
      "Iteration 280, loss = 0.14729152\n",
      "Iteration 281, loss = 0.14651222\n",
      "Iteration 282, loss = 0.14574085\n",
      "Iteration 283, loss = 0.14497753\n",
      "Iteration 284, loss = 0.14422225\n",
      "Iteration 285, loss = 0.14347493\n",
      "Iteration 286, loss = 0.14273570\n",
      "Iteration 287, loss = 0.14200424\n",
      "Iteration 288, loss = 0.14128050\n",
      "Iteration 289, loss = 0.14056452\n",
      "Iteration 290, loss = 0.13985651\n",
      "Iteration 291, loss = 0.13915546\n",
      "Iteration 292, loss = 0.13846230\n",
      "Iteration 293, loss = 0.13777641\n",
      "Iteration 294, loss = 0.13709761\n",
      "Iteration 295, loss = 0.13642578\n",
      "Iteration 296, loss = 0.13576115\n",
      "Iteration 297, loss = 0.13510382\n",
      "Iteration 298, loss = 0.13445326\n",
      "Iteration 299, loss = 0.13380948\n",
      "Iteration 300, loss = 0.13317234\n",
      "Iteration 301, loss = 0.13254190\n",
      "Iteration 302, loss = 0.13191814\n",
      "Iteration 303, loss = 0.13130092\n",
      "Iteration 304, loss = 0.13069003\n",
      "Iteration 305, loss = 0.13008567\n",
      "Iteration 306, loss = 0.12948763\n",
      "Iteration 307, loss = 0.12889568\n",
      "Iteration 308, loss = 0.12830983\n",
      "Iteration 309, loss = 0.12773015\n",
      "Iteration 310, loss = 0.12715673\n",
      "Iteration 311, loss = 0.12658913\n",
      "Iteration 312, loss = 0.12602744\n",
      "Iteration 313, loss = 0.12547157\n",
      "Iteration 314, loss = 0.12492103\n",
      "Iteration 315, loss = 0.12437569\n",
      "Iteration 316, loss = 0.12383635\n",
      "Iteration 317, loss = 0.12330239\n",
      "Iteration 318, loss = 0.12277400\n",
      "Iteration 319, loss = 0.12224995\n",
      "Iteration 320, loss = 0.12173006\n",
      "Iteration 321, loss = 0.12121436\n",
      "Iteration 322, loss = 0.12070414\n",
      "Iteration 323, loss = 0.12019688\n",
      "Iteration 324, loss = 0.11969253\n",
      "Iteration 325, loss = 0.11919202\n",
      "Iteration 326, loss = 0.11869466\n",
      "Iteration 327, loss = 0.11819960\n",
      "Iteration 328, loss = 0.11770709\n",
      "Iteration 329, loss = 0.11721678\n",
      "Iteration 330, loss = 0.11672943\n",
      "Iteration 331, loss = 0.11624374\n",
      "Iteration 332, loss = 0.11575986\n",
      "Iteration 333, loss = 0.11527816\n",
      "Iteration 334, loss = 0.11479808\n",
      "Iteration 335, loss = 0.11431910\n",
      "Iteration 336, loss = 0.11384189\n",
      "Iteration 337, loss = 0.11336620\n",
      "Iteration 338, loss = 0.11289152\n",
      "Iteration 339, loss = 0.11241819\n",
      "Iteration 340, loss = 0.11194692\n",
      "Iteration 341, loss = 0.11147687\n",
      "Iteration 342, loss = 0.11100811\n",
      "Iteration 343, loss = 0.11054031\n",
      "Iteration 344, loss = 0.11007377\n",
      "Iteration 345, loss = 0.10960909\n",
      "Iteration 346, loss = 0.10914466\n",
      "Iteration 347, loss = 0.10868256\n",
      "Iteration 348, loss = 0.10822173\n",
      "Iteration 349, loss = 0.10776147\n",
      "Iteration 350, loss = 0.10730231\n",
      "Iteration 351, loss = 0.10684617\n",
      "Iteration 352, loss = 0.10638913\n",
      "Iteration 353, loss = 0.10593509\n",
      "Iteration 354, loss = 0.10548113\n",
      "Iteration 355, loss = 0.10502735\n",
      "Iteration 356, loss = 0.10457424\n",
      "Iteration 357, loss = 0.10412352\n",
      "Iteration 358, loss = 0.10367141\n",
      "Iteration 359, loss = 0.10322118\n",
      "Iteration 360, loss = 0.10277163\n",
      "Iteration 361, loss = 0.10232157\n",
      "Iteration 362, loss = 0.10187163\n",
      "Iteration 363, loss = 0.10142316\n",
      "Iteration 364, loss = 0.10097552\n",
      "Iteration 365, loss = 0.10052756\n",
      "Iteration 366, loss = 0.10008039\n",
      "Iteration 367, loss = 0.09963480\n",
      "Iteration 368, loss = 0.09919041\n",
      "Iteration 369, loss = 0.09874593\n",
      "Iteration 370, loss = 0.09830310\n",
      "Iteration 371, loss = 0.09786287\n",
      "Iteration 372, loss = 0.09742336\n",
      "Iteration 373, loss = 0.09698499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 374, loss = 0.09654934\n",
      "Iteration 375, loss = 0.09611613\n",
      "Iteration 376, loss = 0.09568463\n",
      "Iteration 377, loss = 0.09525559\n",
      "Iteration 378, loss = 0.09482997\n",
      "Iteration 379, loss = 0.09440676\n",
      "Iteration 380, loss = 0.09398677\n",
      "Iteration 381, loss = 0.09357058\n",
      "Iteration 382, loss = 0.09315731\n",
      "Iteration 383, loss = 0.09274738\n",
      "Iteration 384, loss = 0.09234158\n",
      "Iteration 385, loss = 0.09193930\n",
      "Iteration 386, loss = 0.09154086\n",
      "Iteration 387, loss = 0.09114654\n",
      "Iteration 388, loss = 0.09075601\n",
      "Iteration 389, loss = 0.09036978\n",
      "Iteration 390, loss = 0.08998777\n",
      "Iteration 391, loss = 0.08960963\n",
      "Iteration 392, loss = 0.08923585\n",
      "Iteration 393, loss = 0.08886616\n",
      "Iteration 394, loss = 0.08850048\n",
      "Iteration 395, loss = 0.08813940\n",
      "Iteration 396, loss = 0.08778259\n",
      "Iteration 397, loss = 0.08742977\n",
      "Iteration 398, loss = 0.08708095\n",
      "Iteration 399, loss = 0.08673621\n",
      "Iteration 400, loss = 0.08639577\n",
      "Iteration 401, loss = 0.08605892\n",
      "Iteration 402, loss = 0.08572574\n",
      "Iteration 403, loss = 0.08539663\n",
      "Iteration 404, loss = 0.08507144\n",
      "Iteration 405, loss = 0.08475007\n",
      "Iteration 406, loss = 0.08443220\n",
      "Iteration 407, loss = 0.08411783\n",
      "Iteration 408, loss = 0.08380684\n",
      "Iteration 409, loss = 0.08349967\n",
      "Iteration 410, loss = 0.08319609\n",
      "Iteration 411, loss = 0.08289600\n",
      "Iteration 412, loss = 0.08259938\n",
      "Iteration 413, loss = 0.08230628\n",
      "Iteration 414, loss = 0.08201653\n",
      "Iteration 415, loss = 0.08173042\n",
      "Iteration 416, loss = 0.08144779\n",
      "Iteration 417, loss = 0.08116821\n",
      "Iteration 418, loss = 0.08089210\n",
      "Iteration 419, loss = 0.08061933\n",
      "Iteration 420, loss = 0.08035000\n",
      "Iteration 421, loss = 0.08008387\n",
      "Iteration 422, loss = 0.07982080\n",
      "Iteration 423, loss = 0.07956093\n",
      "Iteration 424, loss = 0.07930439\n",
      "Iteration 425, loss = 0.07905094\n",
      "Iteration 426, loss = 0.07880056\n",
      "Iteration 427, loss = 0.07855342\n",
      "Iteration 428, loss = 0.07830946\n",
      "Iteration 429, loss = 0.07806849\n",
      "Iteration 430, loss = 0.07783056\n",
      "Iteration 431, loss = 0.07759571\n",
      "Iteration 432, loss = 0.07736379\n",
      "Iteration 433, loss = 0.07713491\n",
      "Iteration 434, loss = 0.07690896\n",
      "Iteration 435, loss = 0.07668578\n",
      "Iteration 436, loss = 0.07646549\n",
      "Iteration 437, loss = 0.07624826\n",
      "Iteration 438, loss = 0.07603373\n",
      "Iteration 439, loss = 0.07582201\n",
      "Iteration 440, loss = 0.07561301\n",
      "Iteration 441, loss = 0.07540674\n",
      "Iteration 442, loss = 0.07520315\n",
      "Iteration 443, loss = 0.07500220\n",
      "Iteration 444, loss = 0.07480390\n",
      "Iteration 445, loss = 0.07460815\n",
      "Iteration 446, loss = 0.07441499\n",
      "Iteration 447, loss = 0.07422426\n",
      "Iteration 448, loss = 0.07403603\n",
      "Iteration 449, loss = 0.07385027\n",
      "Iteration 450, loss = 0.07366691\n",
      "Iteration 451, loss = 0.07348599\n",
      "Iteration 452, loss = 0.07330750\n",
      "Iteration 453, loss = 0.07313126\n",
      "Iteration 454, loss = 0.07295731\n",
      "Iteration 455, loss = 0.07278576\n",
      "Iteration 456, loss = 0.07261647\n",
      "Iteration 457, loss = 0.07244939\n",
      "Iteration 458, loss = 0.07228457\n",
      "Iteration 459, loss = 0.07212181\n",
      "Iteration 460, loss = 0.07196115\n",
      "Iteration 461, loss = 0.07180257\n",
      "Iteration 462, loss = 0.07164631\n",
      "Iteration 463, loss = 0.07149174\n",
      "Iteration 464, loss = 0.07133936\n",
      "Iteration 465, loss = 0.07118884\n",
      "Iteration 466, loss = 0.07104014\n",
      "Iteration 467, loss = 0.07089331\n",
      "Iteration 468, loss = 0.07074826\n",
      "Iteration 469, loss = 0.07060496\n",
      "Iteration 470, loss = 0.07046335\n",
      "Iteration 471, loss = 0.07032334\n",
      "Iteration 472, loss = 0.07018492\n",
      "Iteration 473, loss = 0.07004808\n",
      "Iteration 474, loss = 0.06991272\n",
      "Iteration 475, loss = 0.06977882\n",
      "Iteration 476, loss = 0.06964631\n",
      "Iteration 477, loss = 0.06951518\n",
      "Iteration 478, loss = 0.06938536\n",
      "Iteration 479, loss = 0.06925685\n",
      "Iteration 480, loss = 0.06912960\n",
      "Iteration 481, loss = 0.06900356\n",
      "Iteration 482, loss = 0.06887872\n",
      "Iteration 483, loss = 0.06875505\n",
      "Iteration 484, loss = 0.06863248\n",
      "Iteration 485, loss = 0.06851107\n",
      "Iteration 486, loss = 0.06839093\n",
      "Iteration 487, loss = 0.06827154\n",
      "Iteration 488, loss = 0.06815334\n",
      "Iteration 489, loss = 0.06803614\n",
      "Iteration 490, loss = 0.06791996\n",
      "Iteration 491, loss = 0.06780489\n",
      "Iteration 492, loss = 0.06769088\n",
      "Iteration 493, loss = 0.06757789\n",
      "Iteration 494, loss = 0.06746587\n",
      "Iteration 495, loss = 0.06735485\n",
      "Iteration 496, loss = 0.06724480\n",
      "Iteration 497, loss = 0.06713576\n",
      "Iteration 498, loss = 0.06702770\n",
      "Iteration 499, loss = 0.06692061\n",
      "Iteration 500, loss = 0.06681454\n",
      "Iteration 501, loss = 0.06670943\n",
      "Iteration 502, loss = 0.06660534\n",
      "Iteration 503, loss = 0.06650227\n",
      "Iteration 504, loss = 0.06640018\n",
      "Iteration 505, loss = 0.06629909\n",
      "Iteration 506, loss = 0.06619910\n",
      "Iteration 507, loss = 0.06610009\n",
      "Iteration 508, loss = 0.06600213\n",
      "Iteration 509, loss = 0.06590519\n",
      "Iteration 510, loss = 0.06580928\n",
      "Iteration 511, loss = 0.06571446\n",
      "Iteration 512, loss = 0.06562061\n",
      "Iteration 513, loss = 0.06552805\n",
      "Iteration 514, loss = 0.06543639\n",
      "Iteration 515, loss = 0.06534568\n",
      "Iteration 516, loss = 0.06525578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.30362454\n",
      "Iteration 3, loss = 1.26024416\n",
      "Iteration 4, loss = 1.21956087\n",
      "Iteration 5, loss = 1.18133663\n",
      "Iteration 6, loss = 1.14531821\n",
      "Iteration 7, loss = 1.11117818\n",
      "Iteration 8, loss = 1.07874601\n",
      "Iteration 9, loss = 1.04783577\n",
      "Iteration 10, loss = 1.01862248\n",
      "Iteration 11, loss = 0.99111446\n",
      "Iteration 12, loss = 0.96546483\n",
      "Iteration 13, loss = 0.94185821\n",
      "Iteration 14, loss = 0.92036025\n",
      "Iteration 15, loss = 0.90093926\n",
      "Iteration 16, loss = 0.88367761\n",
      "Iteration 17, loss = 0.86839511\n",
      "Iteration 18, loss = 0.85501158\n",
      "Iteration 19, loss = 0.84337657\n",
      "Iteration 20, loss = 0.83328179\n",
      "Iteration 21, loss = 0.82415601\n",
      "Iteration 22, loss = 0.81569687\n",
      "Iteration 23, loss = 0.80759494\n",
      "Iteration 24, loss = 0.79958222\n",
      "Iteration 25, loss = 0.79143979\n",
      "Iteration 26, loss = 0.78303963\n",
      "Iteration 27, loss = 0.77429213\n",
      "Iteration 28, loss = 0.76521756\n",
      "Iteration 29, loss = 0.75583815\n",
      "Iteration 30, loss = 0.74624378\n",
      "Iteration 31, loss = 0.73648564\n",
      "Iteration 32, loss = 0.72665516\n",
      "Iteration 33, loss = 0.71687157\n",
      "Iteration 34, loss = 0.70731085\n",
      "Iteration 35, loss = 0.69786938\n",
      "Iteration 36, loss = 0.68867564\n",
      "Iteration 37, loss = 0.67980943\n",
      "Iteration 38, loss = 0.67129718\n",
      "Iteration 39, loss = 0.66319341\n",
      "Iteration 40, loss = 0.65543463\n",
      "Iteration 41, loss = 0.64811099\n",
      "Iteration 42, loss = 0.64120238\n",
      "Iteration 43, loss = 0.63466728\n",
      "Iteration 44, loss = 0.62842672\n",
      "Iteration 45, loss = 0.62243855\n",
      "Iteration 46, loss = 0.61665539\n",
      "Iteration 47, loss = 0.61109131\n",
      "Iteration 48, loss = 0.60573991\n",
      "Iteration 49, loss = 0.60063339\n",
      "Iteration 50, loss = 0.59565589\n",
      "Iteration 51, loss = 0.59080526\n",
      "Iteration 52, loss = 0.58607017\n",
      "Iteration 53, loss = 0.58143073\n",
      "Iteration 54, loss = 0.57686755\n",
      "Iteration 55, loss = 0.57235996\n",
      "Iteration 56, loss = 0.56789459\n",
      "Iteration 57, loss = 0.56346947\n",
      "Iteration 58, loss = 0.55908212\n",
      "Iteration 59, loss = 0.55473530\n",
      "Iteration 60, loss = 0.55043234\n",
      "Iteration 61, loss = 0.54617859\n",
      "Iteration 62, loss = 0.54199785\n",
      "Iteration 63, loss = 0.53788128\n",
      "Iteration 64, loss = 0.53382489\n",
      "Iteration 65, loss = 0.52984307\n",
      "Iteration 66, loss = 0.52591670\n",
      "Iteration 67, loss = 0.52204305\n",
      "Iteration 68, loss = 0.51822083\n",
      "Iteration 69, loss = 0.51443783\n",
      "Iteration 70, loss = 0.51069695\n",
      "Iteration 71, loss = 0.50699171\n",
      "Iteration 72, loss = 0.50333733\n",
      "Iteration 73, loss = 0.49972427\n",
      "Iteration 74, loss = 0.49615164\n",
      "Iteration 75, loss = 0.49261639\n",
      "Iteration 76, loss = 0.48912036\n",
      "Iteration 77, loss = 0.48567986\n",
      "Iteration 78, loss = 0.48233799\n",
      "Iteration 79, loss = 0.47907888\n",
      "Iteration 80, loss = 0.47589365\n",
      "Iteration 81, loss = 0.47275267\n",
      "Iteration 82, loss = 0.46966040\n",
      "Iteration 83, loss = 0.46660796\n",
      "Iteration 84, loss = 0.46358982\n",
      "Iteration 85, loss = 0.46060892\n",
      "Iteration 86, loss = 0.45766161\n",
      "Iteration 87, loss = 0.45474860\n",
      "Iteration 88, loss = 0.45187188\n",
      "Iteration 89, loss = 0.44903142\n",
      "Iteration 90, loss = 0.44622401\n",
      "Iteration 91, loss = 0.44345148\n",
      "Iteration 92, loss = 0.44071065\n",
      "Iteration 93, loss = 0.43800007\n",
      "Iteration 94, loss = 0.43532211\n",
      "Iteration 95, loss = 0.43267399\n",
      "Iteration 96, loss = 0.43005459\n",
      "Iteration 97, loss = 0.42746294\n",
      "Iteration 98, loss = 0.42489810\n",
      "Iteration 99, loss = 0.42235967\n",
      "Iteration 100, loss = 0.41984698\n",
      "Iteration 101, loss = 0.41735910\n",
      "Iteration 102, loss = 0.41489669\n",
      "Iteration 103, loss = 0.41246037\n",
      "Iteration 104, loss = 0.41004802\n",
      "Iteration 105, loss = 0.40765895\n",
      "Iteration 106, loss = 0.40529191\n",
      "Iteration 107, loss = 0.40294674\n",
      "Iteration 108, loss = 0.40062303\n",
      "Iteration 109, loss = 0.39832030\n",
      "Iteration 110, loss = 0.39603785\n",
      "Iteration 111, loss = 0.39377492\n",
      "Iteration 112, loss = 0.39153094\n",
      "Iteration 113, loss = 0.38930591\n",
      "Iteration 114, loss = 0.38709956\n",
      "Iteration 115, loss = 0.38491100\n",
      "Iteration 116, loss = 0.38273977\n",
      "Iteration 117, loss = 0.38058605\n",
      "Iteration 118, loss = 0.37844972\n",
      "Iteration 119, loss = 0.37632937\n",
      "Iteration 120, loss = 0.37422491\n",
      "Iteration 121, loss = 0.37213609\n",
      "Iteration 122, loss = 0.37006225\n",
      "Iteration 123, loss = 0.36800335\n",
      "Iteration 124, loss = 0.36595897\n",
      "Iteration 125, loss = 0.36392825\n",
      "Iteration 126, loss = 0.36191151\n",
      "Iteration 127, loss = 0.35990785\n",
      "Iteration 128, loss = 0.35791726\n",
      "Iteration 129, loss = 0.35593943\n",
      "Iteration 130, loss = 0.35397415\n",
      "Iteration 131, loss = 0.35202107\n",
      "Iteration 132, loss = 0.35007987\n",
      "Iteration 133, loss = 0.34815083\n",
      "Iteration 134, loss = 0.34623262\n",
      "Iteration 135, loss = 0.34432549\n",
      "Iteration 136, loss = 0.34243024\n",
      "Iteration 137, loss = 0.34054561\n",
      "Iteration 138, loss = 0.33867142\n",
      "Iteration 139, loss = 0.33680816\n",
      "Iteration 140, loss = 0.33495501\n",
      "Iteration 141, loss = 0.33311210\n",
      "Iteration 142, loss = 0.33127948\n",
      "Iteration 143, loss = 0.32945638\n",
      "Iteration 144, loss = 0.32764348\n",
      "Iteration 145, loss = 0.32584456\n",
      "Iteration 146, loss = 0.32405524\n",
      "Iteration 147, loss = 0.32227549\n",
      "Iteration 148, loss = 0.32050505\n",
      "Iteration 149, loss = 0.31874437\n",
      "Iteration 150, loss = 0.31699304\n",
      "Iteration 151, loss = 0.31525109\n",
      "Iteration 152, loss = 0.31351877\n",
      "Iteration 153, loss = 0.31179548\n",
      "Iteration 154, loss = 0.31008119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 155, loss = 0.30837600\n",
      "Iteration 156, loss = 0.30668130\n",
      "Iteration 157, loss = 0.30499914\n",
      "Iteration 158, loss = 0.30332611\n",
      "Iteration 159, loss = 0.30166216\n",
      "Iteration 160, loss = 0.30000720\n",
      "Iteration 161, loss = 0.29836138\n",
      "Iteration 162, loss = 0.29672452\n",
      "Iteration 163, loss = 0.29509666\n",
      "Iteration 164, loss = 0.29347749\n",
      "Iteration 165, loss = 0.29186718\n",
      "Iteration 166, loss = 0.29026597\n",
      "Iteration 167, loss = 0.28867308\n",
      "Iteration 168, loss = 0.28708848\n",
      "Iteration 169, loss = 0.28551289\n",
      "Iteration 170, loss = 0.28394582\n",
      "Iteration 171, loss = 0.28238999\n",
      "Iteration 172, loss = 0.28084272\n",
      "Iteration 173, loss = 0.27930384\n",
      "Iteration 174, loss = 0.27777441\n",
      "Iteration 175, loss = 0.27625335\n",
      "Iteration 176, loss = 0.27474065\n",
      "Iteration 177, loss = 0.27323707\n",
      "Iteration 178, loss = 0.27174204\n",
      "Iteration 179, loss = 0.27025528\n",
      "Iteration 180, loss = 0.26877811\n",
      "Iteration 181, loss = 0.26730995\n",
      "Iteration 182, loss = 0.26585021\n",
      "Iteration 183, loss = 0.26439903\n",
      "Iteration 184, loss = 0.26295589\n",
      "Iteration 185, loss = 0.26151861\n",
      "Iteration 186, loss = 0.26008571\n",
      "Iteration 187, loss = 0.25865501\n",
      "Iteration 188, loss = 0.25722553\n",
      "Iteration 189, loss = 0.25579698\n",
      "Iteration 190, loss = 0.25437208\n",
      "Iteration 191, loss = 0.25297144\n",
      "Iteration 192, loss = 0.25157794\n",
      "Iteration 193, loss = 0.25018765\n",
      "Iteration 194, loss = 0.24880017\n",
      "Iteration 195, loss = 0.24741471\n",
      "Iteration 196, loss = 0.24603259\n",
      "Iteration 197, loss = 0.24465384\n",
      "Iteration 198, loss = 0.24327638\n",
      "Iteration 199, loss = 0.24190023\n",
      "Iteration 200, loss = 0.24052535\n",
      "Iteration 201, loss = 0.23915182\n",
      "Iteration 202, loss = 0.23777996\n",
      "Iteration 203, loss = 0.23641002\n",
      "Iteration 204, loss = 0.23504082\n",
      "Iteration 205, loss = 0.23367451\n",
      "Iteration 206, loss = 0.23231066\n",
      "Iteration 207, loss = 0.23094870\n",
      "Iteration 208, loss = 0.22958867\n",
      "Iteration 209, loss = 0.22823087\n",
      "Iteration 210, loss = 0.22687551\n",
      "Iteration 211, loss = 0.22552285\n",
      "Iteration 212, loss = 0.22417312\n",
      "Iteration 213, loss = 0.22282711\n",
      "Iteration 214, loss = 0.22148460\n",
      "Iteration 215, loss = 0.22014589\n",
      "Iteration 216, loss = 0.21881122\n",
      "Iteration 217, loss = 0.21748084\n",
      "Iteration 218, loss = 0.21615510\n",
      "Iteration 219, loss = 0.21483591\n",
      "Iteration 220, loss = 0.21352191\n",
      "Iteration 221, loss = 0.21221322\n",
      "Iteration 222, loss = 0.21091011\n",
      "Iteration 223, loss = 0.20961284\n",
      "Iteration 224, loss = 0.20832183\n",
      "Iteration 225, loss = 0.20703704\n",
      "Iteration 226, loss = 0.20575879\n",
      "Iteration 227, loss = 0.20448725\n",
      "Iteration 228, loss = 0.20322261\n",
      "Iteration 229, loss = 0.20196503\n",
      "Iteration 230, loss = 0.20071472\n",
      "Iteration 231, loss = 0.19947195\n",
      "Iteration 232, loss = 0.19823686\n",
      "Iteration 233, loss = 0.19700956\n",
      "Iteration 234, loss = 0.19579005\n",
      "Iteration 235, loss = 0.19457862\n",
      "Iteration 236, loss = 0.19337533\n",
      "Iteration 237, loss = 0.19218027\n",
      "Iteration 238, loss = 0.19099352\n",
      "Iteration 239, loss = 0.18981519\n",
      "Iteration 240, loss = 0.18864543\n",
      "Iteration 241, loss = 0.18748434\n",
      "Iteration 242, loss = 0.18633190\n",
      "Iteration 243, loss = 0.18518820\n",
      "Iteration 244, loss = 0.18405356\n",
      "Iteration 245, loss = 0.18292791\n",
      "Iteration 246, loss = 0.18181123\n",
      "Iteration 247, loss = 0.18070366\n",
      "Iteration 248, loss = 0.17960549\n",
      "Iteration 249, loss = 0.17851641\n",
      "Iteration 250, loss = 0.17743640\n",
      "Iteration 251, loss = 0.17636547\n",
      "Iteration 252, loss = 0.17530362\n",
      "Iteration 253, loss = 0.17425108\n",
      "Iteration 254, loss = 0.17320779\n",
      "Iteration 255, loss = 0.17217355\n",
      "Iteration 256, loss = 0.17114842\n",
      "Iteration 257, loss = 0.17013239\n",
      "Iteration 258, loss = 0.16912543\n",
      "Iteration 259, loss = 0.16812778\n",
      "Iteration 260, loss = 0.16713987\n",
      "Iteration 261, loss = 0.16616108\n",
      "Iteration 262, loss = 0.16519149\n",
      "Iteration 263, loss = 0.16423109\n",
      "Iteration 264, loss = 0.16327971\n",
      "Iteration 265, loss = 0.16233735\n",
      "Iteration 266, loss = 0.16140385\n",
      "Iteration 267, loss = 0.16047924\n",
      "Iteration 268, loss = 0.15956344\n",
      "Iteration 269, loss = 0.15865631\n",
      "Iteration 270, loss = 0.15775813\n",
      "Iteration 271, loss = 0.15686852\n",
      "Iteration 272, loss = 0.15598745\n",
      "Iteration 273, loss = 0.15511483\n",
      "Iteration 274, loss = 0.15425067\n",
      "Iteration 275, loss = 0.15339511\n",
      "Iteration 276, loss = 0.15254800\n",
      "Iteration 277, loss = 0.15170914\n",
      "Iteration 278, loss = 0.15087880\n",
      "Iteration 279, loss = 0.15005672\n",
      "Iteration 280, loss = 0.14924272\n",
      "Iteration 281, loss = 0.14843683\n",
      "Iteration 282, loss = 0.14763888\n",
      "Iteration 283, loss = 0.14684886\n",
      "Iteration 284, loss = 0.14606661\n",
      "Iteration 285, loss = 0.14529208\n",
      "Iteration 286, loss = 0.14452536\n",
      "Iteration 287, loss = 0.14376634\n",
      "Iteration 288, loss = 0.14301476\n",
      "Iteration 289, loss = 0.14227060\n",
      "Iteration 290, loss = 0.14153383\n",
      "Iteration 291, loss = 0.14080434\n",
      "Iteration 292, loss = 0.14008204\n",
      "Iteration 293, loss = 0.13936707\n",
      "Iteration 294, loss = 0.13865920\n",
      "Iteration 295, loss = 0.13795842\n",
      "Iteration 296, loss = 0.13726457\n",
      "Iteration 297, loss = 0.13657766\n",
      "Iteration 298, loss = 0.13589756\n",
      "Iteration 299, loss = 0.13522428\n",
      "Iteration 300, loss = 0.13455765\n",
      "Iteration 301, loss = 0.13389764\n",
      "Iteration 302, loss = 0.13324414\n",
      "Iteration 303, loss = 0.13259715\n",
      "Iteration 304, loss = 0.13195660\n",
      "Iteration 305, loss = 0.13132244\n",
      "Iteration 306, loss = 0.13069456\n",
      "Iteration 307, loss = 0.13007267\n",
      "Iteration 308, loss = 0.12945682\n",
      "Iteration 309, loss = 0.12884708\n",
      "Iteration 310, loss = 0.12824362\n",
      "Iteration 311, loss = 0.12764593\n",
      "Iteration 312, loss = 0.12705407\n",
      "Iteration 313, loss = 0.12646763\n",
      "Iteration 314, loss = 0.12588684\n",
      "Iteration 315, loss = 0.12531143\n",
      "Iteration 316, loss = 0.12474213\n",
      "Iteration 317, loss = 0.12417760\n",
      "Iteration 318, loss = 0.12361773\n",
      "Iteration 319, loss = 0.12306293\n",
      "Iteration 320, loss = 0.12251255\n",
      "Iteration 321, loss = 0.12196647\n",
      "Iteration 322, loss = 0.12142413\n",
      "Iteration 323, loss = 0.12088521\n",
      "Iteration 324, loss = 0.12034958\n",
      "Iteration 325, loss = 0.11981655\n",
      "Iteration 326, loss = 0.11928593\n",
      "Iteration 327, loss = 0.11875787\n",
      "Iteration 328, loss = 0.11823196\n",
      "Iteration 329, loss = 0.11770825\n",
      "Iteration 330, loss = 0.11718648\n",
      "Iteration 331, loss = 0.11666654\n",
      "Iteration 332, loss = 0.11614829\n",
      "Iteration 333, loss = 0.11563154\n",
      "Iteration 334, loss = 0.11511584\n",
      "Iteration 335, loss = 0.11460018\n",
      "Iteration 336, loss = 0.11408752\n",
      "Iteration 337, loss = 0.11357505\n",
      "Iteration 338, loss = 0.11306572\n",
      "Iteration 339, loss = 0.11255642\n",
      "Iteration 340, loss = 0.11204456\n",
      "Iteration 341, loss = 0.11153935\n",
      "Iteration 342, loss = 0.11103150\n",
      "Iteration 343, loss = 0.11052596\n",
      "Iteration 344, loss = 0.11001891\n",
      "Iteration 345, loss = 0.10951273\n",
      "Iteration 346, loss = 0.10901068\n",
      "Iteration 347, loss = 0.10850153\n",
      "Iteration 348, loss = 0.10799834\n",
      "Iteration 349, loss = 0.10749615\n",
      "Iteration 350, loss = 0.10699111\n",
      "Iteration 351, loss = 0.10648629\n",
      "Iteration 352, loss = 0.10598292\n",
      "Iteration 353, loss = 0.10547956\n",
      "Iteration 354, loss = 0.10497714\n",
      "Iteration 355, loss = 0.10447403\n",
      "Iteration 356, loss = 0.10397181\n",
      "Iteration 357, loss = 0.10346963\n",
      "Iteration 358, loss = 0.10296877\n",
      "Iteration 359, loss = 0.10246499\n",
      "Iteration 360, loss = 0.10196278\n",
      "Iteration 361, loss = 0.10146203\n",
      "Iteration 362, loss = 0.10095934\n",
      "Iteration 363, loss = 0.10045596\n",
      "Iteration 364, loss = 0.09995356\n",
      "Iteration 365, loss = 0.09945225\n",
      "Iteration 366, loss = 0.09894862\n",
      "Iteration 367, loss = 0.09844664\n",
      "Iteration 368, loss = 0.09794548\n",
      "Iteration 369, loss = 0.09744449\n",
      "Iteration 370, loss = 0.09694438\n",
      "Iteration 371, loss = 0.09644447\n",
      "Iteration 372, loss = 0.09594643\n",
      "Iteration 373, loss = 0.09544963\n",
      "Iteration 374, loss = 0.09495505\n",
      "Iteration 375, loss = 0.09446147\n",
      "Iteration 376, loss = 0.09396952\n",
      "Iteration 377, loss = 0.09347963\n",
      "Iteration 378, loss = 0.09299221\n",
      "Iteration 379, loss = 0.09250754\n",
      "Iteration 380, loss = 0.09202483\n",
      "Iteration 381, loss = 0.09154470\n",
      "Iteration 382, loss = 0.09106799\n",
      "Iteration 383, loss = 0.09059372\n",
      "Iteration 384, loss = 0.09012277\n",
      "Iteration 385, loss = 0.08965505\n",
      "Iteration 386, loss = 0.08919023\n",
      "Iteration 387, loss = 0.08872881\n",
      "Iteration 388, loss = 0.08827101\n",
      "Iteration 389, loss = 0.08781709\n",
      "Iteration 390, loss = 0.08736667\n",
      "Iteration 391, loss = 0.08692011\n",
      "Iteration 392, loss = 0.08647771\n",
      "Iteration 393, loss = 0.08603961\n",
      "Iteration 394, loss = 0.08560574\n",
      "Iteration 395, loss = 0.08517616\n",
      "Iteration 396, loss = 0.08475094\n",
      "Iteration 397, loss = 0.08433023\n",
      "Iteration 398, loss = 0.08391416\n",
      "Iteration 399, loss = 0.08350274\n",
      "Iteration 400, loss = 0.08309624\n",
      "Iteration 401, loss = 0.08269447\n",
      "Iteration 402, loss = 0.08229765\n",
      "Iteration 403, loss = 0.08190577\n",
      "Iteration 404, loss = 0.08151869\n",
      "Iteration 405, loss = 0.08113628\n",
      "Iteration 406, loss = 0.08075873\n",
      "Iteration 407, loss = 0.08038590\n",
      "Iteration 408, loss = 0.08001769\n",
      "Iteration 409, loss = 0.07965400\n",
      "Iteration 410, loss = 0.07929481\n",
      "Iteration 411, loss = 0.07894020\n",
      "Iteration 412, loss = 0.07858977\n",
      "Iteration 413, loss = 0.07824356\n",
      "Iteration 414, loss = 0.07790163\n",
      "Iteration 415, loss = 0.07756372\n",
      "Iteration 416, loss = 0.07722967\n",
      "Iteration 417, loss = 0.07689950\n",
      "Iteration 418, loss = 0.07657312\n",
      "Iteration 419, loss = 0.07625048\n",
      "Iteration 420, loss = 0.07593156\n",
      "Iteration 421, loss = 0.07561626\n",
      "Iteration 422, loss = 0.07530463\n",
      "Iteration 423, loss = 0.07499649\n",
      "Iteration 424, loss = 0.07469198\n",
      "Iteration 425, loss = 0.07439079\n",
      "Iteration 426, loss = 0.07409306\n",
      "Iteration 427, loss = 0.07379869\n",
      "Iteration 428, loss = 0.07350809\n",
      "Iteration 429, loss = 0.07322068\n",
      "Iteration 430, loss = 0.07293642\n",
      "Iteration 431, loss = 0.07265551\n",
      "Iteration 432, loss = 0.07237763\n",
      "Iteration 433, loss = 0.07210268\n",
      "Iteration 434, loss = 0.07183068\n",
      "Iteration 435, loss = 0.07156168\n",
      "Iteration 436, loss = 0.07129586\n",
      "Iteration 437, loss = 0.07103290\n",
      "Iteration 438, loss = 0.07077279\n",
      "Iteration 439, loss = 0.07051573\n",
      "Iteration 440, loss = 0.07026156\n",
      "Iteration 441, loss = 0.07001031\n",
      "Iteration 442, loss = 0.06976181\n",
      "Iteration 443, loss = 0.06951634\n",
      "Iteration 444, loss = 0.06927364\n",
      "Iteration 445, loss = 0.06903373\n",
      "Iteration 446, loss = 0.06879661\n",
      "Iteration 447, loss = 0.06856210\n",
      "Iteration 448, loss = 0.06833026\n",
      "Iteration 449, loss = 0.06810107\n",
      "Iteration 450, loss = 0.06787440\n",
      "Iteration 451, loss = 0.06765003\n",
      "Iteration 452, loss = 0.06742799\n",
      "Iteration 453, loss = 0.06720905\n",
      "Iteration 454, loss = 0.06699263\n",
      "Iteration 455, loss = 0.06677860\n",
      "Iteration 456, loss = 0.06656702\n",
      "Iteration 457, loss = 0.06635792\n",
      "Iteration 458, loss = 0.06615121\n",
      "Iteration 459, loss = 0.06594686\n",
      "Iteration 460, loss = 0.06574481\n",
      "Iteration 461, loss = 0.06554498\n",
      "Iteration 462, loss = 0.06534738\n",
      "Iteration 463, loss = 0.06515192\n",
      "Iteration 464, loss = 0.06495857\n",
      "Iteration 465, loss = 0.06476737\n",
      "Iteration 466, loss = 0.06457820\n",
      "Iteration 467, loss = 0.06439106\n",
      "Iteration 468, loss = 0.06420594\n",
      "Iteration 469, loss = 0.06402276\n",
      "Iteration 470, loss = 0.06384144\n",
      "Iteration 471, loss = 0.06366192\n",
      "Iteration 472, loss = 0.06348415\n",
      "Iteration 473, loss = 0.06330808\n",
      "Iteration 474, loss = 0.06313364\n",
      "Iteration 475, loss = 0.06296079\n",
      "Iteration 476, loss = 0.06278953\n",
      "Iteration 477, loss = 0.06261977\n",
      "Iteration 478, loss = 0.06245149\n",
      "Iteration 479, loss = 0.06228458\n",
      "Iteration 480, loss = 0.06211894\n",
      "Iteration 481, loss = 0.06195457\n",
      "Iteration 482, loss = 0.06179146\n",
      "Iteration 483, loss = 0.06162956\n",
      "Iteration 484, loss = 0.06146880\n",
      "Iteration 485, loss = 0.06130915\n",
      "Iteration 486, loss = 0.06115058\n",
      "Iteration 487, loss = 0.06099304\n",
      "Iteration 488, loss = 0.06083651\n",
      "Iteration 489, loss = 0.06068097\n",
      "Iteration 490, loss = 0.06052638\n",
      "Iteration 491, loss = 0.06037273\n",
      "Iteration 492, loss = 0.06022005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 493, loss = 0.06006829\n",
      "Iteration 494, loss = 0.05991745\n",
      "Iteration 495, loss = 0.05976751\n",
      "Iteration 496, loss = 0.05961847\n",
      "Iteration 497, loss = 0.05947033\n",
      "Iteration 498, loss = 0.05932310\n",
      "Iteration 499, loss = 0.05917680\n",
      "Iteration 500, loss = 0.05903144\n",
      "Iteration 501, loss = 0.05888702\n",
      "Iteration 502, loss = 0.05874349\n",
      "Iteration 503, loss = 0.05860086\n",
      "Iteration 504, loss = 0.05845916\n",
      "Iteration 505, loss = 0.05831842\n",
      "Iteration 506, loss = 0.05817864\n",
      "Iteration 507, loss = 0.05803984\n",
      "Iteration 508, loss = 0.05790196\n",
      "Iteration 509, loss = 0.05776505\n",
      "Iteration 510, loss = 0.05762913\n",
      "Iteration 511, loss = 0.05749422\n",
      "Iteration 512, loss = 0.05736030\n",
      "Iteration 513, loss = 0.05722735\n",
      "Iteration 514, loss = 0.05709540\n",
      "Iteration 515, loss = 0.05696446\n",
      "Iteration 516, loss = 0.05683453\n",
      "Iteration 517, loss = 0.05670564\n",
      "Iteration 518, loss = 0.05657777\n",
      "Iteration 519, loss = 0.05645090\n",
      "Iteration 520, loss = 0.05632509\n",
      "Iteration 521, loss = 0.05620033\n",
      "Iteration 522, loss = 0.05607659\n",
      "Iteration 523, loss = 0.05595391\n",
      "Iteration 524, loss = 0.05583227\n",
      "Iteration 525, loss = 0.05571170\n",
      "Iteration 526, loss = 0.05559217\n",
      "Iteration 527, loss = 0.05547367\n",
      "Iteration 528, loss = 0.05535621\n",
      "Iteration 529, loss = 0.05523977\n",
      "Iteration 530, loss = 0.05512439\n",
      "Iteration 531, loss = 0.05501004\n",
      "Iteration 532, loss = 0.05489671\n",
      "Iteration 533, loss = 0.05478442\n",
      "Iteration 534, loss = 0.05467317\n",
      "Iteration 535, loss = 0.05456295\n",
      "Iteration 536, loss = 0.05445368\n",
      "Iteration 537, loss = 0.05434544\n",
      "Iteration 538, loss = 0.05423817\n",
      "Iteration 539, loss = 0.05413190\n",
      "Iteration 540, loss = 0.05402667\n",
      "Iteration 541, loss = 0.05392245\n",
      "Iteration 542, loss = 0.05381922\n",
      "Iteration 543, loss = 0.05371693\n",
      "Iteration 544, loss = 0.05361558\n",
      "Iteration 545, loss = 0.05351514\n",
      "Iteration 546, loss = 0.05341558\n",
      "Iteration 547, loss = 0.05331688\n",
      "Iteration 548, loss = 0.05321902\n",
      "Iteration 549, loss = 0.05312197\n",
      "Iteration 550, loss = 0.05302573\n",
      "Iteration 551, loss = 0.05293025\n",
      "Iteration 552, loss = 0.05283545\n",
      "Iteration 553, loss = 0.05274140\n",
      "Iteration 554, loss = 0.05264852\n",
      "Iteration 555, loss = 0.05255574\n",
      "Iteration 556, loss = 0.05246408\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.30351331\n",
      "Iteration 3, loss = 1.25974469\n",
      "Iteration 4, loss = 1.21872902\n",
      "Iteration 5, loss = 1.18029929\n",
      "Iteration 6, loss = 1.14417356\n",
      "Iteration 7, loss = 1.10997009\n",
      "Iteration 8, loss = 1.07742790\n",
      "Iteration 9, loss = 1.04643571\n",
      "Iteration 10, loss = 1.01715683\n",
      "Iteration 11, loss = 0.98957305\n",
      "Iteration 12, loss = 0.96375724\n",
      "Iteration 13, loss = 0.93990434\n",
      "Iteration 14, loss = 0.91811204\n",
      "Iteration 15, loss = 0.89845926\n",
      "Iteration 16, loss = 0.88104781\n",
      "Iteration 17, loss = 0.86566010\n",
      "Iteration 18, loss = 0.85218362\n",
      "Iteration 19, loss = 0.84051543\n",
      "Iteration 20, loss = 0.83030192\n",
      "Iteration 21, loss = 0.82109547\n",
      "Iteration 22, loss = 0.81258452\n",
      "Iteration 23, loss = 0.80445866\n",
      "Iteration 24, loss = 0.79645832\n",
      "Iteration 25, loss = 0.78836223\n",
      "Iteration 26, loss = 0.78004419\n",
      "Iteration 27, loss = 0.77140420\n",
      "Iteration 28, loss = 0.76242207\n",
      "Iteration 29, loss = 0.75312600\n",
      "Iteration 30, loss = 0.74360316\n",
      "Iteration 31, loss = 0.73397409\n",
      "Iteration 32, loss = 0.72432988\n",
      "Iteration 33, loss = 0.71476455\n",
      "Iteration 34, loss = 0.70535860\n",
      "Iteration 35, loss = 0.69611458\n",
      "Iteration 36, loss = 0.68706214\n",
      "Iteration 37, loss = 0.67833615\n",
      "Iteration 38, loss = 0.66992846\n",
      "Iteration 39, loss = 0.66191721\n",
      "Iteration 40, loss = 0.65429278\n",
      "Iteration 41, loss = 0.64707810\n",
      "Iteration 42, loss = 0.64019601\n",
      "Iteration 43, loss = 0.63366756\n",
      "Iteration 44, loss = 0.62739320\n",
      "Iteration 45, loss = 0.62137055\n",
      "Iteration 46, loss = 0.61558822\n",
      "Iteration 47, loss = 0.61005743\n",
      "Iteration 48, loss = 0.60477139\n",
      "Iteration 49, loss = 0.59973955\n",
      "Iteration 50, loss = 0.59486606\n",
      "Iteration 51, loss = 0.59011902\n",
      "Iteration 52, loss = 0.58545832\n",
      "Iteration 53, loss = 0.58089762\n",
      "Iteration 54, loss = 0.57640442\n",
      "Iteration 55, loss = 0.57199529\n",
      "Iteration 56, loss = 0.56764427\n",
      "Iteration 57, loss = 0.56333887\n",
      "Iteration 58, loss = 0.55907991\n",
      "Iteration 59, loss = 0.55486698\n",
      "Iteration 60, loss = 0.55070588\n",
      "Iteration 61, loss = 0.54659901\n",
      "Iteration 62, loss = 0.54254604\n",
      "Iteration 63, loss = 0.53855493\n",
      "Iteration 64, loss = 0.53465384\n",
      "Iteration 65, loss = 0.53081363\n",
      "Iteration 66, loss = 0.52703237\n",
      "Iteration 67, loss = 0.52330317\n",
      "Iteration 68, loss = 0.51961198\n",
      "Iteration 69, loss = 0.51594340\n",
      "Iteration 70, loss = 0.51232203\n",
      "Iteration 71, loss = 0.50872961\n",
      "Iteration 72, loss = 0.50518119\n",
      "Iteration 73, loss = 0.50167825\n",
      "Iteration 74, loss = 0.49822020\n",
      "Iteration 75, loss = 0.49481041\n",
      "Iteration 76, loss = 0.49144763\n",
      "Iteration 77, loss = 0.48816364\n",
      "Iteration 78, loss = 0.48494862\n",
      "Iteration 79, loss = 0.48181050\n",
      "Iteration 80, loss = 0.47876484\n",
      "Iteration 81, loss = 0.47576463\n",
      "Iteration 82, loss = 0.47281523\n",
      "Iteration 83, loss = 0.46990375\n",
      "Iteration 84, loss = 0.46702691\n",
      "Iteration 85, loss = 0.46418693\n",
      "Iteration 86, loss = 0.46138100\n",
      "Iteration 87, loss = 0.45860949\n",
      "Iteration 88, loss = 0.45587090\n",
      "Iteration 89, loss = 0.45316502\n",
      "Iteration 90, loss = 0.45049099\n",
      "Iteration 91, loss = 0.44784986\n",
      "Iteration 92, loss = 0.44524011\n",
      "Iteration 93, loss = 0.44266088\n",
      "Iteration 94, loss = 0.44011441\n",
      "Iteration 95, loss = 0.43760051\n",
      "Iteration 96, loss = 0.43511569\n",
      "Iteration 97, loss = 0.43265996\n",
      "Iteration 98, loss = 0.43023382\n",
      "Iteration 99, loss = 0.42783552\n",
      "Iteration 100, loss = 0.42546244\n",
      "Iteration 101, loss = 0.42311358\n",
      "Iteration 102, loss = 0.42078829\n",
      "Iteration 103, loss = 0.41848657\n",
      "Iteration 104, loss = 0.41620747\n",
      "Iteration 105, loss = 0.41395022\n",
      "Iteration 106, loss = 0.41171399\n",
      "Iteration 107, loss = 0.40949902\n",
      "Iteration 108, loss = 0.40730640\n",
      "Iteration 109, loss = 0.40513870\n",
      "Iteration 110, loss = 0.40299080\n",
      "Iteration 111, loss = 0.40086186\n",
      "Iteration 112, loss = 0.39875172\n",
      "Iteration 113, loss = 0.39665981\n",
      "Iteration 114, loss = 0.39458592\n",
      "Iteration 115, loss = 0.39252886\n",
      "Iteration 116, loss = 0.39048915\n",
      "Iteration 117, loss = 0.38846601\n",
      "Iteration 118, loss = 0.38645893\n",
      "Iteration 119, loss = 0.38447013\n",
      "Iteration 120, loss = 0.38249675\n",
      "Iteration 121, loss = 0.38053818\n",
      "Iteration 122, loss = 0.37859406\n",
      "Iteration 123, loss = 0.37666461\n",
      "Iteration 124, loss = 0.37474895\n",
      "Iteration 125, loss = 0.37284738\n",
      "Iteration 126, loss = 0.37095939\n",
      "Iteration 127, loss = 0.36908357\n",
      "Iteration 128, loss = 0.36722052\n",
      "Iteration 129, loss = 0.36537045\n",
      "Iteration 130, loss = 0.36353200\n",
      "Iteration 131, loss = 0.36170551\n",
      "Iteration 132, loss = 0.35989092\n",
      "Iteration 133, loss = 0.35808796\n",
      "Iteration 134, loss = 0.35629614\n",
      "Iteration 135, loss = 0.35451537\n",
      "Iteration 136, loss = 0.35274539\n",
      "Iteration 137, loss = 0.35098595\n",
      "Iteration 138, loss = 0.34923693\n",
      "Iteration 139, loss = 0.34749817\n",
      "Iteration 140, loss = 0.34576951\n",
      "Iteration 141, loss = 0.34405079\n",
      "Iteration 142, loss = 0.34234186\n",
      "Iteration 143, loss = 0.34064306\n",
      "Iteration 144, loss = 0.33895385\n",
      "Iteration 145, loss = 0.33727390\n",
      "Iteration 146, loss = 0.33560317\n",
      "Iteration 147, loss = 0.33394164\n",
      "Iteration 148, loss = 0.33228936\n",
      "Iteration 149, loss = 0.33064575\n",
      "Iteration 150, loss = 0.32901100\n",
      "Iteration 151, loss = 0.32738587\n",
      "Iteration 152, loss = 0.32576907\n",
      "Iteration 153, loss = 0.32416573\n",
      "Iteration 154, loss = 0.32257097\n",
      "Iteration 155, loss = 0.32098457\n",
      "Iteration 156, loss = 0.31940647\n",
      "Iteration 157, loss = 0.31783686\n",
      "Iteration 158, loss = 0.31627530\n",
      "Iteration 159, loss = 0.31472251\n",
      "Iteration 160, loss = 0.31317792\n",
      "Iteration 161, loss = 0.31164154\n",
      "Iteration 162, loss = 0.31011309\n",
      "Iteration 163, loss = 0.30859240\n",
      "Iteration 164, loss = 0.30708152\n",
      "Iteration 165, loss = 0.30558108\n",
      "Iteration 166, loss = 0.30408892\n",
      "Iteration 167, loss = 0.30260518\n",
      "Iteration 168, loss = 0.30112948\n",
      "Iteration 169, loss = 0.29966142\n",
      "Iteration 170, loss = 0.29820238\n",
      "Iteration 171, loss = 0.29675334\n",
      "Iteration 172, loss = 0.29531289\n",
      "Iteration 173, loss = 0.29388015\n",
      "Iteration 174, loss = 0.29245518\n",
      "Iteration 175, loss = 0.29103804\n",
      "Iteration 176, loss = 0.28962899\n",
      "Iteration 177, loss = 0.28822778\n",
      "Iteration 178, loss = 0.28683415\n",
      "Iteration 179, loss = 0.28544866\n",
      "Iteration 180, loss = 0.28407268\n",
      "Iteration 181, loss = 0.28270510\n",
      "Iteration 182, loss = 0.28134482\n",
      "Iteration 183, loss = 0.27999190\n",
      "Iteration 184, loss = 0.27864642\n",
      "Iteration 185, loss = 0.27730860\n",
      "Iteration 186, loss = 0.27597884\n",
      "Iteration 187, loss = 0.27465671\n",
      "Iteration 188, loss = 0.27334226\n",
      "Iteration 189, loss = 0.27203569\n",
      "Iteration 190, loss = 0.27073672\n",
      "Iteration 191, loss = 0.26944625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 192, loss = 0.26816269\n",
      "Iteration 193, loss = 0.26688608\n",
      "Iteration 194, loss = 0.26561828\n",
      "Iteration 195, loss = 0.26435735\n",
      "Iteration 196, loss = 0.26310340\n",
      "Iteration 197, loss = 0.26185752\n",
      "Iteration 198, loss = 0.26061693\n",
      "Iteration 199, loss = 0.25938040\n",
      "Iteration 200, loss = 0.25814793\n",
      "Iteration 201, loss = 0.25691948\n",
      "Iteration 202, loss = 0.25569479\n",
      "Iteration 203, loss = 0.25447633\n",
      "Iteration 204, loss = 0.25326190\n",
      "Iteration 205, loss = 0.25205228\n",
      "Iteration 206, loss = 0.25084640\n",
      "Iteration 207, loss = 0.24964487\n",
      "Iteration 208, loss = 0.24844666\n",
      "Iteration 209, loss = 0.24725145\n",
      "Iteration 210, loss = 0.24606067\n",
      "Iteration 211, loss = 0.24487341\n",
      "Iteration 212, loss = 0.24369163\n",
      "Iteration 213, loss = 0.24251197\n",
      "Iteration 214, loss = 0.24133264\n",
      "Iteration 215, loss = 0.24015064\n",
      "Iteration 216, loss = 0.23896745\n",
      "Iteration 217, loss = 0.23778233\n",
      "Iteration 218, loss = 0.23661136\n",
      "Iteration 219, loss = 0.23544479\n",
      "Iteration 220, loss = 0.23427904\n",
      "Iteration 221, loss = 0.23311463\n",
      "Iteration 222, loss = 0.23195162\n",
      "Iteration 223, loss = 0.23079143\n",
      "Iteration 224, loss = 0.22963243\n",
      "Iteration 225, loss = 0.22847518\n",
      "Iteration 226, loss = 0.22732006\n",
      "Iteration 227, loss = 0.22616864\n",
      "Iteration 228, loss = 0.22502033\n",
      "Iteration 229, loss = 0.22387433\n",
      "Iteration 230, loss = 0.22273137\n",
      "Iteration 231, loss = 0.22159227\n",
      "Iteration 232, loss = 0.22045681\n",
      "Iteration 233, loss = 0.21932520\n",
      "Iteration 234, loss = 0.21819807\n",
      "Iteration 235, loss = 0.21707538\n",
      "Iteration 236, loss = 0.21595794\n",
      "Iteration 237, loss = 0.21484690\n",
      "Iteration 238, loss = 0.21374061\n",
      "Iteration 239, loss = 0.21263915\n",
      "Iteration 240, loss = 0.21154308\n",
      "Iteration 241, loss = 0.21045301\n",
      "Iteration 242, loss = 0.20936861\n",
      "Iteration 243, loss = 0.20829044\n",
      "Iteration 244, loss = 0.20721841\n",
      "Iteration 245, loss = 0.20615286\n",
      "Iteration 246, loss = 0.20509381\n",
      "Iteration 247, loss = 0.20404201\n",
      "Iteration 248, loss = 0.20299648\n",
      "Iteration 249, loss = 0.20195781\n",
      "Iteration 250, loss = 0.20092668\n",
      "Iteration 251, loss = 0.19990243\n",
      "Iteration 252, loss = 0.19888521\n",
      "Iteration 253, loss = 0.19787572\n",
      "Iteration 254, loss = 0.19687315\n",
      "Iteration 255, loss = 0.19587780\n",
      "Iteration 256, loss = 0.19489042\n",
      "Iteration 257, loss = 0.19391064\n",
      "Iteration 258, loss = 0.19293854\n",
      "Iteration 259, loss = 0.19197430\n",
      "Iteration 260, loss = 0.19101792\n",
      "Iteration 261, loss = 0.19006966\n",
      "Iteration 262, loss = 0.18912927\n",
      "Iteration 263, loss = 0.18819683\n",
      "Iteration 264, loss = 0.18727246\n",
      "Iteration 265, loss = 0.18635595\n",
      "Iteration 266, loss = 0.18544766\n",
      "Iteration 267, loss = 0.18454725\n",
      "Iteration 268, loss = 0.18365460\n",
      "Iteration 269, loss = 0.18276987\n",
      "Iteration 270, loss = 0.18189379\n",
      "Iteration 271, loss = 0.18102571\n",
      "Iteration 272, loss = 0.18016554\n",
      "Iteration 273, loss = 0.17931306\n",
      "Iteration 274, loss = 0.17846885\n",
      "Iteration 275, loss = 0.17763252\n",
      "Iteration 276, loss = 0.17680429\n",
      "Iteration 277, loss = 0.17598373\n",
      "Iteration 278, loss = 0.17517141\n",
      "Iteration 279, loss = 0.17436706\n",
      "Iteration 280, loss = 0.17357049\n",
      "Iteration 281, loss = 0.17278171\n",
      "Iteration 282, loss = 0.17200082\n",
      "Iteration 283, loss = 0.17122752\n",
      "Iteration 284, loss = 0.17046193\n",
      "Iteration 285, loss = 0.16970370\n",
      "Iteration 286, loss = 0.16895283\n",
      "Iteration 287, loss = 0.16820965\n",
      "Iteration 288, loss = 0.16747406\n",
      "Iteration 289, loss = 0.16674564\n",
      "Iteration 290, loss = 0.16602433\n",
      "Iteration 291, loss = 0.16531026\n",
      "Iteration 292, loss = 0.16460351\n",
      "Iteration 293, loss = 0.16390359\n",
      "Iteration 294, loss = 0.16321046\n",
      "Iteration 295, loss = 0.16252447\n",
      "Iteration 296, loss = 0.16184550\n",
      "Iteration 297, loss = 0.16117346\n",
      "Iteration 298, loss = 0.16050827\n",
      "Iteration 299, loss = 0.15984995\n",
      "Iteration 300, loss = 0.15919810\n",
      "Iteration 301, loss = 0.15855339\n",
      "Iteration 302, loss = 0.15791515\n",
      "Iteration 303, loss = 0.15728334\n",
      "Iteration 304, loss = 0.15665793\n",
      "Iteration 305, loss = 0.15603886\n",
      "Iteration 306, loss = 0.15542614\n",
      "Iteration 307, loss = 0.15481966\n",
      "Iteration 308, loss = 0.15421939\n",
      "Iteration 309, loss = 0.15362531\n",
      "Iteration 310, loss = 0.15303726\n",
      "Iteration 311, loss = 0.15245518\n",
      "Iteration 312, loss = 0.15187906\n",
      "Iteration 313, loss = 0.15130879\n",
      "Iteration 314, loss = 0.15074416\n",
      "Iteration 315, loss = 0.15018481\n",
      "Iteration 316, loss = 0.14963148\n",
      "Iteration 317, loss = 0.14908360\n",
      "Iteration 318, loss = 0.14854097\n",
      "Iteration 319, loss = 0.14800353\n",
      "Iteration 320, loss = 0.14747117\n",
      "Iteration 321, loss = 0.14694355\n",
      "Iteration 322, loss = 0.14642037\n",
      "Iteration 323, loss = 0.14590116\n",
      "Iteration 324, loss = 0.14538580\n",
      "Iteration 325, loss = 0.14487415\n",
      "Iteration 326, loss = 0.14436614\n",
      "Iteration 327, loss = 0.14386155\n",
      "Iteration 328, loss = 0.14336039\n",
      "Iteration 329, loss = 0.14286220\n",
      "Iteration 330, loss = 0.14236732\n",
      "Iteration 331, loss = 0.14187547\n",
      "Iteration 332, loss = 0.14138666\n",
      "Iteration 333, loss = 0.14090093\n",
      "Iteration 334, loss = 0.14041797\n",
      "Iteration 335, loss = 0.13993660\n",
      "Iteration 336, loss = 0.13946768\n",
      "Iteration 337, loss = 0.13898580\n",
      "Iteration 338, loss = 0.13851712\n",
      "Iteration 339, loss = 0.13805018\n",
      "Iteration 340, loss = 0.13758603\n",
      "Iteration 341, loss = 0.13712490\n",
      "Iteration 342, loss = 0.13666672\n",
      "Iteration 343, loss = 0.13621159\n",
      "Iteration 344, loss = 0.13575950\n",
      "Iteration 345, loss = 0.13531042\n",
      "Iteration 346, loss = 0.13486417\n",
      "Iteration 347, loss = 0.13442116\n",
      "Iteration 348, loss = 0.13398178\n",
      "Iteration 349, loss = 0.13354488\n",
      "Iteration 350, loss = 0.13311093\n",
      "Iteration 351, loss = 0.13267988\n",
      "Iteration 352, loss = 0.13225270\n",
      "Iteration 353, loss = 0.13182630\n",
      "Iteration 354, loss = 0.13140412\n",
      "Iteration 355, loss = 0.13098412\n",
      "Iteration 356, loss = 0.13056585\n",
      "Iteration 357, loss = 0.13014899\n",
      "Iteration 358, loss = 0.12973599\n",
      "Iteration 359, loss = 0.12932326\n",
      "Iteration 360, loss = 0.12891367\n",
      "Iteration 361, loss = 0.12850575\n",
      "Iteration 362, loss = 0.12809835\n",
      "Iteration 363, loss = 0.12769344\n",
      "Iteration 364, loss = 0.12728980\n",
      "Iteration 365, loss = 0.12688699\n",
      "Iteration 366, loss = 0.12648665\n",
      "Iteration 367, loss = 0.12608746\n",
      "Iteration 368, loss = 0.12568879\n",
      "Iteration 369, loss = 0.12529123\n",
      "Iteration 370, loss = 0.12489485\n",
      "Iteration 371, loss = 0.12450078\n",
      "Iteration 372, loss = 0.12410644\n",
      "Iteration 373, loss = 0.12371346\n",
      "Iteration 374, loss = 0.12332158\n",
      "Iteration 375, loss = 0.12293040\n",
      "Iteration 376, loss = 0.12253992\n",
      "Iteration 377, loss = 0.12215052\n",
      "Iteration 378, loss = 0.12176120\n",
      "Iteration 379, loss = 0.12137368\n",
      "Iteration 380, loss = 0.12098679\n",
      "Iteration 381, loss = 0.12060104\n",
      "Iteration 382, loss = 0.12021538\n",
      "Iteration 383, loss = 0.11983004\n",
      "Iteration 384, loss = 0.11944611\n",
      "Iteration 385, loss = 0.11906490\n",
      "Iteration 386, loss = 0.11868281\n",
      "Iteration 387, loss = 0.11830372\n",
      "Iteration 388, loss = 0.11792579\n",
      "Iteration 389, loss = 0.11754893\n",
      "Iteration 390, loss = 0.11717380\n",
      "Iteration 391, loss = 0.11680089\n",
      "Iteration 392, loss = 0.11642897\n",
      "Iteration 393, loss = 0.11605980\n",
      "Iteration 394, loss = 0.11569226\n",
      "Iteration 395, loss = 0.11532519\n",
      "Iteration 396, loss = 0.11495889\n",
      "Iteration 397, loss = 0.11459508\n",
      "Iteration 398, loss = 0.11423296\n",
      "Iteration 399, loss = 0.11387222\n",
      "Iteration 400, loss = 0.11351411\n",
      "Iteration 401, loss = 0.11315791\n",
      "Iteration 402, loss = 0.11280318\n",
      "Iteration 403, loss = 0.11245092\n",
      "Iteration 404, loss = 0.11210055\n",
      "Iteration 405, loss = 0.11175201\n",
      "Iteration 406, loss = 0.11140563\n",
      "Iteration 407, loss = 0.11106097\n",
      "Iteration 408, loss = 0.11071806\n",
      "Iteration 409, loss = 0.11037738\n",
      "Iteration 410, loss = 0.11003870\n",
      "Iteration 411, loss = 0.10970210\n",
      "Iteration 412, loss = 0.10936765\n",
      "Iteration 413, loss = 0.10903522\n",
      "Iteration 414, loss = 0.10870500\n",
      "Iteration 415, loss = 0.10837715\n",
      "Iteration 416, loss = 0.10805165\n",
      "Iteration 417, loss = 0.10772839\n",
      "Iteration 418, loss = 0.10740752\n",
      "Iteration 419, loss = 0.10708927\n",
      "Iteration 420, loss = 0.10677353\n",
      "Iteration 421, loss = 0.10646032\n",
      "Iteration 422, loss = 0.10614969\n",
      "Iteration 423, loss = 0.10584178\n",
      "Iteration 424, loss = 0.10553676\n",
      "Iteration 425, loss = 0.10523445\n",
      "Iteration 426, loss = 0.10493491\n",
      "Iteration 427, loss = 0.10463809\n",
      "Iteration 428, loss = 0.10434452\n",
      "Iteration 429, loss = 0.10405389\n",
      "Iteration 430, loss = 0.10376614\n",
      "Iteration 431, loss = 0.10348130\n",
      "Iteration 432, loss = 0.10319941\n",
      "Iteration 433, loss = 0.10292083\n",
      "Iteration 434, loss = 0.10264535\n",
      "Iteration 435, loss = 0.10237259\n",
      "Iteration 436, loss = 0.10210287\n",
      "Iteration 437, loss = 0.10183626\n",
      "Iteration 438, loss = 0.10157261\n",
      "Iteration 439, loss = 0.10131161\n",
      "Iteration 440, loss = 0.10105191\n",
      "Iteration 441, loss = 0.10079572\n",
      "Iteration 442, loss = 0.10054266\n",
      "Iteration 443, loss = 0.10029222\n",
      "Iteration 444, loss = 0.10004423\n",
      "Iteration 445, loss = 0.09979875\n",
      "Iteration 446, loss = 0.09955576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 447, loss = 0.09931526\n",
      "Iteration 448, loss = 0.09907718\n",
      "Iteration 449, loss = 0.09884165\n",
      "Iteration 450, loss = 0.09860845\n",
      "Iteration 451, loss = 0.09837760\n",
      "Iteration 452, loss = 0.09814918\n",
      "Iteration 453, loss = 0.09792338\n",
      "Iteration 454, loss = 0.09770074\n",
      "Iteration 455, loss = 0.09747998\n",
      "Iteration 456, loss = 0.09726112\n",
      "Iteration 457, loss = 0.09704500\n",
      "Iteration 458, loss = 0.09683153\n",
      "Iteration 459, loss = 0.09662026\n",
      "Iteration 460, loss = 0.09641137\n",
      "Iteration 461, loss = 0.09620471\n",
      "Iteration 462, loss = 0.09600020\n",
      "Iteration 463, loss = 0.09579806\n",
      "Iteration 464, loss = 0.09559814\n",
      "Iteration 465, loss = 0.09540035\n",
      "Iteration 466, loss = 0.09520465\n",
      "Iteration 467, loss = 0.09501141\n",
      "Iteration 468, loss = 0.09482007\n",
      "Iteration 469, loss = 0.09463086\n",
      "Iteration 470, loss = 0.09444381\n",
      "Iteration 471, loss = 0.09425871\n",
      "Iteration 472, loss = 0.09407560\n",
      "Iteration 473, loss = 0.09389462\n",
      "Iteration 474, loss = 0.09371580\n",
      "Iteration 475, loss = 0.09353882\n",
      "Iteration 476, loss = 0.09336379\n",
      "Iteration 477, loss = 0.09319066\n",
      "Iteration 478, loss = 0.09301958\n",
      "Iteration 479, loss = 0.09285034\n",
      "Iteration 480, loss = 0.09268294\n",
      "Iteration 481, loss = 0.09251721\n",
      "Iteration 482, loss = 0.09235328\n",
      "Iteration 483, loss = 0.09219114\n",
      "Iteration 484, loss = 0.09203088\n",
      "Iteration 485, loss = 0.09187232\n",
      "Iteration 486, loss = 0.09171546\n",
      "Iteration 487, loss = 0.09156031\n",
      "Iteration 488, loss = 0.09140682\n",
      "Iteration 489, loss = 0.09125505\n",
      "Iteration 490, loss = 0.09110498\n",
      "Iteration 491, loss = 0.09095653\n",
      "Iteration 492, loss = 0.09080972\n",
      "Iteration 493, loss = 0.09066470\n",
      "Iteration 494, loss = 0.09052126\n",
      "Iteration 495, loss = 0.09037933\n",
      "Iteration 496, loss = 0.09023898\n",
      "Iteration 497, loss = 0.09010023\n",
      "Iteration 498, loss = 0.08996306\n",
      "Iteration 499, loss = 0.08982741\n",
      "Iteration 500, loss = 0.08969334\n",
      "Iteration 501, loss = 0.08956085\n",
      "Iteration 502, loss = 0.08942987\n",
      "Iteration 503, loss = 0.08930044\n",
      "Iteration 504, loss = 0.08917256\n",
      "Iteration 505, loss = 0.08904613\n",
      "Iteration 506, loss = 0.08892118\n",
      "Iteration 507, loss = 0.08879779\n",
      "Iteration 508, loss = 0.08867589\n",
      "Iteration 509, loss = 0.08855547\n",
      "Iteration 510, loss = 0.08843651\n",
      "Iteration 511, loss = 0.08831895\n",
      "Iteration 512, loss = 0.08820281\n",
      "Iteration 513, loss = 0.08808804\n",
      "Iteration 514, loss = 0.08797470\n",
      "Iteration 515, loss = 0.08786271\n",
      "Iteration 516, loss = 0.08775206\n",
      "Iteration 517, loss = 0.08764280\n",
      "Iteration 518, loss = 0.08753489\n",
      "Iteration 519, loss = 0.08742832\n",
      "Iteration 520, loss = 0.08732305\n",
      "Iteration 521, loss = 0.08721902\n",
      "Iteration 522, loss = 0.08711620\n",
      "Iteration 523, loss = 0.08701463\n",
      "Iteration 524, loss = 0.08691435\n",
      "Iteration 525, loss = 0.08681529\n",
      "Iteration 526, loss = 0.08671747\n",
      "Iteration 527, loss = 0.08662081\n",
      "Iteration 528, loss = 0.08652532\n",
      "Iteration 529, loss = 0.08643099\n",
      "Iteration 530, loss = 0.08633778\n",
      "Iteration 531, loss = 0.08624570\n",
      "Iteration 532, loss = 0.08615482\n",
      "Iteration 533, loss = 0.08606523\n",
      "Iteration 534, loss = 0.08597723\n",
      "Iteration 535, loss = 0.08589056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35652749\n",
      "Iteration 2, loss = 1.33251874\n",
      "Iteration 3, loss = 1.30028251\n",
      "Iteration 4, loss = 1.26220532\n",
      "Iteration 5, loss = 1.22036152\n",
      "Iteration 6, loss = 1.17648882\n",
      "Iteration 7, loss = 1.13187949\n",
      "Iteration 8, loss = 1.08757483\n",
      "Iteration 9, loss = 1.04467697\n",
      "Iteration 10, loss = 1.00420713\n",
      "Iteration 11, loss = 0.96728796\n",
      "Iteration 12, loss = 0.93495560\n",
      "Iteration 13, loss = 0.90779413\n",
      "Iteration 14, loss = 0.88610606\n",
      "Iteration 15, loss = 0.86953071\n",
      "Iteration 16, loss = 0.85768043\n",
      "Iteration 17, loss = 0.84939917\n",
      "Iteration 18, loss = 0.84341836\n",
      "Iteration 19, loss = 0.83831328\n",
      "Iteration 20, loss = 0.83279345\n",
      "Iteration 21, loss = 0.82610169\n",
      "Iteration 22, loss = 0.81779481\n",
      "Iteration 23, loss = 0.80814864\n",
      "Iteration 24, loss = 0.79761207\n",
      "Iteration 25, loss = 0.78691860\n",
      "Iteration 26, loss = 0.77596961\n",
      "Iteration 27, loss = 0.76486954\n",
      "Iteration 28, loss = 0.75384094\n",
      "Iteration 29, loss = 0.74322311\n",
      "Iteration 30, loss = 0.73341551\n",
      "Iteration 31, loss = 0.72463527\n",
      "Iteration 32, loss = 0.71671672\n",
      "Iteration 33, loss = 0.70955530\n",
      "Iteration 34, loss = 0.70314690\n",
      "Iteration 35, loss = 0.69738192\n",
      "Iteration 36, loss = 0.69196005\n",
      "Iteration 37, loss = 0.68678635\n",
      "Iteration 38, loss = 0.68182348\n",
      "Iteration 39, loss = 0.67697079\n",
      "Iteration 40, loss = 0.67218381\n",
      "Iteration 41, loss = 0.66744734\n",
      "Iteration 42, loss = 0.66276911\n",
      "Iteration 43, loss = 0.65814624\n",
      "Iteration 44, loss = 0.65356903\n",
      "Iteration 45, loss = 0.64905466\n",
      "Iteration 46, loss = 0.64464812\n",
      "Iteration 47, loss = 0.64032725\n",
      "Iteration 48, loss = 0.63608246\n",
      "Iteration 49, loss = 0.63196474\n",
      "Iteration 50, loss = 0.62801540\n",
      "Iteration 51, loss = 0.62423111\n",
      "Iteration 52, loss = 0.62057959\n",
      "Iteration 53, loss = 0.61709488\n",
      "Iteration 54, loss = 0.61381177\n",
      "Iteration 55, loss = 0.61069376\n",
      "Iteration 56, loss = 0.60767678\n",
      "Iteration 57, loss = 0.60475461\n",
      "Iteration 58, loss = 0.60189720\n",
      "Iteration 59, loss = 0.59908276\n",
      "Iteration 60, loss = 0.59630261\n",
      "Iteration 61, loss = 0.59354767\n",
      "Iteration 62, loss = 0.59081674\n",
      "Iteration 63, loss = 0.58811160\n",
      "Iteration 64, loss = 0.58542954\n",
      "Iteration 65, loss = 0.58277465\n",
      "Iteration 66, loss = 0.58015433\n",
      "Iteration 67, loss = 0.57757430\n",
      "Iteration 68, loss = 0.57503822\n",
      "Iteration 69, loss = 0.57254309\n",
      "Iteration 70, loss = 0.57008648\n",
      "Iteration 71, loss = 0.56767845\n",
      "Iteration 72, loss = 0.56530894\n",
      "Iteration 73, loss = 0.56298990\n",
      "Iteration 74, loss = 0.56071790\n",
      "Iteration 75, loss = 0.55848628\n",
      "Iteration 76, loss = 0.55629236\n",
      "Iteration 77, loss = 0.55413646\n",
      "Iteration 78, loss = 0.55201681\n",
      "Iteration 79, loss = 0.54993498\n",
      "Iteration 80, loss = 0.54788600\n",
      "Iteration 81, loss = 0.54586577\n",
      "Iteration 82, loss = 0.54387624\n",
      "Iteration 83, loss = 0.54191464\n",
      "Iteration 84, loss = 0.53997983\n",
      "Iteration 85, loss = 0.53807149\n",
      "Iteration 86, loss = 0.53618815\n",
      "Iteration 87, loss = 0.53432990\n",
      "Iteration 88, loss = 0.53249662\n",
      "Iteration 89, loss = 0.53068844\n",
      "Iteration 90, loss = 0.52890418\n",
      "Iteration 91, loss = 0.52714357\n",
      "Iteration 92, loss = 0.52540607\n",
      "Iteration 93, loss = 0.52369258\n",
      "Iteration 94, loss = 0.52200188\n",
      "Iteration 95, loss = 0.52033342\n",
      "Iteration 96, loss = 0.51868635\n",
      "Iteration 97, loss = 0.51706030\n",
      "Iteration 98, loss = 0.51545553\n",
      "Iteration 99, loss = 0.51387111\n",
      "Iteration 100, loss = 0.51230558\n",
      "Iteration 101, loss = 0.51075917\n",
      "Iteration 102, loss = 0.50923154\n",
      "Iteration 103, loss = 0.50772157\n",
      "Iteration 104, loss = 0.50622938\n",
      "Iteration 105, loss = 0.50475433\n",
      "Iteration 106, loss = 0.50329699\n",
      "Iteration 107, loss = 0.50185644\n",
      "Iteration 108, loss = 0.50043261\n",
      "Iteration 109, loss = 0.49902488\n",
      "Iteration 110, loss = 0.49763300\n",
      "Iteration 111, loss = 0.49625661\n",
      "Iteration 112, loss = 0.49489591\n",
      "Iteration 113, loss = 0.49355031\n",
      "Iteration 114, loss = 0.49221945\n",
      "Iteration 115, loss = 0.49090300\n",
      "Iteration 116, loss = 0.48960079\n",
      "Iteration 117, loss = 0.48831237\n",
      "Iteration 118, loss = 0.48703705\n",
      "Iteration 119, loss = 0.48577466\n",
      "Iteration 120, loss = 0.48452554\n",
      "Iteration 121, loss = 0.48328965\n",
      "Iteration 122, loss = 0.48206618\n",
      "Iteration 123, loss = 0.48085548\n",
      "Iteration 124, loss = 0.47965719\n",
      "Iteration 125, loss = 0.47847023\n",
      "Iteration 126, loss = 0.47729383\n",
      "Iteration 127, loss = 0.47612856\n",
      "Iteration 128, loss = 0.47497417\n",
      "Iteration 129, loss = 0.47383101\n",
      "Iteration 130, loss = 0.47269906\n",
      "Iteration 131, loss = 0.47157707\n",
      "Iteration 132, loss = 0.47046479\n",
      "Iteration 133, loss = 0.46936306\n",
      "Iteration 134, loss = 0.46826934\n",
      "Iteration 135, loss = 0.46718379\n",
      "Iteration 136, loss = 0.46610740\n",
      "Iteration 137, loss = 0.46503947\n",
      "Iteration 138, loss = 0.46397944\n",
      "Iteration 139, loss = 0.46292695\n",
      "Iteration 140, loss = 0.46188201\n",
      "Iteration 141, loss = 0.46084318\n",
      "Iteration 142, loss = 0.45981034\n",
      "Iteration 143, loss = 0.45878307\n",
      "Iteration 144, loss = 0.45776370\n",
      "Iteration 145, loss = 0.45675174\n",
      "Iteration 146, loss = 0.45574734\n",
      "Iteration 147, loss = 0.45474428\n",
      "Iteration 148, loss = 0.45374183\n",
      "Iteration 149, loss = 0.45274017\n",
      "Iteration 150, loss = 0.45174255\n",
      "Iteration 151, loss = 0.45074611\n",
      "Iteration 152, loss = 0.44974898\n",
      "Iteration 153, loss = 0.44875306\n",
      "Iteration 154, loss = 0.44775528\n",
      "Iteration 155, loss = 0.44676278\n",
      "Iteration 156, loss = 0.44577573\n",
      "Iteration 157, loss = 0.44479286\n",
      "Iteration 158, loss = 0.44380904\n",
      "Iteration 159, loss = 0.44282929\n",
      "Iteration 160, loss = 0.44184805\n",
      "Iteration 161, loss = 0.44084732\n",
      "Iteration 162, loss = 0.43984588\n",
      "Iteration 163, loss = 0.43883661\n",
      "Iteration 164, loss = 0.43782582\n",
      "Iteration 165, loss = 0.43681631\n",
      "Iteration 166, loss = 0.43580727\n",
      "Iteration 167, loss = 0.43480368\n",
      "Iteration 168, loss = 0.43379616\n",
      "Iteration 169, loss = 0.43279441\n",
      "Iteration 170, loss = 0.43178934\n",
      "Iteration 171, loss = 0.43080375\n",
      "Iteration 172, loss = 0.42984062\n",
      "Iteration 173, loss = 0.42888936\n",
      "Iteration 174, loss = 0.42795604\n",
      "Iteration 175, loss = 0.42703714\n",
      "Iteration 176, loss = 0.42613423\n",
      "Iteration 177, loss = 0.42524737\n",
      "Iteration 178, loss = 0.42439121\n",
      "Iteration 179, loss = 0.42355046\n",
      "Iteration 180, loss = 0.42272655\n",
      "Iteration 181, loss = 0.42191387\n",
      "Iteration 182, loss = 0.42111670\n",
      "Iteration 183, loss = 0.42032678\n",
      "Iteration 184, loss = 0.41954537\n",
      "Iteration 185, loss = 0.41877586\n",
      "Iteration 186, loss = 0.41801720\n",
      "Iteration 187, loss = 0.41726853\n",
      "Iteration 188, loss = 0.41652612\n",
      "Iteration 189, loss = 0.41578891\n",
      "Iteration 190, loss = 0.41505945\n",
      "Iteration 191, loss = 0.41433915\n",
      "Iteration 192, loss = 0.41362564\n",
      "Iteration 193, loss = 0.41291725\n",
      "Iteration 194, loss = 0.41221385\n",
      "Iteration 195, loss = 0.41151574\n",
      "Iteration 196, loss = 0.41082167\n",
      "Iteration 197, loss = 0.41013171\n",
      "Iteration 198, loss = 0.40944559\n",
      "Iteration 199, loss = 0.40876463\n",
      "Iteration 200, loss = 0.40808746\n",
      "Iteration 201, loss = 0.40741431\n",
      "Iteration 202, loss = 0.40674540\n",
      "Iteration 203, loss = 0.40608014\n",
      "Iteration 204, loss = 0.40541839\n",
      "Iteration 205, loss = 0.40476034\n",
      "Iteration 206, loss = 0.40410549\n",
      "Iteration 207, loss = 0.40345403\n",
      "Iteration 208, loss = 0.40280593\n",
      "Iteration 209, loss = 0.40216097\n",
      "Iteration 210, loss = 0.40151920\n",
      "Iteration 211, loss = 0.40088049\n",
      "Iteration 212, loss = 0.40024569\n",
      "Iteration 213, loss = 0.39961369\n",
      "Iteration 214, loss = 0.39898481\n",
      "Iteration 215, loss = 0.39835866\n",
      "Iteration 216, loss = 0.39773537\n",
      "Iteration 217, loss = 0.39711503\n",
      "Iteration 218, loss = 0.39649743\n",
      "Iteration 219, loss = 0.39588261\n",
      "Iteration 220, loss = 0.39527050\n",
      "Iteration 221, loss = 0.39466101\n",
      "Iteration 222, loss = 0.39405424\n",
      "Iteration 223, loss = 0.39345034\n",
      "Iteration 224, loss = 0.39284913\n",
      "Iteration 225, loss = 0.39225057\n",
      "Iteration 226, loss = 0.39165460\n",
      "Iteration 227, loss = 0.39106106\n",
      "Iteration 228, loss = 0.39047066\n",
      "Iteration 229, loss = 0.38988292\n",
      "Iteration 230, loss = 0.38929755\n",
      "Iteration 231, loss = 0.38871446\n",
      "Iteration 232, loss = 0.38813442\n",
      "Iteration 233, loss = 0.38755678\n",
      "Iteration 234, loss = 0.38698140\n",
      "Iteration 235, loss = 0.38640851\n",
      "Iteration 236, loss = 0.38583813\n",
      "Iteration 237, loss = 0.38526992\n",
      "Iteration 238, loss = 0.38470392\n",
      "Iteration 239, loss = 0.38414032\n",
      "Iteration 240, loss = 0.38357912\n",
      "Iteration 241, loss = 0.38301963\n",
      "Iteration 242, loss = 0.38246249\n",
      "Iteration 243, loss = 0.38190752\n",
      "Iteration 244, loss = 0.38135461\n",
      "Iteration 245, loss = 0.38080384\n",
      "Iteration 246, loss = 0.38025510\n",
      "Iteration 247, loss = 0.37970830\n",
      "Iteration 248, loss = 0.37916355\n",
      "Iteration 249, loss = 0.37862089\n",
      "Iteration 250, loss = 0.37808023\n",
      "Iteration 251, loss = 0.37754152\n",
      "Iteration 252, loss = 0.37700483\n",
      "Iteration 253, loss = 0.37647005\n",
      "Iteration 254, loss = 0.37593720\n",
      "Iteration 255, loss = 0.37540621\n",
      "Iteration 256, loss = 0.37487711\n",
      "Iteration 257, loss = 0.37434989\n",
      "Iteration 258, loss = 0.37382451\n",
      "Iteration 259, loss = 0.37330094\n",
      "Iteration 260, loss = 0.37277922\n",
      "Iteration 261, loss = 0.37225926\n",
      "Iteration 262, loss = 0.37174099\n",
      "Iteration 263, loss = 0.37122444\n",
      "Iteration 264, loss = 0.37070962\n",
      "Iteration 265, loss = 0.37019647\n",
      "Iteration 266, loss = 0.36968505\n",
      "Iteration 267, loss = 0.36917534\n",
      "Iteration 268, loss = 0.36866730\n",
      "Iteration 269, loss = 0.36816107\n",
      "Iteration 270, loss = 0.36765647\n",
      "Iteration 271, loss = 0.36715352\n",
      "Iteration 272, loss = 0.36665219\n",
      "Iteration 273, loss = 0.36615243\n",
      "Iteration 274, loss = 0.36565431\n",
      "Iteration 275, loss = 0.36515775\n",
      "Iteration 276, loss = 0.36466282\n",
      "Iteration 277, loss = 0.36416928\n",
      "Iteration 278, loss = 0.36367725\n",
      "Iteration 279, loss = 0.36318677\n",
      "Iteration 280, loss = 0.36269776\n",
      "Iteration 281, loss = 0.36221024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 282, loss = 0.36172429\n",
      "Iteration 283, loss = 0.36123977\n",
      "Iteration 284, loss = 0.36075678\n",
      "Iteration 285, loss = 0.36027521\n",
      "Iteration 286, loss = 0.35979508\n",
      "Iteration 287, loss = 0.35931641\n",
      "Iteration 288, loss = 0.35883907\n",
      "Iteration 289, loss = 0.35836317\n",
      "Iteration 290, loss = 0.35788865\n",
      "Iteration 291, loss = 0.35741551\n",
      "Iteration 292, loss = 0.35694393\n",
      "Iteration 293, loss = 0.35647369\n",
      "Iteration 294, loss = 0.35600474\n",
      "Iteration 295, loss = 0.35553713\n",
      "Iteration 296, loss = 0.35507080\n",
      "Iteration 297, loss = 0.35460589\n",
      "Iteration 298, loss = 0.35414232\n",
      "Iteration 299, loss = 0.35368006\n",
      "Iteration 300, loss = 0.35321909\n",
      "Iteration 301, loss = 0.35275942\n",
      "Iteration 302, loss = 0.35230114\n",
      "Iteration 303, loss = 0.35184409\n",
      "Iteration 304, loss = 0.35138825\n",
      "Iteration 305, loss = 0.35093373\n",
      "Iteration 306, loss = 0.35048052\n",
      "Iteration 307, loss = 0.35002850\n",
      "Iteration 308, loss = 0.34957774\n",
      "Iteration 309, loss = 0.34912834\n",
      "Iteration 310, loss = 0.34867999\n",
      "Iteration 311, loss = 0.34823299\n",
      "Iteration 312, loss = 0.34778719\n",
      "Iteration 313, loss = 0.34734259\n",
      "Iteration 314, loss = 0.34689921\n",
      "Iteration 315, loss = 0.34645698\n",
      "Iteration 316, loss = 0.34601598\n",
      "Iteration 317, loss = 0.34557610\n",
      "Iteration 318, loss = 0.34513746\n",
      "Iteration 319, loss = 0.34469991\n",
      "Iteration 320, loss = 0.34426354\n",
      "Iteration 321, loss = 0.34382831\n",
      "Iteration 322, loss = 0.34339425\n",
      "Iteration 323, loss = 0.34296128\n",
      "Iteration 324, loss = 0.34252946\n",
      "Iteration 325, loss = 0.34209873\n",
      "Iteration 326, loss = 0.34166913\n",
      "Iteration 327, loss = 0.34124062\n",
      "Iteration 328, loss = 0.34081327\n",
      "Iteration 329, loss = 0.34038697\n",
      "Iteration 330, loss = 0.33996209\n",
      "Iteration 331, loss = 0.33953837\n",
      "Iteration 332, loss = 0.33911572\n",
      "Iteration 333, loss = 0.33869411\n",
      "Iteration 334, loss = 0.33827357\n",
      "Iteration 335, loss = 0.33785404\n",
      "Iteration 336, loss = 0.33743557\n",
      "Iteration 337, loss = 0.33701811\n",
      "Iteration 338, loss = 0.33660168\n",
      "Iteration 339, loss = 0.33618630\n",
      "Iteration 340, loss = 0.33577190\n",
      "Iteration 341, loss = 0.33535860\n",
      "Iteration 342, loss = 0.33494628\n",
      "Iteration 343, loss = 0.33453500\n",
      "Iteration 344, loss = 0.33412474\n",
      "Iteration 345, loss = 0.33371544\n",
      "Iteration 346, loss = 0.33330717\n",
      "Iteration 347, loss = 0.33289983\n",
      "Iteration 348, loss = 0.33249345\n",
      "Iteration 349, loss = 0.33208801\n",
      "Iteration 350, loss = 0.33168357\n",
      "Iteration 351, loss = 0.33128009\n",
      "Iteration 352, loss = 0.33087759\n",
      "Iteration 353, loss = 0.33047612\n",
      "Iteration 354, loss = 0.33007561\n",
      "Iteration 355, loss = 0.32967597\n",
      "Iteration 356, loss = 0.32927730\n",
      "Iteration 357, loss = 0.32887957\n",
      "Iteration 358, loss = 0.32848283\n",
      "Iteration 359, loss = 0.32808696\n",
      "Iteration 360, loss = 0.32769204\n",
      "Iteration 361, loss = 0.32729804\n",
      "Iteration 362, loss = 0.32690494\n",
      "Iteration 363, loss = 0.32651274\n",
      "Iteration 364, loss = 0.32612146\n",
      "Iteration 365, loss = 0.32573110\n",
      "Iteration 366, loss = 0.32534159\n",
      "Iteration 367, loss = 0.32495298\n",
      "Iteration 368, loss = 0.32456520\n",
      "Iteration 369, loss = 0.32417836\n",
      "Iteration 370, loss = 0.32379262\n",
      "Iteration 371, loss = 0.32340769\n",
      "Iteration 372, loss = 0.32302355\n",
      "Iteration 373, loss = 0.32264047\n",
      "Iteration 374, loss = 0.32225815\n",
      "Iteration 375, loss = 0.32187673\n",
      "Iteration 376, loss = 0.32149613\n",
      "Iteration 377, loss = 0.32111648\n",
      "Iteration 378, loss = 0.32073766\n",
      "Iteration 379, loss = 0.32035961\n",
      "Iteration 380, loss = 0.31998254\n",
      "Iteration 381, loss = 0.31960624\n",
      "Iteration 382, loss = 0.31923078\n",
      "Iteration 383, loss = 0.31885624\n",
      "Iteration 384, loss = 0.31848241\n",
      "Iteration 385, loss = 0.31810945\n",
      "Iteration 386, loss = 0.31773738\n",
      "Iteration 387, loss = 0.31736607\n",
      "Iteration 388, loss = 0.31699558\n",
      "Iteration 389, loss = 0.31662590\n",
      "Iteration 390, loss = 0.31625716\n",
      "Iteration 391, loss = 0.31588907\n",
      "Iteration 392, loss = 0.31552186\n",
      "Iteration 393, loss = 0.31515566\n",
      "Iteration 394, loss = 0.31479049\n",
      "Iteration 395, loss = 0.31442610\n",
      "Iteration 396, loss = 0.31406250\n",
      "Iteration 397, loss = 0.31369988\n",
      "Iteration 398, loss = 0.31333788\n",
      "Iteration 399, loss = 0.31297676\n",
      "Iteration 400, loss = 0.31261653\n",
      "Iteration 401, loss = 0.31225701\n",
      "Iteration 402, loss = 0.31189824\n",
      "Iteration 403, loss = 0.31154024\n",
      "Iteration 404, loss = 0.31118319\n",
      "Iteration 405, loss = 0.31082669\n",
      "Iteration 406, loss = 0.31047106\n",
      "Iteration 407, loss = 0.31011631\n",
      "Iteration 408, loss = 0.30976229\n",
      "Iteration 409, loss = 0.30940902\n",
      "Iteration 410, loss = 0.30905642\n",
      "Iteration 411, loss = 0.30870477\n",
      "Iteration 412, loss = 0.30835374\n",
      "Iteration 413, loss = 0.30800349\n",
      "Iteration 414, loss = 0.30765409\n",
      "Iteration 415, loss = 0.30730535\n",
      "Iteration 416, loss = 0.30695733\n",
      "Iteration 417, loss = 0.30661006\n",
      "Iteration 418, loss = 0.30626367\n",
      "Iteration 419, loss = 0.30591787\n",
      "Iteration 420, loss = 0.30557286\n",
      "Iteration 421, loss = 0.30522861\n",
      "Iteration 422, loss = 0.30488515\n",
      "Iteration 423, loss = 0.30454231\n",
      "Iteration 424, loss = 0.30420020\n",
      "Iteration 425, loss = 0.30385886\n",
      "Iteration 426, loss = 0.30351830\n",
      "Iteration 427, loss = 0.30317838\n",
      "Iteration 428, loss = 0.30283914\n",
      "Iteration 429, loss = 0.30250080\n",
      "Iteration 430, loss = 0.30216295\n",
      "Iteration 431, loss = 0.30182588\n",
      "Iteration 432, loss = 0.30148958\n",
      "Iteration 433, loss = 0.30115390\n",
      "Iteration 434, loss = 0.30081894\n",
      "Iteration 435, loss = 0.30048465\n",
      "Iteration 436, loss = 0.30015114\n",
      "Iteration 437, loss = 0.29981825\n",
      "Iteration 438, loss = 0.29948600\n",
      "Iteration 439, loss = 0.29915449\n",
      "Iteration 440, loss = 0.29882374\n",
      "Iteration 441, loss = 0.29849362\n",
      "Iteration 442, loss = 0.29816411\n",
      "Iteration 443, loss = 0.29783528\n",
      "Iteration 444, loss = 0.29750732\n",
      "Iteration 445, loss = 0.29717974\n",
      "Iteration 446, loss = 0.29685300\n",
      "Iteration 447, loss = 0.29652696\n",
      "Iteration 448, loss = 0.29620154\n",
      "Iteration 449, loss = 0.29587675\n",
      "Iteration 450, loss = 0.29555260\n",
      "Iteration 451, loss = 0.29522924\n",
      "Iteration 452, loss = 0.29490642\n",
      "Iteration 453, loss = 0.29458425\n",
      "Iteration 454, loss = 0.29426292\n",
      "Iteration 455, loss = 0.29394204\n",
      "Iteration 456, loss = 0.29362186\n",
      "Iteration 457, loss = 0.29330235\n",
      "Iteration 458, loss = 0.29298356\n",
      "Iteration 459, loss = 0.29266535\n",
      "Iteration 460, loss = 0.29234775\n",
      "Iteration 461, loss = 0.29203080\n",
      "Iteration 462, loss = 0.29171464\n",
      "Iteration 463, loss = 0.29139890\n",
      "Iteration 464, loss = 0.29108392\n",
      "Iteration 465, loss = 0.29076961\n",
      "Iteration 466, loss = 0.29045585\n",
      "Iteration 467, loss = 0.29014277\n",
      "Iteration 468, loss = 0.28983025\n",
      "Iteration 469, loss = 0.28951851\n",
      "Iteration 470, loss = 0.28920724\n",
      "Iteration 471, loss = 0.28889664\n",
      "Iteration 472, loss = 0.28858679\n",
      "Iteration 473, loss = 0.28827742\n",
      "Iteration 474, loss = 0.28796873\n",
      "Iteration 475, loss = 0.28766065\n",
      "Iteration 476, loss = 0.28735330\n",
      "Iteration 477, loss = 0.28704646\n",
      "Iteration 478, loss = 0.28674024\n",
      "Iteration 479, loss = 0.28643459\n",
      "Iteration 480, loss = 0.28612974\n",
      "Iteration 481, loss = 0.28582527\n",
      "Iteration 482, loss = 0.28552154\n",
      "Iteration 483, loss = 0.28521842\n",
      "Iteration 484, loss = 0.28491588\n",
      "Iteration 485, loss = 0.28461393\n",
      "Iteration 486, loss = 0.28431256\n",
      "Iteration 487, loss = 0.28401187\n",
      "Iteration 488, loss = 0.28371169\n",
      "Iteration 489, loss = 0.28341214\n",
      "Iteration 490, loss = 0.28311325\n",
      "Iteration 491, loss = 0.28281486\n",
      "Iteration 492, loss = 0.28251712\n",
      "Iteration 493, loss = 0.28221991\n",
      "Iteration 494, loss = 0.28192346\n",
      "Iteration 495, loss = 0.28162739\n",
      "Iteration 496, loss = 0.28133196\n",
      "Iteration 497, loss = 0.28103723\n",
      "Iteration 498, loss = 0.28074300\n",
      "Iteration 499, loss = 0.28044934\n",
      "Iteration 500, loss = 0.28015625\n",
      "Iteration 501, loss = 0.27986387\n",
      "Iteration 502, loss = 0.27957192\n",
      "Iteration 503, loss = 0.27928059\n",
      "Iteration 504, loss = 0.27898987\n",
      "Iteration 505, loss = 0.27869976\n",
      "Iteration 506, loss = 0.27841018\n",
      "Iteration 507, loss = 0.27812111\n",
      "Iteration 508, loss = 0.27783268\n",
      "Iteration 509, loss = 0.27754487\n",
      "Iteration 510, loss = 0.27725753\n",
      "Iteration 511, loss = 0.27697081\n",
      "Iteration 512, loss = 0.27668479\n",
      "Iteration 513, loss = 0.27639917\n",
      "Iteration 514, loss = 0.27611413\n",
      "Iteration 515, loss = 0.27582976\n",
      "Iteration 516, loss = 0.27554591\n",
      "Iteration 517, loss = 0.27526256\n",
      "Iteration 518, loss = 0.27497979\n",
      "Iteration 519, loss = 0.27469769\n",
      "Iteration 520, loss = 0.27441600\n",
      "Iteration 521, loss = 0.27413489\n",
      "Iteration 522, loss = 0.27385440\n",
      "Iteration 523, loss = 0.27357446\n",
      "Iteration 524, loss = 0.27329501\n",
      "Iteration 525, loss = 0.27301616\n",
      "Iteration 526, loss = 0.27273781\n",
      "Iteration 527, loss = 0.27246009\n",
      "Iteration 528, loss = 0.27218284\n",
      "Iteration 529, loss = 0.27190614\n",
      "Iteration 530, loss = 0.27163006\n",
      "Iteration 531, loss = 0.27135443\n",
      "Iteration 532, loss = 0.27107937\n",
      "Iteration 533, loss = 0.27080489\n",
      "Iteration 534, loss = 0.27053091\n",
      "Iteration 535, loss = 0.27025751\n",
      "Iteration 536, loss = 0.26998455\n",
      "Iteration 537, loss = 0.26971223\n",
      "Iteration 538, loss = 0.26944041\n",
      "Iteration 539, loss = 0.26916909\n",
      "Iteration 540, loss = 0.26889830\n",
      "Iteration 541, loss = 0.26862815\n",
      "Iteration 542, loss = 0.26835843\n",
      "Iteration 543, loss = 0.26808923\n",
      "Iteration 544, loss = 0.26782054\n",
      "Iteration 545, loss = 0.26755250\n",
      "Iteration 546, loss = 0.26728490\n",
      "Iteration 547, loss = 0.26701778\n",
      "Iteration 548, loss = 0.26675133\n",
      "Iteration 549, loss = 0.26648523\n",
      "Iteration 550, loss = 0.26621974\n",
      "Iteration 551, loss = 0.26595478\n",
      "Iteration 552, loss = 0.26569034\n",
      "Iteration 553, loss = 0.26542636\n",
      "Iteration 554, loss = 0.26516292\n",
      "Iteration 555, loss = 0.26490005\n",
      "Iteration 556, loss = 0.26463761\n",
      "Iteration 557, loss = 0.26437574\n",
      "Iteration 558, loss = 0.26411431\n",
      "Iteration 559, loss = 0.26385353\n",
      "Iteration 560, loss = 0.26359319\n",
      "Iteration 561, loss = 0.26333332\n",
      "Iteration 562, loss = 0.26307396\n",
      "Iteration 563, loss = 0.26281535\n",
      "Iteration 564, loss = 0.26255689\n",
      "Iteration 565, loss = 0.26229921\n",
      "Iteration 566, loss = 0.26204193\n",
      "Iteration 567, loss = 0.26178516\n",
      "Iteration 568, loss = 0.26152888\n",
      "Iteration 569, loss = 0.26127308\n",
      "Iteration 570, loss = 0.26101793\n",
      "Iteration 571, loss = 0.26076309\n",
      "Iteration 572, loss = 0.26050879\n",
      "Iteration 573, loss = 0.26025512\n",
      "Iteration 574, loss = 0.26000184\n",
      "Iteration 575, loss = 0.25974903\n",
      "Iteration 576, loss = 0.25949673\n",
      "Iteration 577, loss = 0.25924494\n",
      "Iteration 578, loss = 0.25899363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 579, loss = 0.25874281\n",
      "Iteration 580, loss = 0.25849249\n",
      "Iteration 581, loss = 0.25824270\n",
      "Iteration 582, loss = 0.25799333\n",
      "Iteration 583, loss = 0.25774443\n",
      "Iteration 584, loss = 0.25749608\n",
      "Iteration 585, loss = 0.25724819\n",
      "Iteration 586, loss = 0.25700074\n",
      "Iteration 587, loss = 0.25675379\n",
      "Iteration 588, loss = 0.25650741\n",
      "Iteration 589, loss = 0.25626142\n",
      "Iteration 590, loss = 0.25601593\n",
      "Iteration 591, loss = 0.25577084\n",
      "Iteration 592, loss = 0.25552646\n",
      "Iteration 593, loss = 0.25528226\n",
      "Iteration 594, loss = 0.25503869\n",
      "Iteration 595, loss = 0.25479563\n",
      "Iteration 596, loss = 0.25455294\n",
      "Iteration 597, loss = 0.25431077\n",
      "Iteration 598, loss = 0.25406901\n",
      "Iteration 599, loss = 0.25382790\n",
      "Iteration 600, loss = 0.25358705\n",
      "Iteration 601, loss = 0.25334673\n",
      "Iteration 602, loss = 0.25310697\n",
      "Iteration 603, loss = 0.25286760\n",
      "Iteration 604, loss = 0.25262867\n",
      "Iteration 605, loss = 0.25239022\n",
      "Iteration 606, loss = 0.25215227\n",
      "Iteration 607, loss = 0.25191475\n",
      "Iteration 608, loss = 0.25167767\n",
      "Iteration 609, loss = 0.25144109\n",
      "Iteration 610, loss = 0.25120502\n",
      "Iteration 611, loss = 0.25096930\n",
      "Iteration 612, loss = 0.25073407\n",
      "Iteration 613, loss = 0.25049939\n",
      "Iteration 614, loss = 0.25026503\n",
      "Iteration 615, loss = 0.25003121\n",
      "Iteration 616, loss = 0.24979780\n",
      "Iteration 617, loss = 0.24956492\n",
      "Iteration 618, loss = 0.24933241\n",
      "Iteration 619, loss = 0.24910034\n",
      "Iteration 620, loss = 0.24886879\n",
      "Iteration 621, loss = 0.24863766\n",
      "Iteration 622, loss = 0.24840694\n",
      "Iteration 623, loss = 0.24817669\n",
      "Iteration 624, loss = 0.24794697\n",
      "Iteration 625, loss = 0.24771757\n",
      "Iteration 626, loss = 0.24748862\n",
      "Iteration 627, loss = 0.24726025\n",
      "Iteration 628, loss = 0.24703218\n",
      "Iteration 629, loss = 0.24680462\n",
      "Iteration 630, loss = 0.24657744\n",
      "Iteration 631, loss = 0.24635082\n",
      "Iteration 632, loss = 0.24612453\n",
      "Iteration 633, loss = 0.24589868\n",
      "Iteration 634, loss = 0.24567332\n",
      "Iteration 635, loss = 0.24544840\n",
      "Iteration 636, loss = 0.24522387\n",
      "Iteration 637, loss = 0.24499979\n",
      "Iteration 638, loss = 0.24477614\n",
      "Iteration 639, loss = 0.24455298\n",
      "Iteration 640, loss = 0.24433019\n",
      "Iteration 641, loss = 0.24410780\n",
      "Iteration 642, loss = 0.24388599\n",
      "Iteration 643, loss = 0.24366443\n",
      "Iteration 644, loss = 0.24344336\n",
      "Iteration 645, loss = 0.24322280\n",
      "Iteration 646, loss = 0.24300262\n",
      "Iteration 647, loss = 0.24278283\n",
      "Iteration 648, loss = 0.24256349\n",
      "Iteration 649, loss = 0.24234467\n",
      "Iteration 650, loss = 0.24212614\n",
      "Iteration 651, loss = 0.24190805\n",
      "Iteration 652, loss = 0.24169043\n",
      "Iteration 653, loss = 0.24147325\n",
      "Iteration 654, loss = 0.24125641\n",
      "Iteration 655, loss = 0.24104006\n",
      "Iteration 656, loss = 0.24082409\n",
      "Iteration 657, loss = 0.24060857\n",
      "Iteration 658, loss = 0.24039341\n",
      "Iteration 659, loss = 0.24017870\n",
      "Iteration 660, loss = 0.23996446\n",
      "Iteration 661, loss = 0.23975054\n",
      "Iteration 662, loss = 0.23953705\n",
      "Iteration 663, loss = 0.23932403\n",
      "Iteration 664, loss = 0.23911140\n",
      "Iteration 665, loss = 0.23889915\n",
      "Iteration 666, loss = 0.23868727\n",
      "Iteration 667, loss = 0.23847593\n",
      "Iteration 668, loss = 0.23826489\n",
      "Iteration 669, loss = 0.23805426\n",
      "Iteration 670, loss = 0.23784408\n",
      "Iteration 671, loss = 0.23763432\n",
      "Iteration 672, loss = 0.23742494\n",
      "Iteration 673, loss = 0.23721593\n",
      "Iteration 674, loss = 0.23700732\n",
      "Iteration 675, loss = 0.23679919\n",
      "Iteration 676, loss = 0.23659140\n",
      "Iteration 677, loss = 0.23638399\n",
      "Iteration 678, loss = 0.23617707\n",
      "Iteration 679, loss = 0.23597047\n",
      "Iteration 680, loss = 0.23576428\n",
      "Iteration 681, loss = 0.23555856\n",
      "Iteration 682, loss = 0.23535317\n",
      "Iteration 683, loss = 0.23514822\n",
      "Iteration 684, loss = 0.23494359\n",
      "Iteration 685, loss = 0.23473946\n",
      "Iteration 686, loss = 0.23453577\n",
      "Iteration 687, loss = 0.23433239\n",
      "Iteration 688, loss = 0.23412946\n",
      "Iteration 689, loss = 0.23392706\n",
      "Iteration 690, loss = 0.23372490\n",
      "Iteration 691, loss = 0.23352319\n",
      "Iteration 692, loss = 0.23332182\n",
      "Iteration 693, loss = 0.23312102\n",
      "Iteration 694, loss = 0.23292036\n",
      "Iteration 695, loss = 0.23272023\n",
      "Iteration 696, loss = 0.23252053\n",
      "Iteration 697, loss = 0.23232114\n",
      "Iteration 698, loss = 0.23212213\n",
      "Iteration 699, loss = 0.23192348\n",
      "Iteration 700, loss = 0.23172532\n",
      "Iteration 701, loss = 0.23152742\n",
      "Iteration 702, loss = 0.23132995\n",
      "Iteration 703, loss = 0.23113294\n",
      "Iteration 704, loss = 0.23093622\n",
      "Iteration 705, loss = 0.23073985\n",
      "Iteration 706, loss = 0.23054393\n",
      "Iteration 707, loss = 0.23034837\n",
      "Iteration 708, loss = 0.23015316\n",
      "Iteration 709, loss = 0.22995835\n",
      "Iteration 710, loss = 0.22976396\n",
      "Iteration 711, loss = 0.22956992\n",
      "Iteration 712, loss = 0.22937622\n",
      "Iteration 713, loss = 0.22918305\n",
      "Iteration 714, loss = 0.22899003\n",
      "Iteration 715, loss = 0.22879752\n",
      "Iteration 716, loss = 0.22860538\n",
      "Iteration 717, loss = 0.22841359\n",
      "Iteration 718, loss = 0.22822211\n",
      "Iteration 719, loss = 0.22803108\n",
      "Iteration 720, loss = 0.22784038\n",
      "Iteration 721, loss = 0.22765002\n",
      "Iteration 722, loss = 0.22746007\n",
      "Iteration 723, loss = 0.22727050\n",
      "Iteration 724, loss = 0.22708123\n",
      "Iteration 725, loss = 0.22689237\n",
      "Iteration 726, loss = 0.22670390\n",
      "Iteration 727, loss = 0.22651573\n",
      "Iteration 728, loss = 0.22632789\n",
      "Iteration 729, loss = 0.22614059\n",
      "Iteration 730, loss = 0.22595340\n",
      "Iteration 731, loss = 0.22576675\n",
      "Iteration 732, loss = 0.22558039\n",
      "Iteration 733, loss = 0.22539438\n",
      "Iteration 734, loss = 0.22520868\n",
      "Iteration 735, loss = 0.22502349\n",
      "Iteration 736, loss = 0.22483847\n",
      "Iteration 737, loss = 0.22465388\n",
      "Iteration 738, loss = 0.22446972\n",
      "Iteration 739, loss = 0.22428585\n",
      "Iteration 740, loss = 0.22410228\n",
      "Iteration 741, loss = 0.22391908\n",
      "Iteration 742, loss = 0.22373630\n",
      "Iteration 743, loss = 0.22355378\n",
      "Iteration 744, loss = 0.22337166\n",
      "Iteration 745, loss = 0.22318989\n",
      "Iteration 746, loss = 0.22300845\n",
      "Iteration 747, loss = 0.22282734\n",
      "Iteration 748, loss = 0.22264665\n",
      "Iteration 749, loss = 0.22246621\n",
      "Iteration 750, loss = 0.22228615\n",
      "Iteration 751, loss = 0.22210648\n",
      "Iteration 752, loss = 0.22192712\n",
      "Iteration 753, loss = 0.22174805\n",
      "Iteration 754, loss = 0.22156946\n",
      "Iteration 755, loss = 0.22139102\n",
      "Iteration 756, loss = 0.22121309\n",
      "Iteration 757, loss = 0.22103546\n",
      "Iteration 758, loss = 0.22085808\n",
      "Iteration 759, loss = 0.22068106\n",
      "Iteration 760, loss = 0.22050442\n",
      "Iteration 761, loss = 0.22032808\n",
      "Iteration 762, loss = 0.22015208\n",
      "Iteration 763, loss = 0.21997648\n",
      "Iteration 764, loss = 0.21980114\n",
      "Iteration 765, loss = 0.21962615\n",
      "Iteration 766, loss = 0.21945158\n",
      "Iteration 767, loss = 0.21927719\n",
      "Iteration 768, loss = 0.21910326\n",
      "Iteration 769, loss = 0.21892960\n",
      "Iteration 770, loss = 0.21875628\n",
      "Iteration 771, loss = 0.21858326\n",
      "Iteration 772, loss = 0.21841068\n",
      "Iteration 773, loss = 0.21823830\n",
      "Iteration 774, loss = 0.21806627\n",
      "Iteration 775, loss = 0.21789469\n",
      "Iteration 776, loss = 0.21772324\n",
      "Iteration 777, loss = 0.21755229\n",
      "Iteration 778, loss = 0.21738158\n",
      "Iteration 779, loss = 0.21721118\n",
      "Iteration 780, loss = 0.21704107\n",
      "Iteration 781, loss = 0.21687144\n",
      "Iteration 782, loss = 0.21670192\n",
      "Iteration 783, loss = 0.21653289\n",
      "Iteration 784, loss = 0.21636412\n",
      "Iteration 785, loss = 0.21619562\n",
      "Iteration 786, loss = 0.21602748\n",
      "Iteration 787, loss = 0.21585969\n",
      "Iteration 788, loss = 0.21569217\n",
      "Iteration 789, loss = 0.21552499\n",
      "Iteration 790, loss = 0.21535812\n",
      "Iteration 791, loss = 0.21519153\n",
      "Iteration 792, loss = 0.21502537\n",
      "Iteration 793, loss = 0.21485941\n",
      "Iteration 794, loss = 0.21469379\n",
      "Iteration 795, loss = 0.21452856\n",
      "Iteration 796, loss = 0.21436355\n",
      "Iteration 797, loss = 0.21419886\n",
      "Iteration 798, loss = 0.21403455\n",
      "Iteration 799, loss = 0.21387044\n",
      "Iteration 800, loss = 0.21370674\n",
      "Iteration 801, loss = 0.21354331\n",
      "Iteration 802, loss = 0.21338018\n",
      "Iteration 803, loss = 0.21321734\n",
      "Iteration 804, loss = 0.21305490\n",
      "Iteration 805, loss = 0.21289268\n",
      "Iteration 806, loss = 0.21273078\n",
      "Iteration 807, loss = 0.21256926\n",
      "Iteration 808, loss = 0.21240793\n",
      "Iteration 809, loss = 0.21224703\n",
      "Iteration 810, loss = 0.21208633\n",
      "Iteration 811, loss = 0.21192592\n",
      "Iteration 812, loss = 0.21176599\n",
      "Iteration 813, loss = 0.21160612\n",
      "Iteration 814, loss = 0.21144672\n",
      "Iteration 815, loss = 0.21128756\n",
      "Iteration 816, loss = 0.21112867\n",
      "Iteration 817, loss = 0.21097010\n",
      "Iteration 818, loss = 0.21081189\n",
      "Iteration 819, loss = 0.21065391\n",
      "Iteration 820, loss = 0.21049619\n",
      "Iteration 821, loss = 0.21033889\n",
      "Iteration 822, loss = 0.21018177\n",
      "Iteration 823, loss = 0.21002500\n",
      "Iteration 824, loss = 0.20986853\n",
      "Iteration 825, loss = 0.20971229\n",
      "Iteration 826, loss = 0.20955647\n",
      "Iteration 827, loss = 0.20940079\n",
      "Iteration 828, loss = 0.20924551\n",
      "Iteration 829, loss = 0.20909051\n",
      "Iteration 830, loss = 0.20893572\n",
      "Iteration 831, loss = 0.20878132\n",
      "Iteration 832, loss = 0.20862718\n",
      "Iteration 833, loss = 0.20847328\n",
      "Iteration 834, loss = 0.20831974\n",
      "Iteration 835, loss = 0.20816644\n",
      "Iteration 836, loss = 0.20801342\n",
      "Iteration 837, loss = 0.20786082\n",
      "Iteration 838, loss = 0.20770830\n",
      "Iteration 839, loss = 0.20755625\n",
      "Iteration 840, loss = 0.20740439\n",
      "Iteration 841, loss = 0.20725279\n",
      "Iteration 842, loss = 0.20710154\n",
      "Iteration 843, loss = 0.20695054\n",
      "Iteration 844, loss = 0.20679981\n",
      "Iteration 845, loss = 0.20664943\n",
      "Iteration 846, loss = 0.20649924\n",
      "Iteration 847, loss = 0.20634938\n",
      "Iteration 848, loss = 0.20619982\n",
      "Iteration 849, loss = 0.20605050\n",
      "Iteration 850, loss = 0.20590143\n",
      "Iteration 851, loss = 0.20575273\n",
      "Iteration 852, loss = 0.20560423\n",
      "Iteration 853, loss = 0.20545603\n",
      "Iteration 854, loss = 0.20530812\n",
      "Iteration 855, loss = 0.20516043\n",
      "Iteration 856, loss = 0.20501313\n",
      "Iteration 857, loss = 0.20486602\n",
      "Iteration 858, loss = 0.20471915\n",
      "Iteration 859, loss = 0.20457270\n",
      "Iteration 860, loss = 0.20442640\n",
      "Iteration 861, loss = 0.20428043\n",
      "Iteration 862, loss = 0.20413472\n",
      "Iteration 863, loss = 0.20398922\n",
      "Iteration 864, loss = 0.20384412\n",
      "Iteration 865, loss = 0.20369916\n",
      "Iteration 866, loss = 0.20355456\n",
      "Iteration 867, loss = 0.20341019\n",
      "Iteration 868, loss = 0.20326605\n",
      "Iteration 869, loss = 0.20312230\n",
      "Iteration 870, loss = 0.20297869\n",
      "Iteration 871, loss = 0.20283543\n",
      "Iteration 872, loss = 0.20269239\n",
      "Iteration 873, loss = 0.20254962\n",
      "Iteration 874, loss = 0.20240712\n",
      "Iteration 875, loss = 0.20226490\n",
      "Iteration 876, loss = 0.20212289\n",
      "Iteration 877, loss = 0.20198127\n",
      "Iteration 878, loss = 0.20183973\n",
      "Iteration 879, loss = 0.20169865\n",
      "Iteration 880, loss = 0.20155771\n",
      "Iteration 881, loss = 0.20141702\n",
      "Iteration 882, loss = 0.20127657\n",
      "Iteration 883, loss = 0.20113647\n",
      "Iteration 884, loss = 0.20099654\n",
      "Iteration 885, loss = 0.20085701\n",
      "Iteration 886, loss = 0.20071754\n",
      "Iteration 887, loss = 0.20057852\n",
      "Iteration 888, loss = 0.20043963\n",
      "Iteration 889, loss = 0.20030099\n",
      "Iteration 890, loss = 0.20016269\n",
      "Iteration 891, loss = 0.20002454\n",
      "Iteration 892, loss = 0.19988676\n",
      "Iteration 893, loss = 0.19974914\n",
      "Iteration 894, loss = 0.19961179\n",
      "Iteration 895, loss = 0.19947481\n",
      "Iteration 896, loss = 0.19933797\n",
      "Iteration 897, loss = 0.19920134\n",
      "Iteration 898, loss = 0.19906514\n",
      "Iteration 899, loss = 0.19892898\n",
      "Iteration 900, loss = 0.19879321\n",
      "Iteration 901, loss = 0.19865762\n",
      "Iteration 902, loss = 0.19852226\n",
      "Iteration 903, loss = 0.19838726\n",
      "Iteration 904, loss = 0.19825242\n",
      "Iteration 905, loss = 0.19811783\n",
      "Iteration 906, loss = 0.19798351\n",
      "Iteration 907, loss = 0.19784941\n",
      "Iteration 908, loss = 0.19771562\n",
      "Iteration 909, loss = 0.19758201\n",
      "Iteration 910, loss = 0.19744867\n",
      "Iteration 911, loss = 0.19731554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 912, loss = 0.19718269\n",
      "Iteration 913, loss = 0.19705012\n",
      "Iteration 914, loss = 0.19691770\n",
      "Iteration 915, loss = 0.19678558\n",
      "Iteration 916, loss = 0.19665369\n",
      "Iteration 917, loss = 0.19652200\n",
      "Iteration 918, loss = 0.19639068\n",
      "Iteration 919, loss = 0.19625947\n",
      "Iteration 920, loss = 0.19612852\n",
      "Iteration 921, loss = 0.19599786\n",
      "Iteration 922, loss = 0.19586735\n",
      "Iteration 923, loss = 0.19573720\n",
      "Iteration 924, loss = 0.19560721\n",
      "Iteration 925, loss = 0.19547747\n",
      "Iteration 926, loss = 0.19534795\n",
      "Iteration 927, loss = 0.19521887\n",
      "Iteration 928, loss = 0.19509016\n",
      "Iteration 929, loss = 0.19496163\n",
      "Iteration 930, loss = 0.19483341\n",
      "Iteration 931, loss = 0.19470538\n",
      "Iteration 932, loss = 0.19457763\n",
      "Iteration 933, loss = 0.19445015\n",
      "Iteration 934, loss = 0.19432282\n",
      "Iteration 935, loss = 0.19419588\n",
      "Iteration 936, loss = 0.19406902\n",
      "Iteration 937, loss = 0.19394255\n",
      "Iteration 938, loss = 0.19381621\n",
      "Iteration 939, loss = 0.19369007\n",
      "Iteration 940, loss = 0.19356430\n",
      "Iteration 941, loss = 0.19343862\n",
      "Iteration 942, loss = 0.19331327\n",
      "Iteration 943, loss = 0.19318809\n",
      "Iteration 944, loss = 0.19306316\n",
      "Iteration 945, loss = 0.19293846\n",
      "Iteration 946, loss = 0.19281398\n",
      "Iteration 947, loss = 0.19268976\n",
      "Iteration 948, loss = 0.19256568\n",
      "Iteration 949, loss = 0.19244194\n",
      "Iteration 950, loss = 0.19231829\n",
      "Iteration 951, loss = 0.19219500\n",
      "Iteration 952, loss = 0.19207180\n",
      "Iteration 953, loss = 0.19194895\n",
      "Iteration 954, loss = 0.19182625\n",
      "Iteration 955, loss = 0.19170378\n",
      "Iteration 956, loss = 0.19158161\n",
      "Iteration 957, loss = 0.19145952\n",
      "Iteration 958, loss = 0.19133779\n",
      "Iteration 959, loss = 0.19121615\n",
      "Iteration 960, loss = 0.19109483\n",
      "Iteration 961, loss = 0.19097371\n",
      "Iteration 962, loss = 0.19085276\n",
      "Iteration 963, loss = 0.19073215\n",
      "Iteration 964, loss = 0.19061159\n",
      "Iteration 965, loss = 0.19049144\n",
      "Iteration 966, loss = 0.19037140\n",
      "Iteration 967, loss = 0.19025154\n",
      "Iteration 968, loss = 0.19013213\n",
      "Iteration 969, loss = 0.19001272\n",
      "Iteration 970, loss = 0.18989365\n",
      "Iteration 971, loss = 0.18977485\n",
      "Iteration 972, loss = 0.18965622\n",
      "Iteration 973, loss = 0.18953780\n",
      "Iteration 974, loss = 0.18941959\n",
      "Iteration 975, loss = 0.18930158\n",
      "Iteration 976, loss = 0.18918378\n",
      "Iteration 977, loss = 0.18906619\n",
      "Iteration 978, loss = 0.18894891\n",
      "Iteration 979, loss = 0.18883169\n",
      "Iteration 980, loss = 0.18871476\n",
      "Iteration 981, loss = 0.18859804\n",
      "Iteration 982, loss = 0.18848155\n",
      "Iteration 983, loss = 0.18836524\n",
      "Iteration 984, loss = 0.18824915\n",
      "Iteration 985, loss = 0.18813324\n",
      "Iteration 986, loss = 0.18801753\n",
      "Iteration 987, loss = 0.18790205\n",
      "Iteration 988, loss = 0.18778686\n",
      "Iteration 989, loss = 0.18767169\n",
      "Iteration 990, loss = 0.18755684\n",
      "Iteration 991, loss = 0.18744218\n",
      "Iteration 992, loss = 0.18732775\n",
      "Iteration 993, loss = 0.18721349\n",
      "Iteration 994, loss = 0.18709945\n",
      "Iteration 995, loss = 0.18698559\n",
      "Iteration 996, loss = 0.18687192\n",
      "Iteration 997, loss = 0.18675845\n",
      "Iteration 998, loss = 0.18664524\n",
      "Iteration 999, loss = 0.18653218\n",
      "Iteration 1000, loss = 0.18641932\n",
      "Iteration 1, loss = 1.35233713\n",
      "Iteration 2, loss = 1.32914641\n",
      "Iteration 3, loss = 1.29803045\n",
      "Iteration 4, loss = 1.26127742\n",
      "Iteration 5, loss = 1.22088319\n",
      "Iteration 6, loss = 1.17847925\n",
      "Iteration 7, loss = 1.13524232\n",
      "Iteration 8, loss = 1.09218546\n",
      "Iteration 9, loss = 1.05030394\n",
      "Iteration 10, loss = 1.01061173\n",
      "Iteration 11, loss = 0.97433069\n",
      "Iteration 12, loss = 0.94227541\n",
      "Iteration 13, loss = 0.91517137\n",
      "Iteration 14, loss = 0.89331221\n",
      "Iteration 15, loss = 0.87672411\n",
      "Iteration 16, loss = 0.86475889\n",
      "Iteration 17, loss = 0.85638016\n",
      "Iteration 18, loss = 0.85039374\n",
      "Iteration 19, loss = 0.84533458\n",
      "Iteration 20, loss = 0.84000697\n",
      "Iteration 21, loss = 0.83359304\n",
      "Iteration 22, loss = 0.82558508\n",
      "Iteration 23, loss = 0.81606064\n",
      "Iteration 24, loss = 0.80542630\n",
      "Iteration 25, loss = 0.79451850\n",
      "Iteration 26, loss = 0.78348353\n",
      "Iteration 27, loss = 0.77244750\n",
      "Iteration 28, loss = 0.76152425\n",
      "Iteration 29, loss = 0.75092757\n",
      "Iteration 30, loss = 0.74097324\n",
      "Iteration 31, loss = 0.73185174\n",
      "Iteration 32, loss = 0.72361954\n",
      "Iteration 33, loss = 0.71623018\n",
      "Iteration 34, loss = 0.70952809\n",
      "Iteration 35, loss = 0.70344600\n",
      "Iteration 36, loss = 0.69782464\n",
      "Iteration 37, loss = 0.69252918\n",
      "Iteration 38, loss = 0.68747814\n",
      "Iteration 39, loss = 0.68258918\n",
      "Iteration 40, loss = 0.67783339\n",
      "Iteration 41, loss = 0.67316289\n",
      "Iteration 42, loss = 0.66853474\n",
      "Iteration 43, loss = 0.66395034\n",
      "Iteration 44, loss = 0.65942748\n",
      "Iteration 45, loss = 0.65497510\n",
      "Iteration 46, loss = 0.65058549\n",
      "Iteration 47, loss = 0.64625485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.64198909\n",
      "Iteration 49, loss = 0.63779874\n",
      "Iteration 50, loss = 0.63369580\n",
      "Iteration 51, loss = 0.62968261\n",
      "Iteration 52, loss = 0.62575691\n",
      "Iteration 53, loss = 0.62193464\n",
      "Iteration 54, loss = 0.61822363\n",
      "Iteration 55, loss = 0.61462222\n",
      "Iteration 56, loss = 0.61113438\n",
      "Iteration 57, loss = 0.60782734\n",
      "Iteration 58, loss = 0.60470550\n",
      "Iteration 59, loss = 0.60173750\n",
      "Iteration 60, loss = 0.59885551\n",
      "Iteration 61, loss = 0.59607587\n",
      "Iteration 62, loss = 0.59333847\n",
      "Iteration 63, loss = 0.59062721\n",
      "Iteration 64, loss = 0.58794047\n",
      "Iteration 65, loss = 0.58526826\n",
      "Iteration 66, loss = 0.58261339\n",
      "Iteration 67, loss = 0.57997770\n",
      "Iteration 68, loss = 0.57736285\n",
      "Iteration 69, loss = 0.57477415\n",
      "Iteration 70, loss = 0.57221913\n",
      "Iteration 71, loss = 0.56969506\n",
      "Iteration 72, loss = 0.56721550\n",
      "Iteration 73, loss = 0.56477325\n",
      "Iteration 74, loss = 0.56238787\n",
      "Iteration 75, loss = 0.56006437\n",
      "Iteration 76, loss = 0.55777732\n",
      "Iteration 77, loss = 0.55552563\n",
      "Iteration 78, loss = 0.55331963\n",
      "Iteration 79, loss = 0.55115518\n",
      "Iteration 80, loss = 0.54902496\n",
      "Iteration 81, loss = 0.54692453\n",
      "Iteration 82, loss = 0.54485463\n",
      "Iteration 83, loss = 0.54281281\n",
      "Iteration 84, loss = 0.54080046\n",
      "Iteration 85, loss = 0.53881264\n",
      "Iteration 86, loss = 0.53684943\n",
      "Iteration 87, loss = 0.53491072\n",
      "Iteration 88, loss = 0.53299607\n",
      "Iteration 89, loss = 0.53110545\n",
      "Iteration 90, loss = 0.52923947\n",
      "Iteration 91, loss = 0.52739916\n",
      "Iteration 92, loss = 0.52558283\n",
      "Iteration 93, loss = 0.52378987\n",
      "Iteration 94, loss = 0.52202135\n",
      "Iteration 95, loss = 0.52027667\n",
      "Iteration 96, loss = 0.51855581\n",
      "Iteration 97, loss = 0.51685670\n",
      "Iteration 98, loss = 0.51517889\n",
      "Iteration 99, loss = 0.51352287\n",
      "Iteration 100, loss = 0.51188731\n",
      "Iteration 101, loss = 0.51027293\n",
      "Iteration 102, loss = 0.50867855\n",
      "Iteration 103, loss = 0.50710272\n",
      "Iteration 104, loss = 0.50554523\n",
      "Iteration 105, loss = 0.50400547\n",
      "Iteration 106, loss = 0.50248279\n",
      "Iteration 107, loss = 0.50097772\n",
      "Iteration 108, loss = 0.49948945\n",
      "Iteration 109, loss = 0.49801831\n",
      "Iteration 110, loss = 0.49656444\n",
      "Iteration 111, loss = 0.49512832\n",
      "Iteration 112, loss = 0.49370813\n",
      "Iteration 113, loss = 0.49230397\n",
      "Iteration 114, loss = 0.49091571\n",
      "Iteration 115, loss = 0.48954248\n",
      "Iteration 116, loss = 0.48818393\n",
      "Iteration 117, loss = 0.48684050\n",
      "Iteration 118, loss = 0.48551120\n",
      "Iteration 119, loss = 0.48419619\n",
      "Iteration 120, loss = 0.48289473\n",
      "Iteration 121, loss = 0.48160643\n",
      "Iteration 122, loss = 0.48033114\n",
      "Iteration 123, loss = 0.47906910\n",
      "Iteration 124, loss = 0.47781921\n",
      "Iteration 125, loss = 0.47658144\n",
      "Iteration 126, loss = 0.47535554\n",
      "Iteration 127, loss = 0.47414156\n",
      "Iteration 128, loss = 0.47293895\n",
      "Iteration 129, loss = 0.47174795\n",
      "Iteration 130, loss = 0.47056847\n",
      "Iteration 131, loss = 0.46939977\n",
      "Iteration 132, loss = 0.46824183\n",
      "Iteration 133, loss = 0.46709454\n",
      "Iteration 134, loss = 0.46595789\n",
      "Iteration 135, loss = 0.46482987\n",
      "Iteration 136, loss = 0.46371191\n",
      "Iteration 137, loss = 0.46260266\n",
      "Iteration 138, loss = 0.46150130\n",
      "Iteration 139, loss = 0.46040726\n",
      "Iteration 140, loss = 0.45932195\n",
      "Iteration 141, loss = 0.45824473\n",
      "Iteration 142, loss = 0.45717570\n",
      "Iteration 143, loss = 0.45611287\n",
      "Iteration 144, loss = 0.45505755\n",
      "Iteration 145, loss = 0.45400872\n",
      "Iteration 146, loss = 0.45296764\n",
      "Iteration 147, loss = 0.45193099\n",
      "Iteration 148, loss = 0.45089998\n",
      "Iteration 149, loss = 0.44987492\n",
      "Iteration 150, loss = 0.44885492\n",
      "Iteration 151, loss = 0.44783867\n",
      "Iteration 152, loss = 0.44682407\n",
      "Iteration 153, loss = 0.44581451\n",
      "Iteration 154, loss = 0.44480812\n",
      "Iteration 155, loss = 0.44380256\n",
      "Iteration 156, loss = 0.44279931\n",
      "Iteration 157, loss = 0.44179676\n",
      "Iteration 158, loss = 0.44079325\n",
      "Iteration 159, loss = 0.43978929\n",
      "Iteration 160, loss = 0.43878677\n",
      "Iteration 161, loss = 0.43778549\n",
      "Iteration 162, loss = 0.43678630\n",
      "Iteration 163, loss = 0.43578828\n",
      "Iteration 164, loss = 0.43479388\n",
      "Iteration 165, loss = 0.43378917\n",
      "Iteration 166, loss = 0.43277305\n",
      "Iteration 167, loss = 0.43175456\n",
      "Iteration 168, loss = 0.43073106\n",
      "Iteration 169, loss = 0.42970438\n",
      "Iteration 170, loss = 0.42867134\n",
      "Iteration 171, loss = 0.42764384\n",
      "Iteration 172, loss = 0.42661927\n",
      "Iteration 173, loss = 0.42559376\n",
      "Iteration 174, loss = 0.42456943\n",
      "Iteration 175, loss = 0.42354952\n",
      "Iteration 176, loss = 0.42255812\n",
      "Iteration 177, loss = 0.42157879\n",
      "Iteration 178, loss = 0.42061625\n",
      "Iteration 179, loss = 0.41967038\n",
      "Iteration 180, loss = 0.41873777\n",
      "Iteration 181, loss = 0.41781403\n",
      "Iteration 182, loss = 0.41690748\n",
      "Iteration 183, loss = 0.41602093\n",
      "Iteration 184, loss = 0.41515414\n",
      "Iteration 185, loss = 0.41430527\n",
      "Iteration 186, loss = 0.41346806\n",
      "Iteration 187, loss = 0.41264231\n",
      "Iteration 188, loss = 0.41182685\n",
      "Iteration 189, loss = 0.41102353\n",
      "Iteration 190, loss = 0.41023456\n",
      "Iteration 191, loss = 0.40945696\n",
      "Iteration 192, loss = 0.40868628\n",
      "Iteration 193, loss = 0.40792327\n",
      "Iteration 194, loss = 0.40717123\n",
      "Iteration 195, loss = 0.40643030\n",
      "Iteration 196, loss = 0.40569603\n",
      "Iteration 197, loss = 0.40496818\n",
      "Iteration 198, loss = 0.40424657\n",
      "Iteration 199, loss = 0.40352927\n",
      "Iteration 200, loss = 0.40281640\n",
      "Iteration 201, loss = 0.40210840\n",
      "Iteration 202, loss = 0.40140441\n",
      "Iteration 203, loss = 0.40070409\n",
      "Iteration 204, loss = 0.40000732\n",
      "Iteration 205, loss = 0.39931411\n",
      "Iteration 206, loss = 0.39862489\n",
      "Iteration 207, loss = 0.39793934\n",
      "Iteration 208, loss = 0.39725725\n",
      "Iteration 209, loss = 0.39657910\n",
      "Iteration 210, loss = 0.39590428\n",
      "Iteration 211, loss = 0.39523281\n",
      "Iteration 212, loss = 0.39456438\n",
      "Iteration 213, loss = 0.39389902\n",
      "Iteration 214, loss = 0.39323674\n",
      "Iteration 215, loss = 0.39257769\n",
      "Iteration 216, loss = 0.39192149\n",
      "Iteration 217, loss = 0.39126849\n",
      "Iteration 218, loss = 0.39061824\n",
      "Iteration 219, loss = 0.38997110\n",
      "Iteration 220, loss = 0.38932675\n",
      "Iteration 221, loss = 0.38868521\n",
      "Iteration 222, loss = 0.38804667\n",
      "Iteration 223, loss = 0.38741063\n",
      "Iteration 224, loss = 0.38677760\n",
      "Iteration 225, loss = 0.38614703\n",
      "Iteration 226, loss = 0.38551919\n",
      "Iteration 227, loss = 0.38489425\n",
      "Iteration 228, loss = 0.38427166\n",
      "Iteration 229, loss = 0.38365177\n",
      "Iteration 230, loss = 0.38303432\n",
      "Iteration 231, loss = 0.38241954\n",
      "Iteration 232, loss = 0.38180711\n",
      "Iteration 233, loss = 0.38119736\n",
      "Iteration 234, loss = 0.38058993\n",
      "Iteration 235, loss = 0.37998524\n",
      "Iteration 236, loss = 0.37938316\n",
      "Iteration 237, loss = 0.37878343\n",
      "Iteration 238, loss = 0.37818609\n",
      "Iteration 239, loss = 0.37759096\n",
      "Iteration 240, loss = 0.37699836\n",
      "Iteration 241, loss = 0.37640791\n",
      "Iteration 242, loss = 0.37581977\n",
      "Iteration 243, loss = 0.37523400\n",
      "Iteration 244, loss = 0.37465044\n",
      "Iteration 245, loss = 0.37406907\n",
      "Iteration 246, loss = 0.37348984\n",
      "Iteration 247, loss = 0.37291282\n",
      "Iteration 248, loss = 0.37233800\n",
      "Iteration 249, loss = 0.37176527\n",
      "Iteration 250, loss = 0.37119467\n",
      "Iteration 251, loss = 0.37062615\n",
      "Iteration 252, loss = 0.37005968\n",
      "Iteration 253, loss = 0.36949529\n",
      "Iteration 254, loss = 0.36893283\n",
      "Iteration 255, loss = 0.36837244\n",
      "Iteration 256, loss = 0.36781402\n",
      "Iteration 257, loss = 0.36725755\n",
      "Iteration 258, loss = 0.36670302\n",
      "Iteration 259, loss = 0.36615048\n",
      "Iteration 260, loss = 0.36559999\n",
      "Iteration 261, loss = 0.36505140\n",
      "Iteration 262, loss = 0.36450468\n",
      "Iteration 263, loss = 0.36395983\n",
      "Iteration 264, loss = 0.36341679\n",
      "Iteration 265, loss = 0.36287558\n",
      "Iteration 266, loss = 0.36233624\n",
      "Iteration 267, loss = 0.36179868\n",
      "Iteration 268, loss = 0.36126294\n",
      "Iteration 269, loss = 0.36072898\n",
      "Iteration 270, loss = 0.36019692\n",
      "Iteration 271, loss = 0.35966653\n",
      "Iteration 272, loss = 0.35913786\n",
      "Iteration 273, loss = 0.35861089\n",
      "Iteration 274, loss = 0.35808565\n",
      "Iteration 275, loss = 0.35756212\n",
      "Iteration 276, loss = 0.35704028\n",
      "Iteration 277, loss = 0.35652015\n",
      "Iteration 278, loss = 0.35600162\n",
      "Iteration 279, loss = 0.35548473\n",
      "Iteration 280, loss = 0.35497023\n",
      "Iteration 281, loss = 0.35445773\n",
      "Iteration 282, loss = 0.35394691\n",
      "Iteration 283, loss = 0.35343781\n",
      "Iteration 284, loss = 0.35293026\n",
      "Iteration 285, loss = 0.35242433\n",
      "Iteration 286, loss = 0.35191997\n",
      "Iteration 287, loss = 0.35141707\n",
      "Iteration 288, loss = 0.35091569\n",
      "Iteration 289, loss = 0.35041583\n",
      "Iteration 290, loss = 0.34991746\n",
      "Iteration 291, loss = 0.34942060\n",
      "Iteration 292, loss = 0.34892522\n",
      "Iteration 293, loss = 0.34843130\n",
      "Iteration 294, loss = 0.34793883\n",
      "Iteration 295, loss = 0.34744784\n",
      "Iteration 296, loss = 0.34695827\n",
      "Iteration 297, loss = 0.34647021\n",
      "Iteration 298, loss = 0.34598354\n",
      "Iteration 299, loss = 0.34549829\n",
      "Iteration 300, loss = 0.34501445\n",
      "Iteration 301, loss = 0.34453204\n",
      "Iteration 302, loss = 0.34405103\n",
      "Iteration 303, loss = 0.34357133\n",
      "Iteration 304, loss = 0.34309299\n",
      "Iteration 305, loss = 0.34261602\n",
      "Iteration 306, loss = 0.34214040\n",
      "Iteration 307, loss = 0.34166619\n",
      "Iteration 308, loss = 0.34119353\n",
      "Iteration 309, loss = 0.34072217\n",
      "Iteration 310, loss = 0.34025216\n",
      "Iteration 311, loss = 0.33978339\n",
      "Iteration 312, loss = 0.33931600\n",
      "Iteration 313, loss = 0.33884979\n",
      "Iteration 314, loss = 0.33838498\n",
      "Iteration 315, loss = 0.33792146\n",
      "Iteration 316, loss = 0.33745916\n",
      "Iteration 317, loss = 0.33699814\n",
      "Iteration 318, loss = 0.33653840\n",
      "Iteration 319, loss = 0.33607991\n",
      "Iteration 320, loss = 0.33562283\n",
      "Iteration 321, loss = 0.33516701\n",
      "Iteration 322, loss = 0.33471241\n",
      "Iteration 323, loss = 0.33425907\n",
      "Iteration 324, loss = 0.33380690\n",
      "Iteration 325, loss = 0.33335589\n",
      "Iteration 326, loss = 0.33290608\n",
      "Iteration 327, loss = 0.33245750\n",
      "Iteration 328, loss = 0.33201006\n",
      "Iteration 329, loss = 0.33156381\n",
      "Iteration 330, loss = 0.33111883\n",
      "Iteration 331, loss = 0.33067502\n",
      "Iteration 332, loss = 0.33023234\n",
      "Iteration 333, loss = 0.32979083\n",
      "Iteration 334, loss = 0.32935048\n",
      "Iteration 335, loss = 0.32891127\n",
      "Iteration 336, loss = 0.32847337\n",
      "Iteration 337, loss = 0.32803653\n",
      "Iteration 338, loss = 0.32760087\n",
      "Iteration 339, loss = 0.32716638\n",
      "Iteration 340, loss = 0.32673292\n",
      "Iteration 341, loss = 0.32630063\n",
      "Iteration 342, loss = 0.32586943\n",
      "Iteration 343, loss = 0.32543925\n",
      "Iteration 344, loss = 0.32501023\n",
      "Iteration 345, loss = 0.32458230\n",
      "Iteration 346, loss = 0.32415540\n",
      "Iteration 347, loss = 0.32372961\n",
      "Iteration 348, loss = 0.32330486\n",
      "Iteration 349, loss = 0.32288115\n",
      "Iteration 350, loss = 0.32245860\n",
      "Iteration 351, loss = 0.32203693\n",
      "Iteration 352, loss = 0.32161645\n",
      "Iteration 353, loss = 0.32119693\n",
      "Iteration 354, loss = 0.32077843\n",
      "Iteration 355, loss = 0.32036101\n",
      "Iteration 356, loss = 0.31994459\n",
      "Iteration 357, loss = 0.31952919\n",
      "Iteration 358, loss = 0.31911480\n",
      "Iteration 359, loss = 0.31870144\n",
      "Iteration 360, loss = 0.31828906\n",
      "Iteration 361, loss = 0.31787767\n",
      "Iteration 362, loss = 0.31746730\n",
      "Iteration 363, loss = 0.31705795\n",
      "Iteration 364, loss = 0.31664952\n",
      "Iteration 365, loss = 0.31624215\n",
      "Iteration 366, loss = 0.31583574\n",
      "Iteration 367, loss = 0.31543028\n",
      "Iteration 368, loss = 0.31502579\n",
      "Iteration 369, loss = 0.31462224\n",
      "Iteration 370, loss = 0.31421974\n",
      "Iteration 371, loss = 0.31381811\n",
      "Iteration 372, loss = 0.31341744\n",
      "Iteration 373, loss = 0.31301774\n",
      "Iteration 374, loss = 0.31261896\n",
      "Iteration 375, loss = 0.31222116\n",
      "Iteration 376, loss = 0.31182424\n",
      "Iteration 377, loss = 0.31142831\n",
      "Iteration 378, loss = 0.31103323\n",
      "Iteration 379, loss = 0.31063916\n",
      "Iteration 380, loss = 0.31024596\n",
      "Iteration 381, loss = 0.30985366\n",
      "Iteration 382, loss = 0.30946230\n",
      "Iteration 383, loss = 0.30907183\n",
      "Iteration 384, loss = 0.30868231\n",
      "Iteration 385, loss = 0.30829370\n",
      "Iteration 386, loss = 0.30790597\n",
      "Iteration 387, loss = 0.30751913\n",
      "Iteration 388, loss = 0.30713321\n",
      "Iteration 389, loss = 0.30674815\n",
      "Iteration 390, loss = 0.30636399\n",
      "Iteration 391, loss = 0.30598073\n",
      "Iteration 392, loss = 0.30559834\n",
      "Iteration 393, loss = 0.30521683\n",
      "Iteration 394, loss = 0.30483621\n",
      "Iteration 395, loss = 0.30445645\n",
      "Iteration 396, loss = 0.30407754\n",
      "Iteration 397, loss = 0.30369953\n",
      "Iteration 398, loss = 0.30332236\n",
      "Iteration 399, loss = 0.30294603\n",
      "Iteration 400, loss = 0.30257055\n",
      "Iteration 401, loss = 0.30219594\n",
      "Iteration 402, loss = 0.30182213\n",
      "Iteration 403, loss = 0.30144925\n",
      "Iteration 404, loss = 0.30107710\n",
      "Iteration 405, loss = 0.30070587\n",
      "Iteration 406, loss = 0.30033542\n",
      "Iteration 407, loss = 0.29996583\n",
      "Iteration 408, loss = 0.29959706\n",
      "Iteration 409, loss = 0.29922910\n",
      "Iteration 410, loss = 0.29886198\n",
      "Iteration 411, loss = 0.29849565\n",
      "Iteration 412, loss = 0.29813017\n",
      "Iteration 413, loss = 0.29776547\n",
      "Iteration 414, loss = 0.29740161\n",
      "Iteration 415, loss = 0.29703851\n",
      "Iteration 416, loss = 0.29667629\n",
      "Iteration 417, loss = 0.29631479\n",
      "Iteration 418, loss = 0.29595415\n",
      "Iteration 419, loss = 0.29559426\n",
      "Iteration 420, loss = 0.29523521\n",
      "Iteration 421, loss = 0.29487691\n",
      "Iteration 422, loss = 0.29451941\n",
      "Iteration 423, loss = 0.29416269\n",
      "Iteration 424, loss = 0.29380677\n",
      "Iteration 425, loss = 0.29345161\n",
      "Iteration 426, loss = 0.29309725\n",
      "Iteration 427, loss = 0.29274369\n",
      "Iteration 428, loss = 0.29239093\n",
      "Iteration 429, loss = 0.29203893\n",
      "Iteration 430, loss = 0.29168771\n",
      "Iteration 431, loss = 0.29133726\n",
      "Iteration 432, loss = 0.29098758\n",
      "Iteration 433, loss = 0.29063865\n",
      "Iteration 434, loss = 0.29029049\n",
      "Iteration 435, loss = 0.28994311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 436, loss = 0.28959644\n",
      "Iteration 437, loss = 0.28925058\n",
      "Iteration 438, loss = 0.28890543\n",
      "Iteration 439, loss = 0.28856104\n",
      "Iteration 440, loss = 0.28821739\n",
      "Iteration 441, loss = 0.28787455\n",
      "Iteration 442, loss = 0.28753241\n",
      "Iteration 443, loss = 0.28719103\n",
      "Iteration 444, loss = 0.28685038\n",
      "Iteration 445, loss = 0.28651049\n",
      "Iteration 446, loss = 0.28617130\n",
      "Iteration 447, loss = 0.28583286\n",
      "Iteration 448, loss = 0.28549515\n",
      "Iteration 449, loss = 0.28515817\n",
      "Iteration 450, loss = 0.28482190\n",
      "Iteration 451, loss = 0.28448638\n",
      "Iteration 452, loss = 0.28415156\n",
      "Iteration 453, loss = 0.28381747\n",
      "Iteration 454, loss = 0.28348408\n",
      "Iteration 455, loss = 0.28315144\n",
      "Iteration 456, loss = 0.28281948\n",
      "Iteration 457, loss = 0.28248824\n",
      "Iteration 458, loss = 0.28215772\n",
      "Iteration 459, loss = 0.28182790\n",
      "Iteration 460, loss = 0.28149880\n",
      "Iteration 461, loss = 0.28117037\n",
      "Iteration 462, loss = 0.28084266\n",
      "Iteration 463, loss = 0.28051565\n",
      "Iteration 464, loss = 0.28018935\n",
      "Iteration 465, loss = 0.27986373\n",
      "Iteration 466, loss = 0.27953880\n",
      "Iteration 467, loss = 0.27921457\n",
      "Iteration 468, loss = 0.27889101\n",
      "Iteration 469, loss = 0.27856816\n",
      "Iteration 470, loss = 0.27824598\n",
      "Iteration 471, loss = 0.27792451\n",
      "Iteration 472, loss = 0.27760370\n",
      "Iteration 473, loss = 0.27728360\n",
      "Iteration 474, loss = 0.27696412\n",
      "Iteration 475, loss = 0.27664537\n",
      "Iteration 476, loss = 0.27632729\n",
      "Iteration 477, loss = 0.27600988\n",
      "Iteration 478, loss = 0.27569312\n",
      "Iteration 479, loss = 0.27537705\n",
      "Iteration 480, loss = 0.27506165\n",
      "Iteration 481, loss = 0.27474696\n",
      "Iteration 482, loss = 0.27443288\n",
      "Iteration 483, loss = 0.27411947\n",
      "Iteration 484, loss = 0.27380680\n",
      "Iteration 485, loss = 0.27349470\n",
      "Iteration 486, loss = 0.27318329\n",
      "Iteration 487, loss = 0.27287255\n",
      "Iteration 488, loss = 0.27256244\n",
      "Iteration 489, loss = 0.27225303\n",
      "Iteration 490, loss = 0.27194419\n",
      "Iteration 491, loss = 0.27163606\n",
      "Iteration 492, loss = 0.27132858\n",
      "Iteration 493, loss = 0.27102175\n",
      "Iteration 494, loss = 0.27071552\n",
      "Iteration 495, loss = 0.27041002\n",
      "Iteration 496, loss = 0.27010506\n",
      "Iteration 497, loss = 0.26980084\n",
      "Iteration 498, loss = 0.26949719\n",
      "Iteration 499, loss = 0.26919422\n",
      "Iteration 500, loss = 0.26889188\n",
      "Iteration 501, loss = 0.26859019\n",
      "Iteration 502, loss = 0.26828910\n",
      "Iteration 503, loss = 0.26798867\n",
      "Iteration 504, loss = 0.26768890\n",
      "Iteration 505, loss = 0.26738971\n",
      "Iteration 506, loss = 0.26709119\n",
      "Iteration 507, loss = 0.26679328\n",
      "Iteration 508, loss = 0.26649601\n",
      "Iteration 509, loss = 0.26619934\n",
      "Iteration 510, loss = 0.26590330\n",
      "Iteration 511, loss = 0.26560791\n",
      "Iteration 512, loss = 0.26531310\n",
      "Iteration 513, loss = 0.26501893\n",
      "Iteration 514, loss = 0.26472538\n",
      "Iteration 515, loss = 0.26443242\n",
      "Iteration 516, loss = 0.26414009\n",
      "Iteration 517, loss = 0.26384837\n",
      "Iteration 518, loss = 0.26355725\n",
      "Iteration 519, loss = 0.26326676\n",
      "Iteration 520, loss = 0.26297687\n",
      "Iteration 521, loss = 0.26268759\n",
      "Iteration 522, loss = 0.26239893\n",
      "Iteration 523, loss = 0.26211086\n",
      "Iteration 524, loss = 0.26182340\n",
      "Iteration 525, loss = 0.26153655\n",
      "Iteration 526, loss = 0.26125027\n",
      "Iteration 527, loss = 0.26096462\n",
      "Iteration 528, loss = 0.26067956\n",
      "Iteration 529, loss = 0.26039508\n",
      "Iteration 530, loss = 0.26011119\n",
      "Iteration 531, loss = 0.25982791\n",
      "Iteration 532, loss = 0.25954520\n",
      "Iteration 533, loss = 0.25926311\n",
      "Iteration 534, loss = 0.25898158\n",
      "Iteration 535, loss = 0.25870064\n",
      "Iteration 536, loss = 0.25842031\n",
      "Iteration 537, loss = 0.25814053\n",
      "Iteration 538, loss = 0.25786134\n",
      "Iteration 539, loss = 0.25758277\n",
      "Iteration 540, loss = 0.25730479\n",
      "Iteration 541, loss = 0.25702736\n",
      "Iteration 542, loss = 0.25675053\n",
      "Iteration 543, loss = 0.25647427\n",
      "Iteration 544, loss = 0.25619859\n",
      "Iteration 545, loss = 0.25592351\n",
      "Iteration 546, loss = 0.25564896\n",
      "Iteration 547, loss = 0.25537500\n",
      "Iteration 548, loss = 0.25510162\n",
      "Iteration 549, loss = 0.25482879\n",
      "Iteration 550, loss = 0.25455653\n",
      "Iteration 551, loss = 0.25428484\n",
      "Iteration 552, loss = 0.25401373\n",
      "Iteration 553, loss = 0.25374317\n",
      "Iteration 554, loss = 0.25347317\n",
      "Iteration 555, loss = 0.25320374\n",
      "Iteration 556, loss = 0.25293486\n",
      "Iteration 557, loss = 0.25266654\n",
      "Iteration 558, loss = 0.25239881\n",
      "Iteration 559, loss = 0.25213158\n",
      "Iteration 560, loss = 0.25186494\n",
      "Iteration 561, loss = 0.25159893\n",
      "Iteration 562, loss = 0.25133346\n",
      "Iteration 563, loss = 0.25106853\n",
      "Iteration 564, loss = 0.25080414\n",
      "Iteration 565, loss = 0.25054032\n",
      "Iteration 566, loss = 0.25027701\n",
      "Iteration 567, loss = 0.25001424\n",
      "Iteration 568, loss = 0.24975203\n",
      "Iteration 569, loss = 0.24949034\n",
      "Iteration 570, loss = 0.24922920\n",
      "Iteration 571, loss = 0.24896860\n",
      "Iteration 572, loss = 0.24870859\n",
      "Iteration 573, loss = 0.24844909\n",
      "Iteration 574, loss = 0.24819015\n",
      "Iteration 575, loss = 0.24793173\n",
      "Iteration 576, loss = 0.24767383\n",
      "Iteration 577, loss = 0.24741646\n",
      "Iteration 578, loss = 0.24715963\n",
      "Iteration 579, loss = 0.24690336\n",
      "Iteration 580, loss = 0.24664760\n",
      "Iteration 581, loss = 0.24639237\n",
      "Iteration 582, loss = 0.24613767\n",
      "Iteration 583, loss = 0.24588352\n",
      "Iteration 584, loss = 0.24562986\n",
      "Iteration 585, loss = 0.24537674\n",
      "Iteration 586, loss = 0.24512417\n",
      "Iteration 587, loss = 0.24487211\n",
      "Iteration 588, loss = 0.24462057\n",
      "Iteration 589, loss = 0.24436954\n",
      "Iteration 590, loss = 0.24411904\n",
      "Iteration 591, loss = 0.24386906\n",
      "Iteration 592, loss = 0.24361962\n",
      "Iteration 593, loss = 0.24337065\n",
      "Iteration 594, loss = 0.24312222\n",
      "Iteration 595, loss = 0.24287431\n",
      "Iteration 596, loss = 0.24262690\n",
      "Iteration 597, loss = 0.24238000\n",
      "Iteration 598, loss = 0.24213367\n",
      "Iteration 599, loss = 0.24188780\n",
      "Iteration 600, loss = 0.24164244\n",
      "Iteration 601, loss = 0.24139758\n",
      "Iteration 602, loss = 0.24115325\n",
      "Iteration 603, loss = 0.24090942\n",
      "Iteration 604, loss = 0.24066607\n",
      "Iteration 605, loss = 0.24042329\n",
      "Iteration 606, loss = 0.24018093\n",
      "Iteration 607, loss = 0.23993912\n",
      "Iteration 608, loss = 0.23969781\n",
      "Iteration 609, loss = 0.23945700\n",
      "Iteration 610, loss = 0.23921667\n",
      "Iteration 611, loss = 0.23897685\n",
      "Iteration 612, loss = 0.23873753\n",
      "Iteration 613, loss = 0.23849869\n",
      "Iteration 614, loss = 0.23826034\n",
      "Iteration 615, loss = 0.23802250\n",
      "Iteration 616, loss = 0.23778515\n",
      "Iteration 617, loss = 0.23754832\n",
      "Iteration 618, loss = 0.23731200\n",
      "Iteration 619, loss = 0.23707615\n",
      "Iteration 620, loss = 0.23684080\n",
      "Iteration 621, loss = 0.23660591\n",
      "Iteration 622, loss = 0.23637156\n",
      "Iteration 623, loss = 0.23613764\n",
      "Iteration 624, loss = 0.23590422\n",
      "Iteration 625, loss = 0.23567129\n",
      "Iteration 626, loss = 0.23543881\n",
      "Iteration 627, loss = 0.23520689\n",
      "Iteration 628, loss = 0.23497537\n",
      "Iteration 629, loss = 0.23474436\n",
      "Iteration 630, loss = 0.23451381\n",
      "Iteration 631, loss = 0.23428373\n",
      "Iteration 632, loss = 0.23405416\n",
      "Iteration 633, loss = 0.23382502\n",
      "Iteration 634, loss = 0.23359639\n",
      "Iteration 635, loss = 0.23336820\n",
      "Iteration 636, loss = 0.23314049\n",
      "Iteration 637, loss = 0.23291327\n",
      "Iteration 638, loss = 0.23268649\n",
      "Iteration 639, loss = 0.23246017\n",
      "Iteration 640, loss = 0.23223437\n",
      "Iteration 641, loss = 0.23200909\n",
      "Iteration 642, loss = 0.23178430\n",
      "Iteration 643, loss = 0.23155996\n",
      "Iteration 644, loss = 0.23133608\n",
      "Iteration 645, loss = 0.23111269\n",
      "Iteration 646, loss = 0.23088973\n",
      "Iteration 647, loss = 0.23066724\n",
      "Iteration 648, loss = 0.23044523\n",
      "Iteration 649, loss = 0.23022363\n",
      "Iteration 650, loss = 0.23000253\n",
      "Iteration 651, loss = 0.22978187\n",
      "Iteration 652, loss = 0.22956166\n",
      "Iteration 653, loss = 0.22934191\n",
      "Iteration 654, loss = 0.22912260\n",
      "Iteration 655, loss = 0.22890374\n",
      "Iteration 656, loss = 0.22868536\n",
      "Iteration 657, loss = 0.22846739\n",
      "Iteration 658, loss = 0.22824989\n",
      "Iteration 659, loss = 0.22803283\n",
      "Iteration 660, loss = 0.22781622\n",
      "Iteration 661, loss = 0.22760005\n",
      "Iteration 662, loss = 0.22738435\n",
      "Iteration 663, loss = 0.22716905\n",
      "Iteration 664, loss = 0.22695422\n",
      "Iteration 665, loss = 0.22673984\n",
      "Iteration 666, loss = 0.22652588\n",
      "Iteration 667, loss = 0.22631236\n",
      "Iteration 668, loss = 0.22609930\n",
      "Iteration 669, loss = 0.22588664\n",
      "Iteration 670, loss = 0.22567444\n",
      "Iteration 671, loss = 0.22546268\n",
      "Iteration 672, loss = 0.22525133\n",
      "Iteration 673, loss = 0.22504042\n",
      "Iteration 674, loss = 0.22482998\n",
      "Iteration 675, loss = 0.22461992\n",
      "Iteration 676, loss = 0.22441030\n",
      "Iteration 677, loss = 0.22420114\n",
      "Iteration 678, loss = 0.22399238\n",
      "Iteration 679, loss = 0.22378405\n",
      "Iteration 680, loss = 0.22357617\n",
      "Iteration 681, loss = 0.22336869\n",
      "Iteration 682, loss = 0.22316164\n",
      "Iteration 683, loss = 0.22295503\n",
      "Iteration 684, loss = 0.22274882\n",
      "Iteration 685, loss = 0.22254306\n",
      "Iteration 686, loss = 0.22233770\n",
      "Iteration 687, loss = 0.22213275\n",
      "Iteration 688, loss = 0.22192822\n",
      "Iteration 689, loss = 0.22172416\n",
      "Iteration 690, loss = 0.22152044\n",
      "Iteration 691, loss = 0.22131719\n",
      "Iteration 692, loss = 0.22111435\n",
      "Iteration 693, loss = 0.22091191\n",
      "Iteration 694, loss = 0.22070987\n",
      "Iteration 695, loss = 0.22050830\n",
      "Iteration 696, loss = 0.22030706\n",
      "Iteration 697, loss = 0.22010629\n",
      "Iteration 698, loss = 0.21990592\n",
      "Iteration 699, loss = 0.21970596\n",
      "Iteration 700, loss = 0.21950640\n",
      "Iteration 701, loss = 0.21930722\n",
      "Iteration 702, loss = 0.21910851\n",
      "Iteration 703, loss = 0.21891015\n",
      "Iteration 704, loss = 0.21871220\n",
      "Iteration 705, loss = 0.21851469\n",
      "Iteration 706, loss = 0.21831752\n",
      "Iteration 707, loss = 0.21812082\n",
      "Iteration 708, loss = 0.21792448\n",
      "Iteration 709, loss = 0.21772855\n",
      "Iteration 710, loss = 0.21753301\n",
      "Iteration 711, loss = 0.21733788\n",
      "Iteration 712, loss = 0.21714315\n",
      "Iteration 713, loss = 0.21694879\n",
      "Iteration 714, loss = 0.21675486\n",
      "Iteration 715, loss = 0.21656130\n",
      "Iteration 716, loss = 0.21636814\n",
      "Iteration 717, loss = 0.21617537\n",
      "Iteration 718, loss = 0.21598301\n",
      "Iteration 719, loss = 0.21579101\n",
      "Iteration 720, loss = 0.21559942\n",
      "Iteration 721, loss = 0.21540824\n",
      "Iteration 722, loss = 0.21521741\n",
      "Iteration 723, loss = 0.21502697\n",
      "Iteration 724, loss = 0.21483693\n",
      "Iteration 725, loss = 0.21464730\n",
      "Iteration 726, loss = 0.21445801\n",
      "Iteration 727, loss = 0.21426913\n",
      "Iteration 728, loss = 0.21408061\n",
      "Iteration 729, loss = 0.21389251\n",
      "Iteration 730, loss = 0.21370476\n",
      "Iteration 731, loss = 0.21351739\n",
      "Iteration 732, loss = 0.21333042\n",
      "Iteration 733, loss = 0.21314382\n",
      "Iteration 734, loss = 0.21295759\n",
      "Iteration 735, loss = 0.21277175\n",
      "Iteration 736, loss = 0.21258627\n",
      "Iteration 737, loss = 0.21240119\n",
      "Iteration 738, loss = 0.21221644\n",
      "Iteration 739, loss = 0.21203212\n",
      "Iteration 740, loss = 0.21184814\n",
      "Iteration 741, loss = 0.21166453\n",
      "Iteration 742, loss = 0.21148131\n",
      "Iteration 743, loss = 0.21129845\n",
      "Iteration 744, loss = 0.21111595\n",
      "Iteration 745, loss = 0.21093385\n",
      "Iteration 746, loss = 0.21075212\n",
      "Iteration 747, loss = 0.21057078\n",
      "Iteration 748, loss = 0.21038978\n",
      "Iteration 749, loss = 0.21020915\n",
      "Iteration 750, loss = 0.21002894\n",
      "Iteration 751, loss = 0.20984904\n",
      "Iteration 752, loss = 0.20966950\n",
      "Iteration 753, loss = 0.20949036\n",
      "Iteration 754, loss = 0.20931155\n",
      "Iteration 755, loss = 0.20913309\n",
      "Iteration 756, loss = 0.20895501\n",
      "Iteration 757, loss = 0.20877728\n",
      "Iteration 758, loss = 0.20859992\n",
      "Iteration 759, loss = 0.20842289\n",
      "Iteration 760, loss = 0.20824625\n",
      "Iteration 761, loss = 0.20806998\n",
      "Iteration 762, loss = 0.20789402\n",
      "Iteration 763, loss = 0.20771844\n",
      "Iteration 764, loss = 0.20754323\n",
      "Iteration 765, loss = 0.20736834\n",
      "Iteration 766, loss = 0.20719381\n",
      "Iteration 767, loss = 0.20701963\n",
      "Iteration 768, loss = 0.20684581\n",
      "Iteration 769, loss = 0.20667234\n",
      "Iteration 770, loss = 0.20649928\n",
      "Iteration 771, loss = 0.20632650\n",
      "Iteration 772, loss = 0.20615410\n",
      "Iteration 773, loss = 0.20598204\n",
      "Iteration 774, loss = 0.20581031\n",
      "Iteration 775, loss = 0.20563894\n",
      "Iteration 776, loss = 0.20546792\n",
      "Iteration 777, loss = 0.20529723\n",
      "Iteration 778, loss = 0.20512688\n",
      "Iteration 779, loss = 0.20495687\n",
      "Iteration 780, loss = 0.20478724\n",
      "Iteration 781, loss = 0.20461793\n",
      "Iteration 782, loss = 0.20444894\n",
      "Iteration 783, loss = 0.20428031\n",
      "Iteration 784, loss = 0.20411201\n",
      "Iteration 785, loss = 0.20394404\n",
      "Iteration 786, loss = 0.20377640\n",
      "Iteration 787, loss = 0.20360908\n",
      "Iteration 788, loss = 0.20344214\n",
      "Iteration 789, loss = 0.20327554\n",
      "Iteration 790, loss = 0.20310921\n",
      "Iteration 791, loss = 0.20294328\n",
      "Iteration 792, loss = 0.20277765\n",
      "Iteration 793, loss = 0.20261235\n",
      "Iteration 794, loss = 0.20244738\n",
      "Iteration 795, loss = 0.20228274\n",
      "Iteration 796, loss = 0.20211845\n",
      "Iteration 797, loss = 0.20195445\n",
      "Iteration 798, loss = 0.20179086\n",
      "Iteration 799, loss = 0.20162751\n",
      "Iteration 800, loss = 0.20146454\n",
      "Iteration 801, loss = 0.20130187\n",
      "Iteration 802, loss = 0.20113953\n",
      "Iteration 803, loss = 0.20097749\n",
      "Iteration 804, loss = 0.20081583\n",
      "Iteration 805, loss = 0.20065446\n",
      "Iteration 806, loss = 0.20049340\n",
      "Iteration 807, loss = 0.20033273\n",
      "Iteration 808, loss = 0.20017227\n",
      "Iteration 809, loss = 0.20001221\n",
      "Iteration 810, loss = 0.19985245\n",
      "Iteration 811, loss = 0.19969298\n",
      "Iteration 812, loss = 0.19953385\n",
      "Iteration 813, loss = 0.19937505\n",
      "Iteration 814, loss = 0.19921653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 815, loss = 0.19905832\n",
      "Iteration 816, loss = 0.19890051\n",
      "Iteration 817, loss = 0.19874293\n",
      "Iteration 818, loss = 0.19858568\n",
      "Iteration 819, loss = 0.19842876\n",
      "Iteration 820, loss = 0.19827214\n",
      "Iteration 821, loss = 0.19811584\n",
      "Iteration 822, loss = 0.19795988\n",
      "Iteration 823, loss = 0.19780418\n",
      "Iteration 824, loss = 0.19764881\n",
      "Iteration 825, loss = 0.19749384\n",
      "Iteration 826, loss = 0.19733904\n",
      "Iteration 827, loss = 0.19718462\n",
      "Iteration 828, loss = 0.19703049\n",
      "Iteration 829, loss = 0.19687667\n",
      "Iteration 830, loss = 0.19672315\n",
      "Iteration 831, loss = 0.19656996\n",
      "Iteration 832, loss = 0.19641703\n",
      "Iteration 833, loss = 0.19626443\n",
      "Iteration 834, loss = 0.19611215\n",
      "Iteration 835, loss = 0.19596017\n",
      "Iteration 836, loss = 0.19580846\n",
      "Iteration 837, loss = 0.19565708\n",
      "Iteration 838, loss = 0.19550596\n",
      "Iteration 839, loss = 0.19535515\n",
      "Iteration 840, loss = 0.19520464\n",
      "Iteration 841, loss = 0.19505442\n",
      "Iteration 842, loss = 0.19490450\n",
      "Iteration 843, loss = 0.19475491\n",
      "Iteration 844, loss = 0.19460559\n",
      "Iteration 845, loss = 0.19445657\n",
      "Iteration 846, loss = 0.19430784\n",
      "Iteration 847, loss = 0.19415938\n",
      "Iteration 848, loss = 0.19401123\n",
      "Iteration 849, loss = 0.19386335\n",
      "Iteration 850, loss = 0.19371578\n",
      "Iteration 851, loss = 0.19356853\n",
      "Iteration 852, loss = 0.19342153\n",
      "Iteration 853, loss = 0.19327482\n",
      "Iteration 854, loss = 0.19312844\n",
      "Iteration 855, loss = 0.19298231\n",
      "Iteration 856, loss = 0.19283647\n",
      "Iteration 857, loss = 0.19269092\n",
      "Iteration 858, loss = 0.19254566\n",
      "Iteration 859, loss = 0.19240069\n",
      "Iteration 860, loss = 0.19225604\n",
      "Iteration 861, loss = 0.19211161\n",
      "Iteration 862, loss = 0.19196751\n",
      "Iteration 863, loss = 0.19182366\n",
      "Iteration 864, loss = 0.19168010\n",
      "Iteration 865, loss = 0.19153683\n",
      "Iteration 866, loss = 0.19139382\n",
      "Iteration 867, loss = 0.19125111\n",
      "Iteration 868, loss = 0.19110873\n",
      "Iteration 869, loss = 0.19096655\n",
      "Iteration 870, loss = 0.19082468\n",
      "Iteration 871, loss = 0.19068308\n",
      "Iteration 872, loss = 0.19054176\n",
      "Iteration 873, loss = 0.19040075\n",
      "Iteration 874, loss = 0.19026000\n",
      "Iteration 875, loss = 0.19011975\n",
      "Iteration 876, loss = 0.18997954\n",
      "Iteration 877, loss = 0.18983970\n",
      "Iteration 878, loss = 0.18970010\n",
      "Iteration 879, loss = 0.18956079\n",
      "Iteration 880, loss = 0.18942181\n",
      "Iteration 881, loss = 0.18928304\n",
      "Iteration 882, loss = 0.18914455\n",
      "Iteration 883, loss = 0.18900637\n",
      "Iteration 884, loss = 0.18886841\n",
      "Iteration 885, loss = 0.18873078\n",
      "Iteration 886, loss = 0.18859334\n",
      "Iteration 887, loss = 0.18845621\n",
      "Iteration 888, loss = 0.18831938\n",
      "Iteration 889, loss = 0.18818278\n",
      "Iteration 890, loss = 0.18804639\n",
      "Iteration 891, loss = 0.18791040\n",
      "Iteration 892, loss = 0.18777457\n",
      "Iteration 893, loss = 0.18763902\n",
      "Iteration 894, loss = 0.18750378\n",
      "Iteration 895, loss = 0.18736873\n",
      "Iteration 896, loss = 0.18723405\n",
      "Iteration 897, loss = 0.18709949\n",
      "Iteration 898, loss = 0.18696531\n",
      "Iteration 899, loss = 0.18683134\n",
      "Iteration 900, loss = 0.18669772\n",
      "Iteration 901, loss = 0.18656455\n",
      "Iteration 902, loss = 0.18643166\n",
      "Iteration 903, loss = 0.18629899\n",
      "Iteration 904, loss = 0.18616668\n",
      "Iteration 905, loss = 0.18603455\n",
      "Iteration 906, loss = 0.18590269\n",
      "Iteration 907, loss = 0.18577118\n",
      "Iteration 908, loss = 0.18563985\n",
      "Iteration 909, loss = 0.18550877\n",
      "Iteration 910, loss = 0.18537805\n",
      "Iteration 911, loss = 0.18524747\n",
      "Iteration 912, loss = 0.18511724\n",
      "Iteration 913, loss = 0.18498722\n",
      "Iteration 914, loss = 0.18485746\n",
      "Iteration 915, loss = 0.18472796\n",
      "Iteration 916, loss = 0.18459873\n",
      "Iteration 917, loss = 0.18446971\n",
      "Iteration 918, loss = 0.18434101\n",
      "Iteration 919, loss = 0.18421252\n",
      "Iteration 920, loss = 0.18408426\n",
      "Iteration 921, loss = 0.18395632\n",
      "Iteration 922, loss = 0.18382854\n",
      "Iteration 923, loss = 0.18370102\n",
      "Iteration 924, loss = 0.18357385\n",
      "Iteration 925, loss = 0.18344683\n",
      "Iteration 926, loss = 0.18332003\n",
      "Iteration 927, loss = 0.18319357\n",
      "Iteration 928, loss = 0.18306724\n",
      "Iteration 929, loss = 0.18294123\n",
      "Iteration 930, loss = 0.18281544\n",
      "Iteration 931, loss = 0.18268987\n",
      "Iteration 932, loss = 0.18256455\n",
      "Iteration 933, loss = 0.18243950\n",
      "Iteration 934, loss = 0.18231463\n",
      "Iteration 935, loss = 0.18219006\n",
      "Iteration 936, loss = 0.18206568\n",
      "Iteration 937, loss = 0.18194153\n",
      "Iteration 938, loss = 0.18181771\n",
      "Iteration 939, loss = 0.18169399\n",
      "Iteration 940, loss = 0.18157057\n",
      "Iteration 941, loss = 0.18144744\n",
      "Iteration 942, loss = 0.18132445\n",
      "Iteration 943, loss = 0.18120169\n",
      "Iteration 944, loss = 0.18107929\n",
      "Iteration 945, loss = 0.18095693\n",
      "Iteration 946, loss = 0.18083497\n",
      "Iteration 947, loss = 0.18071314\n",
      "Iteration 948, loss = 0.18059155\n",
      "Iteration 949, loss = 0.18047027\n",
      "Iteration 950, loss = 0.18034912\n",
      "Iteration 951, loss = 0.18022822\n",
      "Iteration 952, loss = 0.18010763\n",
      "Iteration 953, loss = 0.17998714\n",
      "Iteration 954, loss = 0.17986694\n",
      "Iteration 955, loss = 0.17974702\n",
      "Iteration 956, loss = 0.17962725\n",
      "Iteration 957, loss = 0.17950770\n",
      "Iteration 958, loss = 0.17938845\n",
      "Iteration 959, loss = 0.17926931\n",
      "Iteration 960, loss = 0.17915049\n",
      "Iteration 961, loss = 0.17903186\n",
      "Iteration 962, loss = 0.17891343\n",
      "Iteration 963, loss = 0.17879528\n",
      "Iteration 964, loss = 0.17867732\n",
      "Iteration 965, loss = 0.17855956\n",
      "Iteration 966, loss = 0.17844210\n",
      "Iteration 967, loss = 0.17832478\n",
      "Iteration 968, loss = 0.17820769\n",
      "Iteration 969, loss = 0.17809092\n",
      "Iteration 970, loss = 0.17797426\n",
      "Iteration 971, loss = 0.17785781\n",
      "Iteration 972, loss = 0.17774165\n",
      "Iteration 973, loss = 0.17762563\n",
      "Iteration 974, loss = 0.17750989\n",
      "Iteration 975, loss = 0.17739438\n",
      "Iteration 976, loss = 0.17727903\n",
      "Iteration 977, loss = 0.17716394\n",
      "Iteration 978, loss = 0.17704907\n",
      "Iteration 979, loss = 0.17693437\n",
      "Iteration 980, loss = 0.17681994\n",
      "Iteration 981, loss = 0.17670568\n",
      "Iteration 982, loss = 0.17659163\n",
      "Iteration 983, loss = 0.17647788\n",
      "Iteration 984, loss = 0.17636422\n",
      "Iteration 985, loss = 0.17625083\n",
      "Iteration 986, loss = 0.17613767\n",
      "Iteration 987, loss = 0.17602468\n",
      "Iteration 988, loss = 0.17591187\n",
      "Iteration 989, loss = 0.17579938\n",
      "Iteration 990, loss = 0.17568701\n",
      "Iteration 991, loss = 0.17557483\n",
      "Iteration 992, loss = 0.17546295\n",
      "Iteration 993, loss = 0.17535120\n",
      "Iteration 994, loss = 0.17523968\n",
      "Iteration 995, loss = 0.17512838\n",
      "Iteration 996, loss = 0.17501724\n",
      "Iteration 997, loss = 0.17490641\n",
      "Iteration 998, loss = 0.17479565\n",
      "Iteration 999, loss = 0.17468517\n",
      "Iteration 1000, loss = 0.17457490\n",
      "Iteration 1, loss = 1.34884587\n",
      "Iteration 2, loss = 1.32501816\n",
      "Iteration 3, loss = 1.29301249\n",
      "Iteration 4, loss = 1.25523626\n",
      "Iteration 5, loss = 1.21379618\n",
      "Iteration 6, loss = 1.17034833\n",
      "Iteration 7, loss = 1.12612768\n",
      "Iteration 8, loss = 1.08217530\n",
      "Iteration 9, loss = 1.03956335\n",
      "Iteration 10, loss = 0.99933244\n",
      "Iteration 11, loss = 0.96255168\n",
      "Iteration 12, loss = 0.93020547\n",
      "Iteration 13, loss = 0.90288408\n",
      "Iteration 14, loss = 0.88096243\n",
      "Iteration 15, loss = 0.86421312\n",
      "Iteration 16, loss = 0.85208220\n",
      "Iteration 17, loss = 0.84344670\n",
      "Iteration 18, loss = 0.83723294\n",
      "Iteration 19, loss = 0.83193255\n",
      "Iteration 20, loss = 0.82629072\n",
      "Iteration 21, loss = 0.81957850\n",
      "Iteration 22, loss = 0.81131771\n",
      "Iteration 23, loss = 0.80167984\n",
      "Iteration 24, loss = 0.79097473\n",
      "Iteration 25, loss = 0.77995292\n",
      "Iteration 26, loss = 0.76856715\n",
      "Iteration 27, loss = 0.75719039\n",
      "Iteration 28, loss = 0.74600040\n",
      "Iteration 29, loss = 0.73515884\n",
      "Iteration 30, loss = 0.72499660\n",
      "Iteration 31, loss = 0.71575819\n",
      "Iteration 32, loss = 0.70732857\n",
      "Iteration 33, loss = 0.69974200\n",
      "Iteration 34, loss = 0.69287924\n",
      "Iteration 35, loss = 0.68675955\n",
      "Iteration 36, loss = 0.68115547\n",
      "Iteration 37, loss = 0.67586654\n",
      "Iteration 38, loss = 0.67082517\n",
      "Iteration 39, loss = 0.66599572\n",
      "Iteration 40, loss = 0.66127821\n",
      "Iteration 41, loss = 0.65663419\n",
      "Iteration 42, loss = 0.65207606\n",
      "Iteration 43, loss = 0.64758959\n",
      "Iteration 44, loss = 0.64315950\n",
      "Iteration 45, loss = 0.63877207\n",
      "Iteration 46, loss = 0.63444367\n",
      "Iteration 47, loss = 0.63017294\n",
      "Iteration 48, loss = 0.62599856\n",
      "Iteration 49, loss = 0.62193289\n",
      "Iteration 50, loss = 0.61796922\n",
      "Iteration 51, loss = 0.61416207\n",
      "Iteration 52, loss = 0.61050320\n",
      "Iteration 53, loss = 0.60703121\n",
      "Iteration 54, loss = 0.60374302\n",
      "Iteration 55, loss = 0.60059477\n",
      "Iteration 56, loss = 0.59754162\n",
      "Iteration 57, loss = 0.59455044\n",
      "Iteration 58, loss = 0.59160510\n",
      "Iteration 59, loss = 0.58869383\n",
      "Iteration 60, loss = 0.58581250\n",
      "Iteration 61, loss = 0.58295645\n",
      "Iteration 62, loss = 0.58012571\n",
      "Iteration 63, loss = 0.57732303\n",
      "Iteration 64, loss = 0.57455543\n",
      "Iteration 65, loss = 0.57182714\n",
      "Iteration 66, loss = 0.56912989\n",
      "Iteration 67, loss = 0.56647898\n",
      "Iteration 68, loss = 0.56387303\n",
      "Iteration 69, loss = 0.56132875\n",
      "Iteration 70, loss = 0.55885377\n",
      "Iteration 71, loss = 0.55642281\n",
      "Iteration 72, loss = 0.55403204\n",
      "Iteration 73, loss = 0.55168911\n",
      "Iteration 74, loss = 0.54939394\n",
      "Iteration 75, loss = 0.54713806\n",
      "Iteration 76, loss = 0.54491269\n",
      "Iteration 77, loss = 0.54271815\n",
      "Iteration 78, loss = 0.54055298\n",
      "Iteration 79, loss = 0.53841588\n",
      "Iteration 80, loss = 0.53630643\n",
      "Iteration 81, loss = 0.53422450\n",
      "Iteration 82, loss = 0.53217294\n",
      "Iteration 83, loss = 0.53014992\n",
      "Iteration 84, loss = 0.52815418\n",
      "Iteration 85, loss = 0.52618511\n",
      "Iteration 86, loss = 0.52424229\n",
      "Iteration 87, loss = 0.52232913\n",
      "Iteration 88, loss = 0.52044366\n",
      "Iteration 89, loss = 0.51858646\n",
      "Iteration 90, loss = 0.51675546\n",
      "Iteration 91, loss = 0.51494843\n",
      "Iteration 92, loss = 0.51316421\n",
      "Iteration 93, loss = 0.51140213\n",
      "Iteration 94, loss = 0.50966171\n",
      "Iteration 95, loss = 0.50794252\n",
      "Iteration 96, loss = 0.50624435\n",
      "Iteration 97, loss = 0.50456650\n",
      "Iteration 98, loss = 0.50290930\n",
      "Iteration 99, loss = 0.50127273\n",
      "Iteration 100, loss = 0.49965615\n",
      "Iteration 101, loss = 0.49805892\n",
      "Iteration 102, loss = 0.49648043\n",
      "Iteration 103, loss = 0.49491946\n",
      "Iteration 104, loss = 0.49337857\n",
      "Iteration 105, loss = 0.49185625\n",
      "Iteration 106, loss = 0.49035035\n",
      "Iteration 107, loss = 0.48886119\n",
      "Iteration 108, loss = 0.48738897\n",
      "Iteration 109, loss = 0.48593295\n",
      "Iteration 110, loss = 0.48449344\n",
      "Iteration 111, loss = 0.48307019\n",
      "Iteration 112, loss = 0.48166271\n",
      "Iteration 113, loss = 0.48027081\n",
      "Iteration 114, loss = 0.47889468\n",
      "Iteration 115, loss = 0.47753298\n",
      "Iteration 116, loss = 0.47618577\n",
      "Iteration 117, loss = 0.47485238\n",
      "Iteration 118, loss = 0.47353229\n",
      "Iteration 119, loss = 0.47222595\n",
      "Iteration 120, loss = 0.47093315\n",
      "Iteration 121, loss = 0.46965327\n",
      "Iteration 122, loss = 0.46838533\n",
      "Iteration 123, loss = 0.46712924\n",
      "Iteration 124, loss = 0.46588534\n",
      "Iteration 125, loss = 0.46465314\n",
      "Iteration 126, loss = 0.46343240\n",
      "Iteration 127, loss = 0.46222303\n",
      "Iteration 128, loss = 0.46102491\n",
      "Iteration 129, loss = 0.45983756\n",
      "Iteration 130, loss = 0.45865999\n",
      "Iteration 131, loss = 0.45749206\n",
      "Iteration 132, loss = 0.45633456\n",
      "Iteration 133, loss = 0.45518672\n",
      "Iteration 134, loss = 0.45404790\n",
      "Iteration 135, loss = 0.45291763\n",
      "Iteration 136, loss = 0.45179667\n",
      "Iteration 137, loss = 0.45068481\n",
      "Iteration 138, loss = 0.44958215\n",
      "Iteration 139, loss = 0.44848826\n",
      "Iteration 140, loss = 0.44740341\n",
      "Iteration 141, loss = 0.44632702\n",
      "Iteration 142, loss = 0.44525920\n",
      "Iteration 143, loss = 0.44419919\n",
      "Iteration 144, loss = 0.44314638\n",
      "Iteration 145, loss = 0.44210010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 146, loss = 0.44106060\n",
      "Iteration 147, loss = 0.44002788\n",
      "Iteration 148, loss = 0.43900062\n",
      "Iteration 149, loss = 0.43797918\n",
      "Iteration 150, loss = 0.43696211\n",
      "Iteration 151, loss = 0.43595032\n",
      "Iteration 152, loss = 0.43494497\n",
      "Iteration 153, loss = 0.43394418\n",
      "Iteration 154, loss = 0.43294903\n",
      "Iteration 155, loss = 0.43195286\n",
      "Iteration 156, loss = 0.43095850\n",
      "Iteration 157, loss = 0.42996572\n",
      "Iteration 158, loss = 0.42897163\n",
      "Iteration 159, loss = 0.42797834\n",
      "Iteration 160, loss = 0.42698210\n",
      "Iteration 161, loss = 0.42598761\n",
      "Iteration 162, loss = 0.42499380\n",
      "Iteration 163, loss = 0.42400063\n",
      "Iteration 164, loss = 0.42300705\n",
      "Iteration 165, loss = 0.42201582\n",
      "Iteration 166, loss = 0.42102295\n",
      "Iteration 167, loss = 0.42003342\n",
      "Iteration 168, loss = 0.41904174\n",
      "Iteration 169, loss = 0.41803974\n",
      "Iteration 170, loss = 0.41703466\n",
      "Iteration 171, loss = 0.41602572\n",
      "Iteration 172, loss = 0.41501851\n",
      "Iteration 173, loss = 0.41400406\n",
      "Iteration 174, loss = 0.41298751\n",
      "Iteration 175, loss = 0.41197716\n",
      "Iteration 176, loss = 0.41096343\n",
      "Iteration 177, loss = 0.40995297\n",
      "Iteration 178, loss = 0.40894935\n",
      "Iteration 179, loss = 0.40794869\n",
      "Iteration 180, loss = 0.40696469\n",
      "Iteration 181, loss = 0.40599165\n",
      "Iteration 182, loss = 0.40502415\n",
      "Iteration 183, loss = 0.40406834\n",
      "Iteration 184, loss = 0.40312699\n",
      "Iteration 185, loss = 0.40219932\n",
      "Iteration 186, loss = 0.40129853\n",
      "Iteration 187, loss = 0.40041230\n",
      "Iteration 188, loss = 0.39954409\n",
      "Iteration 189, loss = 0.39869163\n",
      "Iteration 190, loss = 0.39785564\n",
      "Iteration 191, loss = 0.39702934\n",
      "Iteration 192, loss = 0.39621540\n",
      "Iteration 193, loss = 0.39541480\n",
      "Iteration 194, loss = 0.39462969\n",
      "Iteration 195, loss = 0.39385456\n",
      "Iteration 196, loss = 0.39308473\n",
      "Iteration 197, loss = 0.39232186\n",
      "Iteration 198, loss = 0.39156688\n",
      "Iteration 199, loss = 0.39082058\n",
      "Iteration 200, loss = 0.39007996\n",
      "Iteration 201, loss = 0.38934560\n",
      "Iteration 202, loss = 0.38861676\n",
      "Iteration 203, loss = 0.38789227\n",
      "Iteration 204, loss = 0.38717313\n",
      "Iteration 205, loss = 0.38645781\n",
      "Iteration 206, loss = 0.38574621\n",
      "Iteration 207, loss = 0.38503844\n",
      "Iteration 208, loss = 0.38433420\n",
      "Iteration 209, loss = 0.38363346\n",
      "Iteration 210, loss = 0.38293657\n",
      "Iteration 211, loss = 0.38224394\n",
      "Iteration 212, loss = 0.38155559\n",
      "Iteration 213, loss = 0.38087061\n",
      "Iteration 214, loss = 0.38018880\n",
      "Iteration 215, loss = 0.37951008\n",
      "Iteration 216, loss = 0.37883438\n",
      "Iteration 217, loss = 0.37816184\n",
      "Iteration 218, loss = 0.37749272\n",
      "Iteration 219, loss = 0.37682654\n",
      "Iteration 220, loss = 0.37616321\n",
      "Iteration 221, loss = 0.37550281\n",
      "Iteration 222, loss = 0.37484520\n",
      "Iteration 223, loss = 0.37419033\n",
      "Iteration 224, loss = 0.37353832\n",
      "Iteration 225, loss = 0.37288895\n",
      "Iteration 226, loss = 0.37224225\n",
      "Iteration 227, loss = 0.37159822\n",
      "Iteration 228, loss = 0.37095684\n",
      "Iteration 229, loss = 0.37031800\n",
      "Iteration 230, loss = 0.36968175\n",
      "Iteration 231, loss = 0.36904806\n",
      "Iteration 232, loss = 0.36841697\n",
      "Iteration 233, loss = 0.36778824\n",
      "Iteration 234, loss = 0.36716192\n",
      "Iteration 235, loss = 0.36653788\n",
      "Iteration 236, loss = 0.36591622\n",
      "Iteration 237, loss = 0.36529688\n",
      "Iteration 238, loss = 0.36467992\n",
      "Iteration 239, loss = 0.36406523\n",
      "Iteration 240, loss = 0.36345283\n",
      "Iteration 241, loss = 0.36284274\n",
      "Iteration 242, loss = 0.36223505\n",
      "Iteration 243, loss = 0.36162958\n",
      "Iteration 244, loss = 0.36102642\n",
      "Iteration 245, loss = 0.36042540\n",
      "Iteration 246, loss = 0.35982659\n",
      "Iteration 247, loss = 0.35922996\n",
      "Iteration 248, loss = 0.35863543\n",
      "Iteration 249, loss = 0.35804290\n",
      "Iteration 250, loss = 0.35745251\n",
      "Iteration 251, loss = 0.35686414\n",
      "Iteration 252, loss = 0.35627777\n",
      "Iteration 253, loss = 0.35569345\n",
      "Iteration 254, loss = 0.35511114\n",
      "Iteration 255, loss = 0.35453083\n",
      "Iteration 256, loss = 0.35395250\n",
      "Iteration 257, loss = 0.35337612\n",
      "Iteration 258, loss = 0.35280170\n",
      "Iteration 259, loss = 0.35222922\n",
      "Iteration 260, loss = 0.35165864\n",
      "Iteration 261, loss = 0.35108998\n",
      "Iteration 262, loss = 0.35052315\n",
      "Iteration 263, loss = 0.34995828\n",
      "Iteration 264, loss = 0.34939516\n",
      "Iteration 265, loss = 0.34883390\n",
      "Iteration 266, loss = 0.34827449\n",
      "Iteration 267, loss = 0.34771678\n",
      "Iteration 268, loss = 0.34716085\n",
      "Iteration 269, loss = 0.34660683\n",
      "Iteration 270, loss = 0.34605463\n",
      "Iteration 271, loss = 0.34550407\n",
      "Iteration 272, loss = 0.34495536\n",
      "Iteration 273, loss = 0.34440830\n",
      "Iteration 274, loss = 0.34386294\n",
      "Iteration 275, loss = 0.34331931\n",
      "Iteration 276, loss = 0.34277737\n",
      "Iteration 277, loss = 0.34223708\n",
      "Iteration 278, loss = 0.34169846\n",
      "Iteration 279, loss = 0.34116148\n",
      "Iteration 280, loss = 0.34062617\n",
      "Iteration 281, loss = 0.34009242\n",
      "Iteration 282, loss = 0.33956035\n",
      "Iteration 283, loss = 0.33902983\n",
      "Iteration 284, loss = 0.33850090\n",
      "Iteration 285, loss = 0.33797361\n",
      "Iteration 286, loss = 0.33744778\n",
      "Iteration 287, loss = 0.33692357\n",
      "Iteration 288, loss = 0.33640087\n",
      "Iteration 289, loss = 0.33587972\n",
      "Iteration 290, loss = 0.33536024\n",
      "Iteration 291, loss = 0.33484215\n",
      "Iteration 292, loss = 0.33432553\n",
      "Iteration 293, loss = 0.33381039\n",
      "Iteration 294, loss = 0.33329681\n",
      "Iteration 295, loss = 0.33278464\n",
      "Iteration 296, loss = 0.33227395\n",
      "Iteration 297, loss = 0.33176474\n",
      "Iteration 298, loss = 0.33125699\n",
      "Iteration 299, loss = 0.33075058\n",
      "Iteration 300, loss = 0.33024552\n",
      "Iteration 301, loss = 0.32974191\n",
      "Iteration 302, loss = 0.32923967\n",
      "Iteration 303, loss = 0.32873885\n",
      "Iteration 304, loss = 0.32823936\n",
      "Iteration 305, loss = 0.32774131\n",
      "Iteration 306, loss = 0.32724458\n",
      "Iteration 307, loss = 0.32674922\n",
      "Iteration 308, loss = 0.32625522\n",
      "Iteration 309, loss = 0.32576258\n",
      "Iteration 310, loss = 0.32527122\n",
      "Iteration 311, loss = 0.32478114\n",
      "Iteration 312, loss = 0.32429234\n",
      "Iteration 313, loss = 0.32380489\n",
      "Iteration 314, loss = 0.32331872\n",
      "Iteration 315, loss = 0.32283393\n",
      "Iteration 316, loss = 0.32235028\n",
      "Iteration 317, loss = 0.32186800\n",
      "Iteration 318, loss = 0.32138704\n",
      "Iteration 319, loss = 0.32090734\n",
      "Iteration 320, loss = 0.32042886\n",
      "Iteration 321, loss = 0.31995166\n",
      "Iteration 322, loss = 0.31947571\n",
      "Iteration 323, loss = 0.31900100\n",
      "Iteration 324, loss = 0.31852750\n",
      "Iteration 325, loss = 0.31805525\n",
      "Iteration 326, loss = 0.31758424\n",
      "Iteration 327, loss = 0.31711486\n",
      "Iteration 328, loss = 0.31664682\n",
      "Iteration 329, loss = 0.31617997\n",
      "Iteration 330, loss = 0.31571431\n",
      "Iteration 331, loss = 0.31524980\n",
      "Iteration 332, loss = 0.31478650\n",
      "Iteration 333, loss = 0.31432435\n",
      "Iteration 334, loss = 0.31386338\n",
      "Iteration 335, loss = 0.31340356\n",
      "Iteration 336, loss = 0.31294490\n",
      "Iteration 337, loss = 0.31248740\n",
      "Iteration 338, loss = 0.31203109\n",
      "Iteration 339, loss = 0.31157592\n",
      "Iteration 340, loss = 0.31112185\n",
      "Iteration 341, loss = 0.31066894\n",
      "Iteration 342, loss = 0.31021716\n",
      "Iteration 343, loss = 0.30976649\n",
      "Iteration 344, loss = 0.30931695\n",
      "Iteration 345, loss = 0.30886858\n",
      "Iteration 346, loss = 0.30842180\n",
      "Iteration 347, loss = 0.30797612\n",
      "Iteration 348, loss = 0.30753146\n",
      "Iteration 349, loss = 0.30708796\n",
      "Iteration 350, loss = 0.30664544\n",
      "Iteration 351, loss = 0.30620407\n",
      "Iteration 352, loss = 0.30576383\n",
      "Iteration 353, loss = 0.30532467\n",
      "Iteration 354, loss = 0.30488658\n",
      "Iteration 355, loss = 0.30444959\n",
      "Iteration 356, loss = 0.30401371\n",
      "Iteration 357, loss = 0.30357880\n",
      "Iteration 358, loss = 0.30314501\n",
      "Iteration 359, loss = 0.30271223\n",
      "Iteration 360, loss = 0.30228049\n",
      "Iteration 361, loss = 0.30184980\n",
      "Iteration 362, loss = 0.30142012\n",
      "Iteration 363, loss = 0.30099151\n",
      "Iteration 364, loss = 0.30056389\n",
      "Iteration 365, loss = 0.30013743\n",
      "Iteration 366, loss = 0.29971185\n",
      "Iteration 367, loss = 0.29928734\n",
      "Iteration 368, loss = 0.29886397\n",
      "Iteration 369, loss = 0.29844160\n",
      "Iteration 370, loss = 0.29802018\n",
      "Iteration 371, loss = 0.29759974\n",
      "Iteration 372, loss = 0.29718031\n",
      "Iteration 373, loss = 0.29676189\n",
      "Iteration 374, loss = 0.29634446\n",
      "Iteration 375, loss = 0.29592799\n",
      "Iteration 376, loss = 0.29551247\n",
      "Iteration 377, loss = 0.29509805\n",
      "Iteration 378, loss = 0.29468449\n",
      "Iteration 379, loss = 0.29427191\n",
      "Iteration 380, loss = 0.29386031\n",
      "Iteration 381, loss = 0.29344970\n",
      "Iteration 382, loss = 0.29304002\n",
      "Iteration 383, loss = 0.29263132\n",
      "Iteration 384, loss = 0.29222355\n",
      "Iteration 385, loss = 0.29181672\n",
      "Iteration 386, loss = 0.29141084\n",
      "Iteration 387, loss = 0.29100593\n",
      "Iteration 388, loss = 0.29060192\n",
      "Iteration 389, loss = 0.29019896\n",
      "Iteration 390, loss = 0.28979684\n",
      "Iteration 391, loss = 0.28939572\n",
      "Iteration 392, loss = 0.28899549\n",
      "Iteration 393, loss = 0.28859623\n",
      "Iteration 394, loss = 0.28819791\n",
      "Iteration 395, loss = 0.28780057\n",
      "Iteration 396, loss = 0.28740407\n",
      "Iteration 397, loss = 0.28700858\n",
      "Iteration 398, loss = 0.28661404\n",
      "Iteration 399, loss = 0.28622041\n",
      "Iteration 400, loss = 0.28582771\n",
      "Iteration 401, loss = 0.28543590\n",
      "Iteration 402, loss = 0.28504498\n",
      "Iteration 403, loss = 0.28465503\n",
      "Iteration 404, loss = 0.28426591\n",
      "Iteration 405, loss = 0.28387770\n",
      "Iteration 406, loss = 0.28349041\n",
      "Iteration 407, loss = 0.28310395\n",
      "Iteration 408, loss = 0.28271842\n",
      "Iteration 409, loss = 0.28233373\n",
      "Iteration 410, loss = 0.28194994\n",
      "Iteration 411, loss = 0.28156696\n",
      "Iteration 412, loss = 0.28118496\n",
      "Iteration 413, loss = 0.28080375\n",
      "Iteration 414, loss = 0.28042345\n",
      "Iteration 415, loss = 0.28004400\n",
      "Iteration 416, loss = 0.27966539\n",
      "Iteration 417, loss = 0.27928766\n",
      "Iteration 418, loss = 0.27891080\n",
      "Iteration 419, loss = 0.27853474\n",
      "Iteration 420, loss = 0.27815954\n",
      "Iteration 421, loss = 0.27778519\n",
      "Iteration 422, loss = 0.27741167\n",
      "Iteration 423, loss = 0.27703902\n",
      "Iteration 424, loss = 0.27666715\n",
      "Iteration 425, loss = 0.27629618\n",
      "Iteration 426, loss = 0.27592599\n",
      "Iteration 427, loss = 0.27555665\n",
      "Iteration 428, loss = 0.27518814\n",
      "Iteration 429, loss = 0.27482045\n",
      "Iteration 430, loss = 0.27445356\n",
      "Iteration 431, loss = 0.27408753\n",
      "Iteration 432, loss = 0.27372227\n",
      "Iteration 433, loss = 0.27335784\n",
      "Iteration 434, loss = 0.27299423\n",
      "Iteration 435, loss = 0.27263141\n",
      "Iteration 436, loss = 0.27226944\n",
      "Iteration 437, loss = 0.27190824\n",
      "Iteration 438, loss = 0.27154782\n",
      "Iteration 439, loss = 0.27118825\n",
      "Iteration 440, loss = 0.27082943\n",
      "Iteration 441, loss = 0.27047144\n",
      "Iteration 442, loss = 0.27011425\n",
      "Iteration 443, loss = 0.26975787\n",
      "Iteration 444, loss = 0.26940234\n",
      "Iteration 445, loss = 0.26904752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 446, loss = 0.26869353\n",
      "Iteration 447, loss = 0.26834033\n",
      "Iteration 448, loss = 0.26798788\n",
      "Iteration 449, loss = 0.26763626\n",
      "Iteration 450, loss = 0.26728536\n",
      "Iteration 451, loss = 0.26693526\n",
      "Iteration 452, loss = 0.26658596\n",
      "Iteration 453, loss = 0.26623738\n",
      "Iteration 454, loss = 0.26588959\n",
      "Iteration 455, loss = 0.26554260\n",
      "Iteration 456, loss = 0.26519635\n",
      "Iteration 457, loss = 0.26485092\n",
      "Iteration 458, loss = 0.26450620\n",
      "Iteration 459, loss = 0.26416224\n",
      "Iteration 460, loss = 0.26381906\n",
      "Iteration 461, loss = 0.26347661\n",
      "Iteration 462, loss = 0.26313494\n",
      "Iteration 463, loss = 0.26279402\n",
      "Iteration 464, loss = 0.26245382\n",
      "Iteration 465, loss = 0.26211442\n",
      "Iteration 466, loss = 0.26177571\n",
      "Iteration 467, loss = 0.26143778\n",
      "Iteration 468, loss = 0.26110054\n",
      "Iteration 469, loss = 0.26076411\n",
      "Iteration 470, loss = 0.26042836\n",
      "Iteration 471, loss = 0.26009335\n",
      "Iteration 472, loss = 0.25975909\n",
      "Iteration 473, loss = 0.25942552\n",
      "Iteration 474, loss = 0.25909272\n",
      "Iteration 475, loss = 0.25876063\n",
      "Iteration 476, loss = 0.25842925\n",
      "Iteration 477, loss = 0.25809864\n",
      "Iteration 478, loss = 0.25776871\n",
      "Iteration 479, loss = 0.25743951\n",
      "Iteration 480, loss = 0.25711105\n",
      "Iteration 481, loss = 0.25678326\n",
      "Iteration 482, loss = 0.25645624\n",
      "Iteration 483, loss = 0.25612988\n",
      "Iteration 484, loss = 0.25580427\n",
      "Iteration 485, loss = 0.25547934\n",
      "Iteration 486, loss = 0.25515513\n",
      "Iteration 487, loss = 0.25483167\n",
      "Iteration 488, loss = 0.25450883\n",
      "Iteration 489, loss = 0.25418678\n",
      "Iteration 490, loss = 0.25386539\n",
      "Iteration 491, loss = 0.25354468\n",
      "Iteration 492, loss = 0.25322471\n",
      "Iteration 493, loss = 0.25290539\n",
      "Iteration 494, loss = 0.25258682\n",
      "Iteration 495, loss = 0.25226891\n",
      "Iteration 496, loss = 0.25195169\n",
      "Iteration 497, loss = 0.25163517\n",
      "Iteration 498, loss = 0.25131933\n",
      "Iteration 499, loss = 0.25100418\n",
      "Iteration 500, loss = 0.25068971\n",
      "Iteration 501, loss = 0.25037594\n",
      "Iteration 502, loss = 0.25006284\n",
      "Iteration 503, loss = 0.24975040\n",
      "Iteration 504, loss = 0.24943869\n",
      "Iteration 505, loss = 0.24912760\n",
      "Iteration 506, loss = 0.24881723\n",
      "Iteration 507, loss = 0.24850752\n",
      "Iteration 508, loss = 0.24819844\n",
      "Iteration 509, loss = 0.24789009\n",
      "Iteration 510, loss = 0.24758237\n",
      "Iteration 511, loss = 0.24727535\n",
      "Iteration 512, loss = 0.24696896\n",
      "Iteration 513, loss = 0.24666327\n",
      "Iteration 514, loss = 0.24635823\n",
      "Iteration 515, loss = 0.24605385\n",
      "Iteration 516, loss = 0.24575014\n",
      "Iteration 517, loss = 0.24544706\n",
      "Iteration 518, loss = 0.24514468\n",
      "Iteration 519, loss = 0.24484290\n",
      "Iteration 520, loss = 0.24454183\n",
      "Iteration 521, loss = 0.24424141\n",
      "Iteration 522, loss = 0.24394157\n",
      "Iteration 523, loss = 0.24364245\n",
      "Iteration 524, loss = 0.24334393\n",
      "Iteration 525, loss = 0.24304611\n",
      "Iteration 526, loss = 0.24274887\n",
      "Iteration 527, loss = 0.24245230\n",
      "Iteration 528, loss = 0.24215639\n",
      "Iteration 529, loss = 0.24186110\n",
      "Iteration 530, loss = 0.24156645\n",
      "Iteration 531, loss = 0.24127243\n",
      "Iteration 532, loss = 0.24097907\n",
      "Iteration 533, loss = 0.24068632\n",
      "Iteration 534, loss = 0.24039422\n",
      "Iteration 535, loss = 0.24010273\n",
      "Iteration 536, loss = 0.23981190\n",
      "Iteration 537, loss = 0.23952167\n",
      "Iteration 538, loss = 0.23923209\n",
      "Iteration 539, loss = 0.23894312\n",
      "Iteration 540, loss = 0.23865477\n",
      "Iteration 541, loss = 0.23836706\n",
      "Iteration 542, loss = 0.23807994\n",
      "Iteration 543, loss = 0.23779348\n",
      "Iteration 544, loss = 0.23750759\n",
      "Iteration 545, loss = 0.23722237\n",
      "Iteration 546, loss = 0.23693771\n",
      "Iteration 547, loss = 0.23665373\n",
      "Iteration 548, loss = 0.23637033\n",
      "Iteration 549, loss = 0.23608751\n",
      "Iteration 550, loss = 0.23580534\n",
      "Iteration 551, loss = 0.23552376\n",
      "Iteration 552, loss = 0.23524283\n",
      "Iteration 553, loss = 0.23496245\n",
      "Iteration 554, loss = 0.23468269\n",
      "Iteration 555, loss = 0.23440353\n",
      "Iteration 556, loss = 0.23412500\n",
      "Iteration 557, loss = 0.23384703\n",
      "Iteration 558, loss = 0.23356969\n",
      "Iteration 559, loss = 0.23329292\n",
      "Iteration 560, loss = 0.23301678\n",
      "Iteration 561, loss = 0.23274122\n",
      "Iteration 562, loss = 0.23246627\n",
      "Iteration 563, loss = 0.23219194\n",
      "Iteration 564, loss = 0.23191818\n",
      "Iteration 565, loss = 0.23164507\n",
      "Iteration 566, loss = 0.23137248\n",
      "Iteration 567, loss = 0.23110047\n",
      "Iteration 568, loss = 0.23082907\n",
      "Iteration 569, loss = 0.23055824\n",
      "Iteration 570, loss = 0.23028801\n",
      "Iteration 571, loss = 0.23001851\n",
      "Iteration 572, loss = 0.22974950\n",
      "Iteration 573, loss = 0.22948109\n",
      "Iteration 574, loss = 0.22921324\n",
      "Iteration 575, loss = 0.22894594\n",
      "Iteration 576, loss = 0.22867931\n",
      "Iteration 577, loss = 0.22841321\n",
      "Iteration 578, loss = 0.22814767\n",
      "Iteration 579, loss = 0.22788270\n",
      "Iteration 580, loss = 0.22761831\n",
      "Iteration 581, loss = 0.22735452\n",
      "Iteration 582, loss = 0.22709125\n",
      "Iteration 583, loss = 0.22682859\n",
      "Iteration 584, loss = 0.22656646\n",
      "Iteration 585, loss = 0.22630493\n",
      "Iteration 586, loss = 0.22604393\n",
      "Iteration 587, loss = 0.22578351\n",
      "Iteration 588, loss = 0.22552363\n",
      "Iteration 589, loss = 0.22526435\n",
      "Iteration 590, loss = 0.22500558\n",
      "Iteration 591, loss = 0.22474740\n",
      "Iteration 592, loss = 0.22448977\n",
      "Iteration 593, loss = 0.22423268\n",
      "Iteration 594, loss = 0.22397615\n",
      "Iteration 595, loss = 0.22372020\n",
      "Iteration 596, loss = 0.22346477\n",
      "Iteration 597, loss = 0.22320992\n",
      "Iteration 598, loss = 0.22295558\n",
      "Iteration 599, loss = 0.22270177\n",
      "Iteration 600, loss = 0.22244856\n",
      "Iteration 601, loss = 0.22219584\n",
      "Iteration 602, loss = 0.22194372\n",
      "Iteration 603, loss = 0.22169210\n",
      "Iteration 604, loss = 0.22144104\n",
      "Iteration 605, loss = 0.22119050\n",
      "Iteration 606, loss = 0.22094054\n",
      "Iteration 607, loss = 0.22069107\n",
      "Iteration 608, loss = 0.22044217\n",
      "Iteration 609, loss = 0.22019378\n",
      "Iteration 610, loss = 0.21994598\n",
      "Iteration 611, loss = 0.21969866\n",
      "Iteration 612, loss = 0.21945187\n",
      "Iteration 613, loss = 0.21920563\n",
      "Iteration 614, loss = 0.21895992\n",
      "Iteration 615, loss = 0.21871471\n",
      "Iteration 616, loss = 0.21847009\n",
      "Iteration 617, loss = 0.21822593\n",
      "Iteration 618, loss = 0.21798237\n",
      "Iteration 619, loss = 0.21773927\n",
      "Iteration 620, loss = 0.21749673\n",
      "Iteration 621, loss = 0.21725470\n",
      "Iteration 622, loss = 0.21701318\n",
      "Iteration 623, loss = 0.21677236\n",
      "Iteration 624, loss = 0.21653184\n",
      "Iteration 625, loss = 0.21629196\n",
      "Iteration 626, loss = 0.21605269\n",
      "Iteration 627, loss = 0.21581373\n",
      "Iteration 628, loss = 0.21557544\n",
      "Iteration 629, loss = 0.21533764\n",
      "Iteration 630, loss = 0.21510034\n",
      "Iteration 631, loss = 0.21486351\n",
      "Iteration 632, loss = 0.21462730\n",
      "Iteration 633, loss = 0.21439150\n",
      "Iteration 634, loss = 0.21415623\n",
      "Iteration 635, loss = 0.21392156\n",
      "Iteration 636, loss = 0.21368724\n",
      "Iteration 637, loss = 0.21345348\n",
      "Iteration 638, loss = 0.21322032\n",
      "Iteration 639, loss = 0.21298756\n",
      "Iteration 640, loss = 0.21275530\n",
      "Iteration 641, loss = 0.21252350\n",
      "Iteration 642, loss = 0.21229233\n",
      "Iteration 643, loss = 0.21206155\n",
      "Iteration 644, loss = 0.21183125\n",
      "Iteration 645, loss = 0.21160156\n",
      "Iteration 646, loss = 0.21137224\n",
      "Iteration 647, loss = 0.21114344\n",
      "Iteration 648, loss = 0.21091526\n",
      "Iteration 649, loss = 0.21068731\n",
      "Iteration 650, loss = 0.21046012\n",
      "Iteration 651, loss = 0.21023327\n",
      "Iteration 652, loss = 0.21000689\n",
      "Iteration 653, loss = 0.20978097\n",
      "Iteration 654, loss = 0.20955560\n",
      "Iteration 655, loss = 0.20933068\n",
      "Iteration 656, loss = 0.20910624\n",
      "Iteration 657, loss = 0.20888240\n",
      "Iteration 658, loss = 0.20865884\n",
      "Iteration 659, loss = 0.20843588\n",
      "Iteration 660, loss = 0.20821340\n",
      "Iteration 661, loss = 0.20799137\n",
      "Iteration 662, loss = 0.20776975\n",
      "Iteration 663, loss = 0.20754870\n",
      "Iteration 664, loss = 0.20732807\n",
      "Iteration 665, loss = 0.20710792\n",
      "Iteration 666, loss = 0.20688832\n",
      "Iteration 667, loss = 0.20666908\n",
      "Iteration 668, loss = 0.20645035\n",
      "Iteration 669, loss = 0.20623214\n",
      "Iteration 670, loss = 0.20601433\n",
      "Iteration 671, loss = 0.20579698\n",
      "Iteration 672, loss = 0.20558017\n",
      "Iteration 673, loss = 0.20536376\n",
      "Iteration 674, loss = 0.20514779\n",
      "Iteration 675, loss = 0.20493235\n",
      "Iteration 676, loss = 0.20471733\n",
      "Iteration 677, loss = 0.20450275\n",
      "Iteration 678, loss = 0.20428863\n",
      "Iteration 679, loss = 0.20407502\n",
      "Iteration 680, loss = 0.20386181\n",
      "Iteration 681, loss = 0.20364902\n",
      "Iteration 682, loss = 0.20343685\n",
      "Iteration 683, loss = 0.20322488\n",
      "Iteration 684, loss = 0.20301355\n",
      "Iteration 685, loss = 0.20280261\n",
      "Iteration 686, loss = 0.20259209\n",
      "Iteration 687, loss = 0.20238200\n",
      "Iteration 688, loss = 0.20217251\n",
      "Iteration 689, loss = 0.20196324\n",
      "Iteration 690, loss = 0.20175458\n",
      "Iteration 691, loss = 0.20154630\n",
      "Iteration 692, loss = 0.20133848\n",
      "Iteration 693, loss = 0.20113104\n",
      "Iteration 694, loss = 0.20092413\n",
      "Iteration 695, loss = 0.20071759\n",
      "Iteration 696, loss = 0.20051147\n",
      "Iteration 697, loss = 0.20030593\n",
      "Iteration 698, loss = 0.20010064\n",
      "Iteration 699, loss = 0.19989585\n",
      "Iteration 700, loss = 0.19969160\n",
      "Iteration 701, loss = 0.19948767\n",
      "Iteration 702, loss = 0.19928417\n",
      "Iteration 703, loss = 0.19908108\n",
      "Iteration 704, loss = 0.19887850\n",
      "Iteration 705, loss = 0.19867630\n",
      "Iteration 706, loss = 0.19847452\n",
      "Iteration 707, loss = 0.19827321\n",
      "Iteration 708, loss = 0.19807227\n",
      "Iteration 709, loss = 0.19787177\n",
      "Iteration 710, loss = 0.19767174\n",
      "Iteration 711, loss = 0.19747206\n",
      "Iteration 712, loss = 0.19727284\n",
      "Iteration 713, loss = 0.19707407\n",
      "Iteration 714, loss = 0.19687565\n",
      "Iteration 715, loss = 0.19667766\n",
      "Iteration 716, loss = 0.19648017\n",
      "Iteration 717, loss = 0.19628302\n",
      "Iteration 718, loss = 0.19608625\n",
      "Iteration 719, loss = 0.19589004\n",
      "Iteration 720, loss = 0.19569406\n",
      "Iteration 721, loss = 0.19549864\n",
      "Iteration 722, loss = 0.19530353\n",
      "Iteration 723, loss = 0.19510889\n",
      "Iteration 724, loss = 0.19491460\n",
      "Iteration 725, loss = 0.19472083\n",
      "Iteration 726, loss = 0.19452738\n",
      "Iteration 727, loss = 0.19433430\n",
      "Iteration 728, loss = 0.19414175\n",
      "Iteration 729, loss = 0.19394946\n",
      "Iteration 730, loss = 0.19375767\n",
      "Iteration 731, loss = 0.19356629\n",
      "Iteration 732, loss = 0.19337526\n",
      "Iteration 733, loss = 0.19318464\n",
      "Iteration 734, loss = 0.19299446\n",
      "Iteration 735, loss = 0.19280461\n",
      "Iteration 736, loss = 0.19261519\n",
      "Iteration 737, loss = 0.19242624\n",
      "Iteration 738, loss = 0.19223763\n",
      "Iteration 739, loss = 0.19204938\n",
      "Iteration 740, loss = 0.19186155\n",
      "Iteration 741, loss = 0.19167413\n",
      "Iteration 742, loss = 0.19148706\n",
      "Iteration 743, loss = 0.19130046\n",
      "Iteration 744, loss = 0.19111419\n",
      "Iteration 745, loss = 0.19092829\n",
      "Iteration 746, loss = 0.19074289\n",
      "Iteration 747, loss = 0.19055776\n",
      "Iteration 748, loss = 0.19037303\n",
      "Iteration 749, loss = 0.19018880\n",
      "Iteration 750, loss = 0.19000482\n",
      "Iteration 751, loss = 0.18982126\n",
      "Iteration 752, loss = 0.18963821\n",
      "Iteration 753, loss = 0.18945541\n",
      "Iteration 754, loss = 0.18927300\n",
      "Iteration 755, loss = 0.18909108\n",
      "Iteration 756, loss = 0.18890972\n",
      "Iteration 757, loss = 0.18872889\n",
      "Iteration 758, loss = 0.18854852\n",
      "Iteration 759, loss = 0.18836851\n",
      "Iteration 760, loss = 0.18818887\n",
      "Iteration 761, loss = 0.18800962\n",
      "Iteration 762, loss = 0.18783080\n",
      "Iteration 763, loss = 0.18765236\n",
      "Iteration 764, loss = 0.18747424\n",
      "Iteration 765, loss = 0.18729663\n",
      "Iteration 766, loss = 0.18711927\n",
      "Iteration 767, loss = 0.18694233\n",
      "Iteration 768, loss = 0.18676581\n",
      "Iteration 769, loss = 0.18658964\n",
      "Iteration 770, loss = 0.18641380\n",
      "Iteration 771, loss = 0.18623834\n",
      "Iteration 772, loss = 0.18606332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 773, loss = 0.18588859\n",
      "Iteration 774, loss = 0.18571422\n",
      "Iteration 775, loss = 0.18554034\n",
      "Iteration 776, loss = 0.18536672\n",
      "Iteration 777, loss = 0.18519344\n",
      "Iteration 778, loss = 0.18502064\n",
      "Iteration 779, loss = 0.18484807\n",
      "Iteration 780, loss = 0.18467590\n",
      "Iteration 781, loss = 0.18450419\n",
      "Iteration 782, loss = 0.18433274\n",
      "Iteration 783, loss = 0.18416163\n",
      "Iteration 784, loss = 0.18399092\n",
      "Iteration 785, loss = 0.18382058\n",
      "Iteration 786, loss = 0.18365054\n",
      "Iteration 787, loss = 0.18348087\n",
      "Iteration 788, loss = 0.18331160\n",
      "Iteration 789, loss = 0.18314262\n",
      "Iteration 790, loss = 0.18297399\n",
      "Iteration 791, loss = 0.18280583\n",
      "Iteration 792, loss = 0.18263781\n",
      "Iteration 793, loss = 0.18247030\n",
      "Iteration 794, loss = 0.18230307\n",
      "Iteration 795, loss = 0.18213619\n",
      "Iteration 796, loss = 0.18196964\n",
      "Iteration 797, loss = 0.18180350\n",
      "Iteration 798, loss = 0.18163762\n",
      "Iteration 799, loss = 0.18147209\n",
      "Iteration 800, loss = 0.18130704\n",
      "Iteration 801, loss = 0.18114221\n",
      "Iteration 802, loss = 0.18097772\n",
      "Iteration 803, loss = 0.18081361\n",
      "Iteration 804, loss = 0.18064979\n",
      "Iteration 805, loss = 0.18048636\n",
      "Iteration 806, loss = 0.18032321\n",
      "Iteration 807, loss = 0.18016052\n",
      "Iteration 808, loss = 0.17999800\n",
      "Iteration 809, loss = 0.17983592\n",
      "Iteration 810, loss = 0.17967417\n",
      "Iteration 811, loss = 0.17951271\n",
      "Iteration 812, loss = 0.17935161\n",
      "Iteration 813, loss = 0.17919080\n",
      "Iteration 814, loss = 0.17903039\n",
      "Iteration 815, loss = 0.17887027\n",
      "Iteration 816, loss = 0.17871047\n",
      "Iteration 817, loss = 0.17855105\n",
      "Iteration 818, loss = 0.17839190\n",
      "Iteration 819, loss = 0.17823309\n",
      "Iteration 820, loss = 0.17807469\n",
      "Iteration 821, loss = 0.17791646\n",
      "Iteration 822, loss = 0.17775865\n",
      "Iteration 823, loss = 0.17760117\n",
      "Iteration 824, loss = 0.17744397\n",
      "Iteration 825, loss = 0.17728710\n",
      "Iteration 826, loss = 0.17713056\n",
      "Iteration 827, loss = 0.17697435\n",
      "Iteration 828, loss = 0.17681842\n",
      "Iteration 829, loss = 0.17666284\n",
      "Iteration 830, loss = 0.17650758\n",
      "Iteration 831, loss = 0.17635262\n",
      "Iteration 832, loss = 0.17619796\n",
      "Iteration 833, loss = 0.17604372\n",
      "Iteration 834, loss = 0.17588962\n",
      "Iteration 835, loss = 0.17573597\n",
      "Iteration 836, loss = 0.17558259\n",
      "Iteration 837, loss = 0.17542951\n",
      "Iteration 838, loss = 0.17527675\n",
      "Iteration 839, loss = 0.17512431\n",
      "Iteration 840, loss = 0.17497219\n",
      "Iteration 841, loss = 0.17482035\n",
      "Iteration 842, loss = 0.17466885\n",
      "Iteration 843, loss = 0.17451768\n",
      "Iteration 844, loss = 0.17436676\n",
      "Iteration 845, loss = 0.17421615\n",
      "Iteration 846, loss = 0.17406597\n",
      "Iteration 847, loss = 0.17391592\n",
      "Iteration 848, loss = 0.17376637\n",
      "Iteration 849, loss = 0.17361707\n",
      "Iteration 850, loss = 0.17346815\n",
      "Iteration 851, loss = 0.17331943\n",
      "Iteration 852, loss = 0.17317117\n",
      "Iteration 853, loss = 0.17302309\n",
      "Iteration 854, loss = 0.17287530\n",
      "Iteration 855, loss = 0.17272795\n",
      "Iteration 856, loss = 0.17258077\n",
      "Iteration 857, loss = 0.17243391\n",
      "Iteration 858, loss = 0.17228733\n",
      "Iteration 859, loss = 0.17214118\n",
      "Iteration 860, loss = 0.17199515\n",
      "Iteration 861, loss = 0.17184958\n",
      "Iteration 862, loss = 0.17170421\n",
      "Iteration 863, loss = 0.17155910\n",
      "Iteration 864, loss = 0.17141439\n",
      "Iteration 865, loss = 0.17126985\n",
      "Iteration 866, loss = 0.17112571\n",
      "Iteration 867, loss = 0.17098178\n",
      "Iteration 868, loss = 0.17083819\n",
      "Iteration 869, loss = 0.17069487\n",
      "Iteration 870, loss = 0.17055186\n",
      "Iteration 871, loss = 0.17040912\n",
      "Iteration 872, loss = 0.17026664\n",
      "Iteration 873, loss = 0.17012455\n",
      "Iteration 874, loss = 0.16998263\n",
      "Iteration 875, loss = 0.16984106\n",
      "Iteration 876, loss = 0.16969972\n",
      "Iteration 877, loss = 0.16955877\n",
      "Iteration 878, loss = 0.16941797\n",
      "Iteration 879, loss = 0.16927756\n",
      "Iteration 880, loss = 0.16913738\n",
      "Iteration 881, loss = 0.16899747\n",
      "Iteration 882, loss = 0.16885791\n",
      "Iteration 883, loss = 0.16871853\n",
      "Iteration 884, loss = 0.16857952\n",
      "Iteration 885, loss = 0.16844070\n",
      "Iteration 886, loss = 0.16830225\n",
      "Iteration 887, loss = 0.16816398\n",
      "Iteration 888, loss = 0.16802608\n",
      "Iteration 889, loss = 0.16788838\n",
      "Iteration 890, loss = 0.16775096\n",
      "Iteration 891, loss = 0.16761391\n",
      "Iteration 892, loss = 0.16747702\n",
      "Iteration 893, loss = 0.16734044\n",
      "Iteration 894, loss = 0.16720411\n",
      "Iteration 895, loss = 0.16706812\n",
      "Iteration 896, loss = 0.16693232\n",
      "Iteration 897, loss = 0.16679684\n",
      "Iteration 898, loss = 0.16666165\n",
      "Iteration 899, loss = 0.16652667\n",
      "Iteration 900, loss = 0.16639207\n",
      "Iteration 901, loss = 0.16625761\n",
      "Iteration 902, loss = 0.16612351\n",
      "Iteration 903, loss = 0.16598960\n",
      "Iteration 904, loss = 0.16585598\n",
      "Iteration 905, loss = 0.16572266\n",
      "Iteration 906, loss = 0.16558957\n",
      "Iteration 907, loss = 0.16545681\n",
      "Iteration 908, loss = 0.16532421\n",
      "Iteration 909, loss = 0.16519201\n",
      "Iteration 910, loss = 0.16505995\n",
      "Iteration 911, loss = 0.16492823\n",
      "Iteration 912, loss = 0.16479670\n",
      "Iteration 913, loss = 0.16466550\n",
      "Iteration 914, loss = 0.16453451\n",
      "Iteration 915, loss = 0.16440380\n",
      "Iteration 916, loss = 0.16427335\n",
      "Iteration 917, loss = 0.16414314\n",
      "Iteration 918, loss = 0.16401325\n",
      "Iteration 919, loss = 0.16388353\n",
      "Iteration 920, loss = 0.16375412\n",
      "Iteration 921, loss = 0.16362491\n",
      "Iteration 922, loss = 0.16349604\n",
      "Iteration 923, loss = 0.16336731\n",
      "Iteration 924, loss = 0.16323895\n",
      "Iteration 925, loss = 0.16311076\n",
      "Iteration 926, loss = 0.16298287\n",
      "Iteration 927, loss = 0.16285520\n",
      "Iteration 928, loss = 0.16272774\n",
      "Iteration 929, loss = 0.16260067\n",
      "Iteration 930, loss = 0.16247370\n",
      "Iteration 931, loss = 0.16234705\n",
      "Iteration 932, loss = 0.16222061\n",
      "Iteration 933, loss = 0.16209448\n",
      "Iteration 934, loss = 0.16196857\n",
      "Iteration 935, loss = 0.16184304\n",
      "Iteration 936, loss = 0.16171776\n",
      "Iteration 937, loss = 0.16159273\n",
      "Iteration 938, loss = 0.16146801\n",
      "Iteration 939, loss = 0.16134347\n",
      "Iteration 940, loss = 0.16121955\n",
      "Iteration 941, loss = 0.16109577\n",
      "Iteration 942, loss = 0.16097237\n",
      "Iteration 943, loss = 0.16084915\n",
      "Iteration 944, loss = 0.16072616\n",
      "Iteration 945, loss = 0.16060348\n",
      "Iteration 946, loss = 0.16048104\n",
      "Iteration 947, loss = 0.16035881\n",
      "Iteration 948, loss = 0.16023686\n",
      "Iteration 949, loss = 0.16011517\n",
      "Iteration 950, loss = 0.15999368\n",
      "Iteration 951, loss = 0.15987247\n",
      "Iteration 952, loss = 0.15975145\n",
      "Iteration 953, loss = 0.15963073\n",
      "Iteration 954, loss = 0.15951020\n",
      "Iteration 955, loss = 0.15938995\n",
      "Iteration 956, loss = 0.15926988\n",
      "Iteration 957, loss = 0.15915015\n",
      "Iteration 958, loss = 0.15903054\n",
      "Iteration 959, loss = 0.15891118\n",
      "Iteration 960, loss = 0.15879206\n",
      "Iteration 961, loss = 0.15867324\n",
      "Iteration 962, loss = 0.15855456\n",
      "Iteration 963, loss = 0.15843617\n",
      "Iteration 964, loss = 0.15831794\n",
      "Iteration 965, loss = 0.15820006\n",
      "Iteration 966, loss = 0.15808230\n",
      "Iteration 967, loss = 0.15796480\n",
      "Iteration 968, loss = 0.15784752\n",
      "Iteration 969, loss = 0.15773051\n",
      "Iteration 970, loss = 0.15761367\n",
      "Iteration 971, loss = 0.15749713\n",
      "Iteration 972, loss = 0.15738071\n",
      "Iteration 973, loss = 0.15726464\n",
      "Iteration 974, loss = 0.15714867\n",
      "Iteration 975, loss = 0.15703303\n",
      "Iteration 976, loss = 0.15691753\n",
      "Iteration 977, loss = 0.15680229\n",
      "Iteration 978, loss = 0.15668727\n",
      "Iteration 979, loss = 0.15657244\n",
      "Iteration 980, loss = 0.15645788\n",
      "Iteration 981, loss = 0.15634348\n",
      "Iteration 982, loss = 0.15622940\n",
      "Iteration 983, loss = 0.15611543\n",
      "Iteration 984, loss = 0.15600178\n",
      "Iteration 985, loss = 0.15588826\n",
      "Iteration 986, loss = 0.15577500\n",
      "Iteration 987, loss = 0.15566190\n",
      "Iteration 988, loss = 0.15554914\n",
      "Iteration 989, loss = 0.15543648\n",
      "Iteration 990, loss = 0.15532408\n",
      "Iteration 991, loss = 0.15521186\n",
      "Iteration 992, loss = 0.15509994\n",
      "Iteration 993, loss = 0.15498812\n",
      "Iteration 994, loss = 0.15487662\n",
      "Iteration 995, loss = 0.15476523\n",
      "Iteration 996, loss = 0.15465412\n",
      "Iteration 997, loss = 0.15454316\n",
      "Iteration 998, loss = 0.15443251\n",
      "Iteration 999, loss = 0.15432195\n",
      "Iteration 1000, loss = 0.15421173\n",
      "Iteration 1, loss = 1.34975994\n",
      "Iteration 2, loss = 1.32617022\n",
      "Iteration 3, loss = 1.29444823\n",
      "Iteration 4, loss = 1.25692045\n",
      "Iteration 5, loss = 1.21557162\n",
      "Iteration 6, loss = 1.17216352\n",
      "Iteration 7, loss = 1.12785756\n",
      "Iteration 8, loss = 1.08378052\n",
      "Iteration 9, loss = 1.04099714\n",
      "Iteration 10, loss = 1.00059309\n",
      "Iteration 11, loss = 0.96384740\n",
      "Iteration 12, loss = 0.93167760\n",
      "Iteration 13, loss = 0.90456750\n",
      "Iteration 14, loss = 0.88294024\n",
      "Iteration 15, loss = 0.86650962\n",
      "Iteration 16, loss = 0.85461995\n",
      "Iteration 17, loss = 0.84619075\n",
      "Iteration 18, loss = 0.84018062\n",
      "Iteration 19, loss = 0.83506939\n",
      "Iteration 20, loss = 0.82955357\n",
      "Iteration 21, loss = 0.82293840\n",
      "Iteration 22, loss = 0.81475258\n",
      "Iteration 23, loss = 0.80504016\n",
      "Iteration 24, loss = 0.79417991\n",
      "Iteration 25, loss = 0.78303622\n",
      "Iteration 26, loss = 0.77191343\n",
      "Iteration 27, loss = 0.76081623\n",
      "Iteration 28, loss = 0.74986613\n",
      "Iteration 29, loss = 0.73916963\n",
      "Iteration 30, loss = 0.72906942\n",
      "Iteration 31, loss = 0.71971934\n",
      "Iteration 32, loss = 0.71128565\n",
      "Iteration 33, loss = 0.70376162\n",
      "Iteration 34, loss = 0.69701422\n",
      "Iteration 35, loss = 0.69098953\n",
      "Iteration 36, loss = 0.68537301\n",
      "Iteration 37, loss = 0.68007128\n",
      "Iteration 38, loss = 0.67501539\n",
      "Iteration 39, loss = 0.67012521\n",
      "Iteration 40, loss = 0.66535922\n",
      "Iteration 41, loss = 0.66069840\n",
      "Iteration 42, loss = 0.65608235\n",
      "Iteration 43, loss = 0.65151083\n",
      "Iteration 44, loss = 0.64700411\n",
      "Iteration 45, loss = 0.64257711\n",
      "Iteration 46, loss = 0.63823051\n",
      "Iteration 47, loss = 0.63394611\n",
      "Iteration 48, loss = 0.62973184\n",
      "Iteration 49, loss = 0.62559703\n",
      "Iteration 50, loss = 0.62158940\n",
      "Iteration 51, loss = 0.61771054\n",
      "Iteration 52, loss = 0.61396856\n",
      "Iteration 53, loss = 0.61037740\n",
      "Iteration 54, loss = 0.60696316\n",
      "Iteration 55, loss = 0.60370493\n",
      "Iteration 56, loss = 0.60057334\n",
      "Iteration 57, loss = 0.59757731\n",
      "Iteration 58, loss = 0.59464705\n",
      "Iteration 59, loss = 0.59176149\n",
      "Iteration 60, loss = 0.58891423\n",
      "Iteration 61, loss = 0.58609621\n",
      "Iteration 62, loss = 0.58330361\n",
      "Iteration 63, loss = 0.58053381\n",
      "Iteration 64, loss = 0.57779045\n",
      "Iteration 65, loss = 0.57507068\n",
      "Iteration 66, loss = 0.57237960\n",
      "Iteration 67, loss = 0.56972607\n",
      "Iteration 68, loss = 0.56710891\n",
      "Iteration 69, loss = 0.56452771\n",
      "Iteration 70, loss = 0.56198875\n",
      "Iteration 71, loss = 0.55948809\n",
      "Iteration 72, loss = 0.55705717\n",
      "Iteration 73, loss = 0.55467899\n",
      "Iteration 74, loss = 0.55234484\n",
      "Iteration 75, loss = 0.55004536\n",
      "Iteration 76, loss = 0.54778379\n",
      "Iteration 77, loss = 0.54556426\n",
      "Iteration 78, loss = 0.54338085\n",
      "Iteration 79, loss = 0.54122814\n",
      "Iteration 80, loss = 0.53910669\n",
      "Iteration 81, loss = 0.53701367\n",
      "Iteration 82, loss = 0.53494934\n",
      "Iteration 83, loss = 0.53291129\n",
      "Iteration 84, loss = 0.53089937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 85, loss = 0.52891338\n",
      "Iteration 86, loss = 0.52695277\n",
      "Iteration 87, loss = 0.52501757\n",
      "Iteration 88, loss = 0.52310756\n",
      "Iteration 89, loss = 0.52122188\n",
      "Iteration 90, loss = 0.51936020\n",
      "Iteration 91, loss = 0.51752233\n",
      "Iteration 92, loss = 0.51570751\n",
      "Iteration 93, loss = 0.51391564\n",
      "Iteration 94, loss = 0.51214796\n",
      "Iteration 95, loss = 0.51040296\n",
      "Iteration 96, loss = 0.50868317\n",
      "Iteration 97, loss = 0.50698635\n",
      "Iteration 98, loss = 0.50531018\n",
      "Iteration 99, loss = 0.50365540\n",
      "Iteration 100, loss = 0.50201890\n",
      "Iteration 101, loss = 0.50040164\n",
      "Iteration 102, loss = 0.49880408\n",
      "Iteration 103, loss = 0.49722548\n",
      "Iteration 104, loss = 0.49566429\n",
      "Iteration 105, loss = 0.49412111\n",
      "Iteration 106, loss = 0.49259530\n",
      "Iteration 107, loss = 0.49108668\n",
      "Iteration 108, loss = 0.48959517\n",
      "Iteration 109, loss = 0.48812044\n",
      "Iteration 110, loss = 0.48666181\n",
      "Iteration 111, loss = 0.48521898\n",
      "Iteration 112, loss = 0.48379182\n",
      "Iteration 113, loss = 0.48237962\n",
      "Iteration 114, loss = 0.48098268\n",
      "Iteration 115, loss = 0.47960056\n",
      "Iteration 116, loss = 0.47823455\n",
      "Iteration 117, loss = 0.47688187\n",
      "Iteration 118, loss = 0.47554245\n",
      "Iteration 119, loss = 0.47421631\n",
      "Iteration 120, loss = 0.47290355\n",
      "Iteration 121, loss = 0.47160370\n",
      "Iteration 122, loss = 0.47031648\n",
      "Iteration 123, loss = 0.46904127\n",
      "Iteration 124, loss = 0.46777806\n",
      "Iteration 125, loss = 0.46652657\n",
      "Iteration 126, loss = 0.46528649\n",
      "Iteration 127, loss = 0.46405806\n",
      "Iteration 128, loss = 0.46284058\n",
      "Iteration 129, loss = 0.46163391\n",
      "Iteration 130, loss = 0.46043782\n",
      "Iteration 131, loss = 0.45925080\n",
      "Iteration 132, loss = 0.45807460\n",
      "Iteration 133, loss = 0.45690817\n",
      "Iteration 134, loss = 0.45575142\n",
      "Iteration 135, loss = 0.45460296\n",
      "Iteration 136, loss = 0.45346387\n",
      "Iteration 137, loss = 0.45233319\n",
      "Iteration 138, loss = 0.45121166\n",
      "Iteration 139, loss = 0.45009854\n",
      "Iteration 140, loss = 0.44899276\n",
      "Iteration 141, loss = 0.44789379\n",
      "Iteration 142, loss = 0.44680302\n",
      "Iteration 143, loss = 0.44572032\n",
      "Iteration 144, loss = 0.44464468\n",
      "Iteration 145, loss = 0.44357501\n",
      "Iteration 146, loss = 0.44251316\n",
      "Iteration 147, loss = 0.44145733\n",
      "Iteration 148, loss = 0.44040724\n",
      "Iteration 149, loss = 0.43936101\n",
      "Iteration 150, loss = 0.43832156\n",
      "Iteration 151, loss = 0.43728595\n",
      "Iteration 152, loss = 0.43625636\n",
      "Iteration 153, loss = 0.43522776\n",
      "Iteration 154, loss = 0.43420190\n",
      "Iteration 155, loss = 0.43317939\n",
      "Iteration 156, loss = 0.43215781\n",
      "Iteration 157, loss = 0.43113927\n",
      "Iteration 158, loss = 0.43012385\n",
      "Iteration 159, loss = 0.42910901\n",
      "Iteration 160, loss = 0.42809300\n",
      "Iteration 161, loss = 0.42707842\n",
      "Iteration 162, loss = 0.42606359\n",
      "Iteration 163, loss = 0.42505190\n",
      "Iteration 164, loss = 0.42404065\n",
      "Iteration 165, loss = 0.42303321\n",
      "Iteration 166, loss = 0.42202476\n",
      "Iteration 167, loss = 0.42100648\n",
      "Iteration 168, loss = 0.41998453\n",
      "Iteration 169, loss = 0.41895525\n",
      "Iteration 170, loss = 0.41792484\n",
      "Iteration 171, loss = 0.41688542\n",
      "Iteration 172, loss = 0.41584495\n",
      "Iteration 173, loss = 0.41480095\n",
      "Iteration 174, loss = 0.41375098\n",
      "Iteration 175, loss = 0.41270509\n",
      "Iteration 176, loss = 0.41166258\n",
      "Iteration 177, loss = 0.41063203\n",
      "Iteration 178, loss = 0.40961035\n",
      "Iteration 179, loss = 0.40859363\n",
      "Iteration 180, loss = 0.40759452\n",
      "Iteration 181, loss = 0.40660895\n",
      "Iteration 182, loss = 0.40563892\n",
      "Iteration 183, loss = 0.40468641\n",
      "Iteration 184, loss = 0.40375227\n",
      "Iteration 185, loss = 0.40284825\n",
      "Iteration 186, loss = 0.40196363\n",
      "Iteration 187, loss = 0.40109507\n",
      "Iteration 188, loss = 0.40024054\n",
      "Iteration 189, loss = 0.39939776\n",
      "Iteration 190, loss = 0.39856913\n",
      "Iteration 191, loss = 0.39775639\n",
      "Iteration 192, loss = 0.39695267\n",
      "Iteration 193, loss = 0.39615502\n",
      "Iteration 194, loss = 0.39536669\n",
      "Iteration 195, loss = 0.39458915\n",
      "Iteration 196, loss = 0.39382080\n",
      "Iteration 197, loss = 0.39305975\n",
      "Iteration 198, loss = 0.39230628\n",
      "Iteration 199, loss = 0.39155750\n",
      "Iteration 200, loss = 0.39081282\n",
      "Iteration 201, loss = 0.39007245\n",
      "Iteration 202, loss = 0.38933638\n",
      "Iteration 203, loss = 0.38860417\n",
      "Iteration 204, loss = 0.38787583\n",
      "Iteration 205, loss = 0.38715145\n",
      "Iteration 206, loss = 0.38643179\n",
      "Iteration 207, loss = 0.38571678\n",
      "Iteration 208, loss = 0.38500592\n",
      "Iteration 209, loss = 0.38429817\n",
      "Iteration 210, loss = 0.38359381\n",
      "Iteration 211, loss = 0.38289299\n",
      "Iteration 212, loss = 0.38219566\n",
      "Iteration 213, loss = 0.38150208\n",
      "Iteration 214, loss = 0.38081178\n",
      "Iteration 215, loss = 0.38012449\n",
      "Iteration 216, loss = 0.37944013\n",
      "Iteration 217, loss = 0.37875887\n",
      "Iteration 218, loss = 0.37808060\n",
      "Iteration 219, loss = 0.37740542\n",
      "Iteration 220, loss = 0.37673308\n",
      "Iteration 221, loss = 0.37606366\n",
      "Iteration 222, loss = 0.37539705\n",
      "Iteration 223, loss = 0.37473340\n",
      "Iteration 224, loss = 0.37407258\n",
      "Iteration 225, loss = 0.37341437\n",
      "Iteration 226, loss = 0.37275903\n",
      "Iteration 227, loss = 0.37210645\n",
      "Iteration 228, loss = 0.37145647\n",
      "Iteration 229, loss = 0.37080920\n",
      "Iteration 230, loss = 0.37016459\n",
      "Iteration 231, loss = 0.36952243\n",
      "Iteration 232, loss = 0.36888294\n",
      "Iteration 233, loss = 0.36824613\n",
      "Iteration 234, loss = 0.36761180\n",
      "Iteration 235, loss = 0.36697991\n",
      "Iteration 236, loss = 0.36635061\n",
      "Iteration 237, loss = 0.36572346\n",
      "Iteration 238, loss = 0.36509881\n",
      "Iteration 239, loss = 0.36447646\n",
      "Iteration 240, loss = 0.36385651\n",
      "Iteration 241, loss = 0.36323883\n",
      "Iteration 242, loss = 0.36262344\n",
      "Iteration 243, loss = 0.36201041\n",
      "Iteration 244, loss = 0.36139959\n",
      "Iteration 245, loss = 0.36079111\n",
      "Iteration 246, loss = 0.36018476\n",
      "Iteration 247, loss = 0.35958064\n",
      "Iteration 248, loss = 0.35897867\n",
      "Iteration 249, loss = 0.35837883\n",
      "Iteration 250, loss = 0.35778114\n",
      "Iteration 251, loss = 0.35718555\n",
      "Iteration 252, loss = 0.35659249\n",
      "Iteration 253, loss = 0.35600120\n",
      "Iteration 254, loss = 0.35541199\n",
      "Iteration 255, loss = 0.35482479\n",
      "Iteration 256, loss = 0.35423964\n",
      "Iteration 257, loss = 0.35365646\n",
      "Iteration 258, loss = 0.35307524\n",
      "Iteration 259, loss = 0.35249616\n",
      "Iteration 260, loss = 0.35191895\n",
      "Iteration 261, loss = 0.35134374\n",
      "Iteration 262, loss = 0.35077040\n",
      "Iteration 263, loss = 0.35019896\n",
      "Iteration 264, loss = 0.34962946\n",
      "Iteration 265, loss = 0.34906178\n",
      "Iteration 266, loss = 0.34849600\n",
      "Iteration 267, loss = 0.34793205\n",
      "Iteration 268, loss = 0.34737000\n",
      "Iteration 269, loss = 0.34680966\n",
      "Iteration 270, loss = 0.34625119\n",
      "Iteration 271, loss = 0.34569445\n",
      "Iteration 272, loss = 0.34513957\n",
      "Iteration 273, loss = 0.34458640\n",
      "Iteration 274, loss = 0.34403486\n",
      "Iteration 275, loss = 0.34348512\n",
      "Iteration 276, loss = 0.34293704\n",
      "Iteration 277, loss = 0.34239070\n",
      "Iteration 278, loss = 0.34184618\n",
      "Iteration 279, loss = 0.34130325\n",
      "Iteration 280, loss = 0.34076196\n",
      "Iteration 281, loss = 0.34022238\n",
      "Iteration 282, loss = 0.33968442\n",
      "Iteration 283, loss = 0.33914810\n",
      "Iteration 284, loss = 0.33861335\n",
      "Iteration 285, loss = 0.33808028\n",
      "Iteration 286, loss = 0.33754875\n",
      "Iteration 287, loss = 0.33701874\n",
      "Iteration 288, loss = 0.33649031\n",
      "Iteration 289, loss = 0.33596338\n",
      "Iteration 290, loss = 0.33543803\n",
      "Iteration 291, loss = 0.33491416\n",
      "Iteration 292, loss = 0.33439187\n",
      "Iteration 293, loss = 0.33387108\n",
      "Iteration 294, loss = 0.33335180\n",
      "Iteration 295, loss = 0.33283411\n",
      "Iteration 296, loss = 0.33231784\n",
      "Iteration 297, loss = 0.33180305\n",
      "Iteration 298, loss = 0.33128973\n",
      "Iteration 299, loss = 0.33077790\n",
      "Iteration 300, loss = 0.33026748\n",
      "Iteration 301, loss = 0.32975853\n",
      "Iteration 302, loss = 0.32925099\n",
      "Iteration 303, loss = 0.32874485\n",
      "Iteration 304, loss = 0.32824018\n",
      "Iteration 305, loss = 0.32773689\n",
      "Iteration 306, loss = 0.32723503\n",
      "Iteration 307, loss = 0.32673460\n",
      "Iteration 308, loss = 0.32623552\n",
      "Iteration 309, loss = 0.32573781\n",
      "Iteration 310, loss = 0.32524152\n",
      "Iteration 311, loss = 0.32474659\n",
      "Iteration 312, loss = 0.32425302\n",
      "Iteration 313, loss = 0.32376077\n",
      "Iteration 314, loss = 0.32326986\n",
      "Iteration 315, loss = 0.32278029\n",
      "Iteration 316, loss = 0.32229207\n",
      "Iteration 317, loss = 0.32180516\n",
      "Iteration 318, loss = 0.32131953\n",
      "Iteration 319, loss = 0.32083519\n",
      "Iteration 320, loss = 0.32035222\n",
      "Iteration 321, loss = 0.31987042\n",
      "Iteration 322, loss = 0.31938995\n",
      "Iteration 323, loss = 0.31891078\n",
      "Iteration 324, loss = 0.31843298\n",
      "Iteration 325, loss = 0.31795691\n",
      "Iteration 326, loss = 0.31748206\n",
      "Iteration 327, loss = 0.31700852\n",
      "Iteration 328, loss = 0.31653623\n",
      "Iteration 329, loss = 0.31606524\n",
      "Iteration 330, loss = 0.31559547\n",
      "Iteration 331, loss = 0.31512691\n",
      "Iteration 332, loss = 0.31465964\n",
      "Iteration 333, loss = 0.31419352\n",
      "Iteration 334, loss = 0.31372872\n",
      "Iteration 335, loss = 0.31326505\n",
      "Iteration 336, loss = 0.31280262\n",
      "Iteration 337, loss = 0.31234143\n",
      "Iteration 338, loss = 0.31188137\n",
      "Iteration 339, loss = 0.31142251\n",
      "Iteration 340, loss = 0.31096482\n",
      "Iteration 341, loss = 0.31050831\n",
      "Iteration 342, loss = 0.31005294\n",
      "Iteration 343, loss = 0.30959869\n",
      "Iteration 344, loss = 0.30914561\n",
      "Iteration 345, loss = 0.30869369\n",
      "Iteration 346, loss = 0.30824285\n",
      "Iteration 347, loss = 0.30779318\n",
      "Iteration 348, loss = 0.30734461\n",
      "Iteration 349, loss = 0.30689718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 350, loss = 0.30645086\n",
      "Iteration 351, loss = 0.30600561\n",
      "Iteration 352, loss = 0.30556147\n",
      "Iteration 353, loss = 0.30511844\n",
      "Iteration 354, loss = 0.30467647\n",
      "Iteration 355, loss = 0.30423562\n",
      "Iteration 356, loss = 0.30379580\n",
      "Iteration 357, loss = 0.30335713\n",
      "Iteration 358, loss = 0.30291945\n",
      "Iteration 359, loss = 0.30248287\n",
      "Iteration 360, loss = 0.30204732\n",
      "Iteration 361, loss = 0.30161290\n",
      "Iteration 362, loss = 0.30117945\n",
      "Iteration 363, loss = 0.30074708\n",
      "Iteration 364, loss = 0.30031574\n",
      "Iteration 365, loss = 0.29988554\n",
      "Iteration 366, loss = 0.29945631\n",
      "Iteration 367, loss = 0.29902816\n",
      "Iteration 368, loss = 0.29860099\n",
      "Iteration 369, loss = 0.29817493\n",
      "Iteration 370, loss = 0.29774982\n",
      "Iteration 371, loss = 0.29732577\n",
      "Iteration 372, loss = 0.29690268\n",
      "Iteration 373, loss = 0.29648069\n",
      "Iteration 374, loss = 0.29605963\n",
      "Iteration 375, loss = 0.29563967\n",
      "Iteration 376, loss = 0.29522068\n",
      "Iteration 377, loss = 0.29480277\n",
      "Iteration 378, loss = 0.29438578\n",
      "Iteration 379, loss = 0.29396979\n",
      "Iteration 380, loss = 0.29355476\n",
      "Iteration 381, loss = 0.29314073\n",
      "Iteration 382, loss = 0.29272765\n",
      "Iteration 383, loss = 0.29231559\n",
      "Iteration 384, loss = 0.29190442\n",
      "Iteration 385, loss = 0.29149430\n",
      "Iteration 386, loss = 0.29108506\n",
      "Iteration 387, loss = 0.29067677\n",
      "Iteration 388, loss = 0.29026947\n",
      "Iteration 389, loss = 0.28986316\n",
      "Iteration 390, loss = 0.28945801\n",
      "Iteration 391, loss = 0.28905381\n",
      "Iteration 392, loss = 0.28865053\n",
      "Iteration 393, loss = 0.28824826\n",
      "Iteration 394, loss = 0.28784686\n",
      "Iteration 395, loss = 0.28744638\n",
      "Iteration 396, loss = 0.28704686\n",
      "Iteration 397, loss = 0.28664825\n",
      "Iteration 398, loss = 0.28625056\n",
      "Iteration 399, loss = 0.28585380\n",
      "Iteration 400, loss = 0.28545793\n",
      "Iteration 401, loss = 0.28506300\n",
      "Iteration 402, loss = 0.28466892\n",
      "Iteration 403, loss = 0.28427579\n",
      "Iteration 404, loss = 0.28388358\n",
      "Iteration 405, loss = 0.28349219\n",
      "Iteration 406, loss = 0.28310175\n",
      "Iteration 407, loss = 0.28271219\n",
      "Iteration 408, loss = 0.28232353\n",
      "Iteration 409, loss = 0.28193572\n",
      "Iteration 410, loss = 0.28154882\n",
      "Iteration 411, loss = 0.28116279\n",
      "Iteration 412, loss = 0.28077762\n",
      "Iteration 413, loss = 0.28039335\n",
      "Iteration 414, loss = 0.28000993\n",
      "Iteration 415, loss = 0.27962739\n",
      "Iteration 416, loss = 0.27924571\n",
      "Iteration 417, loss = 0.27886487\n",
      "Iteration 418, loss = 0.27848495\n",
      "Iteration 419, loss = 0.27810582\n",
      "Iteration 420, loss = 0.27772759\n",
      "Iteration 421, loss = 0.27735022\n",
      "Iteration 422, loss = 0.27697374\n",
      "Iteration 423, loss = 0.27659806\n",
      "Iteration 424, loss = 0.27622324\n",
      "Iteration 425, loss = 0.27584928\n",
      "Iteration 426, loss = 0.27547613\n",
      "Iteration 427, loss = 0.27510382\n",
      "Iteration 428, loss = 0.27473237\n",
      "Iteration 429, loss = 0.27436172\n",
      "Iteration 430, loss = 0.27399193\n",
      "Iteration 431, loss = 0.27362297\n",
      "Iteration 432, loss = 0.27325483\n",
      "Iteration 433, loss = 0.27288749\n",
      "Iteration 434, loss = 0.27252103\n",
      "Iteration 435, loss = 0.27215535\n",
      "Iteration 436, loss = 0.27179047\n",
      "Iteration 437, loss = 0.27142640\n",
      "Iteration 438, loss = 0.27106319\n",
      "Iteration 439, loss = 0.27070073\n",
      "Iteration 440, loss = 0.27033910\n",
      "Iteration 441, loss = 0.26997826\n",
      "Iteration 442, loss = 0.26961824\n",
      "Iteration 443, loss = 0.26925902\n",
      "Iteration 444, loss = 0.26890063\n",
      "Iteration 445, loss = 0.26854300\n",
      "Iteration 446, loss = 0.26818619\n",
      "Iteration 447, loss = 0.26783016\n",
      "Iteration 448, loss = 0.26747490\n",
      "Iteration 449, loss = 0.26712045\n",
      "Iteration 450, loss = 0.26676678\n",
      "Iteration 451, loss = 0.26641389\n",
      "Iteration 452, loss = 0.26606176\n",
      "Iteration 453, loss = 0.26571042\n",
      "Iteration 454, loss = 0.26535986\n",
      "Iteration 455, loss = 0.26501006\n",
      "Iteration 456, loss = 0.26466102\n",
      "Iteration 457, loss = 0.26431289\n",
      "Iteration 458, loss = 0.26396542\n",
      "Iteration 459, loss = 0.26361871\n",
      "Iteration 460, loss = 0.26327277\n",
      "Iteration 461, loss = 0.26292759\n",
      "Iteration 462, loss = 0.26258319\n",
      "Iteration 463, loss = 0.26223957\n",
      "Iteration 464, loss = 0.26189667\n",
      "Iteration 465, loss = 0.26155457\n",
      "Iteration 466, loss = 0.26121317\n",
      "Iteration 467, loss = 0.26087255\n",
      "Iteration 468, loss = 0.26053266\n",
      "Iteration 469, loss = 0.26019352\n",
      "Iteration 470, loss = 0.25985517\n",
      "Iteration 471, loss = 0.25951748\n",
      "Iteration 472, loss = 0.25918060\n",
      "Iteration 473, loss = 0.25884440\n",
      "Iteration 474, loss = 0.25850899\n",
      "Iteration 475, loss = 0.25817427\n",
      "Iteration 476, loss = 0.25784027\n",
      "Iteration 477, loss = 0.25750703\n",
      "Iteration 478, loss = 0.25717452\n",
      "Iteration 479, loss = 0.25684273\n",
      "Iteration 480, loss = 0.25651165\n",
      "Iteration 481, loss = 0.25618129\n",
      "Iteration 482, loss = 0.25585168\n",
      "Iteration 483, loss = 0.25552273\n",
      "Iteration 484, loss = 0.25519455\n",
      "Iteration 485, loss = 0.25486705\n",
      "Iteration 486, loss = 0.25454029\n",
      "Iteration 487, loss = 0.25421419\n",
      "Iteration 488, loss = 0.25388886\n",
      "Iteration 489, loss = 0.25356415\n",
      "Iteration 490, loss = 0.25324023\n",
      "Iteration 491, loss = 0.25291698\n",
      "Iteration 492, loss = 0.25259440\n",
      "Iteration 493, loss = 0.25227256\n",
      "Iteration 494, loss = 0.25195139\n",
      "Iteration 495, loss = 0.25163099\n",
      "Iteration 496, loss = 0.25131129\n",
      "Iteration 497, loss = 0.25099225\n",
      "Iteration 498, loss = 0.25067401\n",
      "Iteration 499, loss = 0.25035638\n",
      "Iteration 500, loss = 0.25003940\n",
      "Iteration 501, loss = 0.24972323\n",
      "Iteration 502, loss = 0.24940763\n",
      "Iteration 503, loss = 0.24909283\n",
      "Iteration 504, loss = 0.24877860\n",
      "Iteration 505, loss = 0.24846518\n",
      "Iteration 506, loss = 0.24815233\n",
      "Iteration 507, loss = 0.24784012\n",
      "Iteration 508, loss = 0.24752869\n",
      "Iteration 509, loss = 0.24721786\n",
      "Iteration 510, loss = 0.24690774\n",
      "Iteration 511, loss = 0.24659831\n",
      "Iteration 512, loss = 0.24628944\n",
      "Iteration 513, loss = 0.24598138\n",
      "Iteration 514, loss = 0.24567383\n",
      "Iteration 515, loss = 0.24536709\n",
      "Iteration 516, loss = 0.24506088\n",
      "Iteration 517, loss = 0.24475547\n",
      "Iteration 518, loss = 0.24445061\n",
      "Iteration 519, loss = 0.24414654\n",
      "Iteration 520, loss = 0.24384302\n",
      "Iteration 521, loss = 0.24354026\n",
      "Iteration 522, loss = 0.24323804\n",
      "Iteration 523, loss = 0.24293657\n",
      "Iteration 524, loss = 0.24263570\n",
      "Iteration 525, loss = 0.24233546\n",
      "Iteration 526, loss = 0.24203594\n",
      "Iteration 527, loss = 0.24173694\n",
      "Iteration 528, loss = 0.24143874\n",
      "Iteration 529, loss = 0.24114105\n",
      "Iteration 530, loss = 0.24084409\n",
      "Iteration 531, loss = 0.24054766\n",
      "Iteration 532, loss = 0.24025197\n",
      "Iteration 533, loss = 0.23995682\n",
      "Iteration 534, loss = 0.23966239\n",
      "Iteration 535, loss = 0.23936849\n",
      "Iteration 536, loss = 0.23907534\n",
      "Iteration 537, loss = 0.23878270\n",
      "Iteration 538, loss = 0.23849083\n",
      "Iteration 539, loss = 0.23819945\n",
      "Iteration 540, loss = 0.23790868\n",
      "Iteration 541, loss = 0.23761868\n",
      "Iteration 542, loss = 0.23732917\n",
      "Iteration 543, loss = 0.23704026\n",
      "Iteration 544, loss = 0.23675204\n",
      "Iteration 545, loss = 0.23646442\n",
      "Iteration 546, loss = 0.23617739\n",
      "Iteration 547, loss = 0.23589104\n",
      "Iteration 548, loss = 0.23560518\n",
      "Iteration 549, loss = 0.23532008\n",
      "Iteration 550, loss = 0.23503548\n",
      "Iteration 551, loss = 0.23475148\n",
      "Iteration 552, loss = 0.23446815\n",
      "Iteration 553, loss = 0.23418534\n",
      "Iteration 554, loss = 0.23390327\n",
      "Iteration 555, loss = 0.23362164\n",
      "Iteration 556, loss = 0.23334075\n",
      "Iteration 557, loss = 0.23306035\n",
      "Iteration 558, loss = 0.23278063\n",
      "Iteration 559, loss = 0.23250143\n",
      "Iteration 560, loss = 0.23222286\n",
      "Iteration 561, loss = 0.23194486\n",
      "Iteration 562, loss = 0.23166748\n",
      "Iteration 563, loss = 0.23139069\n",
      "Iteration 564, loss = 0.23111444\n",
      "Iteration 565, loss = 0.23083887\n",
      "Iteration 566, loss = 0.23056380\n",
      "Iteration 567, loss = 0.23028937\n",
      "Iteration 568, loss = 0.23001544\n",
      "Iteration 569, loss = 0.22974218\n",
      "Iteration 570, loss = 0.22946945\n",
      "Iteration 571, loss = 0.22919732\n",
      "Iteration 572, loss = 0.22892578\n",
      "Iteration 573, loss = 0.22865475\n",
      "Iteration 574, loss = 0.22838441\n",
      "Iteration 575, loss = 0.22811451\n",
      "Iteration 576, loss = 0.22784551\n",
      "Iteration 577, loss = 0.22757672\n",
      "Iteration 578, loss = 0.22730870\n",
      "Iteration 579, loss = 0.22704124\n",
      "Iteration 580, loss = 0.22677436\n",
      "Iteration 581, loss = 0.22650807\n",
      "Iteration 582, loss = 0.22624232\n",
      "Iteration 583, loss = 0.22597715\n",
      "Iteration 584, loss = 0.22571254\n",
      "Iteration 585, loss = 0.22544849\n",
      "Iteration 586, loss = 0.22518501\n",
      "Iteration 587, loss = 0.22492208\n",
      "Iteration 588, loss = 0.22465972\n",
      "Iteration 589, loss = 0.22439790\n",
      "Iteration 590, loss = 0.22413664\n",
      "Iteration 591, loss = 0.22387593\n",
      "Iteration 592, loss = 0.22361579\n",
      "Iteration 593, loss = 0.22335618\n",
      "Iteration 594, loss = 0.22309712\n",
      "Iteration 595, loss = 0.22283863\n",
      "Iteration 596, loss = 0.22258066\n",
      "Iteration 597, loss = 0.22232324\n",
      "Iteration 598, loss = 0.22206637\n",
      "Iteration 599, loss = 0.22181004\n",
      "Iteration 600, loss = 0.22155425\n",
      "Iteration 601, loss = 0.22129899\n",
      "Iteration 602, loss = 0.22104428\n",
      "Iteration 603, loss = 0.22079011\n",
      "Iteration 604, loss = 0.22053647\n",
      "Iteration 605, loss = 0.22028337\n",
      "Iteration 606, loss = 0.22003080\n",
      "Iteration 607, loss = 0.21977877\n",
      "Iteration 608, loss = 0.21952727\n",
      "Iteration 609, loss = 0.21927629\n",
      "Iteration 610, loss = 0.21902584\n",
      "Iteration 611, loss = 0.21877592\n",
      "Iteration 612, loss = 0.21852651\n",
      "Iteration 613, loss = 0.21827764\n",
      "Iteration 614, loss = 0.21802929\n",
      "Iteration 615, loss = 0.21778146\n",
      "Iteration 616, loss = 0.21753416\n",
      "Iteration 617, loss = 0.21728749\n",
      "Iteration 618, loss = 0.21704133\n",
      "Iteration 619, loss = 0.21679569\n",
      "Iteration 620, loss = 0.21655055\n",
      "Iteration 621, loss = 0.21630593\n",
      "Iteration 622, loss = 0.21606182\n",
      "Iteration 623, loss = 0.21581822\n",
      "Iteration 624, loss = 0.21557514\n",
      "Iteration 625, loss = 0.21533256\n",
      "Iteration 626, loss = 0.21509049\n",
      "Iteration 627, loss = 0.21484893\n",
      "Iteration 628, loss = 0.21460788\n",
      "Iteration 629, loss = 0.21436733\n",
      "Iteration 630, loss = 0.21412728\n",
      "Iteration 631, loss = 0.21388774\n",
      "Iteration 632, loss = 0.21364870\n",
      "Iteration 633, loss = 0.21341016\n",
      "Iteration 634, loss = 0.21317213\n",
      "Iteration 635, loss = 0.21293460\n",
      "Iteration 636, loss = 0.21269757\n",
      "Iteration 637, loss = 0.21246106\n",
      "Iteration 638, loss = 0.21222501\n",
      "Iteration 639, loss = 0.21198948\n",
      "Iteration 640, loss = 0.21175444\n",
      "Iteration 641, loss = 0.21151989\n",
      "Iteration 642, loss = 0.21128582\n",
      "Iteration 643, loss = 0.21105225\n",
      "Iteration 644, loss = 0.21081918\n",
      "Iteration 645, loss = 0.21058663\n",
      "Iteration 646, loss = 0.21035456\n",
      "Iteration 647, loss = 0.21012299\n",
      "Iteration 648, loss = 0.20989188\n",
      "Iteration 649, loss = 0.20966127\n",
      "Iteration 650, loss = 0.20943114\n",
      "Iteration 651, loss = 0.20920148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 652, loss = 0.20897231\n",
      "Iteration 653, loss = 0.20874361\n",
      "Iteration 654, loss = 0.20851539\n",
      "Iteration 655, loss = 0.20828766\n",
      "Iteration 656, loss = 0.20806038\n",
      "Iteration 657, loss = 0.20783359\n",
      "Iteration 658, loss = 0.20760734\n",
      "Iteration 659, loss = 0.20738159\n",
      "Iteration 660, loss = 0.20715629\n",
      "Iteration 661, loss = 0.20693146\n",
      "Iteration 662, loss = 0.20670714\n",
      "Iteration 663, loss = 0.20648326\n",
      "Iteration 664, loss = 0.20625983\n",
      "Iteration 665, loss = 0.20603687\n",
      "Iteration 666, loss = 0.20581433\n",
      "Iteration 667, loss = 0.20559231\n",
      "Iteration 668, loss = 0.20537077\n",
      "Iteration 669, loss = 0.20514968\n",
      "Iteration 670, loss = 0.20492907\n",
      "Iteration 671, loss = 0.20470891\n",
      "Iteration 672, loss = 0.20448921\n",
      "Iteration 673, loss = 0.20427000\n",
      "Iteration 674, loss = 0.20405121\n",
      "Iteration 675, loss = 0.20383289\n",
      "Iteration 676, loss = 0.20361502\n",
      "Iteration 677, loss = 0.20339763\n",
      "Iteration 678, loss = 0.20318067\n",
      "Iteration 679, loss = 0.20296416\n",
      "Iteration 680, loss = 0.20274810\n",
      "Iteration 681, loss = 0.20253250\n",
      "Iteration 682, loss = 0.20231734\n",
      "Iteration 683, loss = 0.20210262\n",
      "Iteration 684, loss = 0.20188835\n",
      "Iteration 685, loss = 0.20167453\n",
      "Iteration 686, loss = 0.20146115\n",
      "Iteration 687, loss = 0.20124821\n",
      "Iteration 688, loss = 0.20103571\n",
      "Iteration 689, loss = 0.20082365\n",
      "Iteration 690, loss = 0.20061205\n",
      "Iteration 691, loss = 0.20040088\n",
      "Iteration 692, loss = 0.20019015\n",
      "Iteration 693, loss = 0.19997984\n",
      "Iteration 694, loss = 0.19976998\n",
      "Iteration 695, loss = 0.19956056\n",
      "Iteration 696, loss = 0.19935157\n",
      "Iteration 697, loss = 0.19914300\n",
      "Iteration 698, loss = 0.19893487\n",
      "Iteration 699, loss = 0.19872716\n",
      "Iteration 700, loss = 0.19851989\n",
      "Iteration 701, loss = 0.19831306\n",
      "Iteration 702, loss = 0.19810664\n",
      "Iteration 703, loss = 0.19790065\n",
      "Iteration 704, loss = 0.19769511\n",
      "Iteration 705, loss = 0.19748999\n",
      "Iteration 706, loss = 0.19728529\n",
      "Iteration 707, loss = 0.19708103\n",
      "Iteration 708, loss = 0.19687717\n",
      "Iteration 709, loss = 0.19667373\n",
      "Iteration 710, loss = 0.19647072\n",
      "Iteration 711, loss = 0.19626812\n",
      "Iteration 712, loss = 0.19606593\n",
      "Iteration 713, loss = 0.19586416\n",
      "Iteration 714, loss = 0.19566281\n",
      "Iteration 715, loss = 0.19546187\n",
      "Iteration 716, loss = 0.19526134\n",
      "Iteration 717, loss = 0.19506122\n",
      "Iteration 718, loss = 0.19486151\n",
      "Iteration 719, loss = 0.19466221\n",
      "Iteration 720, loss = 0.19446331\n",
      "Iteration 721, loss = 0.19426482\n",
      "Iteration 722, loss = 0.19406674\n",
      "Iteration 723, loss = 0.19386906\n",
      "Iteration 724, loss = 0.19367180\n",
      "Iteration 725, loss = 0.19347495\n",
      "Iteration 726, loss = 0.19327852\n",
      "Iteration 727, loss = 0.19308249\n",
      "Iteration 728, loss = 0.19288685\n",
      "Iteration 729, loss = 0.19269160\n",
      "Iteration 730, loss = 0.19249674\n",
      "Iteration 731, loss = 0.19230228\n",
      "Iteration 732, loss = 0.19210821\n",
      "Iteration 733, loss = 0.19191456\n",
      "Iteration 734, loss = 0.19172131\n",
      "Iteration 735, loss = 0.19152845\n",
      "Iteration 736, loss = 0.19133597\n",
      "Iteration 737, loss = 0.19114388\n",
      "Iteration 738, loss = 0.19095218\n",
      "Iteration 739, loss = 0.19076088\n",
      "Iteration 740, loss = 0.19056997\n",
      "Iteration 741, loss = 0.19037944\n",
      "Iteration 742, loss = 0.19018932\n",
      "Iteration 743, loss = 0.18999957\n",
      "Iteration 744, loss = 0.18981022\n",
      "Iteration 745, loss = 0.18962125\n",
      "Iteration 746, loss = 0.18943267\n",
      "Iteration 747, loss = 0.18924446\n",
      "Iteration 748, loss = 0.18905664\n",
      "Iteration 749, loss = 0.18886920\n",
      "Iteration 750, loss = 0.18868214\n",
      "Iteration 751, loss = 0.18849545\n",
      "Iteration 752, loss = 0.18830916\n",
      "Iteration 753, loss = 0.18812323\n",
      "Iteration 754, loss = 0.18793769\n",
      "Iteration 755, loss = 0.18775252\n",
      "Iteration 756, loss = 0.18756774\n",
      "Iteration 757, loss = 0.18738332\n",
      "Iteration 758, loss = 0.18719929\n",
      "Iteration 759, loss = 0.18701563\n",
      "Iteration 760, loss = 0.18683235\n",
      "Iteration 761, loss = 0.18664944\n",
      "Iteration 762, loss = 0.18646689\n",
      "Iteration 763, loss = 0.18628473\n",
      "Iteration 764, loss = 0.18610292\n",
      "Iteration 765, loss = 0.18592148\n",
      "Iteration 766, loss = 0.18574041\n",
      "Iteration 767, loss = 0.18555970\n",
      "Iteration 768, loss = 0.18537936\n",
      "Iteration 769, loss = 0.18519937\n",
      "Iteration 770, loss = 0.18501976\n",
      "Iteration 771, loss = 0.18484050\n",
      "Iteration 772, loss = 0.18466160\n",
      "Iteration 773, loss = 0.18448307\n",
      "Iteration 774, loss = 0.18430489\n",
      "Iteration 775, loss = 0.18412707\n",
      "Iteration 776, loss = 0.18394961\n",
      "Iteration 777, loss = 0.18377250\n",
      "Iteration 778, loss = 0.18359574\n",
      "Iteration 779, loss = 0.18341935\n",
      "Iteration 780, loss = 0.18324331\n",
      "Iteration 781, loss = 0.18306762\n",
      "Iteration 782, loss = 0.18289229\n",
      "Iteration 783, loss = 0.18271731\n",
      "Iteration 784, loss = 0.18254287\n",
      "Iteration 785, loss = 0.18236899\n",
      "Iteration 786, loss = 0.18219548\n",
      "Iteration 787, loss = 0.18202233\n",
      "Iteration 788, loss = 0.18184956\n",
      "Iteration 789, loss = 0.18167714\n",
      "Iteration 790, loss = 0.18150507\n",
      "Iteration 791, loss = 0.18133337\n",
      "Iteration 792, loss = 0.18116204\n",
      "Iteration 793, loss = 0.18099105\n",
      "Iteration 794, loss = 0.18082042\n",
      "Iteration 795, loss = 0.18065012\n",
      "Iteration 796, loss = 0.18048017\n",
      "Iteration 797, loss = 0.18031057\n",
      "Iteration 798, loss = 0.18014132\n",
      "Iteration 799, loss = 0.17997240\n",
      "Iteration 800, loss = 0.17980382\n",
      "Iteration 801, loss = 0.17963558\n",
      "Iteration 802, loss = 0.17946769\n",
      "Iteration 803, loss = 0.17930015\n",
      "Iteration 804, loss = 0.17913296\n",
      "Iteration 805, loss = 0.17896610\n",
      "Iteration 806, loss = 0.17879956\n",
      "Iteration 807, loss = 0.17863334\n",
      "Iteration 808, loss = 0.17846746\n",
      "Iteration 809, loss = 0.17830190\n",
      "Iteration 810, loss = 0.17813669\n",
      "Iteration 811, loss = 0.17797182\n",
      "Iteration 812, loss = 0.17780727\n",
      "Iteration 813, loss = 0.17764304\n",
      "Iteration 814, loss = 0.17747914\n",
      "Iteration 815, loss = 0.17731556\n",
      "Iteration 816, loss = 0.17715231\n",
      "Iteration 817, loss = 0.17698938\n",
      "Iteration 818, loss = 0.17682677\n",
      "Iteration 819, loss = 0.17666448\n",
      "Iteration 820, loss = 0.17650252\n",
      "Iteration 821, loss = 0.17634089\n",
      "Iteration 822, loss = 0.17617957\n",
      "Iteration 823, loss = 0.17601857\n",
      "Iteration 824, loss = 0.17585789\n",
      "Iteration 825, loss = 0.17569753\n",
      "Iteration 826, loss = 0.17553747\n",
      "Iteration 827, loss = 0.17537775\n",
      "Iteration 828, loss = 0.17521832\n",
      "Iteration 829, loss = 0.17505922\n",
      "Iteration 830, loss = 0.17490042\n",
      "Iteration 831, loss = 0.17474194\n",
      "Iteration 832, loss = 0.17458376\n",
      "Iteration 833, loss = 0.17442590\n",
      "Iteration 834, loss = 0.17426834\n",
      "Iteration 835, loss = 0.17411111\n",
      "Iteration 836, loss = 0.17395416\n",
      "Iteration 837, loss = 0.17379754\n",
      "Iteration 838, loss = 0.17364121\n",
      "Iteration 839, loss = 0.17348519\n",
      "Iteration 840, loss = 0.17332947\n",
      "Iteration 841, loss = 0.17317406\n",
      "Iteration 842, loss = 0.17301896\n",
      "Iteration 843, loss = 0.17286416\n",
      "Iteration 844, loss = 0.17270966\n",
      "Iteration 845, loss = 0.17255547\n",
      "Iteration 846, loss = 0.17240158\n",
      "Iteration 847, loss = 0.17224800\n",
      "Iteration 848, loss = 0.17209471\n",
      "Iteration 849, loss = 0.17194172\n",
      "Iteration 850, loss = 0.17178902\n",
      "Iteration 851, loss = 0.17163663\n",
      "Iteration 852, loss = 0.17148453\n",
      "Iteration 853, loss = 0.17133273\n",
      "Iteration 854, loss = 0.17118122\n",
      "Iteration 855, loss = 0.17103000\n",
      "Iteration 856, loss = 0.17087908\n",
      "Iteration 857, loss = 0.17072846\n",
      "Iteration 858, loss = 0.17057813\n",
      "Iteration 859, loss = 0.17042810\n",
      "Iteration 860, loss = 0.17027835\n",
      "Iteration 861, loss = 0.17012890\n",
      "Iteration 862, loss = 0.16997973\n",
      "Iteration 863, loss = 0.16983085\n",
      "Iteration 864, loss = 0.16968226\n",
      "Iteration 865, loss = 0.16953396\n",
      "Iteration 866, loss = 0.16938594\n",
      "Iteration 867, loss = 0.16923821\n",
      "Iteration 868, loss = 0.16909076\n",
      "Iteration 869, loss = 0.16894360\n",
      "Iteration 870, loss = 0.16879672\n",
      "Iteration 871, loss = 0.16865012\n",
      "Iteration 872, loss = 0.16850386\n",
      "Iteration 873, loss = 0.16835811\n",
      "Iteration 874, loss = 0.16821266\n",
      "Iteration 875, loss = 0.16806749\n",
      "Iteration 876, loss = 0.16792261\n",
      "Iteration 877, loss = 0.16777802\n",
      "Iteration 878, loss = 0.16763371\n",
      "Iteration 879, loss = 0.16748969\n",
      "Iteration 880, loss = 0.16734595\n",
      "Iteration 881, loss = 0.16720248\n",
      "Iteration 882, loss = 0.16705931\n",
      "Iteration 883, loss = 0.16691640\n",
      "Iteration 884, loss = 0.16677377\n",
      "Iteration 885, loss = 0.16663145\n",
      "Iteration 886, loss = 0.16648937\n",
      "Iteration 887, loss = 0.16634758\n",
      "Iteration 888, loss = 0.16620605\n",
      "Iteration 889, loss = 0.16606481\n",
      "Iteration 890, loss = 0.16592384\n",
      "Iteration 891, loss = 0.16578314\n",
      "Iteration 892, loss = 0.16564270\n",
      "Iteration 893, loss = 0.16550253\n",
      "Iteration 894, loss = 0.16536264\n",
      "Iteration 895, loss = 0.16522301\n",
      "Iteration 896, loss = 0.16508365\n",
      "Iteration 897, loss = 0.16494456\n",
      "Iteration 898, loss = 0.16480572\n",
      "Iteration 899, loss = 0.16466716\n",
      "Iteration 900, loss = 0.16452886\n",
      "Iteration 901, loss = 0.16439081\n",
      "Iteration 902, loss = 0.16425303\n",
      "Iteration 903, loss = 0.16411551\n",
      "Iteration 904, loss = 0.16397826\n",
      "Iteration 905, loss = 0.16384127\n",
      "Iteration 906, loss = 0.16370455\n",
      "Iteration 907, loss = 0.16356809\n",
      "Iteration 908, loss = 0.16343187\n",
      "Iteration 909, loss = 0.16329592\n",
      "Iteration 910, loss = 0.16316024\n",
      "Iteration 911, loss = 0.16302481\n",
      "Iteration 912, loss = 0.16288964\n",
      "Iteration 913, loss = 0.16275473\n",
      "Iteration 914, loss = 0.16262012\n",
      "Iteration 915, loss = 0.16248579\n",
      "Iteration 916, loss = 0.16235172\n",
      "Iteration 917, loss = 0.16221792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 918, loss = 0.16208435\n",
      "Iteration 919, loss = 0.16195103\n",
      "Iteration 920, loss = 0.16181797\n",
      "Iteration 921, loss = 0.16168516\n",
      "Iteration 922, loss = 0.16155259\n",
      "Iteration 923, loss = 0.16142028\n",
      "Iteration 924, loss = 0.16128821\n",
      "Iteration 925, loss = 0.16115639\n",
      "Iteration 926, loss = 0.16102482\n",
      "Iteration 927, loss = 0.16089349\n",
      "Iteration 928, loss = 0.16076240\n",
      "Iteration 929, loss = 0.16063158\n",
      "Iteration 930, loss = 0.16050098\n",
      "Iteration 931, loss = 0.16037063\n",
      "Iteration 932, loss = 0.16024053\n",
      "Iteration 933, loss = 0.16011067\n",
      "Iteration 934, loss = 0.15998105\n",
      "Iteration 935, loss = 0.15985168\n",
      "Iteration 936, loss = 0.15972254\n",
      "Iteration 937, loss = 0.15959365\n",
      "Iteration 938, loss = 0.15946500\n",
      "Iteration 939, loss = 0.15933658\n",
      "Iteration 940, loss = 0.15920841\n",
      "Iteration 941, loss = 0.15908047\n",
      "Iteration 942, loss = 0.15895277\n",
      "Iteration 943, loss = 0.15882532\n",
      "Iteration 944, loss = 0.15869809\n",
      "Iteration 945, loss = 0.15857111\n",
      "Iteration 946, loss = 0.15844436\n",
      "Iteration 947, loss = 0.15831784\n",
      "Iteration 948, loss = 0.15819156\n",
      "Iteration 949, loss = 0.15806552\n",
      "Iteration 950, loss = 0.15793981\n",
      "Iteration 951, loss = 0.15781437\n",
      "Iteration 952, loss = 0.15768917\n",
      "Iteration 953, loss = 0.15756421\n",
      "Iteration 954, loss = 0.15743950\n",
      "Iteration 955, loss = 0.15731502\n",
      "Iteration 956, loss = 0.15719078\n",
      "Iteration 957, loss = 0.15706677\n",
      "Iteration 958, loss = 0.15694300\n",
      "Iteration 959, loss = 0.15681946\n",
      "Iteration 960, loss = 0.15669615\n",
      "Iteration 961, loss = 0.15657308\n",
      "Iteration 962, loss = 0.15645023\n",
      "Iteration 963, loss = 0.15632762\n",
      "Iteration 964, loss = 0.15620523\n",
      "Iteration 965, loss = 0.15608307\n",
      "Iteration 966, loss = 0.15596113\n",
      "Iteration 967, loss = 0.15583943\n",
      "Iteration 968, loss = 0.15571795\n",
      "Iteration 969, loss = 0.15559670\n",
      "Iteration 970, loss = 0.15547572\n",
      "Iteration 971, loss = 0.15535497\n",
      "Iteration 972, loss = 0.15523443\n",
      "Iteration 973, loss = 0.15511411\n",
      "Iteration 974, loss = 0.15499401\n",
      "Iteration 975, loss = 0.15487411\n",
      "Iteration 976, loss = 0.15475444\n",
      "Iteration 977, loss = 0.15463498\n",
      "Iteration 978, loss = 0.15451574\n",
      "Iteration 979, loss = 0.15439675\n",
      "Iteration 980, loss = 0.15427797\n",
      "Iteration 981, loss = 0.15415940\n",
      "Iteration 982, loss = 0.15404105\n",
      "Iteration 983, loss = 0.15392291\n",
      "Iteration 984, loss = 0.15380499\n",
      "Iteration 985, loss = 0.15368728\n",
      "Iteration 986, loss = 0.15356980\n",
      "Iteration 987, loss = 0.15345253\n",
      "Iteration 988, loss = 0.15333547\n",
      "Iteration 989, loss = 0.15321861\n",
      "Iteration 990, loss = 0.15310199\n",
      "Iteration 991, loss = 0.15298557\n",
      "Iteration 992, loss = 0.15286936\n",
      "Iteration 993, loss = 0.15275338\n",
      "Iteration 994, loss = 0.15263777\n",
      "Iteration 995, loss = 0.15252242\n",
      "Iteration 996, loss = 0.15240729\n",
      "Iteration 997, loss = 0.15229238\n",
      "Iteration 998, loss = 0.15217767\n",
      "Iteration 999, loss = 0.15206319\n",
      "Iteration 1000, loss = 0.15194892\n",
      "Iteration 1, loss = 1.34996548\n",
      "Iteration 2, loss = 1.32612428\n",
      "Iteration 3, loss = 1.29405243\n",
      "Iteration 4, loss = 1.25608480\n",
      "Iteration 5, loss = 1.21428653\n",
      "Iteration 6, loss = 1.17034915\n",
      "Iteration 7, loss = 1.12552698\n",
      "Iteration 8, loss = 1.08090013\n",
      "Iteration 9, loss = 1.03752665\n",
      "Iteration 10, loss = 0.99659600\n",
      "Iteration 11, loss = 0.95937619\n",
      "Iteration 12, loss = 0.92677733\n",
      "Iteration 13, loss = 0.89929481\n",
      "Iteration 14, loss = 0.87740447\n",
      "Iteration 15, loss = 0.86077411\n",
      "Iteration 16, loss = 0.84886351\n",
      "Iteration 17, loss = 0.84062711\n",
      "Iteration 18, loss = 0.83463813\n",
      "Iteration 19, loss = 0.82947762\n",
      "Iteration 20, loss = 0.82393638\n",
      "Iteration 21, loss = 0.81730766\n",
      "Iteration 22, loss = 0.80904078\n",
      "Iteration 23, loss = 0.79928767\n",
      "Iteration 24, loss = 0.78833964\n",
      "Iteration 25, loss = 0.77710158\n",
      "Iteration 26, loss = 0.76564441\n",
      "Iteration 27, loss = 0.75415239\n",
      "Iteration 28, loss = 0.74294333\n",
      "Iteration 29, loss = 0.73218560\n",
      "Iteration 30, loss = 0.72207844\n",
      "Iteration 31, loss = 0.71294576\n",
      "Iteration 32, loss = 0.70475321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.69743311\n",
      "Iteration 34, loss = 0.69085190\n",
      "Iteration 35, loss = 0.68495912\n",
      "Iteration 36, loss = 0.67953014\n",
      "Iteration 37, loss = 0.67441594\n",
      "Iteration 38, loss = 0.66953110\n",
      "Iteration 39, loss = 0.66479665\n",
      "Iteration 40, loss = 0.66016689\n",
      "Iteration 41, loss = 0.65560758\n",
      "Iteration 42, loss = 0.65110709\n",
      "Iteration 43, loss = 0.64664781\n",
      "Iteration 44, loss = 0.64223647\n",
      "Iteration 45, loss = 0.63787459\n",
      "Iteration 46, loss = 0.63356575\n",
      "Iteration 47, loss = 0.62931804\n",
      "Iteration 48, loss = 0.62514051\n",
      "Iteration 49, loss = 0.62104451\n",
      "Iteration 50, loss = 0.61707215\n",
      "Iteration 51, loss = 0.61322626\n",
      "Iteration 52, loss = 0.60952934\n",
      "Iteration 53, loss = 0.60602885\n",
      "Iteration 54, loss = 0.60270710\n",
      "Iteration 55, loss = 0.59956014\n",
      "Iteration 56, loss = 0.59652573\n",
      "Iteration 57, loss = 0.59358191\n",
      "Iteration 58, loss = 0.59069772\n",
      "Iteration 59, loss = 0.58786319\n",
      "Iteration 60, loss = 0.58506999\n",
      "Iteration 61, loss = 0.58232262\n",
      "Iteration 62, loss = 0.57961068\n",
      "Iteration 63, loss = 0.57693138\n",
      "Iteration 64, loss = 0.57428461\n",
      "Iteration 65, loss = 0.57167242\n",
      "Iteration 66, loss = 0.56909143\n",
      "Iteration 67, loss = 0.56654199\n",
      "Iteration 68, loss = 0.56402917\n",
      "Iteration 69, loss = 0.56155161\n",
      "Iteration 70, loss = 0.55910791\n",
      "Iteration 71, loss = 0.55669801\n",
      "Iteration 72, loss = 0.55433110\n",
      "Iteration 73, loss = 0.55199841\n",
      "Iteration 74, loss = 0.54970869\n",
      "Iteration 75, loss = 0.54746486\n",
      "Iteration 76, loss = 0.54526019\n",
      "Iteration 77, loss = 0.54309171\n",
      "Iteration 78, loss = 0.54096707\n",
      "Iteration 79, loss = 0.53888682\n",
      "Iteration 80, loss = 0.53683982\n",
      "Iteration 81, loss = 0.53482314\n",
      "Iteration 82, loss = 0.53283558\n",
      "Iteration 83, loss = 0.53087429\n",
      "Iteration 84, loss = 0.52893934\n",
      "Iteration 85, loss = 0.52703059\n",
      "Iteration 86, loss = 0.52514662\n",
      "Iteration 87, loss = 0.52328802\n",
      "Iteration 88, loss = 0.52145288\n",
      "Iteration 89, loss = 0.51964154\n",
      "Iteration 90, loss = 0.51785544\n",
      "Iteration 91, loss = 0.51609222\n",
      "Iteration 92, loss = 0.51435426\n",
      "Iteration 93, loss = 0.51263929\n",
      "Iteration 94, loss = 0.51094644\n",
      "Iteration 95, loss = 0.50927513\n",
      "Iteration 96, loss = 0.50762488\n",
      "Iteration 97, loss = 0.50599538\n",
      "Iteration 98, loss = 0.50438754\n",
      "Iteration 99, loss = 0.50279992\n",
      "Iteration 100, loss = 0.50123163\n",
      "Iteration 101, loss = 0.49968330\n",
      "Iteration 102, loss = 0.49815435\n",
      "Iteration 103, loss = 0.49664434\n",
      "Iteration 104, loss = 0.49515214\n",
      "Iteration 105, loss = 0.49367794\n",
      "Iteration 106, loss = 0.49222105\n",
      "Iteration 107, loss = 0.49078019\n",
      "Iteration 108, loss = 0.48935541\n",
      "Iteration 109, loss = 0.48794717\n",
      "Iteration 110, loss = 0.48655596\n",
      "Iteration 111, loss = 0.48518037\n",
      "Iteration 112, loss = 0.48381984\n",
      "Iteration 113, loss = 0.48247407\n",
      "Iteration 114, loss = 0.48114274\n",
      "Iteration 115, loss = 0.47982559\n",
      "Iteration 116, loss = 0.47852231\n",
      "Iteration 117, loss = 0.47723265\n",
      "Iteration 118, loss = 0.47595628\n",
      "Iteration 119, loss = 0.47469370\n",
      "Iteration 120, loss = 0.47344420\n",
      "Iteration 121, loss = 0.47220767\n",
      "Iteration 122, loss = 0.47098437\n",
      "Iteration 123, loss = 0.46977287\n",
      "Iteration 124, loss = 0.46857211\n",
      "Iteration 125, loss = 0.46738279\n",
      "Iteration 126, loss = 0.46620479\n",
      "Iteration 127, loss = 0.46503860\n",
      "Iteration 128, loss = 0.46388401\n",
      "Iteration 129, loss = 0.46274118\n",
      "Iteration 130, loss = 0.46160902\n",
      "Iteration 131, loss = 0.46048666\n",
      "Iteration 132, loss = 0.45937363\n",
      "Iteration 133, loss = 0.45827094\n",
      "Iteration 134, loss = 0.45717834\n",
      "Iteration 135, loss = 0.45609541\n",
      "Iteration 136, loss = 0.45502077\n",
      "Iteration 137, loss = 0.45395536\n",
      "Iteration 138, loss = 0.45289829\n",
      "Iteration 139, loss = 0.45184945\n",
      "Iteration 140, loss = 0.45080926\n",
      "Iteration 141, loss = 0.44977565\n",
      "Iteration 142, loss = 0.44874919\n",
      "Iteration 143, loss = 0.44772992\n",
      "Iteration 144, loss = 0.44671854\n",
      "Iteration 145, loss = 0.44571350\n",
      "Iteration 146, loss = 0.44471508\n",
      "Iteration 147, loss = 0.44372382\n",
      "Iteration 148, loss = 0.44273821\n",
      "Iteration 149, loss = 0.44175736\n",
      "Iteration 150, loss = 0.44078323\n",
      "Iteration 151, loss = 0.43981361\n",
      "Iteration 152, loss = 0.43884986\n",
      "Iteration 153, loss = 0.43789008\n",
      "Iteration 154, loss = 0.43693563\n",
      "Iteration 155, loss = 0.43598326\n",
      "Iteration 156, loss = 0.43503338\n",
      "Iteration 157, loss = 0.43408587\n",
      "Iteration 158, loss = 0.43313717\n",
      "Iteration 159, loss = 0.43218996\n",
      "Iteration 160, loss = 0.43124455\n",
      "Iteration 161, loss = 0.43030198\n",
      "Iteration 162, loss = 0.42935876\n",
      "Iteration 163, loss = 0.42841749\n",
      "Iteration 164, loss = 0.42747590\n",
      "Iteration 165, loss = 0.42653786\n",
      "Iteration 166, loss = 0.42560382\n",
      "Iteration 167, loss = 0.42467426\n",
      "Iteration 168, loss = 0.42374572\n",
      "Iteration 169, loss = 0.42280409\n",
      "Iteration 170, loss = 0.42185869\n",
      "Iteration 171, loss = 0.42090683\n",
      "Iteration 172, loss = 0.41995141\n",
      "Iteration 173, loss = 0.41899091\n",
      "Iteration 174, loss = 0.41802667\n",
      "Iteration 175, loss = 0.41706224\n",
      "Iteration 176, loss = 0.41610226\n",
      "Iteration 177, loss = 0.41514429\n",
      "Iteration 178, loss = 0.41418301\n",
      "Iteration 179, loss = 0.41322177\n",
      "Iteration 180, loss = 0.41228831\n",
      "Iteration 181, loss = 0.41136517\n",
      "Iteration 182, loss = 0.41045850\n",
      "Iteration 183, loss = 0.40956276\n",
      "Iteration 184, loss = 0.40867235\n",
      "Iteration 185, loss = 0.40779360\n",
      "Iteration 186, loss = 0.40693810\n",
      "Iteration 187, loss = 0.40610465\n",
      "Iteration 188, loss = 0.40529019\n",
      "Iteration 189, loss = 0.40448875\n",
      "Iteration 190, loss = 0.40369725\n",
      "Iteration 191, loss = 0.40291730\n",
      "Iteration 192, loss = 0.40214537\n",
      "Iteration 193, loss = 0.40137914\n",
      "Iteration 194, loss = 0.40062294\n",
      "Iteration 195, loss = 0.39987875\n",
      "Iteration 196, loss = 0.39914344\n",
      "Iteration 197, loss = 0.39841348\n",
      "Iteration 198, loss = 0.39769481\n",
      "Iteration 199, loss = 0.39698471\n",
      "Iteration 200, loss = 0.39628197\n",
      "Iteration 201, loss = 0.39558572\n",
      "Iteration 202, loss = 0.39489612\n",
      "Iteration 203, loss = 0.39421238\n",
      "Iteration 204, loss = 0.39353331\n",
      "Iteration 205, loss = 0.39285948\n",
      "Iteration 206, loss = 0.39218920\n",
      "Iteration 207, loss = 0.39152273\n",
      "Iteration 208, loss = 0.39085972\n",
      "Iteration 209, loss = 0.39020014\n",
      "Iteration 210, loss = 0.38954399\n",
      "Iteration 211, loss = 0.38889218\n",
      "Iteration 212, loss = 0.38824453\n",
      "Iteration 213, loss = 0.38760057\n",
      "Iteration 214, loss = 0.38695976\n",
      "Iteration 215, loss = 0.38632207\n",
      "Iteration 216, loss = 0.38568728\n",
      "Iteration 217, loss = 0.38505568\n",
      "Iteration 218, loss = 0.38442767\n",
      "Iteration 219, loss = 0.38380255\n",
      "Iteration 220, loss = 0.38318043\n",
      "Iteration 221, loss = 0.38256122\n",
      "Iteration 222, loss = 0.38194475\n",
      "Iteration 223, loss = 0.38133111\n",
      "Iteration 224, loss = 0.38072011\n",
      "Iteration 225, loss = 0.38011175\n",
      "Iteration 226, loss = 0.37950608\n",
      "Iteration 227, loss = 0.37890301\n",
      "Iteration 228, loss = 0.37830251\n",
      "Iteration 229, loss = 0.37770468\n",
      "Iteration 230, loss = 0.37710930\n",
      "Iteration 231, loss = 0.37651626\n",
      "Iteration 232, loss = 0.37592559\n",
      "Iteration 233, loss = 0.37533724\n",
      "Iteration 234, loss = 0.37475128\n",
      "Iteration 235, loss = 0.37416769\n",
      "Iteration 236, loss = 0.37358639\n",
      "Iteration 237, loss = 0.37300750\n",
      "Iteration 238, loss = 0.37243093\n",
      "Iteration 239, loss = 0.37185661\n",
      "Iteration 240, loss = 0.37128465\n",
      "Iteration 241, loss = 0.37071483\n",
      "Iteration 242, loss = 0.37014722\n",
      "Iteration 243, loss = 0.36958179\n",
      "Iteration 244, loss = 0.36901856\n",
      "Iteration 245, loss = 0.36845746\n",
      "Iteration 246, loss = 0.36789847\n",
      "Iteration 247, loss = 0.36734157\n",
      "Iteration 248, loss = 0.36678674\n",
      "Iteration 249, loss = 0.36623395\n",
      "Iteration 250, loss = 0.36568320\n",
      "Iteration 251, loss = 0.36513443\n",
      "Iteration 252, loss = 0.36458761\n",
      "Iteration 253, loss = 0.36404278\n",
      "Iteration 254, loss = 0.36349990\n",
      "Iteration 255, loss = 0.36295894\n",
      "Iteration 256, loss = 0.36241989\n",
      "Iteration 257, loss = 0.36188268\n",
      "Iteration 258, loss = 0.36134736\n",
      "Iteration 259, loss = 0.36081389\n",
      "Iteration 260, loss = 0.36028226\n",
      "Iteration 261, loss = 0.35975247\n",
      "Iteration 262, loss = 0.35922452\n",
      "Iteration 263, loss = 0.35869840\n",
      "Iteration 264, loss = 0.35817407\n",
      "Iteration 265, loss = 0.35765148\n",
      "Iteration 266, loss = 0.35713050\n",
      "Iteration 267, loss = 0.35661125\n",
      "Iteration 268, loss = 0.35609370\n",
      "Iteration 269, loss = 0.35557803\n",
      "Iteration 270, loss = 0.35506395\n",
      "Iteration 271, loss = 0.35455148\n",
      "Iteration 272, loss = 0.35404065\n",
      "Iteration 273, loss = 0.35353144\n",
      "Iteration 274, loss = 0.35302386\n",
      "Iteration 275, loss = 0.35251787\n",
      "Iteration 276, loss = 0.35201367\n",
      "Iteration 277, loss = 0.35151189\n",
      "Iteration 278, loss = 0.35101169\n",
      "Iteration 279, loss = 0.35051306\n",
      "Iteration 280, loss = 0.35001601\n",
      "Iteration 281, loss = 0.34952053\n",
      "Iteration 282, loss = 0.34902663\n",
      "Iteration 283, loss = 0.34853422\n",
      "Iteration 284, loss = 0.34804335\n",
      "Iteration 285, loss = 0.34755390\n",
      "Iteration 286, loss = 0.34706593\n",
      "Iteration 287, loss = 0.34657948\n",
      "Iteration 288, loss = 0.34609452\n",
      "Iteration 289, loss = 0.34561107\n",
      "Iteration 290, loss = 0.34512905\n",
      "Iteration 291, loss = 0.34464847\n",
      "Iteration 292, loss = 0.34416937\n",
      "Iteration 293, loss = 0.34369165\n",
      "Iteration 294, loss = 0.34321543\n",
      "Iteration 295, loss = 0.34274063\n",
      "Iteration 296, loss = 0.34226725\n",
      "Iteration 297, loss = 0.34179527\n",
      "Iteration 298, loss = 0.34132469\n",
      "Iteration 299, loss = 0.34085546\n",
      "Iteration 300, loss = 0.34038762\n",
      "Iteration 301, loss = 0.33992123\n",
      "Iteration 302, loss = 0.33945630\n",
      "Iteration 303, loss = 0.33899269\n",
      "Iteration 304, loss = 0.33853092\n",
      "Iteration 305, loss = 0.33807051\n",
      "Iteration 306, loss = 0.33761136\n",
      "Iteration 307, loss = 0.33715360\n",
      "Iteration 308, loss = 0.33669714\n",
      "Iteration 309, loss = 0.33624198\n",
      "Iteration 310, loss = 0.33578809\n",
      "Iteration 311, loss = 0.33533554\n",
      "Iteration 312, loss = 0.33488427\n",
      "Iteration 313, loss = 0.33443446\n",
      "Iteration 314, loss = 0.33398580\n",
      "Iteration 315, loss = 0.33353844\n",
      "Iteration 316, loss = 0.33309232\n",
      "Iteration 317, loss = 0.33264735\n",
      "Iteration 318, loss = 0.33220369\n",
      "Iteration 319, loss = 0.33176114\n",
      "Iteration 320, loss = 0.33131990\n",
      "Iteration 321, loss = 0.33087976\n",
      "Iteration 322, loss = 0.33044088\n",
      "Iteration 323, loss = 0.33000316\n",
      "Iteration 324, loss = 0.32956658\n",
      "Iteration 325, loss = 0.32913127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 326, loss = 0.32869701\n",
      "Iteration 327, loss = 0.32826400\n",
      "Iteration 328, loss = 0.32783207\n",
      "Iteration 329, loss = 0.32740130\n",
      "Iteration 330, loss = 0.32697171\n",
      "Iteration 331, loss = 0.32654329\n",
      "Iteration 332, loss = 0.32611597\n",
      "Iteration 333, loss = 0.32568978\n",
      "Iteration 334, loss = 0.32526480\n",
      "Iteration 335, loss = 0.32484093\n",
      "Iteration 336, loss = 0.32441825\n",
      "Iteration 337, loss = 0.32399660\n",
      "Iteration 338, loss = 0.32357606\n",
      "Iteration 339, loss = 0.32315665\n",
      "Iteration 340, loss = 0.32273832\n",
      "Iteration 341, loss = 0.32232106\n",
      "Iteration 342, loss = 0.32190493\n",
      "Iteration 343, loss = 0.32148982\n",
      "Iteration 344, loss = 0.32107585\n",
      "Iteration 345, loss = 0.32066286\n",
      "Iteration 346, loss = 0.32025108\n",
      "Iteration 347, loss = 0.31984026\n",
      "Iteration 348, loss = 0.31943052\n",
      "Iteration 349, loss = 0.31902190\n",
      "Iteration 350, loss = 0.31861438\n",
      "Iteration 351, loss = 0.31820787\n",
      "Iteration 352, loss = 0.31780241\n",
      "Iteration 353, loss = 0.31739798\n",
      "Iteration 354, loss = 0.31699453\n",
      "Iteration 355, loss = 0.31659209\n",
      "Iteration 356, loss = 0.31619074\n",
      "Iteration 357, loss = 0.31579029\n",
      "Iteration 358, loss = 0.31539090\n",
      "Iteration 359, loss = 0.31499248\n",
      "Iteration 360, loss = 0.31459507\n",
      "Iteration 361, loss = 0.31419861\n",
      "Iteration 362, loss = 0.31380320\n",
      "Iteration 363, loss = 0.31340878\n",
      "Iteration 364, loss = 0.31301556\n",
      "Iteration 365, loss = 0.31262326\n",
      "Iteration 366, loss = 0.31223181\n",
      "Iteration 367, loss = 0.31184119\n",
      "Iteration 368, loss = 0.31145172\n",
      "Iteration 369, loss = 0.31106325\n",
      "Iteration 370, loss = 0.31067577\n",
      "Iteration 371, loss = 0.31028916\n",
      "Iteration 372, loss = 0.30990345\n",
      "Iteration 373, loss = 0.30951869\n",
      "Iteration 374, loss = 0.30913497\n",
      "Iteration 375, loss = 0.30875202\n",
      "Iteration 376, loss = 0.30837004\n",
      "Iteration 377, loss = 0.30798904\n",
      "Iteration 378, loss = 0.30760888\n",
      "Iteration 379, loss = 0.30722968\n",
      "Iteration 380, loss = 0.30685129\n",
      "Iteration 381, loss = 0.30647398\n",
      "Iteration 382, loss = 0.30609740\n",
      "Iteration 383, loss = 0.30572176\n",
      "Iteration 384, loss = 0.30534697\n",
      "Iteration 385, loss = 0.30497322\n",
      "Iteration 386, loss = 0.30460021\n",
      "Iteration 387, loss = 0.30422818\n",
      "Iteration 388, loss = 0.30385699\n",
      "Iteration 389, loss = 0.30348673\n",
      "Iteration 390, loss = 0.30311736\n",
      "Iteration 391, loss = 0.30274883\n",
      "Iteration 392, loss = 0.30238120\n",
      "Iteration 393, loss = 0.30201445\n",
      "Iteration 394, loss = 0.30164861\n",
      "Iteration 395, loss = 0.30128356\n",
      "Iteration 396, loss = 0.30091936\n",
      "Iteration 397, loss = 0.30055603\n",
      "Iteration 398, loss = 0.30019361\n",
      "Iteration 399, loss = 0.29983193\n",
      "Iteration 400, loss = 0.29947114\n",
      "Iteration 401, loss = 0.29911116\n",
      "Iteration 402, loss = 0.29875208\n",
      "Iteration 403, loss = 0.29839377\n",
      "Iteration 404, loss = 0.29803625\n",
      "Iteration 405, loss = 0.29767962\n",
      "Iteration 406, loss = 0.29732384\n",
      "Iteration 407, loss = 0.29696884\n",
      "Iteration 408, loss = 0.29661460\n",
      "Iteration 409, loss = 0.29626120\n",
      "Iteration 410, loss = 0.29590866\n",
      "Iteration 411, loss = 0.29555695\n",
      "Iteration 412, loss = 0.29520592\n",
      "Iteration 413, loss = 0.29485577\n",
      "Iteration 414, loss = 0.29450645\n",
      "Iteration 415, loss = 0.29415788\n",
      "Iteration 416, loss = 0.29381014\n",
      "Iteration 417, loss = 0.29346314\n",
      "Iteration 418, loss = 0.29311703\n",
      "Iteration 419, loss = 0.29277160\n",
      "Iteration 420, loss = 0.29242701\n",
      "Iteration 421, loss = 0.29208318\n",
      "Iteration 422, loss = 0.29174015\n",
      "Iteration 423, loss = 0.29139784\n",
      "Iteration 424, loss = 0.29105637\n",
      "Iteration 425, loss = 0.29071561\n",
      "Iteration 426, loss = 0.29037568\n",
      "Iteration 427, loss = 0.29003645\n",
      "Iteration 428, loss = 0.28969800\n",
      "Iteration 429, loss = 0.28936034\n",
      "Iteration 430, loss = 0.28902343\n",
      "Iteration 431, loss = 0.28868722\n",
      "Iteration 432, loss = 0.28835181\n",
      "Iteration 433, loss = 0.28801718\n",
      "Iteration 434, loss = 0.28768324\n",
      "Iteration 435, loss = 0.28735006\n",
      "Iteration 436, loss = 0.28701758\n",
      "Iteration 437, loss = 0.28668602\n",
      "Iteration 438, loss = 0.28635497\n",
      "Iteration 439, loss = 0.28602477\n",
      "Iteration 440, loss = 0.28569525\n",
      "Iteration 441, loss = 0.28536656\n",
      "Iteration 442, loss = 0.28503855\n",
      "Iteration 443, loss = 0.28471129\n",
      "Iteration 444, loss = 0.28438468\n",
      "Iteration 445, loss = 0.28405890\n",
      "Iteration 446, loss = 0.28373375\n",
      "Iteration 447, loss = 0.28340938\n",
      "Iteration 448, loss = 0.28308570\n",
      "Iteration 449, loss = 0.28276283\n",
      "Iteration 450, loss = 0.28244056\n",
      "Iteration 451, loss = 0.28211902\n",
      "Iteration 452, loss = 0.28179822\n",
      "Iteration 453, loss = 0.28147826\n",
      "Iteration 454, loss = 0.28115878\n",
      "Iteration 455, loss = 0.28084009\n",
      "Iteration 456, loss = 0.28052217\n",
      "Iteration 457, loss = 0.28020483\n",
      "Iteration 458, loss = 0.27988826\n",
      "Iteration 459, loss = 0.27957234\n",
      "Iteration 460, loss = 0.27925717\n",
      "Iteration 461, loss = 0.27894272\n",
      "Iteration 462, loss = 0.27862887\n",
      "Iteration 463, loss = 0.27831573\n",
      "Iteration 464, loss = 0.27800333\n",
      "Iteration 465, loss = 0.27769158\n",
      "Iteration 466, loss = 0.27738048\n",
      "Iteration 467, loss = 0.27707009\n",
      "Iteration 468, loss = 0.27676042\n",
      "Iteration 469, loss = 0.27645137\n",
      "Iteration 470, loss = 0.27614296\n",
      "Iteration 471, loss = 0.27583528\n",
      "Iteration 472, loss = 0.27552830\n",
      "Iteration 473, loss = 0.27522196\n",
      "Iteration 474, loss = 0.27491623\n",
      "Iteration 475, loss = 0.27461121\n",
      "Iteration 476, loss = 0.27430695\n",
      "Iteration 477, loss = 0.27400315\n",
      "Iteration 478, loss = 0.27370011\n",
      "Iteration 479, loss = 0.27339780\n",
      "Iteration 480, loss = 0.27309604\n",
      "Iteration 481, loss = 0.27279496\n",
      "Iteration 482, loss = 0.27249457\n",
      "Iteration 483, loss = 0.27219490\n",
      "Iteration 484, loss = 0.27189576\n",
      "Iteration 485, loss = 0.27159733\n",
      "Iteration 486, loss = 0.27129949\n",
      "Iteration 487, loss = 0.27100242\n",
      "Iteration 488, loss = 0.27070590\n",
      "Iteration 489, loss = 0.27041006\n",
      "Iteration 490, loss = 0.27011478\n",
      "Iteration 491, loss = 0.26982028\n",
      "Iteration 492, loss = 0.26952628\n",
      "Iteration 493, loss = 0.26923295\n",
      "Iteration 494, loss = 0.26894031\n",
      "Iteration 495, loss = 0.26864827\n",
      "Iteration 496, loss = 0.26835682\n",
      "Iteration 497, loss = 0.26806602\n",
      "Iteration 498, loss = 0.26777592\n",
      "Iteration 499, loss = 0.26748638\n",
      "Iteration 500, loss = 0.26719745\n",
      "Iteration 501, loss = 0.26690915\n",
      "Iteration 502, loss = 0.26662160\n",
      "Iteration 503, loss = 0.26633447\n",
      "Iteration 504, loss = 0.26604806\n",
      "Iteration 505, loss = 0.26576227\n",
      "Iteration 506, loss = 0.26547711\n",
      "Iteration 507, loss = 0.26519252\n",
      "Iteration 508, loss = 0.26490853\n",
      "Iteration 509, loss = 0.26462517\n",
      "Iteration 510, loss = 0.26434252\n",
      "Iteration 511, loss = 0.26406039\n",
      "Iteration 512, loss = 0.26377886\n",
      "Iteration 513, loss = 0.26349804\n",
      "Iteration 514, loss = 0.26321772\n",
      "Iteration 515, loss = 0.26293803\n",
      "Iteration 516, loss = 0.26265895\n",
      "Iteration 517, loss = 0.26238053\n",
      "Iteration 518, loss = 0.26210263\n",
      "Iteration 519, loss = 0.26182535\n",
      "Iteration 520, loss = 0.26154861\n",
      "Iteration 521, loss = 0.26127261\n",
      "Iteration 522, loss = 0.26099704\n",
      "Iteration 523, loss = 0.26072211\n",
      "Iteration 524, loss = 0.26044784\n",
      "Iteration 525, loss = 0.26017412\n",
      "Iteration 526, loss = 0.25990094\n",
      "Iteration 527, loss = 0.25962837\n",
      "Iteration 528, loss = 0.25935636\n",
      "Iteration 529, loss = 0.25908498\n",
      "Iteration 530, loss = 0.25881413\n",
      "Iteration 531, loss = 0.25854386\n",
      "Iteration 532, loss = 0.25827430\n",
      "Iteration 533, loss = 0.25800508\n",
      "Iteration 534, loss = 0.25773658\n",
      "Iteration 535, loss = 0.25746869\n",
      "Iteration 536, loss = 0.25720133\n",
      "Iteration 537, loss = 0.25693450\n",
      "Iteration 538, loss = 0.25666825\n",
      "Iteration 539, loss = 0.25640252\n",
      "Iteration 540, loss = 0.25613746\n",
      "Iteration 541, loss = 0.25587289\n",
      "Iteration 542, loss = 0.25560890\n",
      "Iteration 543, loss = 0.25534555\n",
      "Iteration 544, loss = 0.25508264\n",
      "Iteration 545, loss = 0.25482030\n",
      "Iteration 546, loss = 0.25455862\n",
      "Iteration 547, loss = 0.25429742\n",
      "Iteration 548, loss = 0.25403676\n",
      "Iteration 549, loss = 0.25377666\n",
      "Iteration 550, loss = 0.25351721\n",
      "Iteration 551, loss = 0.25325822\n",
      "Iteration 552, loss = 0.25299978\n",
      "Iteration 553, loss = 0.25274189\n",
      "Iteration 554, loss = 0.25248461\n",
      "Iteration 555, loss = 0.25222784\n",
      "Iteration 556, loss = 0.25197158\n",
      "Iteration 557, loss = 0.25171584\n",
      "Iteration 558, loss = 0.25146078\n",
      "Iteration 559, loss = 0.25120614\n",
      "Iteration 560, loss = 0.25095205\n",
      "Iteration 561, loss = 0.25069857\n",
      "Iteration 562, loss = 0.25044555\n",
      "Iteration 563, loss = 0.25019306\n",
      "Iteration 564, loss = 0.24994116\n",
      "Iteration 565, loss = 0.24968979\n",
      "Iteration 566, loss = 0.24943896\n",
      "Iteration 567, loss = 0.24918860\n",
      "Iteration 568, loss = 0.24893879\n",
      "Iteration 569, loss = 0.24868955\n",
      "Iteration 570, loss = 0.24844081\n",
      "Iteration 571, loss = 0.24819256\n",
      "Iteration 572, loss = 0.24794493\n",
      "Iteration 573, loss = 0.24769773\n",
      "Iteration 574, loss = 0.24745106\n",
      "Iteration 575, loss = 0.24720497\n",
      "Iteration 576, loss = 0.24695938\n",
      "Iteration 577, loss = 0.24671429\n",
      "Iteration 578, loss = 0.24646969\n",
      "Iteration 579, loss = 0.24622572\n",
      "Iteration 580, loss = 0.24598213\n",
      "Iteration 581, loss = 0.24573911\n",
      "Iteration 582, loss = 0.24549659\n",
      "Iteration 583, loss = 0.24525465\n",
      "Iteration 584, loss = 0.24501316\n",
      "Iteration 585, loss = 0.24477218\n",
      "Iteration 586, loss = 0.24453169\n",
      "Iteration 587, loss = 0.24429179\n",
      "Iteration 588, loss = 0.24405232\n",
      "Iteration 589, loss = 0.24381337\n",
      "Iteration 590, loss = 0.24357499\n",
      "Iteration 591, loss = 0.24333700\n",
      "Iteration 592, loss = 0.24309955\n",
      "Iteration 593, loss = 0.24286266\n",
      "Iteration 594, loss = 0.24262623\n",
      "Iteration 595, loss = 0.24239031\n",
      "Iteration 596, loss = 0.24215489\n",
      "Iteration 597, loss = 0.24192001\n",
      "Iteration 598, loss = 0.24168563\n",
      "Iteration 599, loss = 0.24145176\n",
      "Iteration 600, loss = 0.24121840\n",
      "Iteration 601, loss = 0.24098552\n",
      "Iteration 602, loss = 0.24075310\n",
      "Iteration 603, loss = 0.24052115\n",
      "Iteration 604, loss = 0.24028979\n",
      "Iteration 605, loss = 0.24005888\n",
      "Iteration 606, loss = 0.23982839\n",
      "Iteration 607, loss = 0.23959842\n",
      "Iteration 608, loss = 0.23936905\n",
      "Iteration 609, loss = 0.23913997\n",
      "Iteration 610, loss = 0.23891144\n",
      "Iteration 611, loss = 0.23868349\n",
      "Iteration 612, loss = 0.23845592\n",
      "Iteration 613, loss = 0.23822883\n",
      "Iteration 614, loss = 0.23800221\n",
      "Iteration 615, loss = 0.23777617\n",
      "Iteration 616, loss = 0.23755053\n",
      "Iteration 617, loss = 0.23732534\n",
      "Iteration 618, loss = 0.23710062\n",
      "Iteration 619, loss = 0.23687653\n",
      "Iteration 620, loss = 0.23665267\n",
      "Iteration 621, loss = 0.23642945\n",
      "Iteration 622, loss = 0.23620665\n",
      "Iteration 623, loss = 0.23598429\n",
      "Iteration 624, loss = 0.23576243\n",
      "Iteration 625, loss = 0.23554100\n",
      "Iteration 626, loss = 0.23532011\n",
      "Iteration 627, loss = 0.23509961\n",
      "Iteration 628, loss = 0.23487956\n",
      "Iteration 629, loss = 0.23466007\n",
      "Iteration 630, loss = 0.23444093\n",
      "Iteration 631, loss = 0.23422228\n",
      "Iteration 632, loss = 0.23400412\n",
      "Iteration 633, loss = 0.23378639\n",
      "Iteration 634, loss = 0.23356913\n",
      "Iteration 635, loss = 0.23335226\n",
      "Iteration 636, loss = 0.23313598\n",
      "Iteration 637, loss = 0.23292000\n",
      "Iteration 638, loss = 0.23270455\n",
      "Iteration 639, loss = 0.23248957\n",
      "Iteration 640, loss = 0.23227499\n",
      "Iteration 641, loss = 0.23206088\n",
      "Iteration 642, loss = 0.23184718\n",
      "Iteration 643, loss = 0.23163406\n",
      "Iteration 644, loss = 0.23142121\n",
      "Iteration 645, loss = 0.23120888\n",
      "Iteration 646, loss = 0.23099702\n",
      "Iteration 647, loss = 0.23078557\n",
      "Iteration 648, loss = 0.23057455\n",
      "Iteration 649, loss = 0.23036395\n",
      "Iteration 650, loss = 0.23015392\n",
      "Iteration 651, loss = 0.22994414\n",
      "Iteration 652, loss = 0.22973490\n",
      "Iteration 653, loss = 0.22952613\n",
      "Iteration 654, loss = 0.22931770\n",
      "Iteration 655, loss = 0.22910972\n",
      "Iteration 656, loss = 0.22890223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 657, loss = 0.22869518\n",
      "Iteration 658, loss = 0.22848851\n",
      "Iteration 659, loss = 0.22828226\n",
      "Iteration 660, loss = 0.22807643\n",
      "Iteration 661, loss = 0.22787111\n",
      "Iteration 662, loss = 0.22766614\n",
      "Iteration 663, loss = 0.22746163\n",
      "Iteration 664, loss = 0.22725760\n",
      "Iteration 665, loss = 0.22705391\n",
      "Iteration 666, loss = 0.22685065\n",
      "Iteration 667, loss = 0.22664792\n",
      "Iteration 668, loss = 0.22644551\n",
      "Iteration 669, loss = 0.22624355\n",
      "Iteration 670, loss = 0.22604194\n",
      "Iteration 671, loss = 0.22584088\n",
      "Iteration 672, loss = 0.22564013\n",
      "Iteration 673, loss = 0.22543982\n",
      "Iteration 674, loss = 0.22524003\n",
      "Iteration 675, loss = 0.22504054\n",
      "Iteration 676, loss = 0.22484149\n",
      "Iteration 677, loss = 0.22464289\n",
      "Iteration 678, loss = 0.22444470\n",
      "Iteration 679, loss = 0.22424688\n",
      "Iteration 680, loss = 0.22404949\n",
      "Iteration 681, loss = 0.22385252\n",
      "Iteration 682, loss = 0.22365597\n",
      "Iteration 683, loss = 0.22345977\n",
      "Iteration 684, loss = 0.22326400\n",
      "Iteration 685, loss = 0.22306868\n",
      "Iteration 686, loss = 0.22287372\n",
      "Iteration 687, loss = 0.22267914\n",
      "Iteration 688, loss = 0.22248505\n",
      "Iteration 689, loss = 0.22229126\n",
      "Iteration 690, loss = 0.22209791\n",
      "Iteration 691, loss = 0.22190498\n",
      "Iteration 692, loss = 0.22171247\n",
      "Iteration 693, loss = 0.22152029\n",
      "Iteration 694, loss = 0.22132855\n",
      "Iteration 695, loss = 0.22113725\n",
      "Iteration 696, loss = 0.22094626\n",
      "Iteration 697, loss = 0.22075569\n",
      "Iteration 698, loss = 0.22056562\n",
      "Iteration 699, loss = 0.22037582\n",
      "Iteration 700, loss = 0.22018643\n",
      "Iteration 701, loss = 0.21999742\n",
      "Iteration 702, loss = 0.21980906\n",
      "Iteration 703, loss = 0.21962079\n",
      "Iteration 704, loss = 0.21943316\n",
      "Iteration 705, loss = 0.21924582\n",
      "Iteration 706, loss = 0.21905887\n",
      "Iteration 707, loss = 0.21887233\n",
      "Iteration 708, loss = 0.21868618\n",
      "Iteration 709, loss = 0.21850049\n",
      "Iteration 710, loss = 0.21831504\n",
      "Iteration 711, loss = 0.21813008\n",
      "Iteration 712, loss = 0.21794544\n",
      "Iteration 713, loss = 0.21776120\n",
      "Iteration 714, loss = 0.21757735\n",
      "Iteration 715, loss = 0.21739395\n",
      "Iteration 716, loss = 0.21721082\n",
      "Iteration 717, loss = 0.21702808\n",
      "Iteration 718, loss = 0.21684583\n",
      "Iteration 719, loss = 0.21666385\n",
      "Iteration 720, loss = 0.21648226\n",
      "Iteration 721, loss = 0.21630099\n",
      "Iteration 722, loss = 0.21612029\n",
      "Iteration 723, loss = 0.21593970\n",
      "Iteration 724, loss = 0.21575969\n",
      "Iteration 725, loss = 0.21558000\n",
      "Iteration 726, loss = 0.21540063\n",
      "Iteration 727, loss = 0.21522168\n",
      "Iteration 728, loss = 0.21504305\n",
      "Iteration 729, loss = 0.21486487\n",
      "Iteration 730, loss = 0.21468698\n",
      "Iteration 731, loss = 0.21450955\n",
      "Iteration 732, loss = 0.21433240\n",
      "Iteration 733, loss = 0.21415562\n",
      "Iteration 734, loss = 0.21397923\n",
      "Iteration 735, loss = 0.21380321\n",
      "Iteration 736, loss = 0.21362751\n",
      "Iteration 737, loss = 0.21345218\n",
      "Iteration 738, loss = 0.21327725\n",
      "Iteration 739, loss = 0.21310265\n",
      "Iteration 740, loss = 0.21292834\n",
      "Iteration 741, loss = 0.21275455\n",
      "Iteration 742, loss = 0.21258092\n",
      "Iteration 743, loss = 0.21240776\n",
      "Iteration 744, loss = 0.21223495\n",
      "Iteration 745, loss = 0.21206245\n",
      "Iteration 746, loss = 0.21189032\n",
      "Iteration 747, loss = 0.21171855\n",
      "Iteration 748, loss = 0.21154711\n",
      "Iteration 749, loss = 0.21137597\n",
      "Iteration 750, loss = 0.21120536\n",
      "Iteration 751, loss = 0.21103490\n",
      "Iteration 752, loss = 0.21086482\n",
      "Iteration 753, loss = 0.21069520\n",
      "Iteration 754, loss = 0.21052579\n",
      "Iteration 755, loss = 0.21035680\n",
      "Iteration 756, loss = 0.21018815\n",
      "Iteration 757, loss = 0.21001981\n",
      "Iteration 758, loss = 0.20985179\n",
      "Iteration 759, loss = 0.20968419\n",
      "Iteration 760, loss = 0.20951689\n",
      "Iteration 761, loss = 0.20934989\n",
      "Iteration 762, loss = 0.20918325\n",
      "Iteration 763, loss = 0.20901700\n",
      "Iteration 764, loss = 0.20885107\n",
      "Iteration 765, loss = 0.20868540\n",
      "Iteration 766, loss = 0.20852020\n",
      "Iteration 767, loss = 0.20835514\n",
      "Iteration 768, loss = 0.20819056\n",
      "Iteration 769, loss = 0.20802635\n",
      "Iteration 770, loss = 0.20786233\n",
      "Iteration 771, loss = 0.20769868\n",
      "Iteration 772, loss = 0.20753545\n",
      "Iteration 773, loss = 0.20737244\n",
      "Iteration 774, loss = 0.20720980\n",
      "Iteration 775, loss = 0.20704756\n",
      "Iteration 776, loss = 0.20688555\n",
      "Iteration 777, loss = 0.20672388\n",
      "Iteration 778, loss = 0.20656257\n",
      "Iteration 779, loss = 0.20640155\n",
      "Iteration 780, loss = 0.20624086\n",
      "Iteration 781, loss = 0.20608058\n",
      "Iteration 782, loss = 0.20592046\n",
      "Iteration 783, loss = 0.20576076\n",
      "Iteration 784, loss = 0.20560143\n",
      "Iteration 785, loss = 0.20544234\n",
      "Iteration 786, loss = 0.20528357\n",
      "Iteration 787, loss = 0.20512518\n",
      "Iteration 788, loss = 0.20496708\n",
      "Iteration 789, loss = 0.20480931\n",
      "Iteration 790, loss = 0.20465187\n",
      "Iteration 791, loss = 0.20449471\n",
      "Iteration 792, loss = 0.20433783\n",
      "Iteration 793, loss = 0.20418139\n",
      "Iteration 794, loss = 0.20402513\n",
      "Iteration 795, loss = 0.20386921\n",
      "Iteration 796, loss = 0.20371372\n",
      "Iteration 797, loss = 0.20355845\n",
      "Iteration 798, loss = 0.20340346\n",
      "Iteration 799, loss = 0.20324879\n",
      "Iteration 800, loss = 0.20309449\n",
      "Iteration 801, loss = 0.20294047\n",
      "Iteration 802, loss = 0.20278677\n",
      "Iteration 803, loss = 0.20263337\n",
      "Iteration 804, loss = 0.20248023\n",
      "Iteration 805, loss = 0.20232753\n",
      "Iteration 806, loss = 0.20217497\n",
      "Iteration 807, loss = 0.20202278\n",
      "Iteration 808, loss = 0.20187098\n",
      "Iteration 809, loss = 0.20171941\n",
      "Iteration 810, loss = 0.20156808\n",
      "Iteration 811, loss = 0.20141714\n",
      "Iteration 812, loss = 0.20126646\n",
      "Iteration 813, loss = 0.20111609\n",
      "Iteration 814, loss = 0.20096607\n",
      "Iteration 815, loss = 0.20081625\n",
      "Iteration 816, loss = 0.20066680\n",
      "Iteration 817, loss = 0.20051764\n",
      "Iteration 818, loss = 0.20036879\n",
      "Iteration 819, loss = 0.20022015\n",
      "Iteration 820, loss = 0.20007195\n",
      "Iteration 821, loss = 0.19992390\n",
      "Iteration 822, loss = 0.19977622\n",
      "Iteration 823, loss = 0.19962889\n",
      "Iteration 824, loss = 0.19948174\n",
      "Iteration 825, loss = 0.19933493\n",
      "Iteration 826, loss = 0.19918849\n",
      "Iteration 827, loss = 0.19904218\n",
      "Iteration 828, loss = 0.19889634\n",
      "Iteration 829, loss = 0.19875068\n",
      "Iteration 830, loss = 0.19860531\n",
      "Iteration 831, loss = 0.19846019\n",
      "Iteration 832, loss = 0.19831548\n",
      "Iteration 833, loss = 0.19817092\n",
      "Iteration 834, loss = 0.19802676\n",
      "Iteration 835, loss = 0.19788282\n",
      "Iteration 836, loss = 0.19773915\n",
      "Iteration 837, loss = 0.19759586\n",
      "Iteration 838, loss = 0.19745275\n",
      "Iteration 839, loss = 0.19730994\n",
      "Iteration 840, loss = 0.19716751\n",
      "Iteration 841, loss = 0.19702528\n",
      "Iteration 842, loss = 0.19688331\n",
      "Iteration 843, loss = 0.19674165\n",
      "Iteration 844, loss = 0.19660025\n",
      "Iteration 845, loss = 0.19645913\n",
      "Iteration 846, loss = 0.19631839\n",
      "Iteration 847, loss = 0.19617776\n",
      "Iteration 848, loss = 0.19603756\n",
      "Iteration 849, loss = 0.19589753\n",
      "Iteration 850, loss = 0.19575782\n",
      "Iteration 851, loss = 0.19561839\n",
      "Iteration 852, loss = 0.19547923\n",
      "Iteration 853, loss = 0.19534033\n",
      "Iteration 854, loss = 0.19520171\n",
      "Iteration 855, loss = 0.19506338\n",
      "Iteration 856, loss = 0.19492530\n",
      "Iteration 857, loss = 0.19478750\n",
      "Iteration 858, loss = 0.19464999\n",
      "Iteration 859, loss = 0.19451268\n",
      "Iteration 860, loss = 0.19437576\n",
      "Iteration 861, loss = 0.19423900\n",
      "Iteration 862, loss = 0.19410255\n",
      "Iteration 863, loss = 0.19396636\n",
      "Iteration 864, loss = 0.19383046\n",
      "Iteration 865, loss = 0.19369476\n",
      "Iteration 866, loss = 0.19355943\n",
      "Iteration 867, loss = 0.19342430\n",
      "Iteration 868, loss = 0.19328939\n",
      "Iteration 869, loss = 0.19315485\n",
      "Iteration 870, loss = 0.19302049\n",
      "Iteration 871, loss = 0.19288640\n",
      "Iteration 872, loss = 0.19275261\n",
      "Iteration 873, loss = 0.19261903\n",
      "Iteration 874, loss = 0.19248580\n",
      "Iteration 875, loss = 0.19235272\n",
      "Iteration 876, loss = 0.19221996\n",
      "Iteration 877, loss = 0.19208749\n",
      "Iteration 878, loss = 0.19195521\n",
      "Iteration 879, loss = 0.19182320\n",
      "Iteration 880, loss = 0.19169148\n",
      "Iteration 881, loss = 0.19156000\n",
      "Iteration 882, loss = 0.19142876\n",
      "Iteration 883, loss = 0.19129781\n",
      "Iteration 884, loss = 0.19116706\n",
      "Iteration 885, loss = 0.19103662\n",
      "Iteration 886, loss = 0.19090640\n",
      "Iteration 887, loss = 0.19077643\n",
      "Iteration 888, loss = 0.19064706\n",
      "Iteration 889, loss = 0.19051791\n",
      "Iteration 890, loss = 0.19038900\n",
      "Iteration 891, loss = 0.19026043\n",
      "Iteration 892, loss = 0.19013205\n",
      "Iteration 893, loss = 0.19000395\n",
      "Iteration 894, loss = 0.18987613\n",
      "Iteration 895, loss = 0.18974853\n",
      "Iteration 896, loss = 0.18962129\n",
      "Iteration 897, loss = 0.18949414\n",
      "Iteration 898, loss = 0.18936741\n",
      "Iteration 899, loss = 0.18924084\n",
      "Iteration 900, loss = 0.18911448\n",
      "Iteration 901, loss = 0.18898840\n",
      "Iteration 902, loss = 0.18886260\n",
      "Iteration 903, loss = 0.18873699\n",
      "Iteration 904, loss = 0.18861174\n",
      "Iteration 905, loss = 0.18848660\n",
      "Iteration 906, loss = 0.18836176\n",
      "Iteration 907, loss = 0.18823718\n",
      "Iteration 908, loss = 0.18811278\n",
      "Iteration 909, loss = 0.18798872\n",
      "Iteration 910, loss = 0.18786479\n",
      "Iteration 911, loss = 0.18774122\n",
      "Iteration 912, loss = 0.18761781\n",
      "Iteration 913, loss = 0.18749462\n",
      "Iteration 914, loss = 0.18737167\n",
      "Iteration 915, loss = 0.18724899\n",
      "Iteration 916, loss = 0.18712651\n",
      "Iteration 917, loss = 0.18700437\n",
      "Iteration 918, loss = 0.18688231\n",
      "Iteration 919, loss = 0.18676062\n",
      "Iteration 920, loss = 0.18663908\n",
      "Iteration 921, loss = 0.18651775\n",
      "Iteration 922, loss = 0.18639672\n",
      "Iteration 923, loss = 0.18627588\n",
      "Iteration 924, loss = 0.18615532\n",
      "Iteration 925, loss = 0.18603496\n",
      "Iteration 926, loss = 0.18591480\n",
      "Iteration 927, loss = 0.18579496\n",
      "Iteration 928, loss = 0.18567523\n",
      "Iteration 929, loss = 0.18555582\n",
      "Iteration 930, loss = 0.18543660\n",
      "Iteration 931, loss = 0.18531758\n",
      "Iteration 932, loss = 0.18519888\n",
      "Iteration 933, loss = 0.18508031\n",
      "Iteration 934, loss = 0.18496200\n",
      "Iteration 935, loss = 0.18484396\n",
      "Iteration 936, loss = 0.18472607\n",
      "Iteration 937, loss = 0.18460844\n",
      "Iteration 938, loss = 0.18449106\n",
      "Iteration 939, loss = 0.18437383\n",
      "Iteration 940, loss = 0.18425693\n",
      "Iteration 941, loss = 0.18414014\n",
      "Iteration 942, loss = 0.18402370\n",
      "Iteration 943, loss = 0.18390739\n",
      "Iteration 944, loss = 0.18379127\n",
      "Iteration 945, loss = 0.18367541\n",
      "Iteration 946, loss = 0.18355976\n",
      "Iteration 947, loss = 0.18344433\n",
      "Iteration 948, loss = 0.18332916\n",
      "Iteration 949, loss = 0.18321414\n",
      "Iteration 950, loss = 0.18309939\n",
      "Iteration 951, loss = 0.18298481\n",
      "Iteration 952, loss = 0.18287049\n",
      "Iteration 953, loss = 0.18275637\n",
      "Iteration 954, loss = 0.18264245\n",
      "Iteration 955, loss = 0.18252878\n",
      "Iteration 956, loss = 0.18241528\n",
      "Iteration 957, loss = 0.18230203\n",
      "Iteration 958, loss = 0.18218902\n",
      "Iteration 959, loss = 0.18207617\n",
      "Iteration 960, loss = 0.18196355\n",
      "Iteration 961, loss = 0.18185114\n",
      "Iteration 962, loss = 0.18173892\n",
      "Iteration 963, loss = 0.18162699\n",
      "Iteration 964, loss = 0.18151519\n",
      "Iteration 965, loss = 0.18140362\n",
      "Iteration 966, loss = 0.18129227\n",
      "Iteration 967, loss = 0.18118108\n",
      "Iteration 968, loss = 0.18107023\n",
      "Iteration 969, loss = 0.18095941\n",
      "Iteration 970, loss = 0.18084895\n",
      "Iteration 971, loss = 0.18073862\n",
      "Iteration 972, loss = 0.18062846\n",
      "Iteration 973, loss = 0.18051859\n",
      "Iteration 974, loss = 0.18040882\n",
      "Iteration 975, loss = 0.18029943\n",
      "Iteration 976, loss = 0.18019034\n",
      "Iteration 977, loss = 0.18008152\n",
      "Iteration 978, loss = 0.17997295\n",
      "Iteration 979, loss = 0.17986454\n",
      "Iteration 980, loss = 0.17975635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 981, loss = 0.17964837\n",
      "Iteration 982, loss = 0.17954059\n",
      "Iteration 983, loss = 0.17943307\n",
      "Iteration 984, loss = 0.17932569\n",
      "Iteration 985, loss = 0.17921857\n",
      "Iteration 986, loss = 0.17911159\n",
      "Iteration 987, loss = 0.17900487\n",
      "Iteration 988, loss = 0.17889832\n",
      "Iteration 989, loss = 0.17879196\n",
      "Iteration 990, loss = 0.17868586\n",
      "Iteration 991, loss = 0.17857989\n",
      "Iteration 992, loss = 0.17847416\n",
      "Iteration 993, loss = 0.17836859\n",
      "Iteration 994, loss = 0.17826326\n",
      "Iteration 995, loss = 0.17815811\n",
      "Iteration 996, loss = 0.17805312\n",
      "Iteration 997, loss = 0.17794845\n",
      "Iteration 998, loss = 0.17784384\n",
      "Iteration 999, loss = 0.17773953\n",
      "Iteration 1000, loss = 0.17763535\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 8.48599119\n",
      "Iteration 3, loss = 2.60225949\n",
      "Iteration 4, loss = 1.16398347\n",
      "Iteration 5, loss = 0.75822460\n",
      "Iteration 6, loss = 0.82416403\n",
      "Iteration 7, loss = 0.60039815\n",
      "Iteration 8, loss = 0.69049817\n",
      "Iteration 9, loss = 0.56169170\n",
      "Iteration 10, loss = 0.58872034\n",
      "Iteration 11, loss = 0.51183010\n",
      "Iteration 12, loss = 0.51362385\n",
      "Iteration 13, loss = 0.48888984\n",
      "Iteration 14, loss = 0.42390159\n",
      "Iteration 15, loss = 0.42824143\n",
      "Iteration 16, loss = 0.38775651\n",
      "Iteration 17, loss = 0.36001153\n",
      "Iteration 18, loss = 0.35942426\n",
      "Iteration 19, loss = 0.30903459\n",
      "Iteration 20, loss = 0.30931126\n",
      "Iteration 21, loss = 0.27665442\n",
      "Iteration 22, loss = 0.26147529\n",
      "Iteration 23, loss = 0.24863740\n",
      "Iteration 24, loss = 0.22517660\n",
      "Iteration 25, loss = 0.22364138\n",
      "Iteration 26, loss = 0.19774956\n",
      "Iteration 27, loss = 0.20196634\n",
      "Iteration 28, loss = 0.17895617\n",
      "Iteration 29, loss = 0.18339945\n",
      "Iteration 30, loss = 0.16619985\n",
      "Iteration 31, loss = 0.17042974\n",
      "Iteration 32, loss = 0.15656713\n",
      "Iteration 33, loss = 0.15944901\n",
      "Iteration 34, loss = 0.14988716\n",
      "Iteration 35, loss = 0.15154873\n",
      "Iteration 36, loss = 0.14383710\n",
      "Iteration 37, loss = 0.14507373\n",
      "Iteration 38, loss = 0.13961261\n",
      "Iteration 39, loss = 0.14027385\n",
      "Iteration 40, loss = 0.13546905\n",
      "Iteration 41, loss = 0.13626577\n",
      "Iteration 42, loss = 0.13238324\n",
      "Iteration 43, loss = 0.13317833\n",
      "Iteration 44, loss = 0.12932586\n",
      "Iteration 45, loss = 0.13026492\n",
      "Iteration 46, loss = 0.12694571\n",
      "Iteration 47, loss = 0.12797637\n",
      "Iteration 48, loss = 0.12485779\n",
      "Iteration 49, loss = 0.12549407\n",
      "Iteration 50, loss = 0.12312616\n",
      "Iteration 51, loss = 0.12332498\n",
      "Iteration 52, loss = 0.12184290\n",
      "Iteration 53, loss = 0.12115026\n",
      "Iteration 54, loss = 0.12061923\n",
      "Iteration 55, loss = 0.11918583\n",
      "Iteration 56, loss = 0.11925148\n",
      "Iteration 57, loss = 0.11769836\n",
      "Iteration 58, loss = 0.11772655\n",
      "Iteration 59, loss = 0.11661507\n",
      "Iteration 60, loss = 0.11599111\n",
      "Iteration 61, loss = 0.11561178\n",
      "Iteration 62, loss = 0.11448139\n",
      "Iteration 63, loss = 0.11432994\n",
      "Iteration 64, loss = 0.11343006\n",
      "Iteration 65, loss = 0.11288849\n",
      "Iteration 66, loss = 0.11252024\n",
      "Iteration 67, loss = 0.11165918\n",
      "Iteration 68, loss = 0.11138761\n",
      "Iteration 69, loss = 0.11078027\n",
      "Iteration 70, loss = 0.11012998\n",
      "Iteration 71, loss = 0.10985469\n",
      "Iteration 72, loss = 0.10918891\n",
      "Iteration 73, loss = 0.10868555\n",
      "Iteration 74, loss = 0.10835557\n",
      "Iteration 75, loss = 0.10773006\n",
      "Iteration 76, loss = 0.10728678\n",
      "Iteration 77, loss = 0.10693094\n",
      "Iteration 78, loss = 0.10636643\n",
      "Iteration 79, loss = 0.10593856\n",
      "Iteration 80, loss = 0.10558797\n",
      "Iteration 81, loss = 0.10508077\n",
      "Iteration 82, loss = 0.10464458\n",
      "Iteration 83, loss = 0.10430363\n",
      "Iteration 84, loss = 0.10385597\n",
      "Iteration 85, loss = 0.10340956\n",
      "Iteration 86, loss = 0.10306775\n",
      "Iteration 87, loss = 0.10268135\n",
      "Iteration 88, loss = 0.10224157\n",
      "Iteration 89, loss = 0.10187633\n",
      "Iteration 90, loss = 0.10153491\n",
      "Iteration 91, loss = 0.10113418\n",
      "Iteration 92, loss = 0.10074361\n",
      "Iteration 93, loss = 0.10040625\n",
      "Iteration 94, loss = 0.10006269\n",
      "Iteration 95, loss = 0.09968719\n",
      "Iteration 96, loss = 0.09932483\n",
      "Iteration 97, loss = 0.09899748\n",
      "Iteration 98, loss = 0.09866941\n",
      "Iteration 99, loss = 0.09831963\n",
      "Iteration 100, loss = 0.09797545\n",
      "Iteration 101, loss = 0.09765341\n",
      "Iteration 102, loss = 0.09734374\n",
      "Iteration 103, loss = 0.09702570\n",
      "Iteration 104, loss = 0.09669968\n",
      "Iteration 105, loss = 0.09638080\n",
      "Iteration 106, loss = 0.09607553\n",
      "Iteration 107, loss = 0.09577662\n",
      "Iteration 108, loss = 0.09547594\n",
      "Iteration 109, loss = 0.09517321\n",
      "Iteration 110, loss = 0.09487191\n",
      "Iteration 111, loss = 0.09457628\n",
      "Iteration 112, loss = 0.09430761\n",
      "Iteration 113, loss = 0.09400660\n",
      "Iteration 114, loss = 0.09372870\n",
      "Iteration 115, loss = 0.09345411\n",
      "Iteration 116, loss = 0.09318227\n",
      "Iteration 117, loss = 0.09291275\n",
      "Iteration 118, loss = 0.09264500\n",
      "Iteration 119, loss = 0.09237879\n",
      "Iteration 120, loss = 0.09211457\n",
      "Iteration 121, loss = 0.09185255\n",
      "Iteration 122, loss = 0.09159312\n",
      "Iteration 123, loss = 0.09133663\n",
      "Iteration 124, loss = 0.09108326\n",
      "Iteration 125, loss = 0.09083312\n",
      "Iteration 126, loss = 0.09058606\n",
      "Iteration 127, loss = 0.09034214"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 128, loss = 0.09010703\n",
      "Iteration 129, loss = 0.08986878\n",
      "Iteration 130, loss = 0.08963399\n",
      "Iteration 131, loss = 0.08940532\n",
      "Iteration 132, loss = 0.08919007\n",
      "Iteration 133, loss = 0.08901475\n",
      "Iteration 134, loss = 0.08897207\n",
      "Iteration 135, loss = 0.08933124\n",
      "Iteration 136, loss = 0.09138010\n",
      "Iteration 137, loss = 0.09697011\n",
      "Iteration 138, loss = 0.11936056\n",
      "Iteration 139, loss = 0.12880286\n",
      "Iteration 140, loss = 0.17711953\n",
      "Iteration 141, loss = 0.09751805\n",
      "Iteration 142, loss = 0.10190675\n",
      "Iteration 143, loss = 0.16120187\n",
      "Iteration 144, loss = 0.09288593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 8.63164286\n",
      "Iteration 3, loss = 2.48761382\n",
      "Iteration 4, loss = 1.58268781\n",
      "Iteration 5, loss = 0.81047525\n",
      "Iteration 6, loss = 0.80881919\n",
      "Iteration 7, loss = 0.69271222\n",
      "Iteration 8, loss = 0.63907295\n",
      "Iteration 9, loss = 0.57748550\n",
      "Iteration 10, loss = 0.55928681\n",
      "Iteration 11, loss = 0.53236987\n",
      "Iteration 12, loss = 0.51357496\n",
      "Iteration 13, loss = 0.47087308\n",
      "Iteration 14, loss = 0.44663580\n",
      "Iteration 15, loss = 0.40914971\n",
      "Iteration 16, loss = 0.38864373\n",
      "Iteration 17, loss = 0.35007806\n",
      "Iteration 18, loss = 0.32776539\n",
      "Iteration 19, loss = 0.29537032\n",
      "Iteration 20, loss = 0.27698835\n",
      "Iteration 21, loss = 0.24789305\n",
      "Iteration 22, loss = 0.22521872\n",
      "Iteration 23, loss = 0.21479536\n",
      "Iteration 24, loss = 0.19211188\n",
      "Iteration 25, loss = 0.17604237\n",
      "Iteration 26, loss = 0.17152221\n",
      "Iteration 27, loss = 0.15849514\n",
      "Iteration 28, loss = 0.14815137\n",
      "Iteration 29, loss = 0.14963677\n",
      "Iteration 30, loss = 0.14358896\n",
      "Iteration 31, loss = 0.13483052\n",
      "Iteration 32, loss = 0.13797167\n",
      "Iteration 33, loss = 0.13384985\n",
      "Iteration 34, loss = 0.12602436\n",
      "Iteration 35, loss = 0.12895776\n",
      "Iteration 36, loss = 0.12844137\n",
      "Iteration 37, loss = 0.12147697\n",
      "Iteration 38, loss = 0.12541062\n",
      "Iteration 39, loss = 0.12568379\n",
      "Iteration 40, loss = 0.11867301\n",
      "Iteration 41, loss = 0.12394076\n",
      "Iteration 42, loss = 0.12364618\n",
      "Iteration 43, loss = 0.11670488\n",
      "Iteration 44, loss = 0.12407439\n",
      "Iteration 45, loss = 0.12268005\n",
      "Iteration 46, loss = 0.11560056\n",
      "Iteration 47, loss = 0.12539277\n",
      "Iteration 48, loss = 0.12151306\n",
      "Iteration 49, loss = 0.11550048\n",
      "Iteration 50, loss = 0.12613053\n",
      "Iteration 51, loss = 0.11871288\n",
      "Iteration 52, loss = 0.11577352\n",
      "Iteration 53, loss = 0.12349809\n",
      "Iteration 54, loss = 0.11401848\n",
      "Iteration 55, loss = 0.11510398\n",
      "Iteration 56, loss = 0.11733401\n",
      "Iteration 57, loss = 0.11034903\n",
      "Iteration 58, loss = 0.11331906\n",
      "Iteration 59, loss = 0.11147447\n",
      "Iteration 60, loss = 0.10908850\n",
      "Iteration 61, loss = 0.11124241\n",
      "Iteration 62, loss = 0.10827977\n",
      "Iteration 63, loss = 0.10889597\n",
      "Iteration 64, loss = 0.10930192\n",
      "Iteration 65, loss = 0.10675350\n",
      "Iteration 66, loss = 0.10839227\n",
      "Iteration 67, loss = 0.10759963\n",
      "Iteration 68, loss = 0.10562424\n",
      "Iteration 69, loss = 0.10737872\n",
      "Iteration 70, loss = 0.10618501\n",
      "Iteration 71, loss = 0.10443433\n",
      "Iteration 72, loss = 0.10612759\n",
      "Iteration 73, loss = 0.10507966\n",
      "Iteration 74, loss = 0.10317289\n",
      "Iteration 75, loss = 0.10481714\n",
      "Iteration 76, loss = 0.10427076\n",
      "Iteration 77, loss = 0.10194626\n",
      "Iteration 78, loss = 0.10344426\n",
      "Iteration 79, loss = 0.10366125\n",
      "Iteration 80, loss = 0.10088209\n",
      "Iteration 81, loss = 0.10188880\n",
      "Iteration 82, loss = 0.10292528\n",
      "Iteration 83, loss = 0.10008014\n",
      "Iteration 84, loss = 0.10010142\n",
      "Iteration 85, loss = 0.10153639\n",
      "Iteration 86, loss = 0.09952449\n",
      "Iteration 87, loss = 0.09840564\n",
      "Iteration 88, loss = 0.09927335\n",
      "Iteration 89, loss = 0.09888983\n",
      "Iteration 90, loss = 0.09761073\n",
      "Iteration 91, loss = 0.09711141\n",
      "Iteration 92, loss = 0.09751671\n",
      "Iteration 93, loss = 0.09744961\n",
      "Iteration 94, loss = 0.09633797\n",
      "Iteration 95, loss = 0.09574905\n",
      "Iteration 96, loss = 0.09590834\n",
      "Iteration 97, loss = 0.09587643\n",
      "Iteration 98, loss = 0.09539693\n",
      "Iteration 99, loss = 0.09462472\n",
      "Iteration 100, loss = 0.09421502\n",
      "Iteration 101, loss = 0.09418384\n",
      "Iteration 102, loss = 0.09416338\n",
      "Iteration 103, loss = 0.09401385\n",
      "Iteration 104, loss = 0.09352682\n",
      "Iteration 105, loss = 0.09302918\n",
      "Iteration 106, loss = 0.09255154\n",
      "Iteration 107, loss = 0.09219068\n",
      "Iteration 108, loss = 0.09193218\n",
      "Iteration 109, loss = 0.09175170\n",
      "Iteration 110, loss = 0.09165892\n",
      "Iteration 111, loss = 0.09166511\n",
      "Iteration 112, loss = 0.09202912\n",
      "Iteration 113, loss = 0.09275548\n",
      "Iteration 114, loss = 0.09541400\n",
      "Iteration 115, loss = 0.09832263\n",
      "Iteration 116, loss = 0.10833073\n",
      "Iteration 117, loss = 0.10390841\n",
      "Iteration 118, loss = 0.10405902\n",
      "Iteration 119, loss = 0.09173487\n",
      "Iteration 120, loss = 0.08959358\n",
      "Iteration 121, loss = 0.09561233\n",
      "Iteration 122, loss = 0.09594287\n",
      "Iteration 123, loss = 0.09360941\n",
      "Iteration 124, loss = 0.08840321\n",
      "Iteration 125, loss = 0.09009093\n",
      "Iteration 126, loss = 0.09462835\n",
      "Iteration 127, loss = 0.09083149\n",
      "Iteration 128, loss = 0.08758720\n",
      "Iteration 129, loss = 0.08824841\n",
      "Iteration 130, loss = 0.09003457\n",
      "Iteration 131, loss = 0.08996377\n",
      "Iteration 132, loss = 0.08707092\n",
      "Iteration 133, loss = 0.08688411\n",
      "Iteration 134, loss = 0.08864547\n",
      "Iteration 135, loss = 0.08804762\n",
      "Iteration 136, loss = 0.08649488\n",
      "Iteration 137, loss = 0.08569052\n",
      "Iteration 138, loss = 0.08640760\n",
      "Iteration 139, loss = 0.08714701\n",
      "Iteration 140, loss = 0.08611643\n",
      "Iteration 141, loss = 0.08504258\n",
      "Iteration 142, loss = 0.08481076\n",
      "Iteration 143, loss = 0.08525514\n",
      "Iteration 144, loss = 0.08558613\n",
      "Iteration 145, loss = 0.08497941\n",
      "Iteration 146, loss = 0.08424405\n",
      "Iteration 147, loss = 0.08376687\n",
      "Iteration 148, loss = 0.08375647\n",
      "Iteration 149, loss = 0.08399213\n",
      "Iteration 150, loss = 0.08404670\n",
      "Iteration 151, loss = 0.08398882\n",
      "Iteration 152, loss = 0.08356393\n",
      "Iteration 153, loss = 0.08316559\n",
      "Iteration 154, loss = 0.08274183\n",
      "Iteration 155, loss = 0.08242440\n",
      "Iteration 156, loss = 0.08218492\n",
      "Iteration 157, loss = 0.08200991\n",
      "Iteration 158, loss = 0.08188364\n",
      "Iteration 159, loss = 0.08180656\n",
      "Iteration 160, loss = 0.08182541\n",
      "Iteration 161, loss = 0.08200545\n",
      "Iteration 162, loss = 0.08273735\n",
      "Iteration 163, loss = 0.08422981\n",
      "Iteration 164, loss = 0.08937164\n",
      "Iteration 165, loss = 0.09539576\n",
      "Iteration 166, loss = 0.11836887\n",
      "Iteration 167, loss = 0.10585499\n",
      "Iteration 168, loss = 0.10688523\n",
      "Iteration 169, loss = 0.08336976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 8.47521900\n",
      "Iteration 3, loss = 2.56026408\n",
      "Iteration 4, loss = 1.22072997\n",
      "Iteration 5, loss = 0.77801084\n",
      "Iteration 6, loss = 0.84203086\n",
      "Iteration 7, loss = 0.58301111\n",
      "Iteration 8, loss = 0.77113592\n",
      "Iteration 9, loss = 0.50291924\n",
      "Iteration 10, loss = 0.60023651\n",
      "Iteration 11, loss = 0.48567269\n",
      "Iteration 12, loss = 0.49430307\n",
      "Iteration 13, loss = 0.48213436\n",
      "Iteration 14, loss = 0.39997072\n",
      "Iteration 15, loss = 0.43416585\n",
      "Iteration 16, loss = 0.36584402\n",
      "Iteration 17, loss = 0.34221338\n",
      "Iteration 18, loss = 0.34535994\n",
      "Iteration 19, loss = 0.27719165\n",
      "Iteration 20, loss = 0.29768882\n",
      "Iteration 21, loss = 0.24251833\n",
      "Iteration 22, loss = 0.23984834\n",
      "Iteration 23, loss = 0.21677464\n",
      "Iteration 24, loss = 0.18831707\n",
      "Iteration 25, loss = 0.19475038\n",
      "Iteration 26, loss = 0.16016617\n",
      "Iteration 27, loss = 0.17088742\n",
      "Iteration 28, loss = 0.14830492\n",
      "Iteration 29, loss = 0.14507885\n",
      "Iteration 30, loss = 0.14412265\n",
      "Iteration 31, loss = 0.12747701\n",
      "Iteration 32, loss = 0.13564185\n",
      "Iteration 33, loss = 0.12238709\n",
      "Iteration 34, loss = 0.12289834\n",
      "Iteration 35, loss = 0.12274681\n",
      "Iteration 36, loss = 0.11384253\n",
      "Iteration 37, loss = 0.11835804\n",
      "Iteration 38, loss = 0.11076721\n",
      "Iteration 39, loss = 0.11230201\n",
      "Iteration 40, loss = 0.10980500\n",
      "Iteration 41, loss = 0.10645638\n",
      "Iteration 42, loss = 0.10849118\n",
      "Iteration 43, loss = 0.10350725\n",
      "Iteration 44, loss = 0.10531966\n",
      "Iteration 45, loss = 0.10298079\n",
      "Iteration 46, loss = 0.10150641\n",
      "Iteration 47, loss = 0.10233544\n",
      "Iteration 48, loss = 0.09935972\n",
      "Iteration 49, loss = 0.10031704\n",
      "Iteration 50, loss = 0.09867003\n",
      "Iteration 51, loss = 0.09773087\n",
      "Iteration 52, loss = 0.09789806\n",
      "Iteration 53, loss = 0.09593472\n",
      "Iteration 54, loss = 0.09637106\n",
      "Iteration 55, loss = 0.09488232\n",
      "Iteration 56, loss = 0.09450480\n",
      "Iteration 57, loss = 0.09395358\n",
      "Iteration 58, loss = 0.09287130\n",
      "Iteration 59, loss = 0.09283909\n",
      "Iteration 60, loss = 0.09152048\n",
      "Iteration 61, loss = 0.09155400\n",
      "Iteration 62, loss = 0.09036574\n",
      "Iteration 63, loss = 0.09025228\n",
      "Iteration 64, loss = 0.08930549\n",
      "Iteration 65, loss = 0.08900043\n",
      "Iteration 66, loss = 0.08817964\n",
      "Iteration 67, loss = 0.08783227\n",
      "Iteration 68, loss = 0.08709075\n",
      "Iteration 69, loss = 0.08673948\n",
      "Iteration 70, loss = 0.08597020\n",
      "Iteration 71, loss = 0.08568449\n",
      "Iteration 72, loss = 0.08491489\n",
      "Iteration 73, loss = 0.08466175\n",
      "Iteration 74, loss = 0.08398141\n",
      "Iteration 75, loss = 0.08353843\n",
      "Iteration 76, loss = 0.08315523\n",
      "Iteration 77, loss = 0.08249293\n",
      "Iteration 78, loss = 0.08220161\n",
      "Iteration 79, loss = 0.08175095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80, loss = 0.08115361\n",
      "Iteration 81, loss = 0.08085379\n",
      "Iteration 82, loss = 0.08052702\n",
      "Iteration 83, loss = 0.07998001\n",
      "Iteration 84, loss = 0.07953018\n",
      "Iteration 85, loss = 0.07923555\n",
      "Iteration 86, loss = 0.07894832\n",
      "Iteration 87, loss = 0.07858376\n",
      "Iteration 88, loss = 0.07816462\n",
      "Iteration 89, loss = 0.07775691\n",
      "Iteration 90, loss = 0.07737244\n",
      "Iteration 91, loss = 0.07701981\n",
      "Iteration 92, loss = 0.07670026\n",
      "Iteration 93, loss = 0.07641120\n",
      "Iteration 94, loss = 0.07622115\n",
      "Iteration 95, loss = 0.07632636\n",
      "Iteration 96, loss = 0.07739741\n",
      "Iteration 97, loss = 0.08146500\n",
      "Iteration 98, loss = 0.08129623\n",
      "Iteration 99, loss = 0.07653503\n",
      "Iteration 100, loss = 0.07542323\n",
      "Iteration 101, loss = 0.07838404\n",
      "Iteration 102, loss = 0.07503549\n",
      "Iteration 103, loss = 0.07497014\n",
      "Iteration 104, loss = 0.07615631\n",
      "Iteration 105, loss = 0.07329561\n",
      "Iteration 106, loss = 0.07470964\n",
      "Iteration 107, loss = 0.07352764\n",
      "Iteration 108, loss = 0.07297757\n",
      "Iteration 109, loss = 0.07361275\n",
      "Iteration 110, loss = 0.07206490\n",
      "Iteration 111, loss = 0.07298335\n",
      "Iteration 112, loss = 0.07196016\n",
      "Iteration 113, loss = 0.07176558\n",
      "Iteration 114, loss = 0.07189041\n",
      "Iteration 115, loss = 0.07085922\n",
      "Iteration 116, loss = 0.07129152\n",
      "Iteration 117, loss = 0.07052990\n",
      "Iteration 118, loss = 0.07043809\n",
      "Iteration 119, loss = 0.07037554\n",
      "Iteration 120, loss = 0.06972694\n",
      "Iteration 121, loss = 0.06994466\n",
      "Iteration 122, loss = 0.06941892\n",
      "Iteration 123, loss = 0.06920795\n",
      "Iteration 124, loss = 0.06919450\n",
      "Iteration 125, loss = 0.06865952\n",
      "Iteration 126, loss = 0.06867537\n",
      "Iteration 127, loss = 0.06844006\n",
      "Iteration 128, loss = 0.06804606\n",
      "Iteration 129, loss = 0.06806002\n",
      "Iteration 130, loss = 0.06778531\n",
      "Iteration 131, loss = 0.06744509\n",
      "Iteration 132, loss = 0.06740637\n",
      "Iteration 133, loss = 0.06719190\n",
      "Iteration 134, loss = 0.06685848\n",
      "Iteration 135, loss = 0.06673770\n",
      "Iteration 136, loss = 0.06659280\n",
      "Iteration 137, loss = 0.06630482\n",
      "Iteration 138, loss = 0.06611822\n",
      "Iteration 139, loss = 0.06600198\n",
      "Iteration 140, loss = 0.06577787\n",
      "Iteration 141, loss = 0.06554728\n",
      "Iteration 142, loss = 0.06541698\n",
      "Iteration 143, loss = 0.06525834\n",
      "Iteration 144, loss = 0.06503011\n",
      "Iteration 145, loss = 0.06485025\n",
      "Iteration 146, loss = 0.06471738\n",
      "Iteration 147, loss = 0.06454529\n",
      "Iteration 148, loss = 0.06434395\n",
      "Iteration 149, loss = 0.06417458\n",
      "Iteration 150, loss = 0.06403545\n",
      "Iteration 151, loss = 0.06387902\n",
      "Iteration 152, loss = 0.06369856\n",
      "Iteration 153, loss = 0.06352484\n",
      "Iteration 154, loss = 0.06337515\n",
      "Iteration 155, loss = 0.06323455\n",
      "Iteration 156, loss = 0.06308393\n",
      "Iteration 157, loss = 0.06292087\n",
      "Iteration 158, loss = 0.06275781\n",
      "Iteration 159, loss = 0.06260472\n",
      "Iteration 160, loss = 0.06246217\n",
      "Iteration 161, loss = 0.06232429\n",
      "Iteration 162, loss = 0.06218517\n",
      "Iteration 163, loss = 0.06204644\n",
      "Iteration 164, loss = 0.06190358\n",
      "Iteration 165, loss = 0.06176107\n",
      "Iteration 166, loss = 0.06162018\n",
      "Iteration 167, loss = 0.06148267\n",
      "Iteration 168, loss = 0.06134769\n",
      "Iteration 169, loss = 0.06121840\n",
      "Iteration 170, loss = 0.06109533\n",
      "Iteration 171, loss = 0.06098661\n",
      "Iteration 172, loss = 0.06090105\n",
      "Iteration 173, loss = 0.06087502\n",
      "Iteration 174, loss = 0.06095874\n",
      "Iteration 175, loss = 0.06137672\n",
      "Iteration 176, loss = 0.06234510\n",
      "Iteration 177, loss = 0.06516265\n",
      "Iteration 178, loss = 0.06886207\n",
      "Iteration 179, loss = 0.07721965\n",
      "Iteration 180, loss = 0.07471281\n",
      "Iteration 181, loss = 0.06651034\n",
      "Iteration 182, loss = 0.06035318\n",
      "Iteration 183, loss = 0.06949405\n",
      "Iteration 184, loss = 0.06794121\n",
      "Iteration 185, loss = 0.05976432\n",
      "Iteration 186, loss = 0.06918807\n",
      "Iteration 187, loss = 0.06526539\n",
      "Iteration 188, loss = 0.06106093\n",
      "Iteration 189, loss = 0.06793015\n",
      "Iteration 190, loss = 0.06032245\n",
      "Iteration 191, loss = 0.06317213\n",
      "Iteration 192, loss = 0.06222558\n",
      "Iteration 193, loss = 0.05941772\n",
      "Iteration 194, loss = 0.06250919\n",
      "Iteration 195, loss = 0.05860564\n",
      "Iteration 196, loss = 0.06198504\n",
      "Iteration 197, loss = 0.05924824\n",
      "Iteration 198, loss = 0.06006931\n",
      "Iteration 199, loss = 0.05973663\n",
      "Iteration 200, loss = 0.05856089\n",
      "Iteration 201, loss = 0.05981594\n",
      "Iteration 202, loss = 0.05798712\n",
      "Iteration 203, loss = 0.05952350\n",
      "Iteration 204, loss = 0.05796803\n",
      "Iteration 205, loss = 0.05861067\n",
      "Iteration 206, loss = 0.05810869\n",
      "Iteration 207, loss = 0.05774949\n",
      "Iteration 208, loss = 0.05816876\n",
      "Iteration 209, loss = 0.05725704\n",
      "Iteration 210, loss = 0.05796241\n",
      "Iteration 211, loss = 0.05711890\n",
      "Iteration 212, loss = 0.05738963\n",
      "Iteration 213, loss = 0.05714833\n",
      "Iteration 214, loss = 0.05682790\n",
      "Iteration 215, loss = 0.05708081\n",
      "Iteration 216, loss = 0.05652409\n",
      "Iteration 217, loss = 0.05679825\n",
      "Iteration 218, loss = 0.05646934\n",
      "Iteration 219, loss = 0.05635757\n",
      "Iteration 220, loss = 0.05642986\n",
      "Iteration 221, loss = 0.05605165\n",
      "Iteration 222, loss = 0.05618166\n",
      "Iteration 223, loss = 0.05596093\n",
      "Iteration 224, loss = 0.05584333\n",
      "Iteration 225, loss = 0.05588238\n",
      "Iteration 226, loss = 0.05562535\n",
      "Iteration 227, loss = 0.05564989\n",
      "Iteration 228, loss = 0.05555111\n",
      "Iteration 229, loss = 0.05538076\n",
      "Iteration 230, loss = 0.05541014\n",
      "Iteration 231, loss = 0.05525271\n",
      "Iteration 232, loss = 0.05515858\n",
      "Iteration 233, loss = 0.05514921\n",
      "Iteration 234, loss = 0.05499410\n",
      "Iteration 235, loss = 0.05493576\n",
      "Iteration 236, loss = 0.05489533\n",
      "Iteration 237, loss = 0.05476312\n",
      "Iteration 238, loss = 0.05471321\n",
      "Iteration 239, loss = 0.05465846\n",
      "Iteration 240, loss = 0.05454436\n",
      "Iteration 241, loss = 0.05449445\n",
      "Iteration 242, loss = 0.05443598\n",
      "Iteration 243, loss = 0.05433576\n",
      "Iteration 244, loss = 0.05428313\n",
      "Iteration 245, loss = 0.05422131\n",
      "Iteration 246, loss = 0.05413079\n",
      "Iteration 247, loss = 0.05407698\n",
      "Iteration 248, loss = 0.05402320\n",
      "Iteration 249, loss = 0.05394014\n",
      "Iteration 250, loss = 0.05387459\n",
      "Iteration 251, loss = 0.05382256\n",
      "Iteration 252, loss = 0.05375205\n",
      "Iteration 253, loss = 0.05368635\n",
      "Iteration 254, loss = 0.05363027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 8.62092997\n",
      "Iteration 3, loss = 2.58485209\n",
      "Iteration 4, loss = 1.23685213\n",
      "Iteration 5, loss = 0.83495112\n",
      "Iteration 6, loss = 0.88844412\n",
      "Iteration 7, loss = 0.63806050\n",
      "Iteration 8, loss = 0.64404551\n",
      "Iteration 9, loss = 0.58482290\n",
      "Iteration 10, loss = 0.52086290\n",
      "Iteration 11, loss = 0.52245671\n",
      "Iteration 12, loss = 0.48038665\n",
      "Iteration 13, loss = 0.46025534\n",
      "Iteration 14, loss = 0.39907421\n",
      "Iteration 15, loss = 0.38879286\n",
      "Iteration 16, loss = 0.34028840\n",
      "Iteration 17, loss = 0.33101977\n",
      "Iteration 18, loss = 0.28788143\n",
      "Iteration 19, loss = 0.27682016\n",
      "Iteration 20, loss = 0.23973128\n",
      "Iteration 21, loss = 0.23237050\n",
      "Iteration 22, loss = 0.20098895\n",
      "Iteration 23, loss = 0.19455671\n",
      "Iteration 24, loss = 0.17136979\n",
      "Iteration 25, loss = 0.16764760\n",
      "Iteration 26, loss = 0.14931881\n",
      "Iteration 27, loss = 0.14636731\n",
      "Iteration 28, loss = 0.13421720\n",
      "Iteration 29, loss = 0.13228312\n",
      "Iteration 30, loss = 0.12414957\n",
      "Iteration 31, loss = 0.12185580\n",
      "Iteration 32, loss = 0.11678426\n",
      "Iteration 33, loss = 0.11350000\n",
      "Iteration 34, loss = 0.11087387\n",
      "Iteration 35, loss = 0.10744882\n",
      "Iteration 36, loss = 0.10658257\n",
      "Iteration 37, loss = 0.10292745\n",
      "Iteration 38, loss = 0.10302945\n",
      "Iteration 39, loss = 0.09965758\n",
      "Iteration 40, loss = 0.09984436\n",
      "Iteration 41, loss = 0.09703526\n",
      "Iteration 42, loss = 0.09713987\n",
      "Iteration 43, loss = 0.09481654\n",
      "Iteration 44, loss = 0.09469033\n",
      "Iteration 45, loss = 0.09292471\n",
      "Iteration 46, loss = 0.09258119\n",
      "Iteration 47, loss = 0.09112943\n",
      "Iteration 48, loss = 0.09068659\n",
      "Iteration 49, loss = 0.08950100\n",
      "Iteration 50, loss = 0.08898823\n",
      "Iteration 51, loss = 0.08792046\n",
      "Iteration 52, loss = 0.08743496\n",
      "Iteration 53, loss = 0.08646584\n",
      "Iteration 54, loss = 0.08600860\n",
      "Iteration 55, loss = 0.08507029\n",
      "Iteration 56, loss = 0.08466965\n",
      "Iteration 57, loss = 0.08377674\n",
      "Iteration 58, loss = 0.08341912\n",
      "Iteration 59, loss = 0.08255181\n",
      "Iteration 60, loss = 0.08220660\n",
      "Iteration 61, loss = 0.08140853\n",
      "Iteration 62, loss = 0.08104909\n",
      "Iteration 63, loss = 0.08033606\n",
      "Iteration 64, loss = 0.07991372\n",
      "Iteration 65, loss = 0.07931669\n",
      "Iteration 66, loss = 0.07882172\n",
      "Iteration 67, loss = 0.07834012\n",
      "Iteration 68, loss = 0.07778206\n",
      "Iteration 69, loss = 0.07737967\n",
      "Iteration 70, loss = 0.07680557\n",
      "Iteration 71, loss = 0.07641768\n",
      "Iteration 72, loss = 0.07589191\n",
      "Iteration 73, loss = 0.07546780\n",
      "Iteration 74, loss = 0.07502430\n",
      "Iteration 75, loss = 0.07454638\n",
      "Iteration 76, loss = 0.07416033\n",
      "Iteration 77, loss = 0.07368170\n",
      "Iteration 78, loss = 0.07328923\n",
      "Iteration 79, loss = 0.07286091\n",
      "Iteration 80, loss = 0.07243308\n",
      "Iteration 81, loss = 0.07205206\n",
      "Iteration 82, loss = 0.07161910\n",
      "Iteration 83, loss = 0.07123934\n",
      "Iteration 84, loss = 0.07084413\n",
      "Iteration 85, loss = 0.07044106\n",
      "Iteration 86, loss = 0.07007740\n",
      "Iteration 87, loss = 0.06968373\n",
      "Iteration 88, loss = 0.06931006\n",
      "Iteration 89, loss = 0.06895043\n",
      "Iteration 90, loss = 0.06857104\n",
      "Iteration 91, loss = 0.06821662\n",
      "Iteration 92, loss = 0.06786357\n",
      "Iteration 93, loss = 0.06750156\n",
      "Iteration 94, loss = 0.06715965\n",
      "Iteration 95, loss = 0.06681623\n",
      "Iteration 96, loss = 0.06646926\n",
      "Iteration 97, loss = 0.06613772\n",
      "Iteration 98, loss = 0.06580553\n",
      "Iteration 99, loss = 0.06547205\n",
      "Iteration 100, loss = 0.06515090\n",
      "Iteration 101, loss = 0.06483171\n",
      "Iteration 102, loss = 0.06451123\n",
      "Iteration 103, loss = 0.06420014\n",
      "Iteration 104, loss = 0.06389331\n",
      "Iteration 105, loss = 0.06358558\n",
      "Iteration 106, loss = 0.06328402\n",
      "Iteration 107, loss = 0.06298856\n",
      "Iteration 108, loss = 0.06269337\n",
      "Iteration 109, loss = 0.06240138\n",
      "Iteration 110, loss = 0.06211579\n",
      "Iteration 111, loss = 0.06183279\n",
      "Iteration 112, loss = 0.06155108\n",
      "Iteration 113, loss = 0.06127471\n",
      "Iteration 114, loss = 0.06100314\n",
      "Iteration 115, loss = 0.06073404\n",
      "Iteration 116, loss = 0.06046739\n",
      "Iteration 117, loss = 0.06020497\n",
      "Iteration 118, loss = 0.05994636\n",
      "Iteration 119, loss = 0.05969017\n",
      "Iteration 120, loss = 0.05943669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 121, loss = 0.05918662\n",
      "Iteration 122, loss = 0.05893976\n",
      "Iteration 123, loss = 0.05869501\n",
      "Iteration 124, loss = 0.05845220\n",
      "Iteration 125, loss = 0.05821224\n",
      "Iteration 126, loss = 0.05797543\n",
      "Iteration 127, loss = 0.05774144\n",
      "Iteration 128, loss = 0.05750997\n",
      "Iteration 129, loss = 0.05728104\n",
      "Iteration 130, loss = 0.05705482\n",
      "Iteration 131, loss = 0.05683139\n",
      "Iteration 132, loss = 0.05661057\n",
      "Iteration 133, loss = 0.05639208\n",
      "Iteration 134, loss = 0.05617582\n",
      "Iteration 135, loss = 0.05596185\n",
      "Iteration 136, loss = 0.05575024\n",
      "Iteration 137, loss = 0.05554100\n",
      "Iteration 138, loss = 0.05533416\n",
      "Iteration 139, loss = 0.05512973\n",
      "Iteration 140, loss = 0.05492748\n",
      "Iteration 141, loss = 0.05472733\n",
      "Iteration 142, loss = 0.05452931\n",
      "Iteration 143, loss = 0.05433344\n",
      "Iteration 144, loss = 0.05413965\n",
      "Iteration 145, loss = 0.05394791\n",
      "Iteration 146, loss = 0.05375819\n",
      "Iteration 147, loss = 0.05357045\n",
      "Iteration 148, loss = 0.05338466\n",
      "Iteration 149, loss = 0.05320078\n",
      "Iteration 150, loss = 0.05301884\n",
      "Iteration 151, loss = 0.05283878\n",
      "Iteration 152, loss = 0.05266062\n",
      "Iteration 153, loss = 0.05248439\n",
      "Iteration 154, loss = 0.05231024\n",
      "Iteration 155, loss = 0.05213852\n",
      "Iteration 156, loss = 0.05196993\n",
      "Iteration 157, loss = 0.05180608\n",
      "Iteration 158, loss = 0.05165095\n",
      "Iteration 159, loss = 0.05151348\n",
      "Iteration 160, loss = 0.05141956\n",
      "Iteration 161, loss = 0.05141626\n",
      "Iteration 162, loss = 0.05169226\n",
      "Iteration 163, loss = 0.05244698\n",
      "Iteration 164, loss = 0.05516096\n",
      "Iteration 165, loss = 0.05889475\n",
      "Iteration 166, loss = 0.07192338\n",
      "Iteration 167, loss = 0.06707075\n",
      "Iteration 168, loss = 0.06840417\n",
      "Iteration 169, loss = 0.05236976\n",
      "Iteration 170, loss = 0.05152766\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 8.80107288\n",
      "Iteration 3, loss = 2.55850546\n",
      "Iteration 4, loss = 1.56288407\n",
      "Iteration 5, loss = 0.89429446\n",
      "Iteration 6, loss = 0.77944632\n",
      "Iteration 7, loss = 0.70546707\n",
      "Iteration 8, loss = 0.62035377\n",
      "Iteration 9, loss = 0.60915667\n",
      "Iteration 10, loss = 0.56287074\n",
      "Iteration 11, loss = 0.55608995\n",
      "Iteration 12, loss = 0.51334410\n",
      "Iteration 13, loss = 0.49604037\n",
      "Iteration 14, loss = 0.45114785\n",
      "Iteration 15, loss = 0.44431816\n",
      "Iteration 16, loss = 0.39959402\n",
      "Iteration 17, loss = 0.38250633\n",
      "Iteration 18, loss = 0.34593313\n",
      "Iteration 19, loss = 0.33248652\n",
      "Iteration 20, loss = 0.29697744\n",
      "Iteration 21, loss = 0.28616999\n",
      "Iteration 22, loss = 0.25261681\n",
      "Iteration 23, loss = 0.23889302\n",
      "Iteration 24, loss = 0.22040619\n",
      "Iteration 25, loss = 0.19946076\n",
      "Iteration 26, loss = 0.19429093\n",
      "Iteration 27, loss = 0.17817064\n",
      "Iteration 28, loss = 0.17025651\n",
      "Iteration 29, loss = 0.16635321\n",
      "Iteration 30, loss = 0.15430969\n",
      "Iteration 31, loss = 0.15196847\n",
      "Iteration 32, loss = 0.14954862\n",
      "Iteration 33, loss = 0.14119736\n",
      "Iteration 34, loss = 0.14180709\n",
      "Iteration 35, loss = 0.14304149\n",
      "Iteration 36, loss = 0.13657316\n",
      "Iteration 37, loss = 0.13215634\n",
      "Iteration 38, loss = 0.13371446\n",
      "Iteration 39, loss = 0.13083282\n",
      "Iteration 40, loss = 0.12802714\n",
      "Iteration 41, loss = 0.12886275\n",
      "Iteration 42, loss = 0.12707426\n",
      "Iteration 43, loss = 0.12483660\n",
      "Iteration 44, loss = 0.12496059\n",
      "Iteration 45, loss = 0.12432457\n",
      "Iteration 46, loss = 0.12254268\n",
      "Iteration 47, loss = 0.12174634\n",
      "Iteration 48, loss = 0.12178087\n",
      "Iteration 49, loss = 0.12100421\n",
      "Iteration 50, loss = 0.11954888\n",
      "Iteration 51, loss = 0.11903424\n",
      "Iteration 52, loss = 0.11900761\n",
      "Iteration 53, loss = 0.11819047\n",
      "Iteration 54, loss = 0.11707312\n",
      "Iteration 55, loss = 0.11637346\n",
      "Iteration 56, loss = 0.11613919\n",
      "Iteration 57, loss = 0.11582657\n",
      "Iteration 58, loss = 0.11501845\n",
      "Iteration 59, loss = 0.11416042\n",
      "Iteration 60, loss = 0.11355728\n",
      "Iteration 61, loss = 0.11322163\n",
      "Iteration 62, loss = 0.11292232\n",
      "Iteration 63, loss = 0.11240379\n",
      "Iteration 64, loss = 0.11176831\n",
      "Iteration 65, loss = 0.11108777\n",
      "Iteration 66, loss = 0.11051293\n",
      "Iteration 67, loss = 0.11006334\n",
      "Iteration 68, loss = 0.10969073\n",
      "Iteration 69, loss = 0.10934470\n",
      "Iteration 70, loss = 0.10895543\n",
      "Iteration 71, loss = 0.10856550\n",
      "Iteration 72, loss = 0.10810588\n",
      "Iteration 73, loss = 0.10767292\n",
      "Iteration 74, loss = 0.10720686\n",
      "Iteration 75, loss = 0.10679910\n",
      "Iteration 76, loss = 0.10639011\n",
      "Iteration 77, loss = 0.10608233\n",
      "Iteration 78, loss = 0.10580897\n",
      "Iteration 79, loss = 0.10579034\n",
      "Iteration 80, loss = 0.10588457\n",
      "Iteration 81, loss = 0.10679958\n",
      "Iteration 82, loss = 0.10774262\n",
      "Iteration 83, loss = 0.11106692\n",
      "Iteration 84, loss = 0.11151795\n",
      "Iteration 85, loss = 0.11474304\n",
      "Iteration 86, loss = 0.10881283\n",
      "Iteration 87, loss = 0.10462863\n",
      "Iteration 88, loss = 0.10142263\n",
      "Iteration 89, loss = 0.10197773\n",
      "Iteration 90, loss = 0.10487063\n",
      "Iteration 91, loss = 0.10536271\n",
      "Iteration 92, loss = 0.10495938\n",
      "Iteration 93, loss = 0.10126098\n",
      "Iteration 94, loss = 0.09931814\n",
      "Iteration 95, loss = 0.09967444\n",
      "Iteration 96, loss = 0.10097840\n",
      "Iteration 97, loss = 0.10207806\n",
      "Iteration 98, loss = 0.10046530\n",
      "Iteration 99, loss = 0.09872789\n",
      "Iteration 100, loss = 0.09745238\n",
      "Iteration 101, loss = 0.09745565\n",
      "Iteration 102, loss = 0.09821662\n",
      "Iteration 103, loss = 0.09845369\n",
      "Iteration 104, loss = 0.09828612\n",
      "Iteration 105, loss = 0.09705629\n",
      "Iteration 106, loss = 0.09601024\n",
      "Iteration 107, loss = 0.09538841\n",
      "Iteration 108, loss = 0.09531299\n",
      "Iteration 109, loss = 0.09555761\n",
      "Iteration 110, loss = 0.09568303\n",
      "Iteration 111, loss = 0.09573924\n",
      "Iteration 112, loss = 0.09525884\n",
      "Iteration 113, loss = 0.09477315\n",
      "Iteration 114, loss = 0.09408247\n",
      "Iteration 115, loss = 0.09352336\n",
      "Iteration 116, loss = 0.09306526\n",
      "Iteration 117, loss = 0.09274247\n",
      "Iteration 118, loss = 0.09252457\n",
      "Iteration 119, loss = 0.09238388\n",
      "Iteration 120, loss = 0.09232050\n",
      "Iteration 121, loss = 0.09230519\n",
      "Iteration 122, loss = 0.09246429\n",
      "Iteration 123, loss = 0.09268011\n",
      "Iteration 124, loss = 0.09346147\n",
      "Iteration 125, loss = 0.09421921\n",
      "Iteration 126, loss = 0.09664847\n",
      "Iteration 127, loss = 0.09788452\n",
      "Iteration 128, loss = 0.10268424\n",
      "Iteration 129, loss = 0.10097817\n",
      "Iteration 130, loss = 0.10247403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 4.39083993\n",
      "Iteration 3, loss = 1.89489041\n",
      "Iteration 4, loss = 1.82186881\n",
      "Iteration 5, loss = 0.88409022\n",
      "Iteration 6, loss = 0.78858701\n",
      "Iteration 7, loss = 0.68333428\n",
      "Iteration 8, loss = 0.58800165\n",
      "Iteration 9, loss = 0.51936083\n",
      "Iteration 10, loss = 0.52051493\n",
      "Iteration 11, loss = 0.99336514\n",
      "Iteration 12, loss = 1.74142114\n",
      "Iteration 13, loss = 0.88760903\n",
      "Iteration 14, loss = 0.57485548\n",
      "Iteration 15, loss = 0.55686279\n",
      "Iteration 16, loss = 0.46392314\n",
      "Iteration 17, loss = 0.44489883\n",
      "Iteration 18, loss = 0.42734409\n",
      "Iteration 19, loss = 0.40488538\n",
      "Iteration 20, loss = 0.37876511\n",
      "Iteration 21, loss = 0.35025195\n",
      "Iteration 22, loss = 0.32083972\n",
      "Iteration 23, loss = 0.29251211\n",
      "Iteration 24, loss = 0.26643520\n",
      "Iteration 25, loss = 0.24344732\n",
      "Iteration 26, loss = 0.22377981\n",
      "Iteration 27, loss = 0.20772218\n",
      "Iteration 28, loss = 0.19603849\n",
      "Iteration 29, loss = 0.20926262\n",
      "Iteration 30, loss = 0.54823162\n",
      "Iteration 31, loss = 3.16387486\n",
      "Iteration 32, loss = 3.22660028\n",
      "Iteration 33, loss = 0.40606235\n",
      "Iteration 34, loss = 0.56629922\n",
      "Iteration 35, loss = 0.57114698\n",
      "Iteration 36, loss = 0.48632282\n",
      "Iteration 37, loss = 0.39506840\n",
      "Iteration 38, loss = 0.40862521\n",
      "Iteration 39, loss = 0.38678243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 4.41618898\n",
      "Iteration 3, loss = 1.74759010\n",
      "Iteration 4, loss = 1.74195402\n",
      "Iteration 5, loss = 0.85961758\n",
      "Iteration 6, loss = 0.76691075\n",
      "Iteration 7, loss = 0.65340863\n",
      "Iteration 8, loss = 0.55549976\n",
      "Iteration 9, loss = 0.51263934\n",
      "Iteration 10, loss = 0.60408880\n",
      "Iteration 11, loss = 1.29894739\n",
      "Iteration 12, loss = 0.79676351\n",
      "Iteration 13, loss = 0.48908505\n",
      "Iteration 14, loss = 0.42298642\n",
      "Iteration 15, loss = 0.40790814\n",
      "Iteration 16, loss = 0.38551556\n",
      "Iteration 17, loss = 0.36252527\n",
      "Iteration 18, loss = 0.33664178\n",
      "Iteration 19, loss = 0.30910434\n",
      "Iteration 20, loss = 0.28173115\n",
      "Iteration 21, loss = 0.25664149\n",
      "Iteration 22, loss = 0.23951047\n",
      "Iteration 23, loss = 0.27179716\n",
      "Iteration 24, loss = 0.74533713\n",
      "Iteration 25, loss = 1.81108554\n",
      "Iteration 26, loss = 0.42158479\n",
      "Iteration 27, loss = 0.64896617\n",
      "Iteration 28, loss = 0.43008388\n",
      "Iteration 29, loss = 0.41800047\n",
      "Iteration 30, loss = 0.40271076\n",
      "Iteration 31, loss = 0.38843313\n",
      "Iteration 32, loss = 0.37445631\n",
      "Iteration 33, loss = 0.36018337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 4.36051732\n",
      "Iteration 3, loss = 1.87250406\n",
      "Iteration 4, loss = 1.81400858\n",
      "Iteration 5, loss = 0.88420955\n",
      "Iteration 6, loss = 0.78635344\n",
      "Iteration 7, loss = 0.67022016\n",
      "Iteration 8, loss = 0.57518139\n",
      "Iteration 9, loss = 0.47842070\n",
      "Iteration 10, loss = 0.44442426\n",
      "Iteration 11, loss = 0.70921992\n",
      "Iteration 12, loss = 1.83780504\n",
      "Iteration 13, loss = 0.69784760\n",
      "Iteration 14, loss = 0.46194162\n",
      "Iteration 15, loss = 0.40921263\n",
      "Iteration 16, loss = 0.38854977\n",
      "Iteration 17, loss = 0.37226674\n",
      "Iteration 18, loss = 0.35047936\n",
      "Iteration 19, loss = 0.32485444\n",
      "Iteration 20, loss = 0.29596539\n",
      "Iteration 21, loss = 0.26657287\n",
      "Iteration 22, loss = 0.23922073\n",
      "Iteration 23, loss = 0.21501982\n",
      "Iteration 24, loss = 0.19442407\n",
      "Iteration 25, loss = 0.17841589\n",
      "Iteration 26, loss = 0.17211130\n",
      "Iteration 27, loss = 0.23582623\n",
      "Iteration 28, loss = 1.08241684\n",
      "Iteration 29, loss = 3.81575699\n",
      "Iteration 30, loss = 2.10380653\n",
      "Iteration 31, loss = 0.48808555\n",
      "Iteration 32, loss = 0.55487240\n",
      "Iteration 33, loss = 0.41225628\n",
      "Iteration 34, loss = 0.43225483\n",
      "Iteration 35, loss = 0.43029372\n",
      "Iteration 36, loss = 0.42320334\n",
      "Iteration 37, loss = 0.41134300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 4.37516371\n",
      "Iteration 3, loss = 1.81350084\n",
      "Iteration 4, loss = 1.80657328\n",
      "Iteration 5, loss = 0.88251817\n",
      "Iteration 6, loss = 0.79720470\n",
      "Iteration 7, loss = 0.68077535\n",
      "Iteration 8, loss = 0.58062375\n",
      "Iteration 9, loss = 0.48762086\n",
      "Iteration 10, loss = 0.41881796\n",
      "Iteration 11, loss = 0.37335275\n",
      "Iteration 12, loss = 0.37390306\n",
      "Iteration 13, loss = 1.07047519\n",
      "Iteration 14, loss = 2.60845198\n",
      "Iteration 15, loss = 0.71479540\n",
      "Iteration 16, loss = 0.56698548\n",
      "Iteration 17, loss = 0.43879507\n",
      "Iteration 18, loss = 0.40572174\n",
      "Iteration 19, loss = 0.39478713\n",
      "Iteration 20, loss = 0.37291577\n",
      "Iteration 21, loss = 0.34300742\n",
      "Iteration 22, loss = 0.31520765\n",
      "Iteration 23, loss = 0.28719506\n",
      "Iteration 24, loss = 0.26059840\n",
      "Iteration 25, loss = 0.23705954\n",
      "Iteration 26, loss = 0.21688680\n",
      "Iteration 27, loss = 0.20787415\n",
      "Iteration 28, loss = 0.28815120\n",
      "Iteration 29, loss = 1.27741583\n",
      "Iteration 30, loss = 3.23924369\n",
      "Iteration 31, loss = 0.37230799\n",
      "Iteration 32, loss = 0.79189646\n",
      "Iteration 33, loss = 0.42824892\n",
      "Iteration 34, loss = 0.41390954\n",
      "Iteration 35, loss = 0.40721742\n",
      "Iteration 36, loss = 0.40004486\n",
      "Iteration 37, loss = 0.39185218\n",
      "Iteration 38, loss = 0.38268359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 4.47111157\n",
      "Iteration 3, loss = 1.79900008\n",
      "Iteration 4, loss = 1.76188637\n",
      "Iteration 5, loss = 0.86230030\n",
      "Iteration 6, loss = 0.78205127\n",
      "Iteration 7, loss = 0.66464241\n",
      "Iteration 8, loss = 0.57070453\n",
      "Iteration 9, loss = 0.48485765\n",
      "Iteration 10, loss = 0.43306198\n",
      "Iteration 11, loss = 0.45182582\n",
      "Iteration 12, loss = 1.26448691\n",
      "Iteration 13, loss = 1.72435996\n",
      "Iteration 14, loss = 0.57999866\n",
      "Iteration 15, loss = 0.49091486\n",
      "Iteration 16, loss = 0.45940213\n",
      "Iteration 17, loss = 0.42228627\n",
      "Iteration 18, loss = 0.38974609\n",
      "Iteration 19, loss = 0.36062385\n",
      "Iteration 20, loss = 0.33275881\n",
      "Iteration 21, loss = 0.30544828\n",
      "Iteration 22, loss = 0.27915797\n",
      "Iteration 23, loss = 0.25514931\n",
      "Iteration 24, loss = 0.23395640\n",
      "Iteration 25, loss = 0.21598206\n",
      "Iteration 26, loss = 0.20119654\n",
      "Iteration 27, loss = 0.19650503\n",
      "Iteration 28, loss = 0.28970331\n",
      "Iteration 29, loss = 1.58388562\n",
      "Iteration 30, loss = 4.11715260\n",
      "Iteration 31, loss = 0.43958814\n",
      "Iteration 32, loss = 1.11873898\n",
      "Iteration 33, loss = 0.67672991\n",
      "Iteration 34, loss = 0.48240117\n",
      "Iteration 35, loss = 0.47242234\n",
      "Iteration 36, loss = 0.45939748\n",
      "Iteration 37, loss = 0.44490255\n",
      "Iteration 38, loss = 0.42585180\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.22620921\n",
      "Iteration 3, loss = 1.22826617\n",
      "Iteration 4, loss = 1.09457918\n",
      "Iteration 5, loss = 0.90222519\n",
      "Iteration 6, loss = 0.80778746\n",
      "Iteration 7, loss = 0.78755742\n",
      "Iteration 8, loss = 0.75013240\n",
      "Iteration 9, loss = 0.68179250\n",
      "Iteration 10, loss = 0.61887536\n",
      "Iteration 11, loss = 0.58738415\n",
      "Iteration 12, loss = 0.57441758\n",
      "Iteration 13, loss = 0.55410832\n",
      "Iteration 14, loss = 0.52455796\n",
      "Iteration 15, loss = 0.50265363\n",
      "Iteration 16, loss = 0.49359897\n",
      "Iteration 17, loss = 0.47954609\n",
      "Iteration 18, loss = 0.45592868\n",
      "Iteration 19, loss = 0.43778253\n",
      "Iteration 20, loss = 0.42985611\n",
      "Iteration 21, loss = 0.42143300\n",
      "Iteration 22, loss = 0.40705102\n",
      "Iteration 23, loss = 0.39355801\n",
      "Iteration 24, loss = 0.38501423\n",
      "Iteration 25, loss = 0.37323462\n",
      "Iteration 26, loss = 0.35799682\n",
      "Iteration 27, loss = 0.34773484\n",
      "Iteration 28, loss = 0.33769501\n",
      "Iteration 29, loss = 0.32347130\n",
      "Iteration 30, loss = 0.31257947\n",
      "Iteration 31, loss = 0.30367421\n",
      "Iteration 32, loss = 0.29098365\n",
      "Iteration 33, loss = 0.28010486\n",
      "Iteration 34, loss = 0.27060760\n",
      "Iteration 35, loss = 0.25818859\n",
      "Iteration 36, loss = 0.24824913\n",
      "Iteration 37, loss = 0.23840543\n",
      "Iteration 38, loss = 0.22734101\n",
      "Iteration 39, loss = 0.21911291\n",
      "Iteration 40, loss = 0.20899325\n",
      "Iteration 41, loss = 0.20093635\n",
      "Iteration 42, loss = 0.19249101\n",
      "Iteration 43, loss = 0.18420831\n",
      "Iteration 44, loss = 0.17731656\n",
      "Iteration 45, loss = 0.16957140\n",
      "Iteration 46, loss = 0.16368701\n",
      "Iteration 47, loss = 0.15677165\n",
      "Iteration 48, loss = 0.15164611\n",
      "Iteration 49, loss = 0.14556939\n",
      "Iteration 50, loss = 0.14109296\n",
      "Iteration 51, loss = 0.13584926\n",
      "Iteration 52, loss = 0.13212408\n",
      "Iteration 53, loss = 0.12753642\n",
      "Iteration 54, loss = 0.12419379\n",
      "Iteration 55, loss = 0.12040522\n",
      "Iteration 56, loss = 0.11748761\n",
      "Iteration 57, loss = 0.11439747\n",
      "Iteration 58, loss = 0.11169860\n",
      "Iteration 59, loss = 0.10927176\n",
      "Iteration 60, loss = 0.10678826\n",
      "Iteration 61, loss = 0.10488589\n",
      "Iteration 62, loss = 0.10270988\n",
      "Iteration 63, loss = 0.10106052\n",
      "Iteration 64, loss = 0.09935640\n",
      "Iteration 65, loss = 0.09773889\n",
      "Iteration 66, loss = 0.09647038\n",
      "Iteration 67, loss = 0.09502527\n",
      "Iteration 68, loss = 0.09383880\n",
      "Iteration 69, loss = 0.09281679\n",
      "Iteration 70, loss = 0.09165870\n",
      "Iteration 71, loss = 0.09070409\n",
      "Iteration 72, loss = 0.08991321\n",
      "Iteration 73, loss = 0.08901916\n",
      "Iteration 74, loss = 0.08817531\n",
      "Iteration 75, loss = 0.08751573\n",
      "Iteration 76, loss = 0.08690015\n",
      "Iteration 77, loss = 0.08623022\n",
      "Iteration 78, loss = 0.08557723\n",
      "Iteration 79, loss = 0.08504108\n",
      "Iteration 80, loss = 0.08458923\n",
      "Iteration 81, loss = 0.08414003\n",
      "Iteration 82, loss = 0.08366276\n",
      "Iteration 83, loss = 0.08317475\n",
      "Iteration 84, loss = 0.08272914\n",
      "Iteration 85, loss = 0.08234999\n",
      "Iteration 86, loss = 0.08202639\n",
      "Iteration 87, loss = 0.08174469\n",
      "Iteration 88, loss = 0.08148214\n",
      "Iteration 89, loss = 0.08122729\n",
      "Iteration 90, loss = 0.08091264\n",
      "Iteration 91, loss = 0.08056572\n",
      "Iteration 92, loss = 0.08021969\n",
      "Iteration 93, loss = 0.07994086\n",
      "Iteration 94, loss = 0.07973249\n",
      "Iteration 95, loss = 0.07956688\n",
      "Iteration 96, loss = 0.07941955\n",
      "Iteration 97, loss = 0.07924410\n",
      "Iteration 98, loss = 0.07903472\n",
      "Iteration 99, loss = 0.07876408\n",
      "Iteration 100, loss = 0.07849939\n",
      "Iteration 101, loss = 0.07828142\n",
      "Iteration 102, loss = 0.07812502\n",
      "Iteration 103, loss = 0.07800993\n",
      "Iteration 104, loss = 0.07790724\n",
      "Iteration 105, loss = 0.07780196\n",
      "Iteration 106, loss = 0.07765861\n",
      "Iteration 107, loss = 0.07749019\n",
      "Iteration 108, loss = 0.07728301\n",
      "Iteration 109, loss = 0.07708259\n",
      "Iteration 110, loss = 0.07690746\n",
      "Iteration 111, loss = 0.07676585\n",
      "Iteration 112, loss = 0.07665301\n",
      "Iteration 113, loss = 0.07657578\n",
      "Iteration 114, loss = 0.07655986\n",
      "Iteration 115, loss = 0.07664045\n",
      "Iteration 116, loss = 0.07692217\n",
      "Iteration 117, loss = 0.07698199\n",
      "Iteration 118, loss = 0.07668588\n",
      "Iteration 119, loss = 0.07595778\n",
      "Iteration 120, loss = 0.07581757\n",
      "Iteration 121, loss = 0.07615713\n",
      "Iteration 122, loss = 0.07605244\n",
      "Iteration 123, loss = 0.07558778\n",
      "Iteration 124, loss = 0.07541038\n",
      "Iteration 125, loss = 0.07560287\n",
      "Iteration 126, loss = 0.07557242\n",
      "Iteration 127, loss = 0.07519397\n",
      "Iteration 128, loss = 0.07508947\n",
      "Iteration 129, loss = 0.07522318\n",
      "Iteration 130, loss = 0.07510005\n",
      "Iteration 131, loss = 0.07483872\n",
      "Iteration 132, loss = 0.07477701\n",
      "Iteration 133, loss = 0.07483368\n",
      "Iteration 134, loss = 0.07473911\n",
      "Iteration 135, loss = 0.07453474\n",
      "Iteration 136, loss = 0.07447205\n",
      "Iteration 137, loss = 0.07449822\n",
      "Iteration 138, loss = 0.07441311\n",
      "Iteration 139, loss = 0.07425788\n",
      "Iteration 140, loss = 0.07417338\n",
      "Iteration 141, loss = 0.07416654\n",
      "Iteration 142, loss = 0.07412676\n",
      "Iteration 143, loss = 0.07400960\n",
      "Iteration 144, loss = 0.07390137\n",
      "Iteration 145, loss = 0.07385316\n",
      "Iteration 146, loss = 0.07382811\n",
      "Iteration 147, loss = 0.07377242\n",
      "Iteration 148, loss = 0.07367688\n",
      "Iteration 149, loss = 0.07358589\n",
      "Iteration 150, loss = 0.07352561\n",
      "Iteration 151, loss = 0.07348719\n",
      "Iteration 152, loss = 0.07344642\n",
      "Iteration 153, loss = 0.07338511\n",
      "Iteration 154, loss = 0.07331001\n",
      "Iteration 155, loss = 0.07323189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.21265948\n",
      "Iteration 3, loss = 1.21364439\n",
      "Iteration 4, loss = 1.08280137\n",
      "Iteration 5, loss = 0.89682994\n",
      "Iteration 6, loss = 0.80906724\n",
      "Iteration 7, loss = 0.78975307\n",
      "Iteration 8, loss = 0.75041579\n",
      "Iteration 9, loss = 0.68692382\n",
      "Iteration 10, loss = 0.63059185\n",
      "Iteration 11, loss = 0.59961480\n",
      "Iteration 12, loss = 0.58125877\n",
      "Iteration 13, loss = 0.55631603\n",
      "Iteration 14, loss = 0.53080627\n",
      "Iteration 15, loss = 0.51501941\n",
      "Iteration 16, loss = 0.50439885\n",
      "Iteration 17, loss = 0.48307697\n",
      "Iteration 18, loss = 0.45885494\n",
      "Iteration 19, loss = 0.44318968\n",
      "Iteration 20, loss = 0.43430300\n",
      "Iteration 21, loss = 0.42273370\n",
      "Iteration 22, loss = 0.40823403\n",
      "Iteration 23, loss = 0.39718881\n",
      "Iteration 24, loss = 0.38785812\n",
      "Iteration 25, loss = 0.37364336\n",
      "Iteration 26, loss = 0.35888852\n",
      "Iteration 27, loss = 0.34845614\n",
      "Iteration 28, loss = 0.33621750\n",
      "Iteration 29, loss = 0.32267552\n",
      "Iteration 30, loss = 0.31237975\n",
      "Iteration 31, loss = 0.30148190\n",
      "Iteration 32, loss = 0.28864770\n",
      "Iteration 33, loss = 0.27836736\n",
      "Iteration 34, loss = 0.26719400\n",
      "Iteration 35, loss = 0.25482684\n",
      "Iteration 36, loss = 0.24493174\n",
      "Iteration 37, loss = 0.23379091\n",
      "Iteration 38, loss = 0.22341271\n",
      "Iteration 39, loss = 0.21398371\n",
      "Iteration 40, loss = 0.20377636\n",
      "Iteration 41, loss = 0.19542959\n",
      "Iteration 42, loss = 0.18612448\n",
      "Iteration 43, loss = 0.17841420\n",
      "Iteration 44, loss = 0.17031183\n",
      "Iteration 45, loss = 0.16312537\n",
      "Iteration 46, loss = 0.15615410\n",
      "Iteration 47, loss = 0.14975298\n",
      "Iteration 48, loss = 0.14376866\n",
      "Iteration 49, loss = 0.13821856\n",
      "Iteration 50, loss = 0.13295518\n",
      "Iteration 51, loss = 0.12827447\n",
      "Iteration 52, loss = 0.12370284\n",
      "Iteration 53, loss = 0.11977408\n",
      "Iteration 54, loss = 0.11575880\n",
      "Iteration 55, loss = 0.11246453\n",
      "Iteration 56, loss = 0.10903887\n",
      "Iteration 57, loss = 0.10615309\n",
      "Iteration 58, loss = 0.10341835\n",
      "Iteration 59, loss = 0.10077821\n",
      "Iteration 60, loss = 0.09862538\n",
      "Iteration 61, loss = 0.09642062\n",
      "Iteration 62, loss = 0.09445357\n",
      "Iteration 63, loss = 0.09278915\n",
      "Iteration 64, loss = 0.09108870\n",
      "Iteration 65, loss = 0.08956416\n",
      "Iteration 66, loss = 0.08830989\n",
      "Iteration 67, loss = 0.08706891\n",
      "Iteration 68, loss = 0.08585113\n",
      "Iteration 69, loss = 0.08484715\n",
      "Iteration 70, loss = 0.08398940\n",
      "Iteration 71, loss = 0.08312574\n",
      "Iteration 72, loss = 0.08227648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.08154153\n",
      "Iteration 74, loss = 0.08093884\n",
      "Iteration 75, loss = 0.08041006\n",
      "Iteration 76, loss = 0.07989142\n",
      "Iteration 77, loss = 0.07936669\n",
      "Iteration 78, loss = 0.07885284\n",
      "Iteration 79, loss = 0.07840470\n",
      "Iteration 80, loss = 0.07803400\n",
      "Iteration 81, loss = 0.07772316\n",
      "Iteration 82, loss = 0.07746003\n",
      "Iteration 83, loss = 0.07721585\n",
      "Iteration 84, loss = 0.07696539\n",
      "Iteration 85, loss = 0.07665703\n",
      "Iteration 86, loss = 0.07633683\n",
      "Iteration 87, loss = 0.07606405\n",
      "Iteration 88, loss = 0.07587072\n",
      "Iteration 89, loss = 0.07573281\n",
      "Iteration 90, loss = 0.07560768\n",
      "Iteration 91, loss = 0.07547098\n",
      "Iteration 92, loss = 0.07527414\n",
      "Iteration 93, loss = 0.07505347\n",
      "Iteration 94, loss = 0.07484555\n",
      "Iteration 95, loss = 0.07469346\n",
      "Iteration 96, loss = 0.07459043\n",
      "Iteration 97, loss = 0.07450764\n",
      "Iteration 98, loss = 0.07442429\n",
      "Iteration 99, loss = 0.07430426\n",
      "Iteration 100, loss = 0.07415594\n",
      "Iteration 101, loss = 0.07398470\n",
      "Iteration 102, loss = 0.07383203\n",
      "Iteration 103, loss = 0.07371639\n",
      "Iteration 104, loss = 0.07363369\n",
      "Iteration 105, loss = 0.07356797\n",
      "Iteration 106, loss = 0.07349923\n",
      "Iteration 107, loss = 0.07342357\n",
      "Iteration 108, loss = 0.07331933\n",
      "Iteration 109, loss = 0.07320155\n",
      "Iteration 110, loss = 0.07307164\n",
      "Iteration 111, loss = 0.07295321\n",
      "Iteration 112, loss = 0.07285282\n",
      "Iteration 113, loss = 0.07277137\n",
      "Iteration 114, loss = 0.07270296\n",
      "Iteration 115, loss = 0.07264102\n",
      "Iteration 116, loss = 0.07258559\n",
      "Iteration 117, loss = 0.07252679\n",
      "Iteration 118, loss = 0.07247010\n",
      "Iteration 119, loss = 0.07239565\n",
      "Iteration 120, loss = 0.07231412\n",
      "Iteration 121, loss = 0.07220916\n",
      "Iteration 122, loss = 0.07210209\n",
      "Iteration 123, loss = 0.07199642\n",
      "Iteration 124, loss = 0.07190539\n",
      "Iteration 125, loss = 0.07183044\n",
      "Iteration 126, loss = 0.07176876\n",
      "Iteration 127, loss = 0.07171638\n",
      "Iteration 128, loss = 0.07166963\n",
      "Iteration 129, loss = 0.07163065\n",
      "Iteration 130, loss = 0.07159219\n",
      "Iteration 131, loss = 0.07156154\n",
      "Iteration 132, loss = 0.07151494\n",
      "Iteration 133, loss = 0.07146256\n",
      "Iteration 134, loss = 0.07137171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.22005569\n",
      "Iteration 3, loss = 1.21559090\n",
      "Iteration 4, loss = 1.08868676\n",
      "Iteration 5, loss = 0.89972886\n",
      "Iteration 6, loss = 0.80172195\n",
      "Iteration 7, loss = 0.78376667\n",
      "Iteration 8, loss = 0.75133693\n",
      "Iteration 9, loss = 0.68606113\n",
      "Iteration 10, loss = 0.62221588\n",
      "Iteration 11, loss = 0.58906517\n",
      "Iteration 12, loss = 0.57557467\n",
      "Iteration 13, loss = 0.55595744\n",
      "Iteration 14, loss = 0.52609735\n",
      "Iteration 15, loss = 0.50184182\n",
      "Iteration 16, loss = 0.49127344\n",
      "Iteration 17, loss = 0.47858871\n",
      "Iteration 18, loss = 0.45474320\n",
      "Iteration 19, loss = 0.43315138\n",
      "Iteration 20, loss = 0.42238489\n",
      "Iteration 21, loss = 0.41433542\n",
      "Iteration 22, loss = 0.40047916\n",
      "Iteration 23, loss = 0.38447817\n",
      "Iteration 24, loss = 0.37381622\n",
      "Iteration 25, loss = 0.36270793\n",
      "Iteration 26, loss = 0.34691035\n",
      "Iteration 27, loss = 0.33230133\n",
      "Iteration 28, loss = 0.32117222\n",
      "Iteration 29, loss = 0.30809317\n",
      "Iteration 30, loss = 0.29433014\n",
      "Iteration 31, loss = 0.28387886\n",
      "Iteration 32, loss = 0.27171877\n",
      "Iteration 33, loss = 0.25855768\n",
      "Iteration 34, loss = 0.24792771\n",
      "Iteration 35, loss = 0.23558079\n",
      "Iteration 36, loss = 0.22339130\n",
      "Iteration 37, loss = 0.21323020\n",
      "Iteration 38, loss = 0.20158326\n",
      "Iteration 39, loss = 0.19176457\n",
      "Iteration 40, loss = 0.18179817\n",
      "Iteration 41, loss = 0.17215504\n",
      "Iteration 42, loss = 0.16371157\n",
      "Iteration 43, loss = 0.15469022\n",
      "Iteration 44, loss = 0.14734820\n",
      "Iteration 45, loss = 0.13933989\n",
      "Iteration 46, loss = 0.13281637\n",
      "Iteration 47, loss = 0.12599539\n",
      "Iteration 48, loss = 0.12032099\n",
      "Iteration 49, loss = 0.11461802\n",
      "Iteration 50, loss = 0.10964072\n",
      "Iteration 51, loss = 0.10485316\n",
      "Iteration 52, loss = 0.10055489\n",
      "Iteration 53, loss = 0.09658454\n",
      "Iteration 54, loss = 0.09291746\n",
      "Iteration 55, loss = 0.08959757\n",
      "Iteration 56, loss = 0.08653841\n",
      "Iteration 57, loss = 0.08377582\n",
      "Iteration 58, loss = 0.08125592\n",
      "Iteration 59, loss = 0.07892126\n",
      "Iteration 60, loss = 0.07687213\n",
      "Iteration 61, loss = 0.07491001\n",
      "Iteration 62, loss = 0.07323475\n",
      "Iteration 63, loss = 0.07158852\n",
      "Iteration 64, loss = 0.07023222\n",
      "Iteration 65, loss = 0.06885555\n",
      "Iteration 66, loss = 0.06773864\n",
      "Iteration 67, loss = 0.06661006\n",
      "Iteration 68, loss = 0.06566409\n",
      "Iteration 69, loss = 0.06476676\n",
      "Iteration 70, loss = 0.06393310\n",
      "Iteration 71, loss = 0.06323025\n",
      "Iteration 72, loss = 0.06250987\n",
      "Iteration 73, loss = 0.06190804\n",
      "Iteration 74, loss = 0.06135001\n",
      "Iteration 75, loss = 0.06078627\n",
      "Iteration 76, loss = 0.06032568\n",
      "Iteration 77, loss = 0.05989323\n",
      "Iteration 78, loss = 0.05944428\n",
      "Iteration 79, loss = 0.05906893\n",
      "Iteration 80, loss = 0.05874185\n",
      "Iteration 81, loss = 0.05839488\n",
      "Iteration 82, loss = 0.05806668\n",
      "Iteration 83, loss = 0.05779473\n",
      "Iteration 84, loss = 0.05754880\n",
      "Iteration 85, loss = 0.05729451\n",
      "Iteration 86, loss = 0.05703797\n",
      "Iteration 87, loss = 0.05681133\n",
      "Iteration 88, loss = 0.05661922\n",
      "Iteration 89, loss = 0.05644680\n",
      "Iteration 90, loss = 0.05627768\n",
      "Iteration 91, loss = 0.05609912\n",
      "Iteration 92, loss = 0.05592139\n",
      "Iteration 93, loss = 0.05576177\n",
      "Iteration 94, loss = 0.05562680\n",
      "Iteration 95, loss = 0.05551050\n",
      "Iteration 96, loss = 0.05540618\n",
      "Iteration 97, loss = 0.05530643\n",
      "Iteration 98, loss = 0.05519447\n",
      "Iteration 99, loss = 0.05506791\n",
      "Iteration 100, loss = 0.05494179\n",
      "Iteration 101, loss = 0.05483872\n",
      "Iteration 102, loss = 0.05475975\n",
      "Iteration 103, loss = 0.05469131\n",
      "Iteration 104, loss = 0.05461871\n",
      "Iteration 105, loss = 0.05452919\n",
      "Iteration 106, loss = 0.05442873\n",
      "Iteration 107, loss = 0.05433314\n",
      "Iteration 108, loss = 0.05425466\n",
      "Iteration 109, loss = 0.05419070\n",
      "Iteration 110, loss = 0.05413120\n",
      "Iteration 111, loss = 0.05406794\n",
      "Iteration 112, loss = 0.05399431\n",
      "Iteration 113, loss = 0.05391169\n",
      "Iteration 114, loss = 0.05382996\n",
      "Iteration 115, loss = 0.05375582\n",
      "Iteration 116, loss = 0.05369097\n",
      "Iteration 117, loss = 0.05363288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.21550273\n",
      "Iteration 3, loss = 1.21129926\n",
      "Iteration 4, loss = 1.07841007\n",
      "Iteration 5, loss = 0.89151813\n",
      "Iteration 6, loss = 0.80165782\n",
      "Iteration 7, loss = 0.78219728\n",
      "Iteration 8, loss = 0.74374576\n",
      "Iteration 9, loss = 0.67866538\n",
      "Iteration 10, loss = 0.62065792\n",
      "Iteration 11, loss = 0.58987387\n",
      "Iteration 12, loss = 0.57229443\n",
      "Iteration 13, loss = 0.54905778\n",
      "Iteration 14, loss = 0.52190804\n",
      "Iteration 15, loss = 0.50374776\n",
      "Iteration 16, loss = 0.49267119\n",
      "Iteration 17, loss = 0.47352135\n",
      "Iteration 18, loss = 0.44877555\n",
      "Iteration 19, loss = 0.43145556\n",
      "Iteration 20, loss = 0.42225339\n",
      "Iteration 21, loss = 0.41097431\n",
      "Iteration 22, loss = 0.39562415\n",
      "Iteration 23, loss = 0.38290793\n",
      "Iteration 24, loss = 0.37294887\n",
      "Iteration 25, loss = 0.35870798\n",
      "Iteration 26, loss = 0.34310848\n",
      "Iteration 27, loss = 0.33160198\n",
      "Iteration 28, loss = 0.31948588\n",
      "Iteration 29, loss = 0.30502828\n",
      "Iteration 30, loss = 0.29351237\n",
      "Iteration 31, loss = 0.28283318\n",
      "Iteration 32, loss = 0.26963048\n",
      "Iteration 33, loss = 0.25831160\n",
      "Iteration 34, loss = 0.24736054\n",
      "Iteration 35, loss = 0.23461704\n",
      "Iteration 36, loss = 0.22391782\n",
      "Iteration 37, loss = 0.21317244\n",
      "Iteration 38, loss = 0.20187545\n",
      "Iteration 39, loss = 0.19267791\n",
      "Iteration 40, loss = 0.18214904\n",
      "Iteration 41, loss = 0.17337766\n",
      "Iteration 42, loss = 0.16446633\n",
      "Iteration 43, loss = 0.15597542\n",
      "Iteration 44, loss = 0.14842592\n",
      "Iteration 45, loss = 0.14045965\n",
      "Iteration 46, loss = 0.13397999\n",
      "Iteration 47, loss = 0.12699173\n",
      "Iteration 48, loss = 0.12135090\n",
      "Iteration 49, loss = 0.11534824\n",
      "Iteration 50, loss = 0.11039120\n",
      "Iteration 51, loss = 0.10531768\n",
      "Iteration 52, loss = 0.10113828\n",
      "Iteration 53, loss = 0.09676043\n",
      "Iteration 54, loss = 0.09316298\n",
      "Iteration 55, loss = 0.08942726\n",
      "Iteration 56, loss = 0.08640674\n",
      "Iteration 57, loss = 0.08321575\n",
      "Iteration 58, loss = 0.08066004\n",
      "Iteration 59, loss = 0.07793377\n",
      "Iteration 60, loss = 0.07578005\n",
      "Iteration 61, loss = 0.07346314\n",
      "Iteration 62, loss = 0.07162938\n",
      "Iteration 63, loss = 0.06968881\n",
      "Iteration 64, loss = 0.06805871\n",
      "Iteration 65, loss = 0.06647964\n",
      "Iteration 66, loss = 0.06497659\n",
      "Iteration 67, loss = 0.06371636\n",
      "Iteration 68, loss = 0.06237153\n",
      "Iteration 69, loss = 0.06128457\n",
      "Iteration 70, loss = 0.06018877\n",
      "Iteration 71, loss = 0.05914802\n",
      "Iteration 72, loss = 0.05828447\n",
      "Iteration 73, loss = 0.05736235\n",
      "Iteration 74, loss = 0.05653848\n",
      "Iteration 75, loss = 0.05583052\n",
      "Iteration 76, loss = 0.05507619\n",
      "Iteration 77, loss = 0.05438938\n",
      "Iteration 78, loss = 0.05380734\n",
      "Iteration 79, loss = 0.05320729\n",
      "Iteration 80, loss = 0.05260794\n",
      "Iteration 81, loss = 0.05210229\n",
      "Iteration 82, loss = 0.05164259\n",
      "Iteration 83, loss = 0.05115578\n",
      "Iteration 84, loss = 0.05067835\n",
      "Iteration 85, loss = 0.05026187\n",
      "Iteration 86, loss = 0.04989056\n",
      "Iteration 87, loss = 0.04952452\n",
      "Iteration 88, loss = 0.04914530\n",
      "Iteration 89, loss = 0.04877400\n",
      "Iteration 90, loss = 0.04843205\n",
      "Iteration 91, loss = 0.04812305\n",
      "Iteration 92, loss = 0.04783877\n",
      "Iteration 93, loss = 0.04757209\n",
      "Iteration 94, loss = 0.04732013\n",
      "Iteration 95, loss = 0.04706526\n",
      "Iteration 96, loss = 0.04680379\n",
      "Iteration 97, loss = 0.04652698\n",
      "Iteration 98, loss = 0.04626033\n",
      "Iteration 99, loss = 0.04602099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, loss = 0.04581176\n",
      "Iteration 101, loss = 0.04562602\n",
      "Iteration 102, loss = 0.04545834\n",
      "Iteration 103, loss = 0.04530920\n",
      "Iteration 104, loss = 0.04515751\n",
      "Iteration 105, loss = 0.04499167\n",
      "Iteration 106, loss = 0.04477256\n",
      "Iteration 107, loss = 0.04454332\n",
      "Iteration 108, loss = 0.04434672\n",
      "Iteration 109, loss = 0.04420567\n",
      "Iteration 110, loss = 0.04410148\n",
      "Iteration 111, loss = 0.04399889\n",
      "Iteration 112, loss = 0.04387859\n",
      "Iteration 113, loss = 0.04371333\n",
      "Iteration 114, loss = 0.04353517\n",
      "Iteration 115, loss = 0.04337220\n",
      "Iteration 116, loss = 0.04324668\n",
      "Iteration 117, loss = 0.04315155\n",
      "Iteration 118, loss = 0.04306715\n",
      "Iteration 119, loss = 0.04298075\n",
      "Iteration 120, loss = 0.04287089\n",
      "Iteration 121, loss = 0.04274604\n",
      "Iteration 122, loss = 0.04260712\n",
      "Iteration 123, loss = 0.04247755\n",
      "Iteration 124, loss = 0.04236728\n",
      "Iteration 125, loss = 0.04227643\n",
      "Iteration 126, loss = 0.04219891\n",
      "Iteration 127, loss = 0.04212783\n",
      "Iteration 128, loss = 0.04206227\n",
      "Iteration 129, loss = 0.04199210\n",
      "Iteration 130, loss = 0.04192021\n",
      "Iteration 131, loss = 0.04182898\n",
      "Iteration 132, loss = 0.04172860\n",
      "Iteration 133, loss = 0.04161709\n",
      "Iteration 134, loss = 0.04151161\n",
      "Iteration 135, loss = 0.04141912\n",
      "Iteration 136, loss = 0.04134231\n",
      "Iteration 137, loss = 0.04127805\n",
      "Iteration 138, loss = 0.04122164\n",
      "Iteration 139, loss = 0.04117064\n",
      "Iteration 140, loss = 0.04112025\n",
      "Iteration 141, loss = 0.04107273\n",
      "Iteration 142, loss = 0.04101671\n",
      "Iteration 143, loss = 0.04095784\n",
      "Iteration 144, loss = 0.04088086\n",
      "Iteration 145, loss = 0.04079764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.22060516\n",
      "Iteration 3, loss = 1.22075016\n",
      "Iteration 4, loss = 1.07659161\n",
      "Iteration 5, loss = 0.88346167\n",
      "Iteration 6, loss = 0.79778681\n",
      "Iteration 7, loss = 0.77755050\n",
      "Iteration 8, loss = 0.73420261\n",
      "Iteration 9, loss = 0.66687033\n",
      "Iteration 10, loss = 0.61171224\n",
      "Iteration 11, loss = 0.58577516\n",
      "Iteration 12, loss = 0.56993295\n",
      "Iteration 13, loss = 0.54498869\n",
      "Iteration 14, loss = 0.51661834\n",
      "Iteration 15, loss = 0.50112076\n",
      "Iteration 16, loss = 0.49297861\n",
      "Iteration 17, loss = 0.47365242\n",
      "Iteration 18, loss = 0.44977715\n",
      "Iteration 19, loss = 0.43473388\n",
      "Iteration 20, loss = 0.42686930\n",
      "Iteration 21, loss = 0.41571375\n",
      "Iteration 22, loss = 0.40087608\n",
      "Iteration 23, loss = 0.38991747\n",
      "Iteration 24, loss = 0.38137514\n",
      "Iteration 25, loss = 0.36785462\n",
      "Iteration 26, loss = 0.35389823\n",
      "Iteration 27, loss = 0.34425243\n",
      "Iteration 28, loss = 0.33301004\n",
      "Iteration 29, loss = 0.31973533\n",
      "Iteration 30, loss = 0.30970989\n",
      "Iteration 31, loss = 0.29978998\n",
      "Iteration 32, loss = 0.28726561\n",
      "Iteration 33, loss = 0.27714723\n",
      "Iteration 34, loss = 0.26699079\n",
      "Iteration 35, loss = 0.25504395\n",
      "Iteration 36, loss = 0.24549917\n",
      "Iteration 37, loss = 0.23508299\n",
      "Iteration 38, loss = 0.22459978\n",
      "Iteration 39, loss = 0.21593155\n",
      "Iteration 40, loss = 0.20570132\n",
      "Iteration 41, loss = 0.19767067\n",
      "Iteration 42, loss = 0.18861993\n",
      "Iteration 43, loss = 0.18100227\n",
      "Iteration 44, loss = 0.17316822\n",
      "Iteration 45, loss = 0.16600680\n",
      "Iteration 46, loss = 0.15924038\n",
      "Iteration 47, loss = 0.15297378\n",
      "Iteration 48, loss = 0.14710638\n",
      "Iteration 49, loss = 0.14163036\n",
      "Iteration 50, loss = 0.13656913\n",
      "Iteration 51, loss = 0.13199760\n",
      "Iteration 52, loss = 0.12755503\n",
      "Iteration 53, loss = 0.12387512\n",
      "Iteration 54, loss = 0.11990718\n",
      "Iteration 55, loss = 0.11683178\n",
      "Iteration 56, loss = 0.11352680\n",
      "Iteration 57, loss = 0.11090464\n",
      "Iteration 58, loss = 0.10824249\n",
      "Iteration 59, loss = 0.10588508\n",
      "Iteration 60, loss = 0.10382614\n",
      "Iteration 61, loss = 0.10170563\n",
      "Iteration 62, loss = 0.10004255\n",
      "Iteration 63, loss = 0.09830798\n",
      "Iteration 64, loss = 0.09678405\n",
      "Iteration 65, loss = 0.09546977\n",
      "Iteration 66, loss = 0.09408198\n",
      "Iteration 67, loss = 0.09294693\n",
      "Iteration 68, loss = 0.09188117\n",
      "Iteration 69, loss = 0.09078386\n",
      "Iteration 70, loss = 0.08988385\n",
      "Iteration 71, loss = 0.08906683\n",
      "Iteration 72, loss = 0.08821800\n",
      "Iteration 73, loss = 0.08741484\n",
      "Iteration 74, loss = 0.08676250\n",
      "Iteration 75, loss = 0.08616508\n",
      "Iteration 76, loss = 0.08554349\n",
      "Iteration 77, loss = 0.08492222\n",
      "Iteration 78, loss = 0.08436691\n",
      "Iteration 79, loss = 0.08389004\n",
      "Iteration 80, loss = 0.08347760\n",
      "Iteration 81, loss = 0.08313007\n",
      "Iteration 82, loss = 0.08280729\n",
      "Iteration 83, loss = 0.08247175\n",
      "Iteration 84, loss = 0.08200404\n",
      "Iteration 85, loss = 0.08152641\n",
      "Iteration 86, loss = 0.08116510\n",
      "Iteration 87, loss = 0.08094092\n",
      "Iteration 88, loss = 0.08077127\n",
      "Iteration 89, loss = 0.08054081\n",
      "Iteration 90, loss = 0.08022795\n",
      "Iteration 91, loss = 0.07986329\n",
      "Iteration 92, loss = 0.07957198\n",
      "Iteration 93, loss = 0.07938465\n",
      "Iteration 94, loss = 0.07925221\n",
      "Iteration 95, loss = 0.07911561\n",
      "Iteration 96, loss = 0.07891191\n",
      "Iteration 97, loss = 0.07865959\n",
      "Iteration 98, loss = 0.07839365\n",
      "Iteration 99, loss = 0.07817881\n",
      "Iteration 100, loss = 0.07802694\n",
      "Iteration 101, loss = 0.07791520\n",
      "Iteration 102, loss = 0.07781997\n",
      "Iteration 103, loss = 0.07770704\n",
      "Iteration 104, loss = 0.07757211\n",
      "Iteration 105, loss = 0.07738380\n",
      "Iteration 106, loss = 0.07717801\n",
      "Iteration 107, loss = 0.07698050\n",
      "Iteration 108, loss = 0.07682088\n",
      "Iteration 109, loss = 0.07670124\n",
      "Iteration 110, loss = 0.07661100\n",
      "Iteration 111, loss = 0.07654137\n",
      "Iteration 112, loss = 0.07647905\n",
      "Iteration 113, loss = 0.07642692\n",
      "Iteration 114, loss = 0.07633739\n",
      "Iteration 115, loss = 0.07621902\n",
      "Iteration 116, loss = 0.07602978\n",
      "Iteration 117, loss = 0.07583393\n",
      "Iteration 118, loss = 0.07567036\n",
      "Iteration 119, loss = 0.07556681\n",
      "Iteration 120, loss = 0.07551089\n",
      "Iteration 121, loss = 0.07547290\n",
      "Iteration 122, loss = 0.07543440\n",
      "Iteration 123, loss = 0.07536009\n",
      "Iteration 124, loss = 0.07526084\n",
      "Iteration 125, loss = 0.07511738\n",
      "Iteration 126, loss = 0.07497168\n",
      "Iteration 127, loss = 0.07484008\n",
      "Iteration 128, loss = 0.07474125\n",
      "Iteration 129, loss = 0.07467030\n",
      "Iteration 130, loss = 0.07461745\n",
      "Iteration 131, loss = 0.07457596\n",
      "Iteration 132, loss = 0.07453953\n",
      "Iteration 133, loss = 0.07450184\n",
      "Iteration 134, loss = 0.07445173\n",
      "Iteration 135, loss = 0.07439109\n",
      "Iteration 136, loss = 0.07429251\n",
      "Iteration 137, loss = 0.07418026\n",
      "Iteration 138, loss = 0.07405343\n",
      "Iteration 139, loss = 0.07393890\n",
      "Iteration 140, loss = 0.07384554\n",
      "Iteration 141, loss = 0.07377597\n",
      "Iteration 142, loss = 0.07372468\n",
      "Iteration 143, loss = 0.07368491\n",
      "Iteration 144, loss = 0.07365616\n",
      "Iteration 145, loss = 0.07363438\n",
      "Iteration 146, loss = 0.07362992\n",
      "Iteration 147, loss = 0.07360640\n",
      "Iteration 148, loss = 0.07359044\n",
      "Iteration 149, loss = 0.07352424\n",
      "Iteration 150, loss = 0.07343102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.36077107\n",
      "Iteration 3, loss = 1.21525789\n",
      "Iteration 4, loss = 1.17954466\n",
      "Iteration 5, loss = 1.08999806\n",
      "Iteration 6, loss = 0.98318756\n",
      "Iteration 7, loss = 0.90460028\n",
      "Iteration 8, loss = 0.85120918\n",
      "Iteration 9, loss = 0.80619209\n",
      "Iteration 10, loss = 0.76375149\n",
      "Iteration 11, loss = 0.72490423\n",
      "Iteration 12, loss = 0.68872095\n",
      "Iteration 13, loss = 0.65702137\n",
      "Iteration 14, loss = 0.62846302\n",
      "Iteration 15, loss = 0.60234698\n",
      "Iteration 16, loss = 0.57916056\n",
      "Iteration 17, loss = 0.55938091\n",
      "Iteration 18, loss = 0.54198706\n",
      "Iteration 19, loss = 0.52642743\n",
      "Iteration 20, loss = 0.51245960\n",
      "Iteration 21, loss = 0.49979672\n",
      "Iteration 22, loss = 0.48823170\n",
      "Iteration 23, loss = 0.47759609\n",
      "Iteration 24, loss = 0.46774718\n",
      "Iteration 25, loss = 0.45853649\n",
      "Iteration 26, loss = 0.44984395\n",
      "Iteration 27, loss = 0.44156738\n",
      "Iteration 28, loss = 0.43374103\n",
      "Iteration 29, loss = 0.42639265\n",
      "Iteration 30, loss = 0.41947509\n",
      "Iteration 31, loss = 0.41289507\n",
      "Iteration 32, loss = 0.40655816\n",
      "Iteration 33, loss = 0.40045143\n",
      "Iteration 34, loss = 0.39453810\n",
      "Iteration 35, loss = 0.38880429\n",
      "Iteration 36, loss = 0.38322760\n",
      "Iteration 37, loss = 0.37779697\n",
      "Iteration 38, loss = 0.37249899\n",
      "Iteration 39, loss = 0.36732821\n",
      "Iteration 40, loss = 0.36228756\n",
      "Iteration 41, loss = 0.35736579\n",
      "Iteration 42, loss = 0.35256087\n",
      "Iteration 43, loss = 0.34786286\n",
      "Iteration 44, loss = 0.34326736\n",
      "Iteration 45, loss = 0.33877878\n",
      "Iteration 46, loss = 0.33438472\n",
      "Iteration 47, loss = 0.33008680\n",
      "Iteration 48, loss = 0.32588298\n",
      "Iteration 49, loss = 0.32177193\n",
      "Iteration 50, loss = 0.31775566\n",
      "Iteration 51, loss = 0.31382000\n",
      "Iteration 52, loss = 0.30996951\n",
      "Iteration 53, loss = 0.30620485\n",
      "Iteration 54, loss = 0.30251947\n",
      "Iteration 55, loss = 0.29890466\n",
      "Iteration 56, loss = 0.29535276\n",
      "Iteration 57, loss = 0.29187221\n",
      "Iteration 58, loss = 0.28846009\n",
      "Iteration 59, loss = 0.28510632\n",
      "Iteration 60, loss = 0.28178098\n",
      "Iteration 61, loss = 0.27844625\n",
      "Iteration 62, loss = 0.27503109\n",
      "Iteration 63, loss = 0.27150019\n",
      "Iteration 64, loss = 0.26797202\n",
      "Iteration 65, loss = 0.26460850\n",
      "Iteration 66, loss = 0.26156779\n",
      "Iteration 67, loss = 0.25870140\n",
      "Iteration 68, loss = 0.25594523\n",
      "Iteration 69, loss = 0.25329656\n",
      "Iteration 70, loss = 0.25071338\n",
      "Iteration 71, loss = 0.24818860\n",
      "Iteration 72, loss = 0.24571793\n",
      "Iteration 73, loss = 0.24329918\n",
      "Iteration 74, loss = 0.24093117\n",
      "Iteration 75, loss = 0.23861411\n",
      "Iteration 76, loss = 0.23634578\n",
      "Iteration 77, loss = 0.23412510\n",
      "Iteration 78, loss = 0.23194964\n",
      "Iteration 79, loss = 0.22981930\n",
      "Iteration 80, loss = 0.22773311\n",
      "Iteration 81, loss = 0.22569008\n",
      "Iteration 82, loss = 0.22369080\n",
      "Iteration 83, loss = 0.22173227\n",
      "Iteration 84, loss = 0.21981548\n",
      "Iteration 85, loss = 0.21793522\n",
      "Iteration 86, loss = 0.21609438\n",
      "Iteration 87, loss = 0.21429049\n",
      "Iteration 88, loss = 0.21252359\n",
      "Iteration 89, loss = 0.21079105\n",
      "Iteration 90, loss = 0.20909243\n",
      "Iteration 91, loss = 0.20742499\n",
      "Iteration 92, loss = 0.20578924\n",
      "Iteration 93, loss = 0.20418664\n",
      "Iteration 94, loss = 0.20261384\n",
      "Iteration 95, loss = 0.20107164\n",
      "Iteration 96, loss = 0.19955985\n",
      "Iteration 97, loss = 0.19807616\n",
      "Iteration 98, loss = 0.19661929\n",
      "Iteration 99, loss = 0.19519008\n",
      "Iteration 100, loss = 0.19378842\n",
      "Iteration 101, loss = 0.19241374\n",
      "Iteration 102, loss = 0.19106482\n",
      "Iteration 103, loss = 0.18974047\n",
      "Iteration 104, loss = 0.18844087\n",
      "Iteration 105, loss = 0.18716403\n",
      "Iteration 106, loss = 0.18591394\n",
      "Iteration 107, loss = 0.18468895\n",
      "Iteration 108, loss = 0.18348801\n",
      "Iteration 109, loss = 0.18230822\n",
      "Iteration 110, loss = 0.18114991\n",
      "Iteration 111, loss = 0.18001575\n",
      "Iteration 112, loss = 0.17891232\n",
      "Iteration 113, loss = 0.17783610\n",
      "Iteration 114, loss = 0.17678243\n",
      "Iteration 115, loss = 0.17574904\n",
      "Iteration 116, loss = 0.17473734\n",
      "Iteration 117, loss = 0.17374419\n",
      "Iteration 118, loss = 0.17276854\n",
      "Iteration 119, loss = 0.17181061\n",
      "Iteration 120, loss = 0.17086977\n",
      "Iteration 121, loss = 0.16994604\n",
      "Iteration 122, loss = 0.16903843\n",
      "Iteration 123, loss = 0.16814712\n",
      "Iteration 124, loss = 0.16727168\n",
      "Iteration 125, loss = 0.16641180\n",
      "Iteration 126, loss = 0.16556718\n",
      "Iteration 127, loss = 0.16473756\n",
      "Iteration 128, loss = 0.16392253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 129, loss = 0.16312178\n",
      "Iteration 130, loss = 0.16233498\n",
      "Iteration 131, loss = 0.16156174\n",
      "Iteration 132, loss = 0.16080175\n",
      "Iteration 133, loss = 0.16005540\n",
      "Iteration 134, loss = 0.15932190\n",
      "Iteration 135, loss = 0.15860166\n",
      "Iteration 136, loss = 0.15789372\n",
      "Iteration 137, loss = 0.15719808\n",
      "Iteration 138, loss = 0.15651488\n",
      "Iteration 139, loss = 0.15584247\n",
      "Iteration 140, loss = 0.15518063\n",
      "Iteration 141, loss = 0.15452920\n",
      "Iteration 142, loss = 0.15388838\n",
      "Iteration 143, loss = 0.15325854\n",
      "Iteration 144, loss = 0.15263882\n",
      "Iteration 145, loss = 0.15202930\n",
      "Iteration 146, loss = 0.15143145\n",
      "Iteration 147, loss = 0.15084384\n",
      "Iteration 148, loss = 0.15026584\n",
      "Iteration 149, loss = 0.14969678\n",
      "Iteration 150, loss = 0.14913654\n",
      "Iteration 151, loss = 0.14858511\n",
      "Iteration 152, loss = 0.14804304\n",
      "Iteration 153, loss = 0.14750978\n",
      "Iteration 154, loss = 0.14698448\n",
      "Iteration 155, loss = 0.14646721\n",
      "Iteration 156, loss = 0.14595805\n",
      "Iteration 157, loss = 0.14545692\n",
      "Iteration 158, loss = 0.14496303\n",
      "Iteration 159, loss = 0.14447729\n",
      "Iteration 160, loss = 0.14399891\n",
      "Iteration 161, loss = 0.14353006\n",
      "Iteration 162, loss = 0.14306833\n",
      "Iteration 163, loss = 0.14261329\n",
      "Iteration 164, loss = 0.14216433\n",
      "Iteration 165, loss = 0.14172155\n",
      "Iteration 166, loss = 0.14128500\n",
      "Iteration 167, loss = 0.14085409\n",
      "Iteration 168, loss = 0.14042917\n",
      "Iteration 169, loss = 0.14000992\n",
      "Iteration 170, loss = 0.13959624\n",
      "Iteration 171, loss = 0.13918803\n",
      "Iteration 172, loss = 0.13878535\n",
      "Iteration 173, loss = 0.13838781\n",
      "Iteration 174, loss = 0.13799561\n",
      "Iteration 175, loss = 0.13760857\n",
      "Iteration 176, loss = 0.13722651\n",
      "Iteration 177, loss = 0.13684940\n",
      "Iteration 178, loss = 0.13647717\n",
      "Iteration 179, loss = 0.13610963\n",
      "Iteration 180, loss = 0.13574688\n",
      "Iteration 181, loss = 0.13538862\n",
      "Iteration 182, loss = 0.13503497\n",
      "Iteration 183, loss = 0.13468578\n",
      "Iteration 184, loss = 0.13434091\n",
      "Iteration 185, loss = 0.13400042\n",
      "Iteration 186, loss = 0.13366405\n",
      "Iteration 187, loss = 0.13333185\n",
      "Iteration 188, loss = 0.13300384\n",
      "Iteration 189, loss = 0.13267980\n",
      "Iteration 190, loss = 0.13235980\n",
      "Iteration 191, loss = 0.13204375\n",
      "Iteration 192, loss = 0.13173144\n",
      "Iteration 193, loss = 0.13142310\n",
      "Iteration 194, loss = 0.13111831\n",
      "Iteration 195, loss = 0.13081717\n",
      "Iteration 196, loss = 0.13051939\n",
      "Iteration 197, loss = 0.13022498\n",
      "Iteration 198, loss = 0.12993408\n",
      "Iteration 199, loss = 0.12964641\n",
      "Iteration 200, loss = 0.12936221\n",
      "Iteration 201, loss = 0.12908112\n",
      "Iteration 202, loss = 0.12880327\n",
      "Iteration 203, loss = 0.12852854\n",
      "Iteration 204, loss = 0.12825685\n",
      "Iteration 205, loss = 0.12798830\n",
      "Iteration 206, loss = 0.12772258\n",
      "Iteration 207, loss = 0.12745994\n",
      "Iteration 208, loss = 0.12720009\n",
      "Iteration 209, loss = 0.12694305\n",
      "Iteration 210, loss = 0.12668891\n",
      "Iteration 211, loss = 0.12643737\n",
      "Iteration 212, loss = 0.12618861\n",
      "Iteration 213, loss = 0.12594244\n",
      "Iteration 214, loss = 0.12569899\n",
      "Iteration 215, loss = 0.12545800\n",
      "Iteration 216, loss = 0.12521968\n",
      "Iteration 217, loss = 0.12498376\n",
      "Iteration 218, loss = 0.12475044\n",
      "Iteration 219, loss = 0.12451955\n",
      "Iteration 220, loss = 0.12429111\n",
      "Iteration 221, loss = 0.12406506\n",
      "Iteration 222, loss = 0.12384123\n",
      "Iteration 223, loss = 0.12361979\n",
      "Iteration 224, loss = 0.12340047\n",
      "Iteration 225, loss = 0.12318346\n",
      "Iteration 226, loss = 0.12296853\n",
      "Iteration 227, loss = 0.12275582\n",
      "Iteration 228, loss = 0.12254515\n",
      "Iteration 229, loss = 0.12233665\n",
      "Iteration 230, loss = 0.12213010\n",
      "Iteration 231, loss = 0.12192565\n",
      "Iteration 232, loss = 0.12172312\n",
      "Iteration 233, loss = 0.12152264\n",
      "Iteration 234, loss = 0.12132401\n",
      "Iteration 235, loss = 0.12112737\n",
      "Iteration 236, loss = 0.12093254\n",
      "Iteration 237, loss = 0.12073963\n",
      "Iteration 238, loss = 0.12054847\n",
      "Iteration 239, loss = 0.12035923\n",
      "Iteration 240, loss = 0.12017165\n",
      "Iteration 241, loss = 0.11998586\n",
      "Iteration 242, loss = 0.11980177\n",
      "Iteration 243, loss = 0.11961946\n",
      "Iteration 244, loss = 0.11943874\n",
      "Iteration 245, loss = 0.11925976\n",
      "Iteration 246, loss = 0.11908233\n",
      "Iteration 247, loss = 0.11890659\n",
      "Iteration 248, loss = 0.11873239\n",
      "Iteration 249, loss = 0.11855983\n",
      "Iteration 250, loss = 0.11838874\n",
      "Iteration 251, loss = 0.11821924\n",
      "Iteration 252, loss = 0.11805119\n",
      "Iteration 253, loss = 0.11788470\n",
      "Iteration 254, loss = 0.11771959\n",
      "Iteration 255, loss = 0.11755602\n",
      "Iteration 256, loss = 0.11739380\n",
      "Iteration 257, loss = 0.11723304\n",
      "Iteration 258, loss = 0.11707363\n",
      "Iteration 259, loss = 0.11691566\n",
      "Iteration 260, loss = 0.11675897\n",
      "Iteration 261, loss = 0.11660359\n",
      "Iteration 262, loss = 0.11644929\n",
      "Iteration 263, loss = 0.11629637\n",
      "Iteration 264, loss = 0.11614462\n",
      "Iteration 265, loss = 0.11599409\n",
      "Iteration 266, loss = 0.11584487\n",
      "Iteration 267, loss = 0.11569681\n",
      "Iteration 268, loss = 0.11555002\n",
      "Iteration 269, loss = 0.11540437\n",
      "Iteration 270, loss = 0.11526000\n",
      "Iteration 271, loss = 0.11511675\n",
      "Iteration 272, loss = 0.11497467\n",
      "Iteration 273, loss = 0.11483375\n",
      "Iteration 274, loss = 0.11469398\n",
      "Iteration 275, loss = 0.11455529\n",
      "Iteration 276, loss = 0.11441777\n",
      "Iteration 277, loss = 0.11428128\n",
      "Iteration 278, loss = 0.11414594\n",
      "Iteration 279, loss = 0.11401163\n",
      "Iteration 280, loss = 0.11387839\n",
      "Iteration 281, loss = 0.11374619\n",
      "Iteration 282, loss = 0.11361505\n",
      "Iteration 283, loss = 0.11348484\n",
      "Iteration 284, loss = 0.11335565\n",
      "Iteration 285, loss = 0.11322753\n",
      "Iteration 286, loss = 0.11310036\n",
      "Iteration 287, loss = 0.11297424\n",
      "Iteration 288, loss = 0.11284904\n",
      "Iteration 289, loss = 0.11272485\n",
      "Iteration 290, loss = 0.11260159\n",
      "Iteration 291, loss = 0.11247930\n",
      "Iteration 292, loss = 0.11235792\n",
      "Iteration 293, loss = 0.11223755\n",
      "Iteration 294, loss = 0.11211810\n",
      "Iteration 295, loss = 0.11199967\n",
      "Iteration 296, loss = 0.11188216\n",
      "Iteration 297, loss = 0.11176549\n",
      "Iteration 298, loss = 0.11164986\n",
      "Iteration 299, loss = 0.11153511\n",
      "Iteration 300, loss = 0.11142140\n",
      "Iteration 301, loss = 0.11130837\n",
      "Iteration 302, loss = 0.11119605\n",
      "Iteration 303, loss = 0.11108453\n",
      "Iteration 304, loss = 0.11097368\n",
      "Iteration 305, loss = 0.11086377\n",
      "Iteration 306, loss = 0.11075462\n",
      "Iteration 307, loss = 0.11064625\n",
      "Iteration 308, loss = 0.11053872\n",
      "Iteration 309, loss = 0.11043197\n",
      "Iteration 310, loss = 0.11032593\n",
      "Iteration 311, loss = 0.11022063\n",
      "Iteration 312, loss = 0.11011609\n",
      "Iteration 313, loss = 0.11001223\n",
      "Iteration 314, loss = 0.10990911\n",
      "Iteration 315, loss = 0.10980672\n",
      "Iteration 316, loss = 0.10970503\n",
      "Iteration 317, loss = 0.10960405\n",
      "Iteration 318, loss = 0.10950374\n",
      "Iteration 319, loss = 0.10940414\n",
      "Iteration 320, loss = 0.10930517\n",
      "Iteration 321, loss = 0.10920693\n",
      "Iteration 322, loss = 0.10910930\n",
      "Iteration 323, loss = 0.10901234\n",
      "Iteration 324, loss = 0.10891605\n",
      "Iteration 325, loss = 0.10882036\n",
      "Iteration 326, loss = 0.10872534\n",
      "Iteration 327, loss = 0.10863092\n",
      "Iteration 328, loss = 0.10853714\n",
      "Iteration 329, loss = 0.10844398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.35337589\n",
      "Iteration 3, loss = 1.20795950\n",
      "Iteration 4, loss = 1.17426737\n",
      "Iteration 5, loss = 1.08914072\n",
      "Iteration 6, loss = 0.98422810\n",
      "Iteration 7, loss = 0.90761662\n",
      "Iteration 8, loss = 0.85447258\n",
      "Iteration 9, loss = 0.81110076\n",
      "Iteration 10, loss = 0.76959131\n",
      "Iteration 11, loss = 0.73093697\n",
      "Iteration 12, loss = 0.69536078\n",
      "Iteration 13, loss = 0.66412887\n",
      "Iteration 14, loss = 0.63623767\n",
      "Iteration 15, loss = 0.61144530\n",
      "Iteration 16, loss = 0.58930160\n",
      "Iteration 17, loss = 0.56944066\n",
      "Iteration 18, loss = 0.55157919\n",
      "Iteration 19, loss = 0.53548808\n",
      "Iteration 20, loss = 0.52092587\n",
      "Iteration 21, loss = 0.50765146\n",
      "Iteration 22, loss = 0.49548196\n",
      "Iteration 23, loss = 0.48425006\n",
      "Iteration 24, loss = 0.47381844\n",
      "Iteration 25, loss = 0.46409087\n",
      "Iteration 26, loss = 0.45496038\n",
      "Iteration 27, loss = 0.44633497\n",
      "Iteration 28, loss = 0.43811687\n",
      "Iteration 29, loss = 0.43026053\n",
      "Iteration 30, loss = 0.42272527\n",
      "Iteration 31, loss = 0.41545817\n",
      "Iteration 32, loss = 0.40847920\n",
      "Iteration 33, loss = 0.40183311\n",
      "Iteration 34, loss = 0.39552191\n",
      "Iteration 35, loss = 0.38944327\n",
      "Iteration 36, loss = 0.38353253\n",
      "Iteration 37, loss = 0.37778156\n",
      "Iteration 38, loss = 0.37217493\n",
      "Iteration 39, loss = 0.36670326\n",
      "Iteration 40, loss = 0.36135699\n",
      "Iteration 41, loss = 0.35612935\n",
      "Iteration 42, loss = 0.35101663\n",
      "Iteration 43, loss = 0.34601529\n",
      "Iteration 44, loss = 0.34112575\n",
      "Iteration 45, loss = 0.33634555\n",
      "Iteration 46, loss = 0.33166819\n",
      "Iteration 47, loss = 0.32709121\n",
      "Iteration 48, loss = 0.32261886\n",
      "Iteration 49, loss = 0.31824481\n",
      "Iteration 50, loss = 0.31396839\n",
      "Iteration 51, loss = 0.30978625\n",
      "Iteration 52, loss = 0.30570325\n",
      "Iteration 53, loss = 0.30171697\n",
      "Iteration 54, loss = 0.29781692\n",
      "Iteration 55, loss = 0.29400447\n",
      "Iteration 56, loss = 0.29027274\n",
      "Iteration 57, loss = 0.28662020\n",
      "Iteration 58, loss = 0.28304605\n",
      "Iteration 59, loss = 0.27954477\n",
      "Iteration 60, loss = 0.27611529\n",
      "Iteration 61, loss = 0.27275923\n",
      "Iteration 62, loss = 0.26947452\n",
      "Iteration 63, loss = 0.26626052\n",
      "Iteration 64, loss = 0.26311528\n",
      "Iteration 65, loss = 0.26003738\n",
      "Iteration 66, loss = 0.25702530\n",
      "Iteration 67, loss = 0.25407672\n",
      "Iteration 68, loss = 0.25118823\n",
      "Iteration 69, loss = 0.24835381\n",
      "Iteration 70, loss = 0.24557868\n",
      "Iteration 71, loss = 0.24286046\n",
      "Iteration 72, loss = 0.24019947\n",
      "Iteration 73, loss = 0.23758439\n",
      "Iteration 74, loss = 0.23499834\n",
      "Iteration 75, loss = 0.23243807\n",
      "Iteration 76, loss = 0.22988344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77, loss = 0.22727980\n",
      "Iteration 78, loss = 0.22461355\n",
      "Iteration 79, loss = 0.22196658\n",
      "Iteration 80, loss = 0.21939724\n",
      "Iteration 81, loss = 0.21704499\n",
      "Iteration 82, loss = 0.21483835\n",
      "Iteration 83, loss = 0.21271278\n",
      "Iteration 84, loss = 0.21066583\n",
      "Iteration 85, loss = 0.20868764\n",
      "Iteration 86, loss = 0.20675795\n",
      "Iteration 87, loss = 0.20487037\n",
      "Iteration 88, loss = 0.20302419\n",
      "Iteration 89, loss = 0.20121666\n",
      "Iteration 90, loss = 0.19944630\n",
      "Iteration 91, loss = 0.19771326\n",
      "Iteration 92, loss = 0.19601576\n",
      "Iteration 93, loss = 0.19435289\n",
      "Iteration 94, loss = 0.19272551\n",
      "Iteration 95, loss = 0.19113203\n",
      "Iteration 96, loss = 0.18957128\n",
      "Iteration 97, loss = 0.18804062\n",
      "Iteration 98, loss = 0.18654136\n",
      "Iteration 99, loss = 0.18507351\n",
      "Iteration 100, loss = 0.18364339\n",
      "Iteration 101, loss = 0.18225560\n",
      "Iteration 102, loss = 0.18090596\n",
      "Iteration 103, loss = 0.17958692\n",
      "Iteration 104, loss = 0.17829816\n",
      "Iteration 105, loss = 0.17703799\n",
      "Iteration 106, loss = 0.17580436\n",
      "Iteration 107, loss = 0.17459761\n",
      "Iteration 108, loss = 0.17341660\n",
      "Iteration 109, loss = 0.17225909\n",
      "Iteration 110, loss = 0.17112179\n",
      "Iteration 111, loss = 0.17000726\n",
      "Iteration 112, loss = 0.16891479\n",
      "Iteration 113, loss = 0.16784409\n",
      "Iteration 114, loss = 0.16679544\n",
      "Iteration 115, loss = 0.16576802\n",
      "Iteration 116, loss = 0.16476095\n",
      "Iteration 117, loss = 0.16377403\n",
      "Iteration 118, loss = 0.16280667\n",
      "Iteration 119, loss = 0.16185854\n",
      "Iteration 120, loss = 0.16092889\n",
      "Iteration 121, loss = 0.16001720\n",
      "Iteration 122, loss = 0.15912308\n",
      "Iteration 123, loss = 0.15824760\n",
      "Iteration 124, loss = 0.15739149\n",
      "Iteration 125, loss = 0.15655353\n",
      "Iteration 126, loss = 0.15573149\n",
      "Iteration 127, loss = 0.15492483\n",
      "Iteration 128, loss = 0.15413321\n",
      "Iteration 129, loss = 0.15335634\n",
      "Iteration 130, loss = 0.15259373\n",
      "Iteration 131, loss = 0.15184501\n",
      "Iteration 132, loss = 0.15111023\n",
      "Iteration 133, loss = 0.15039083\n",
      "Iteration 134, loss = 0.14968789\n",
      "Iteration 135, loss = 0.14900176\n",
      "Iteration 136, loss = 0.14832782\n",
      "Iteration 137, loss = 0.14766603\n",
      "Iteration 138, loss = 0.14701656\n",
      "Iteration 139, loss = 0.14637781\n",
      "Iteration 140, loss = 0.14574843\n",
      "Iteration 141, loss = 0.14512842\n",
      "Iteration 142, loss = 0.14451789\n",
      "Iteration 143, loss = 0.14391676\n",
      "Iteration 144, loss = 0.14332479\n",
      "Iteration 145, loss = 0.14274358\n",
      "Iteration 146, loss = 0.14217243\n",
      "Iteration 147, loss = 0.14161051\n",
      "Iteration 148, loss = 0.14105762\n",
      "Iteration 149, loss = 0.14051387\n",
      "Iteration 150, loss = 0.13997890\n",
      "Iteration 151, loss = 0.13945228\n",
      "Iteration 152, loss = 0.13893382\n",
      "Iteration 153, loss = 0.13842349\n",
      "Iteration 154, loss = 0.13792105\n",
      "Iteration 155, loss = 0.13742630\n",
      "Iteration 156, loss = 0.13693828\n",
      "Iteration 157, loss = 0.13645708\n",
      "Iteration 158, loss = 0.13598307\n",
      "Iteration 159, loss = 0.13551576\n",
      "Iteration 160, loss = 0.13505513\n",
      "Iteration 161, loss = 0.13460129\n",
      "Iteration 162, loss = 0.13415397\n",
      "Iteration 163, loss = 0.13371318\n",
      "Iteration 164, loss = 0.13327869\n",
      "Iteration 165, loss = 0.13285056\n",
      "Iteration 166, loss = 0.13242808\n",
      "Iteration 167, loss = 0.13201146\n",
      "Iteration 168, loss = 0.13160097\n",
      "Iteration 169, loss = 0.13119643\n",
      "Iteration 170, loss = 0.13079841\n",
      "Iteration 171, loss = 0.13040621\n",
      "Iteration 172, loss = 0.13001979\n",
      "Iteration 173, loss = 0.12963945\n",
      "Iteration 174, loss = 0.12926441\n",
      "Iteration 175, loss = 0.12889522\n",
      "Iteration 176, loss = 0.12853078\n",
      "Iteration 177, loss = 0.12817104\n",
      "Iteration 178, loss = 0.12781592\n",
      "Iteration 179, loss = 0.12746540\n",
      "Iteration 180, loss = 0.12711924\n",
      "Iteration 181, loss = 0.12677740\n",
      "Iteration 182, loss = 0.12643987\n",
      "Iteration 183, loss = 0.12610663\n",
      "Iteration 184, loss = 0.12577763\n",
      "Iteration 185, loss = 0.12545276\n",
      "Iteration 186, loss = 0.12513197\n",
      "Iteration 187, loss = 0.12481562\n",
      "Iteration 188, loss = 0.12450327\n",
      "Iteration 189, loss = 0.12419491\n",
      "Iteration 190, loss = 0.12389039\n",
      "Iteration 191, loss = 0.12358964\n",
      "Iteration 192, loss = 0.12329253\n",
      "Iteration 193, loss = 0.12299904\n",
      "Iteration 194, loss = 0.12270911\n",
      "Iteration 195, loss = 0.12242258\n",
      "Iteration 196, loss = 0.12213947\n",
      "Iteration 197, loss = 0.12185973\n",
      "Iteration 198, loss = 0.12158326\n",
      "Iteration 199, loss = 0.12131006\n",
      "Iteration 200, loss = 0.12104007\n",
      "Iteration 201, loss = 0.12077323\n",
      "Iteration 202, loss = 0.12050946\n",
      "Iteration 203, loss = 0.12024872\n",
      "Iteration 204, loss = 0.11999096\n",
      "Iteration 205, loss = 0.11973620\n",
      "Iteration 206, loss = 0.11948431\n",
      "Iteration 207, loss = 0.11923527\n",
      "Iteration 208, loss = 0.11898906\n",
      "Iteration 209, loss = 0.11874561\n",
      "Iteration 210, loss = 0.11850486\n",
      "Iteration 211, loss = 0.11826681\n",
      "Iteration 212, loss = 0.11803134\n",
      "Iteration 213, loss = 0.11779843\n",
      "Iteration 214, loss = 0.11756802\n",
      "Iteration 215, loss = 0.11734010\n",
      "Iteration 216, loss = 0.11711460\n",
      "Iteration 217, loss = 0.11689151\n",
      "Iteration 218, loss = 0.11667077\n",
      "Iteration 219, loss = 0.11645232\n",
      "Iteration 220, loss = 0.11623618\n",
      "Iteration 221, loss = 0.11602228\n",
      "Iteration 222, loss = 0.11581061\n",
      "Iteration 223, loss = 0.11560114\n",
      "Iteration 224, loss = 0.11539381\n",
      "Iteration 225, loss = 0.11518857\n",
      "Iteration 226, loss = 0.11498539\n",
      "Iteration 227, loss = 0.11478427\n",
      "Iteration 228, loss = 0.11458519\n",
      "Iteration 229, loss = 0.11438813\n",
      "Iteration 230, loss = 0.11419298\n",
      "Iteration 231, loss = 0.11399964\n",
      "Iteration 232, loss = 0.11380823\n",
      "Iteration 233, loss = 0.11361868\n",
      "Iteration 234, loss = 0.11343077\n",
      "Iteration 235, loss = 0.11324464\n",
      "Iteration 236, loss = 0.11306031\n",
      "Iteration 237, loss = 0.11287775\n",
      "Iteration 238, loss = 0.11269684\n",
      "Iteration 239, loss = 0.11251765\n",
      "Iteration 240, loss = 0.11233999\n",
      "Iteration 241, loss = 0.11216393\n",
      "Iteration 242, loss = 0.11198930\n",
      "Iteration 243, loss = 0.11181601\n",
      "Iteration 244, loss = 0.11164408\n",
      "Iteration 245, loss = 0.11147363\n",
      "Iteration 246, loss = 0.11130460\n",
      "Iteration 247, loss = 0.11113653\n",
      "Iteration 248, loss = 0.11096906\n",
      "Iteration 249, loss = 0.11080171\n",
      "Iteration 250, loss = 0.11063544\n",
      "Iteration 251, loss = 0.11047020\n",
      "Iteration 252, loss = 0.11030548\n",
      "Iteration 253, loss = 0.11014036\n",
      "Iteration 254, loss = 0.10997555\n",
      "Iteration 255, loss = 0.10980843\n",
      "Iteration 256, loss = 0.10963720\n",
      "Iteration 257, loss = 0.10946010\n",
      "Iteration 258, loss = 0.10928064\n",
      "Iteration 259, loss = 0.10911129\n",
      "Iteration 260, loss = 0.10895085\n",
      "Iteration 261, loss = 0.10879305\n",
      "Iteration 262, loss = 0.10863696\n",
      "Iteration 263, loss = 0.10848757\n",
      "Iteration 264, loss = 0.10834443\n",
      "Iteration 265, loss = 0.10820366\n",
      "Iteration 266, loss = 0.10806702\n",
      "Iteration 267, loss = 0.10793010\n",
      "Iteration 268, loss = 0.10779117\n",
      "Iteration 269, loss = 0.10765461\n",
      "Iteration 270, loss = 0.10752001\n",
      "Iteration 271, loss = 0.10738677\n",
      "Iteration 272, loss = 0.10725488\n",
      "Iteration 273, loss = 0.10712428\n",
      "Iteration 274, loss = 0.10699496\n",
      "Iteration 275, loss = 0.10686700\n",
      "Iteration 276, loss = 0.10674023\n",
      "Iteration 277, loss = 0.10661458\n",
      "Iteration 278, loss = 0.10648994\n",
      "Iteration 279, loss = 0.10636653\n",
      "Iteration 280, loss = 0.10624406\n",
      "Iteration 281, loss = 0.10612271\n",
      "Iteration 282, loss = 0.10600236\n",
      "Iteration 283, loss = 0.10588308\n",
      "Iteration 284, loss = 0.10576466\n",
      "Iteration 285, loss = 0.10564726\n",
      "Iteration 286, loss = 0.10553074\n",
      "Iteration 287, loss = 0.10541519\n",
      "Iteration 288, loss = 0.10530051\n",
      "Iteration 289, loss = 0.10518679\n",
      "Iteration 290, loss = 0.10507390\n",
      "Iteration 291, loss = 0.10496196\n",
      "Iteration 292, loss = 0.10485081\n",
      "Iteration 293, loss = 0.10474059\n",
      "Iteration 294, loss = 0.10463115\n",
      "Iteration 295, loss = 0.10452260\n",
      "Iteration 296, loss = 0.10441486\n",
      "Iteration 297, loss = 0.10430799\n",
      "Iteration 298, loss = 0.10420191\n",
      "Iteration 299, loss = 0.10409661\n",
      "Iteration 300, loss = 0.10399213\n",
      "Iteration 301, loss = 0.10388837\n",
      "Iteration 302, loss = 0.10378546\n",
      "Iteration 303, loss = 0.10368322\n",
      "Iteration 304, loss = 0.10358181\n",
      "Iteration 305, loss = 0.10348108\n",
      "Iteration 306, loss = 0.10338112\n",
      "Iteration 307, loss = 0.10328184\n",
      "Iteration 308, loss = 0.10318336\n",
      "Iteration 309, loss = 0.10308550\n",
      "Iteration 310, loss = 0.10298841\n",
      "Iteration 311, loss = 0.10289196\n",
      "Iteration 312, loss = 0.10279625\n",
      "Iteration 313, loss = 0.10270116\n",
      "Iteration 314, loss = 0.10260681\n",
      "Iteration 315, loss = 0.10251306\n",
      "Iteration 316, loss = 0.10242004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.36112728\n",
      "Iteration 3, loss = 1.21108036\n",
      "Iteration 4, loss = 1.17313393\n",
      "Iteration 5, loss = 1.08500463\n",
      "Iteration 6, loss = 0.97688363\n",
      "Iteration 7, loss = 0.89692474\n",
      "Iteration 8, loss = 0.84484180\n",
      "Iteration 9, loss = 0.79932624\n",
      "Iteration 10, loss = 0.75600817\n",
      "Iteration 11, loss = 0.71521870\n",
      "Iteration 12, loss = 0.67755475\n",
      "Iteration 13, loss = 0.64420701\n",
      "Iteration 14, loss = 0.61651205\n",
      "Iteration 15, loss = 0.59179412\n",
      "Iteration 16, loss = 0.56976566\n",
      "Iteration 17, loss = 0.54997590\n",
      "Iteration 18, loss = 0.53213137\n",
      "Iteration 19, loss = 0.51595548\n",
      "Iteration 20, loss = 0.50135848\n",
      "Iteration 21, loss = 0.48807440\n",
      "Iteration 22, loss = 0.47578563\n",
      "Iteration 23, loss = 0.46446747\n",
      "Iteration 24, loss = 0.45409718\n",
      "Iteration 25, loss = 0.44449595\n",
      "Iteration 26, loss = 0.43543802\n",
      "Iteration 27, loss = 0.42685138\n",
      "Iteration 28, loss = 0.41867417\n",
      "Iteration 29, loss = 0.41084550\n",
      "Iteration 30, loss = 0.40332208\n",
      "Iteration 31, loss = 0.39608342\n",
      "Iteration 32, loss = 0.38908885\n",
      "Iteration 33, loss = 0.38231743\n",
      "Iteration 34, loss = 0.37575290\n",
      "Iteration 35, loss = 0.36937643\n",
      "Iteration 36, loss = 0.36317316\n",
      "Iteration 37, loss = 0.35713763\n",
      "Iteration 38, loss = 0.35125185\n",
      "Iteration 39, loss = 0.34551044\n",
      "Iteration 40, loss = 0.33991496\n",
      "Iteration 41, loss = 0.33445750\n",
      "Iteration 42, loss = 0.32912522\n",
      "Iteration 43, loss = 0.32392186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44, loss = 0.31885013\n",
      "Iteration 45, loss = 0.31389718\n",
      "Iteration 46, loss = 0.30906201\n",
      "Iteration 47, loss = 0.30433637\n",
      "Iteration 48, loss = 0.29971722\n",
      "Iteration 49, loss = 0.29519990\n",
      "Iteration 50, loss = 0.29078819\n",
      "Iteration 51, loss = 0.28648033\n",
      "Iteration 52, loss = 0.28227437\n",
      "Iteration 53, loss = 0.27816501\n",
      "Iteration 54, loss = 0.27415322\n",
      "Iteration 55, loss = 0.27023409\n",
      "Iteration 56, loss = 0.26640751\n",
      "Iteration 57, loss = 0.26266889\n",
      "Iteration 58, loss = 0.25901503\n",
      "Iteration 59, loss = 0.25544508\n",
      "Iteration 60, loss = 0.25195744\n",
      "Iteration 61, loss = 0.24854966\n",
      "Iteration 62, loss = 0.24521959\n",
      "Iteration 63, loss = 0.24196626\n",
      "Iteration 64, loss = 0.23878751\n",
      "Iteration 65, loss = 0.23568109\n",
      "Iteration 66, loss = 0.23264437\n",
      "Iteration 67, loss = 0.22967629\n",
      "Iteration 68, loss = 0.22677630\n",
      "Iteration 69, loss = 0.22394350\n",
      "Iteration 70, loss = 0.22117530\n",
      "Iteration 71, loss = 0.21847269\n",
      "Iteration 72, loss = 0.21583094\n",
      "Iteration 73, loss = 0.21325074\n",
      "Iteration 74, loss = 0.21073123\n",
      "Iteration 75, loss = 0.20826850\n",
      "Iteration 76, loss = 0.20586315\n",
      "Iteration 77, loss = 0.20351284\n",
      "Iteration 78, loss = 0.20121287\n",
      "Iteration 79, loss = 0.19896482\n",
      "Iteration 80, loss = 0.19676743\n",
      "Iteration 81, loss = 0.19461698\n",
      "Iteration 82, loss = 0.19251249\n",
      "Iteration 83, loss = 0.19045565\n",
      "Iteration 84, loss = 0.18844511\n",
      "Iteration 85, loss = 0.18647793\n",
      "Iteration 86, loss = 0.18455472\n",
      "Iteration 87, loss = 0.18267507\n",
      "Iteration 88, loss = 0.18083476\n",
      "Iteration 89, loss = 0.17903373\n",
      "Iteration 90, loss = 0.17727112\n",
      "Iteration 91, loss = 0.17554717\n",
      "Iteration 92, loss = 0.17385963\n",
      "Iteration 93, loss = 0.17220792\n",
      "Iteration 94, loss = 0.17059131\n",
      "Iteration 95, loss = 0.16900975\n",
      "Iteration 96, loss = 0.16746183\n",
      "Iteration 97, loss = 0.16594765\n",
      "Iteration 98, loss = 0.16446894\n",
      "Iteration 99, loss = 0.16302373\n",
      "Iteration 100, loss = 0.16160870\n",
      "Iteration 101, loss = 0.16022423\n",
      "Iteration 102, loss = 0.15887158\n",
      "Iteration 103, loss = 0.15755592\n",
      "Iteration 104, loss = 0.15627655\n",
      "Iteration 105, loss = 0.15502973\n",
      "Iteration 106, loss = 0.15381240\n",
      "Iteration 107, loss = 0.15262053\n",
      "Iteration 108, loss = 0.15145412\n",
      "Iteration 109, loss = 0.15031752\n",
      "Iteration 110, loss = 0.14919783\n",
      "Iteration 111, loss = 0.14809714\n",
      "Iteration 112, loss = 0.14701957\n",
      "Iteration 113, loss = 0.14596656\n",
      "Iteration 114, loss = 0.14493633\n",
      "Iteration 115, loss = 0.14392739\n",
      "Iteration 116, loss = 0.14293901\n",
      "Iteration 117, loss = 0.14197045\n",
      "Iteration 118, loss = 0.14102145\n",
      "Iteration 119, loss = 0.14009165\n",
      "Iteration 120, loss = 0.13918048\n",
      "Iteration 121, loss = 0.13828742\n",
      "Iteration 122, loss = 0.13741191\n",
      "Iteration 123, loss = 0.13655349\n",
      "Iteration 124, loss = 0.13571197\n",
      "Iteration 125, loss = 0.13488689\n",
      "Iteration 126, loss = 0.13407761\n",
      "Iteration 127, loss = 0.13328373\n",
      "Iteration 128, loss = 0.13250583\n",
      "Iteration 129, loss = 0.13174232\n",
      "Iteration 130, loss = 0.13099445\n",
      "Iteration 131, loss = 0.13025951\n",
      "Iteration 132, loss = 0.12953776\n",
      "Iteration 133, loss = 0.12882896\n",
      "Iteration 134, loss = 0.12813280\n",
      "Iteration 135, loss = 0.12745063\n",
      "Iteration 136, loss = 0.12678121\n",
      "Iteration 137, loss = 0.12612375\n",
      "Iteration 138, loss = 0.12547797\n",
      "Iteration 139, loss = 0.12484422\n",
      "Iteration 140, loss = 0.12422209\n",
      "Iteration 141, loss = 0.12361111\n",
      "Iteration 142, loss = 0.12300996\n",
      "Iteration 143, loss = 0.12241977\n",
      "Iteration 144, loss = 0.12183871\n",
      "Iteration 145, loss = 0.12126206\n",
      "Iteration 146, loss = 0.12068885\n",
      "Iteration 147, loss = 0.12011907\n",
      "Iteration 148, loss = 0.11955526\n",
      "Iteration 149, loss = 0.11898974\n",
      "Iteration 150, loss = 0.11842359\n",
      "Iteration 151, loss = 0.11785464\n",
      "Iteration 152, loss = 0.11726980\n",
      "Iteration 153, loss = 0.11669132\n",
      "Iteration 154, loss = 0.11612860\n",
      "Iteration 155, loss = 0.11558233\n",
      "Iteration 156, loss = 0.11506637\n",
      "Iteration 157, loss = 0.11458480\n",
      "Iteration 158, loss = 0.11411395\n",
      "Iteration 159, loss = 0.11365046\n",
      "Iteration 160, loss = 0.11320243\n",
      "Iteration 161, loss = 0.11276439\n",
      "Iteration 162, loss = 0.11233375\n",
      "Iteration 163, loss = 0.11190596\n",
      "Iteration 164, loss = 0.11148853\n",
      "Iteration 165, loss = 0.11107802\n",
      "Iteration 166, loss = 0.11067379\n",
      "Iteration 167, loss = 0.11027725\n",
      "Iteration 168, loss = 0.10988841\n",
      "Iteration 169, loss = 0.10950593\n",
      "Iteration 170, loss = 0.10912894\n",
      "Iteration 171, loss = 0.10875757\n",
      "Iteration 172, loss = 0.10839144\n",
      "Iteration 173, loss = 0.10803057\n",
      "Iteration 174, loss = 0.10767474\n",
      "Iteration 175, loss = 0.10732375\n",
      "Iteration 176, loss = 0.10697757\n",
      "Iteration 177, loss = 0.10663599\n",
      "Iteration 178, loss = 0.10629896\n",
      "Iteration 179, loss = 0.10596645\n",
      "Iteration 180, loss = 0.10563827\n",
      "Iteration 181, loss = 0.10531437\n",
      "Iteration 182, loss = 0.10499471\n",
      "Iteration 183, loss = 0.10467917\n",
      "Iteration 184, loss = 0.10436768\n",
      "Iteration 185, loss = 0.10406015\n",
      "Iteration 186, loss = 0.10375658\n",
      "Iteration 187, loss = 0.10345679\n",
      "Iteration 188, loss = 0.10316074\n",
      "Iteration 189, loss = 0.10286847\n",
      "Iteration 190, loss = 0.10257967\n",
      "Iteration 191, loss = 0.10229448\n",
      "Iteration 192, loss = 0.10201264\n",
      "Iteration 193, loss = 0.10173354\n",
      "Iteration 194, loss = 0.10145767\n",
      "Iteration 195, loss = 0.10118507\n",
      "Iteration 196, loss = 0.10091553\n",
      "Iteration 197, loss = 0.10064911\n",
      "Iteration 198, loss = 0.10038583\n",
      "Iteration 199, loss = 0.10012545\n",
      "Iteration 200, loss = 0.09986808\n",
      "Iteration 201, loss = 0.09961382\n",
      "Iteration 202, loss = 0.09936237\n",
      "Iteration 203, loss = 0.09911376\n",
      "Iteration 204, loss = 0.09886798\n",
      "Iteration 205, loss = 0.09862495\n",
      "Iteration 206, loss = 0.09838454\n",
      "Iteration 207, loss = 0.09814672\n",
      "Iteration 208, loss = 0.09791170\n",
      "Iteration 209, loss = 0.09767941\n",
      "Iteration 210, loss = 0.09745004\n",
      "Iteration 211, loss = 0.09722335\n",
      "Iteration 212, loss = 0.09699924\n",
      "Iteration 213, loss = 0.09677763\n",
      "Iteration 214, loss = 0.09655857\n",
      "Iteration 215, loss = 0.09634230\n",
      "Iteration 216, loss = 0.09612833\n",
      "Iteration 217, loss = 0.09591626\n",
      "Iteration 218, loss = 0.09570649\n",
      "Iteration 219, loss = 0.09549909\n",
      "Iteration 220, loss = 0.09529381\n",
      "Iteration 221, loss = 0.09509064\n",
      "Iteration 222, loss = 0.09488962\n",
      "Iteration 223, loss = 0.09469070\n",
      "Iteration 224, loss = 0.09449389\n",
      "Iteration 225, loss = 0.09429942\n",
      "Iteration 226, loss = 0.09410703\n",
      "Iteration 227, loss = 0.09391651\n",
      "Iteration 228, loss = 0.09372777\n",
      "Iteration 229, loss = 0.09354093\n",
      "Iteration 230, loss = 0.09335591\n",
      "Iteration 231, loss = 0.09317270\n",
      "Iteration 232, loss = 0.09299128\n",
      "Iteration 233, loss = 0.09281163\n",
      "Iteration 234, loss = 0.09263376\n",
      "Iteration 235, loss = 0.09245757\n",
      "Iteration 236, loss = 0.09228304\n",
      "Iteration 237, loss = 0.09211018\n",
      "Iteration 238, loss = 0.09193898\n",
      "Iteration 239, loss = 0.09176931\n",
      "Iteration 240, loss = 0.09160130\n",
      "Iteration 241, loss = 0.09143496\n",
      "Iteration 242, loss = 0.09126955\n",
      "Iteration 243, loss = 0.09110555\n",
      "Iteration 244, loss = 0.09094301\n",
      "Iteration 245, loss = 0.09078204\n",
      "Iteration 246, loss = 0.09062252\n",
      "Iteration 247, loss = 0.09046437\n",
      "Iteration 248, loss = 0.09030760\n",
      "Iteration 249, loss = 0.09015212\n",
      "Iteration 250, loss = 0.08999648\n",
      "Iteration 251, loss = 0.08984238\n",
      "Iteration 252, loss = 0.08968940\n",
      "Iteration 253, loss = 0.08953750\n",
      "Iteration 254, loss = 0.08938762\n",
      "Iteration 255, loss = 0.08923901\n",
      "Iteration 256, loss = 0.08909167\n",
      "Iteration 257, loss = 0.08894568\n",
      "Iteration 258, loss = 0.08880137\n",
      "Iteration 259, loss = 0.08865767\n",
      "Iteration 260, loss = 0.08851439\n",
      "Iteration 261, loss = 0.08837257\n",
      "Iteration 262, loss = 0.08823127\n",
      "Iteration 263, loss = 0.08809094\n",
      "Iteration 264, loss = 0.08795216\n",
      "Iteration 265, loss = 0.08781752\n",
      "Iteration 266, loss = 0.08768508\n",
      "Iteration 267, loss = 0.08755365\n",
      "Iteration 268, loss = 0.08742322\n",
      "Iteration 269, loss = 0.08729383\n",
      "Iteration 270, loss = 0.08716544\n",
      "Iteration 271, loss = 0.08703808\n",
      "Iteration 272, loss = 0.08691174\n",
      "Iteration 273, loss = 0.08678643\n",
      "Iteration 274, loss = 0.08666214\n",
      "Iteration 275, loss = 0.08653886\n",
      "Iteration 276, loss = 0.08641657\n",
      "Iteration 277, loss = 0.08629524\n",
      "Iteration 278, loss = 0.08617491\n",
      "Iteration 279, loss = 0.08605558\n",
      "Iteration 280, loss = 0.08593713\n",
      "Iteration 281, loss = 0.08581968\n",
      "Iteration 282, loss = 0.08570312\n",
      "Iteration 283, loss = 0.08558751\n",
      "Iteration 284, loss = 0.08547280\n",
      "Iteration 285, loss = 0.08535896\n",
      "Iteration 286, loss = 0.08524607\n",
      "Iteration 287, loss = 0.08513399\n",
      "Iteration 288, loss = 0.08502283\n",
      "Iteration 289, loss = 0.08491250\n",
      "Iteration 290, loss = 0.08480302\n",
      "Iteration 291, loss = 0.08469443\n",
      "Iteration 292, loss = 0.08458662\n",
      "Iteration 293, loss = 0.08447965\n",
      "Iteration 294, loss = 0.08437349\n",
      "Iteration 295, loss = 0.08426814\n",
      "Iteration 296, loss = 0.08416359\n",
      "Iteration 297, loss = 0.08405980\n",
      "Iteration 298, loss = 0.08395683\n",
      "Iteration 299, loss = 0.08385457\n",
      "Iteration 300, loss = 0.08375312\n",
      "Iteration 301, loss = 0.08365238\n",
      "Iteration 302, loss = 0.08355240\n",
      "Iteration 303, loss = 0.08345317\n",
      "Iteration 304, loss = 0.08335463\n",
      "Iteration 305, loss = 0.08325682\n",
      "Iteration 306, loss = 0.08315973\n",
      "Iteration 307, loss = 0.08306332\n",
      "Iteration 308, loss = 0.08296763\n",
      "Iteration 309, loss = 0.08287259\n",
      "Iteration 310, loss = 0.08277827\n",
      "Iteration 311, loss = 0.08268460\n",
      "Iteration 312, loss = 0.08259159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.35751008\n",
      "Iteration 3, loss = 1.20865399\n",
      "Iteration 4, loss = 1.17084382\n",
      "Iteration 5, loss = 1.08516738\n",
      "Iteration 6, loss = 0.97862066\n",
      "Iteration 7, loss = 0.90065408\n",
      "Iteration 8, loss = 0.84984695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.80518887\n",
      "Iteration 10, loss = 0.76325585\n",
      "Iteration 11, loss = 0.72412730\n",
      "Iteration 12, loss = 0.68719622\n",
      "Iteration 13, loss = 0.65527763\n",
      "Iteration 14, loss = 0.62731432\n",
      "Iteration 15, loss = 0.60237800\n",
      "Iteration 16, loss = 0.58004948\n",
      "Iteration 17, loss = 0.55991978\n",
      "Iteration 18, loss = 0.54179116\n",
      "Iteration 19, loss = 0.52548575\n",
      "Iteration 20, loss = 0.51065356\n",
      "Iteration 21, loss = 0.49701799\n",
      "Iteration 22, loss = 0.48435954\n",
      "Iteration 23, loss = 0.47270135\n",
      "Iteration 24, loss = 0.46201479\n",
      "Iteration 25, loss = 0.45214794\n",
      "Iteration 26, loss = 0.44285924\n",
      "Iteration 27, loss = 0.43404460\n",
      "Iteration 28, loss = 0.42566189\n",
      "Iteration 29, loss = 0.41765051\n",
      "Iteration 30, loss = 0.40996240\n",
      "Iteration 31, loss = 0.40256331\n",
      "Iteration 32, loss = 0.39541628\n",
      "Iteration 33, loss = 0.38850690\n",
      "Iteration 34, loss = 0.38180181\n",
      "Iteration 35, loss = 0.37528940\n",
      "Iteration 36, loss = 0.36896140\n",
      "Iteration 37, loss = 0.36280403\n",
      "Iteration 38, loss = 0.35680484\n",
      "Iteration 39, loss = 0.35095543\n",
      "Iteration 40, loss = 0.34525012\n",
      "Iteration 41, loss = 0.33968204\n",
      "Iteration 42, loss = 0.33425518\n",
      "Iteration 43, loss = 0.32896767\n",
      "Iteration 44, loss = 0.32381140\n",
      "Iteration 45, loss = 0.31877186\n",
      "Iteration 46, loss = 0.31384591\n",
      "Iteration 47, loss = 0.30903005\n",
      "Iteration 48, loss = 0.30432867\n",
      "Iteration 49, loss = 0.29973598\n",
      "Iteration 50, loss = 0.29524817\n",
      "Iteration 51, loss = 0.29086336\n",
      "Iteration 52, loss = 0.28657976\n",
      "Iteration 53, loss = 0.28239452\n",
      "Iteration 54, loss = 0.27830457\n",
      "Iteration 55, loss = 0.27430907\n",
      "Iteration 56, loss = 0.27040476\n",
      "Iteration 57, loss = 0.26658726\n",
      "Iteration 58, loss = 0.26285585\n",
      "Iteration 59, loss = 0.25920858\n",
      "Iteration 60, loss = 0.25564229\n",
      "Iteration 61, loss = 0.25215288\n",
      "Iteration 62, loss = 0.24873999\n",
      "Iteration 63, loss = 0.24540384\n",
      "Iteration 64, loss = 0.24214327\n",
      "Iteration 65, loss = 0.23895460\n",
      "Iteration 66, loss = 0.23583668\n",
      "Iteration 67, loss = 0.23278861\n",
      "Iteration 68, loss = 0.22980788\n",
      "Iteration 69, loss = 0.22689205\n",
      "Iteration 70, loss = 0.22404093\n",
      "Iteration 71, loss = 0.22125298\n",
      "Iteration 72, loss = 0.21852518\n",
      "Iteration 73, loss = 0.21585733\n",
      "Iteration 74, loss = 0.21324527\n",
      "Iteration 75, loss = 0.21068694\n",
      "Iteration 76, loss = 0.20818294\n",
      "Iteration 77, loss = 0.20573305\n",
      "Iteration 78, loss = 0.20333433\n",
      "Iteration 79, loss = 0.20098728\n",
      "Iteration 80, loss = 0.19868976\n",
      "Iteration 81, loss = 0.19643860\n",
      "Iteration 82, loss = 0.19423539\n",
      "Iteration 83, loss = 0.19208061\n",
      "Iteration 84, loss = 0.18997205\n",
      "Iteration 85, loss = 0.18790746\n",
      "Iteration 86, loss = 0.18588597\n",
      "Iteration 87, loss = 0.18390583\n",
      "Iteration 88, loss = 0.18196570\n",
      "Iteration 89, loss = 0.18006795\n",
      "Iteration 90, loss = 0.17821501\n",
      "Iteration 91, loss = 0.17640132\n",
      "Iteration 92, loss = 0.17462371\n",
      "Iteration 93, loss = 0.17288301\n",
      "Iteration 94, loss = 0.17118721\n",
      "Iteration 95, loss = 0.16953185\n",
      "Iteration 96, loss = 0.16791010\n",
      "Iteration 97, loss = 0.16632682\n",
      "Iteration 98, loss = 0.16478108\n",
      "Iteration 99, loss = 0.16326965\n",
      "Iteration 100, loss = 0.16178474\n",
      "Iteration 101, loss = 0.16032819\n",
      "Iteration 102, loss = 0.15889547\n",
      "Iteration 103, loss = 0.15748010\n",
      "Iteration 104, loss = 0.15607360\n",
      "Iteration 105, loss = 0.15467622\n",
      "Iteration 106, loss = 0.15327065\n",
      "Iteration 107, loss = 0.15185380\n",
      "Iteration 108, loss = 0.15048441\n",
      "Iteration 109, loss = 0.14915192\n",
      "Iteration 110, loss = 0.14787252\n",
      "Iteration 111, loss = 0.14666496\n",
      "Iteration 112, loss = 0.14551041\n",
      "Iteration 113, loss = 0.14439368\n",
      "Iteration 114, loss = 0.14330341\n",
      "Iteration 115, loss = 0.14223776\n",
      "Iteration 116, loss = 0.14119786\n",
      "Iteration 117, loss = 0.14018080\n",
      "Iteration 118, loss = 0.13918398\n",
      "Iteration 119, loss = 0.13820627\n",
      "Iteration 120, loss = 0.13724902\n",
      "Iteration 121, loss = 0.13631074\n",
      "Iteration 122, loss = 0.13539031\n",
      "Iteration 123, loss = 0.13448837\n",
      "Iteration 124, loss = 0.13360470\n",
      "Iteration 125, loss = 0.13273870\n",
      "Iteration 126, loss = 0.13189292\n",
      "Iteration 127, loss = 0.13106360\n",
      "Iteration 128, loss = 0.13024944\n",
      "Iteration 129, loss = 0.12945012\n",
      "Iteration 130, loss = 0.12866708\n",
      "Iteration 131, loss = 0.12789345\n",
      "Iteration 132, loss = 0.12713354\n",
      "Iteration 133, loss = 0.12638667\n",
      "Iteration 134, loss = 0.12565196\n",
      "Iteration 135, loss = 0.12492908\n",
      "Iteration 136, loss = 0.12421810\n",
      "Iteration 137, loss = 0.12351900\n",
      "Iteration 138, loss = 0.12283183\n",
      "Iteration 139, loss = 0.12215594\n",
      "Iteration 140, loss = 0.12149133\n",
      "Iteration 141, loss = 0.12083750\n",
      "Iteration 142, loss = 0.12019411\n",
      "Iteration 143, loss = 0.11956104\n",
      "Iteration 144, loss = 0.11893807\n",
      "Iteration 145, loss = 0.11832490\n",
      "Iteration 146, loss = 0.11772130\n",
      "Iteration 147, loss = 0.11712628\n",
      "Iteration 148, loss = 0.11653938\n",
      "Iteration 149, loss = 0.11596122\n",
      "Iteration 150, loss = 0.11539180\n",
      "Iteration 151, loss = 0.11483112\n",
      "Iteration 152, loss = 0.11427880\n",
      "Iteration 153, loss = 0.11373471\n",
      "Iteration 154, loss = 0.11319857\n",
      "Iteration 155, loss = 0.11267026\n",
      "Iteration 156, loss = 0.11214955\n",
      "Iteration 157, loss = 0.11163594\n",
      "Iteration 158, loss = 0.11112932\n",
      "Iteration 159, loss = 0.11063035\n",
      "Iteration 160, loss = 0.11013929\n",
      "Iteration 161, loss = 0.10965548\n",
      "Iteration 162, loss = 0.10917915\n",
      "Iteration 163, loss = 0.10870975\n",
      "Iteration 164, loss = 0.10824693\n",
      "Iteration 165, loss = 0.10779185\n",
      "Iteration 166, loss = 0.10734290\n",
      "Iteration 167, loss = 0.10689929\n",
      "Iteration 168, loss = 0.10646149\n",
      "Iteration 169, loss = 0.10602941\n",
      "Iteration 170, loss = 0.10560296\n",
      "Iteration 171, loss = 0.10518202\n",
      "Iteration 172, loss = 0.10476657\n",
      "Iteration 173, loss = 0.10435628\n",
      "Iteration 174, loss = 0.10395045\n",
      "Iteration 175, loss = 0.10355014\n",
      "Iteration 176, loss = 0.10315561\n",
      "Iteration 177, loss = 0.10276640\n",
      "Iteration 178, loss = 0.10238197\n",
      "Iteration 179, loss = 0.10200231\n",
      "Iteration 180, loss = 0.10162737\n",
      "Iteration 181, loss = 0.10125704\n",
      "Iteration 182, loss = 0.10089121\n",
      "Iteration 183, loss = 0.10052984\n",
      "Iteration 184, loss = 0.10017278\n",
      "Iteration 185, loss = 0.09982001\n",
      "Iteration 186, loss = 0.09947144\n",
      "Iteration 187, loss = 0.09912699\n",
      "Iteration 188, loss = 0.09878675\n",
      "Iteration 189, loss = 0.09845066\n",
      "Iteration 190, loss = 0.09811844\n",
      "Iteration 191, loss = 0.09779005\n",
      "Iteration 192, loss = 0.09746543\n",
      "Iteration 193, loss = 0.09714453\n",
      "Iteration 194, loss = 0.09682733\n",
      "Iteration 195, loss = 0.09651390\n",
      "Iteration 196, loss = 0.09620390\n",
      "Iteration 197, loss = 0.09589736\n",
      "Iteration 198, loss = 0.09559433\n",
      "Iteration 199, loss = 0.09529465\n",
      "Iteration 200, loss = 0.09499834\n",
      "Iteration 201, loss = 0.09470538\n",
      "Iteration 202, loss = 0.09441559\n",
      "Iteration 203, loss = 0.09412899\n",
      "Iteration 204, loss = 0.09384554\n",
      "Iteration 205, loss = 0.09356511\n",
      "Iteration 206, loss = 0.09328770\n",
      "Iteration 207, loss = 0.09301330\n",
      "Iteration 208, loss = 0.09274177\n",
      "Iteration 209, loss = 0.09247317\n",
      "Iteration 210, loss = 0.09220737\n",
      "Iteration 211, loss = 0.09194437\n",
      "Iteration 212, loss = 0.09168414\n",
      "Iteration 213, loss = 0.09142657\n",
      "Iteration 214, loss = 0.09117174\n",
      "Iteration 215, loss = 0.09091946\n",
      "Iteration 216, loss = 0.09066982\n",
      "Iteration 217, loss = 0.09042269\n",
      "Iteration 218, loss = 0.09017804\n",
      "Iteration 219, loss = 0.08993591\n",
      "Iteration 220, loss = 0.08969614\n",
      "Iteration 221, loss = 0.08945884\n",
      "Iteration 222, loss = 0.08922387\n",
      "Iteration 223, loss = 0.08899128\n",
      "Iteration 224, loss = 0.08876101\n",
      "Iteration 225, loss = 0.08853301\n",
      "Iteration 226, loss = 0.08830724\n",
      "Iteration 227, loss = 0.08808371\n",
      "Iteration 228, loss = 0.08786227\n",
      "Iteration 229, loss = 0.08764300\n",
      "Iteration 230, loss = 0.08742582\n",
      "Iteration 231, loss = 0.08721070\n",
      "Iteration 232, loss = 0.08699762\n",
      "Iteration 233, loss = 0.08678657\n",
      "Iteration 234, loss = 0.08657753\n",
      "Iteration 235, loss = 0.08637042\n",
      "Iteration 236, loss = 0.08616524\n",
      "Iteration 237, loss = 0.08596202\n",
      "Iteration 238, loss = 0.08576067\n",
      "Iteration 239, loss = 0.08556117\n",
      "Iteration 240, loss = 0.08536351\n",
      "Iteration 241, loss = 0.08516764\n",
      "Iteration 242, loss = 0.08497354\n",
      "Iteration 243, loss = 0.08478118\n",
      "Iteration 244, loss = 0.08459055\n",
      "Iteration 245, loss = 0.08440166\n",
      "Iteration 246, loss = 0.08421442\n",
      "Iteration 247, loss = 0.08402892\n",
      "Iteration 248, loss = 0.08384507\n",
      "Iteration 249, loss = 0.08366286\n",
      "Iteration 250, loss = 0.08348221\n",
      "Iteration 251, loss = 0.08330316\n",
      "Iteration 252, loss = 0.08312566\n",
      "Iteration 253, loss = 0.08294970\n",
      "Iteration 254, loss = 0.08277526\n",
      "Iteration 255, loss = 0.08260233\n",
      "Iteration 256, loss = 0.08243088\n",
      "Iteration 257, loss = 0.08226088\n",
      "Iteration 258, loss = 0.08209232\n",
      "Iteration 259, loss = 0.08192518\n",
      "Iteration 260, loss = 0.08175945\n",
      "Iteration 261, loss = 0.08159512\n",
      "Iteration 262, loss = 0.08143215\n",
      "Iteration 263, loss = 0.08127055\n",
      "Iteration 264, loss = 0.08111032\n",
      "Iteration 265, loss = 0.08095141\n",
      "Iteration 266, loss = 0.08079380\n",
      "Iteration 267, loss = 0.08063748\n",
      "Iteration 268, loss = 0.08048246\n",
      "Iteration 269, loss = 0.08032866\n",
      "Iteration 270, loss = 0.08017612\n",
      "Iteration 271, loss = 0.08002481\n",
      "Iteration 272, loss = 0.07987471\n",
      "Iteration 273, loss = 0.07972581\n",
      "Iteration 274, loss = 0.07957809\n",
      "Iteration 275, loss = 0.07943155\n",
      "Iteration 276, loss = 0.07928616\n",
      "Iteration 277, loss = 0.07914192\n",
      "Iteration 278, loss = 0.07899883\n",
      "Iteration 279, loss = 0.07885682\n",
      "Iteration 280, loss = 0.07871592\n",
      "Iteration 281, loss = 0.07857612\n",
      "Iteration 282, loss = 0.07843740\n",
      "Iteration 283, loss = 0.07829973\n",
      "Iteration 284, loss = 0.07816313\n",
      "Iteration 285, loss = 0.07802757\n",
      "Iteration 286, loss = 0.07789304\n",
      "Iteration 287, loss = 0.07775952\n",
      "Iteration 288, loss = 0.07762700\n",
      "Iteration 289, loss = 0.07749549\n",
      "Iteration 290, loss = 0.07736496\n",
      "Iteration 291, loss = 0.07723541\n",
      "Iteration 292, loss = 0.07710682\n",
      "Iteration 293, loss = 0.07697918\n",
      "Iteration 294, loss = 0.07685247\n",
      "Iteration 295, loss = 0.07672670\n",
      "Iteration 296, loss = 0.07660185\n",
      "Iteration 297, loss = 0.07647792\n",
      "Iteration 298, loss = 0.07635488\n",
      "Iteration 299, loss = 0.07623274\n",
      "Iteration 300, loss = 0.07611152\n",
      "Iteration 301, loss = 0.07599117\n",
      "Iteration 302, loss = 0.07587169\n",
      "Iteration 303, loss = 0.07575306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 304, loss = 0.07563528\n",
      "Iteration 305, loss = 0.07551835\n",
      "Iteration 306, loss = 0.07540224\n",
      "Iteration 307, loss = 0.07528695\n",
      "Iteration 308, loss = 0.07517248\n",
      "Iteration 309, loss = 0.07505880\n",
      "Iteration 310, loss = 0.07494592\n",
      "Iteration 311, loss = 0.07483382\n",
      "Iteration 312, loss = 0.07472250\n",
      "Iteration 313, loss = 0.07461195\n",
      "Iteration 314, loss = 0.07450216\n",
      "Iteration 315, loss = 0.07439313\n",
      "Iteration 316, loss = 0.07428485\n",
      "Iteration 317, loss = 0.07417732\n",
      "Iteration 318, loss = 0.07407052\n",
      "Iteration 319, loss = 0.07396444\n",
      "Iteration 320, loss = 0.07385907\n",
      "Iteration 321, loss = 0.07375442\n",
      "Iteration 322, loss = 0.07365047\n",
      "Iteration 323, loss = 0.07354721\n",
      "Iteration 324, loss = 0.07344464\n",
      "Iteration 325, loss = 0.07334275\n",
      "Iteration 326, loss = 0.07324154\n",
      "Iteration 327, loss = 0.07314100\n",
      "Iteration 328, loss = 0.07304112\n",
      "Iteration 329, loss = 0.07294190\n",
      "Iteration 330, loss = 0.07284334\n",
      "Iteration 331, loss = 0.07274541\n",
      "Iteration 332, loss = 0.07264811\n",
      "Iteration 333, loss = 0.07255144\n",
      "Iteration 334, loss = 0.07245539\n",
      "Iteration 335, loss = 0.07235996\n",
      "Iteration 336, loss = 0.07226514\n",
      "Iteration 337, loss = 0.07217094\n",
      "Iteration 338, loss = 0.07207733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.35530244\n",
      "Iteration 3, loss = 1.21011263\n",
      "Iteration 4, loss = 1.17353181\n",
      "Iteration 5, loss = 1.07855536\n",
      "Iteration 6, loss = 0.97029392\n",
      "Iteration 7, loss = 0.89171227\n",
      "Iteration 8, loss = 0.83869525\n",
      "Iteration 9, loss = 0.79357123\n",
      "Iteration 10, loss = 0.75201646\n",
      "Iteration 11, loss = 0.71424351\n",
      "Iteration 12, loss = 0.68023966\n",
      "Iteration 13, loss = 0.64981228\n",
      "Iteration 14, loss = 0.62274891\n",
      "Iteration 15, loss = 0.59864517\n",
      "Iteration 16, loss = 0.57691476\n",
      "Iteration 17, loss = 0.55653301\n",
      "Iteration 18, loss = 0.53672367\n",
      "Iteration 19, loss = 0.52096495\n",
      "Iteration 20, loss = 0.50741829\n",
      "Iteration 21, loss = 0.49506520\n",
      "Iteration 22, loss = 0.48370283\n",
      "Iteration 23, loss = 0.47316233\n",
      "Iteration 24, loss = 0.46332089\n",
      "Iteration 25, loss = 0.45407922\n",
      "Iteration 26, loss = 0.44535112\n",
      "Iteration 27, loss = 0.43707187\n",
      "Iteration 28, loss = 0.42918346\n",
      "Iteration 29, loss = 0.42163832\n",
      "Iteration 30, loss = 0.41439900\n",
      "Iteration 31, loss = 0.40744466\n",
      "Iteration 32, loss = 0.40073676\n",
      "Iteration 33, loss = 0.39424986\n",
      "Iteration 34, loss = 0.38804400\n",
      "Iteration 35, loss = 0.38214462\n",
      "Iteration 36, loss = 0.37637313\n",
      "Iteration 37, loss = 0.37070540\n",
      "Iteration 38, loss = 0.36518131\n",
      "Iteration 39, loss = 0.35982434\n",
      "Iteration 40, loss = 0.35461203\n",
      "Iteration 41, loss = 0.34953116\n",
      "Iteration 42, loss = 0.34456152\n",
      "Iteration 43, loss = 0.33969834\n",
      "Iteration 44, loss = 0.33494176\n",
      "Iteration 45, loss = 0.33029476\n",
      "Iteration 46, loss = 0.32575730\n",
      "Iteration 47, loss = 0.32132503\n",
      "Iteration 48, loss = 0.31699507\n",
      "Iteration 49, loss = 0.31276755\n",
      "Iteration 50, loss = 0.30863877\n",
      "Iteration 51, loss = 0.30460487\n",
      "Iteration 52, loss = 0.30066163\n",
      "Iteration 53, loss = 0.29680816\n",
      "Iteration 54, loss = 0.29304040\n",
      "Iteration 55, loss = 0.28935801\n",
      "Iteration 56, loss = 0.28575632\n",
      "Iteration 57, loss = 0.28222608\n",
      "Iteration 58, loss = 0.27876843\n",
      "Iteration 59, loss = 0.27538694\n",
      "Iteration 60, loss = 0.27208033\n",
      "Iteration 61, loss = 0.26884340\n",
      "Iteration 62, loss = 0.26564787\n",
      "Iteration 63, loss = 0.26246951\n",
      "Iteration 64, loss = 0.25930906\n",
      "Iteration 65, loss = 0.25610976\n",
      "Iteration 66, loss = 0.25283771\n",
      "Iteration 67, loss = 0.24958304\n",
      "Iteration 68, loss = 0.24646276\n",
      "Iteration 69, loss = 0.24358746\n",
      "Iteration 70, loss = 0.24086519\n",
      "Iteration 71, loss = 0.23825080\n",
      "Iteration 72, loss = 0.23575079\n",
      "Iteration 73, loss = 0.23333690\n",
      "Iteration 74, loss = 0.23098350\n",
      "Iteration 75, loss = 0.22868426\n",
      "Iteration 76, loss = 0.22643460\n",
      "Iteration 77, loss = 0.22423517\n",
      "Iteration 78, loss = 0.22208624\n",
      "Iteration 79, loss = 0.21998508\n",
      "Iteration 80, loss = 0.21792931\n",
      "Iteration 81, loss = 0.21591622\n",
      "Iteration 82, loss = 0.21394702\n",
      "Iteration 83, loss = 0.21202146\n",
      "Iteration 84, loss = 0.21013808\n",
      "Iteration 85, loss = 0.20829552\n",
      "Iteration 86, loss = 0.20649339\n",
      "Iteration 87, loss = 0.20473135\n",
      "Iteration 88, loss = 0.20300760\n",
      "Iteration 89, loss = 0.20132181\n",
      "Iteration 90, loss = 0.19967141\n",
      "Iteration 91, loss = 0.19805672\n",
      "Iteration 92, loss = 0.19647557\n",
      "Iteration 93, loss = 0.19492691\n",
      "Iteration 94, loss = 0.19341070\n",
      "Iteration 95, loss = 0.19192631\n",
      "Iteration 96, loss = 0.19047172\n",
      "Iteration 97, loss = 0.18904643\n",
      "Iteration 98, loss = 0.18765068\n",
      "Iteration 99, loss = 0.18628369\n",
      "Iteration 100, loss = 0.18494556\n",
      "Iteration 101, loss = 0.18363950\n",
      "Iteration 102, loss = 0.18235965\n",
      "Iteration 103, loss = 0.18110600\n",
      "Iteration 104, loss = 0.17987891\n",
      "Iteration 105, loss = 0.17868363\n",
      "Iteration 106, loss = 0.17752403\n",
      "Iteration 107, loss = 0.17639183\n",
      "Iteration 108, loss = 0.17528487\n",
      "Iteration 109, loss = 0.17420121\n",
      "Iteration 110, loss = 0.17313969\n",
      "Iteration 111, loss = 0.17210697\n",
      "Iteration 112, loss = 0.17108921\n",
      "Iteration 113, loss = 0.17008807\n",
      "Iteration 114, loss = 0.16910351\n",
      "Iteration 115, loss = 0.16814041\n",
      "Iteration 116, loss = 0.16719990\n",
      "Iteration 117, loss = 0.16627834\n",
      "Iteration 118, loss = 0.16537490\n",
      "Iteration 119, loss = 0.16448937\n",
      "Iteration 120, loss = 0.16362105\n",
      "Iteration 121, loss = 0.16276943\n",
      "Iteration 122, loss = 0.16193412\n",
      "Iteration 123, loss = 0.16111481\n",
      "Iteration 124, loss = 0.16031114\n",
      "Iteration 125, loss = 0.15952343\n",
      "Iteration 126, loss = 0.15875252\n",
      "Iteration 127, loss = 0.15799623\n",
      "Iteration 128, loss = 0.15725479\n",
      "Iteration 129, loss = 0.15652827\n",
      "Iteration 130, loss = 0.15581543\n",
      "Iteration 131, loss = 0.15511563\n",
      "Iteration 132, loss = 0.15442921\n",
      "Iteration 133, loss = 0.15375602\n",
      "Iteration 134, loss = 0.15309740\n",
      "Iteration 135, loss = 0.15245183\n",
      "Iteration 136, loss = 0.15182155\n",
      "Iteration 137, loss = 0.15120337\n",
      "Iteration 138, loss = 0.15059523\n",
      "Iteration 139, loss = 0.14999780\n",
      "Iteration 140, loss = 0.14941114\n",
      "Iteration 141, loss = 0.14883445\n",
      "Iteration 142, loss = 0.14826660\n",
      "Iteration 143, loss = 0.14770770\n",
      "Iteration 144, loss = 0.14715774\n",
      "Iteration 145, loss = 0.14661631\n",
      "Iteration 146, loss = 0.14608331\n",
      "Iteration 147, loss = 0.14555865\n",
      "Iteration 148, loss = 0.14504232\n",
      "Iteration 149, loss = 0.14453432\n",
      "Iteration 150, loss = 0.14403481\n",
      "Iteration 151, loss = 0.14354318\n",
      "Iteration 152, loss = 0.14305898\n",
      "Iteration 153, loss = 0.14258262\n",
      "Iteration 154, loss = 0.14211354\n",
      "Iteration 155, loss = 0.14165183\n",
      "Iteration 156, loss = 0.14119721\n",
      "Iteration 157, loss = 0.14074939\n",
      "Iteration 158, loss = 0.14030827\n",
      "Iteration 159, loss = 0.13987373\n",
      "Iteration 160, loss = 0.13944567\n",
      "Iteration 161, loss = 0.13902382\n",
      "Iteration 162, loss = 0.13860818\n",
      "Iteration 163, loss = 0.13819868\n",
      "Iteration 164, loss = 0.13779510\n",
      "Iteration 165, loss = 0.13739729\n",
      "Iteration 166, loss = 0.13700513\n",
      "Iteration 167, loss = 0.13661852\n",
      "Iteration 168, loss = 0.13623735\n",
      "Iteration 169, loss = 0.13586145\n",
      "Iteration 170, loss = 0.13549076\n",
      "Iteration 171, loss = 0.13512525\n",
      "Iteration 172, loss = 0.13476463\n",
      "Iteration 173, loss = 0.13440897\n",
      "Iteration 174, loss = 0.13405812\n",
      "Iteration 175, loss = 0.13371197\n",
      "Iteration 176, loss = 0.13337044\n",
      "Iteration 177, loss = 0.13303344\n",
      "Iteration 178, loss = 0.13270088\n",
      "Iteration 179, loss = 0.13237267\n",
      "Iteration 180, loss = 0.13204873\n",
      "Iteration 181, loss = 0.13172898\n",
      "Iteration 182, loss = 0.13141341\n",
      "Iteration 183, loss = 0.13110179\n",
      "Iteration 184, loss = 0.13079417\n",
      "Iteration 185, loss = 0.13049043\n",
      "Iteration 186, loss = 0.13019052\n",
      "Iteration 187, loss = 0.12989435\n",
      "Iteration 188, loss = 0.12960186\n",
      "Iteration 189, loss = 0.12931300\n",
      "Iteration 190, loss = 0.12902775\n",
      "Iteration 191, loss = 0.12874599\n",
      "Iteration 192, loss = 0.12846764\n",
      "Iteration 193, loss = 0.12819267\n",
      "Iteration 194, loss = 0.12792101\n",
      "Iteration 195, loss = 0.12765268\n",
      "Iteration 196, loss = 0.12738748\n",
      "Iteration 197, loss = 0.12712545\n",
      "Iteration 198, loss = 0.12686651\n",
      "Iteration 199, loss = 0.12661060\n",
      "Iteration 200, loss = 0.12635767\n",
      "Iteration 201, loss = 0.12610769\n",
      "Iteration 202, loss = 0.12586060\n",
      "Iteration 203, loss = 0.12561633\n",
      "Iteration 204, loss = 0.12537492\n",
      "Iteration 205, loss = 0.12513624\n",
      "Iteration 206, loss = 0.12490026\n",
      "Iteration 207, loss = 0.12466692\n",
      "Iteration 208, loss = 0.12443619\n",
      "Iteration 209, loss = 0.12420800\n",
      "Iteration 210, loss = 0.12398235\n",
      "Iteration 211, loss = 0.12375917\n",
      "Iteration 212, loss = 0.12353843\n",
      "Iteration 213, loss = 0.12332009\n",
      "Iteration 214, loss = 0.12310413\n",
      "Iteration 215, loss = 0.12289048\n",
      "Iteration 216, loss = 0.12267912\n",
      "Iteration 217, loss = 0.12247003\n",
      "Iteration 218, loss = 0.12226314\n",
      "Iteration 219, loss = 0.12205843\n",
      "Iteration 220, loss = 0.12185587\n",
      "Iteration 221, loss = 0.12165543\n",
      "Iteration 222, loss = 0.12145706\n",
      "Iteration 223, loss = 0.12126075\n",
      "Iteration 224, loss = 0.12106646\n",
      "Iteration 225, loss = 0.12087415\n",
      "Iteration 226, loss = 0.12068379\n",
      "Iteration 227, loss = 0.12049537\n",
      "Iteration 228, loss = 0.12030884\n",
      "Iteration 229, loss = 0.12012417\n",
      "Iteration 230, loss = 0.11994135\n",
      "Iteration 231, loss = 0.11976034\n",
      "Iteration 232, loss = 0.11958112\n",
      "Iteration 233, loss = 0.11940366\n",
      "Iteration 234, loss = 0.11922794\n",
      "Iteration 235, loss = 0.11905393\n",
      "Iteration 236, loss = 0.11888160\n",
      "Iteration 237, loss = 0.11871093\n",
      "Iteration 238, loss = 0.11854189\n",
      "Iteration 239, loss = 0.11837447\n",
      "Iteration 240, loss = 0.11820863\n",
      "Iteration 241, loss = 0.11804436\n",
      "Iteration 242, loss = 0.11788164\n",
      "Iteration 243, loss = 0.11772043\n",
      "Iteration 244, loss = 0.11756073\n",
      "Iteration 245, loss = 0.11740251\n",
      "Iteration 246, loss = 0.11724574\n",
      "Iteration 247, loss = 0.11709041\n",
      "Iteration 248, loss = 0.11693650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 249, loss = 0.11678399\n",
      "Iteration 250, loss = 0.11663286\n",
      "Iteration 251, loss = 0.11648308\n",
      "Iteration 252, loss = 0.11633465\n",
      "Iteration 253, loss = 0.11618754\n",
      "Iteration 254, loss = 0.11604174\n",
      "Iteration 255, loss = 0.11589722\n",
      "Iteration 256, loss = 0.11575398\n",
      "Iteration 257, loss = 0.11561199\n",
      "Iteration 258, loss = 0.11547124\n",
      "Iteration 259, loss = 0.11533171\n",
      "Iteration 260, loss = 0.11519338\n",
      "Iteration 261, loss = 0.11505624\n",
      "Iteration 262, loss = 0.11492028\n",
      "Iteration 263, loss = 0.11478547\n",
      "Iteration 264, loss = 0.11465181\n",
      "Iteration 265, loss = 0.11451936\n",
      "Iteration 266, loss = 0.11438807\n",
      "Iteration 267, loss = 0.11425787\n",
      "Iteration 268, loss = 0.11412875\n",
      "Iteration 269, loss = 0.11400071\n",
      "Iteration 270, loss = 0.11387371\n",
      "Iteration 271, loss = 0.11374776\n",
      "Iteration 272, loss = 0.11362284\n",
      "Iteration 273, loss = 0.11349894\n",
      "Iteration 274, loss = 0.11337604\n",
      "Iteration 275, loss = 0.11325414\n",
      "Iteration 276, loss = 0.11313322\n",
      "Iteration 277, loss = 0.11301331\n",
      "Iteration 278, loss = 0.11289433\n",
      "Iteration 279, loss = 0.11277629\n",
      "Iteration 280, loss = 0.11265918\n",
      "Iteration 281, loss = 0.11254303\n",
      "Iteration 282, loss = 0.11242776\n",
      "Iteration 283, loss = 0.11231340\n",
      "Iteration 284, loss = 0.11219992\n",
      "Iteration 285, loss = 0.11208734\n",
      "Iteration 286, loss = 0.11197562\n",
      "Iteration 287, loss = 0.11186475\n",
      "Iteration 288, loss = 0.11175472\n",
      "Iteration 289, loss = 0.11164554\n",
      "Iteration 290, loss = 0.11153721\n",
      "Iteration 291, loss = 0.11142967\n",
      "Iteration 292, loss = 0.11132295\n",
      "Iteration 293, loss = 0.11121701\n",
      "Iteration 294, loss = 0.11111187\n",
      "Iteration 295, loss = 0.11100752\n",
      "Iteration 296, loss = 0.11090392\n",
      "Iteration 297, loss = 0.11080109\n",
      "Iteration 298, loss = 0.11069902\n",
      "Iteration 299, loss = 0.11059770\n",
      "Iteration 300, loss = 0.11049710\n",
      "Iteration 301, loss = 0.11039724\n",
      "Iteration 302, loss = 0.11029809\n",
      "Iteration 303, loss = 0.11019968\n",
      "Iteration 304, loss = 0.11010194\n",
      "Iteration 305, loss = 0.11000491\n",
      "Iteration 306, loss = 0.10990856\n",
      "Iteration 307, loss = 0.10981289\n",
      "Iteration 308, loss = 0.10971791\n",
      "Iteration 309, loss = 0.10962358\n",
      "Iteration 310, loss = 0.10952990\n",
      "Iteration 311, loss = 0.10943689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.59494215\n",
      "Iteration 3, loss = 1.53166961\n",
      "Iteration 4, loss = 1.47259591\n",
      "Iteration 5, loss = 1.41822357\n",
      "Iteration 6, loss = 1.36880665\n",
      "Iteration 7, loss = 1.32470150\n",
      "Iteration 8, loss = 1.28625946\n",
      "Iteration 9, loss = 1.25356433\n",
      "Iteration 10, loss = 1.22645956\n",
      "Iteration 11, loss = 1.20440601\n",
      "Iteration 12, loss = 1.18656250\n",
      "Iteration 13, loss = 1.17189561\n",
      "Iteration 14, loss = 1.15924097\n",
      "Iteration 15, loss = 1.14748199\n",
      "Iteration 16, loss = 1.13571561\n",
      "Iteration 17, loss = 1.12331906\n",
      "Iteration 18, loss = 1.10984897\n",
      "Iteration 19, loss = 1.09521930\n",
      "Iteration 20, loss = 1.07959348\n",
      "Iteration 21, loss = 1.06321173\n",
      "Iteration 22, loss = 1.04642308\n",
      "Iteration 23, loss = 1.02964117\n",
      "Iteration 24, loss = 1.01317603\n",
      "Iteration 25, loss = 0.99728768\n",
      "Iteration 26, loss = 0.98220536\n",
      "Iteration 27, loss = 0.96794020\n",
      "Iteration 28, loss = 0.95453523\n",
      "Iteration 29, loss = 0.94191425\n",
      "Iteration 30, loss = 0.92983503\n",
      "Iteration 31, loss = 0.91821509\n",
      "Iteration 32, loss = 0.90689808\n",
      "Iteration 33, loss = 0.89582061\n",
      "Iteration 34, loss = 0.88486964\n",
      "Iteration 35, loss = 0.87401647\n",
      "Iteration 36, loss = 0.86330742\n",
      "Iteration 37, loss = 0.85279519\n",
      "Iteration 38, loss = 0.84251538\n",
      "Iteration 39, loss = 0.83255678\n",
      "Iteration 40, loss = 0.82294774\n",
      "Iteration 41, loss = 0.81363301\n",
      "Iteration 42, loss = 0.80462993\n",
      "Iteration 43, loss = 0.79590536\n",
      "Iteration 44, loss = 0.78746613\n",
      "Iteration 45, loss = 0.77929079\n",
      "Iteration 46, loss = 0.77137454\n",
      "Iteration 47, loss = 0.76368956\n",
      "Iteration 48, loss = 0.75620945\n",
      "Iteration 49, loss = 0.74893641\n",
      "Iteration 50, loss = 0.74186662\n",
      "Iteration 51, loss = 0.73496959\n",
      "Iteration 52, loss = 0.72823547\n",
      "Iteration 53, loss = 0.72165027\n",
      "Iteration 54, loss = 0.71521358\n",
      "Iteration 55, loss = 0.70890400\n",
      "Iteration 56, loss = 0.70273761\n",
      "Iteration 57, loss = 0.69669815\n",
      "Iteration 58, loss = 0.69078458\n",
      "Iteration 59, loss = 0.68496118\n",
      "Iteration 60, loss = 0.67923894\n",
      "Iteration 61, loss = 0.67364631\n",
      "Iteration 62, loss = 0.66822280\n",
      "Iteration 63, loss = 0.66294281\n",
      "Iteration 64, loss = 0.65783104\n",
      "Iteration 65, loss = 0.65293295\n",
      "Iteration 66, loss = 0.64823973\n",
      "Iteration 67, loss = 0.64371443\n",
      "Iteration 68, loss = 0.63931861\n",
      "Iteration 69, loss = 0.63502156\n",
      "Iteration 70, loss = 0.63081621\n",
      "Iteration 71, loss = 0.62670403\n",
      "Iteration 72, loss = 0.62265573\n",
      "Iteration 73, loss = 0.61867536\n",
      "Iteration 74, loss = 0.61475194\n",
      "Iteration 75, loss = 0.61087857\n",
      "Iteration 76, loss = 0.60706180\n",
      "Iteration 77, loss = 0.60330363\n",
      "Iteration 78, loss = 0.59960310\n",
      "Iteration 79, loss = 0.59596041\n",
      "Iteration 80, loss = 0.59237881\n",
      "Iteration 81, loss = 0.58885741\n",
      "Iteration 82, loss = 0.58538669\n",
      "Iteration 83, loss = 0.58196808\n",
      "Iteration 84, loss = 0.57859966\n",
      "Iteration 85, loss = 0.57528178\n",
      "Iteration 86, loss = 0.57201420\n",
      "Iteration 87, loss = 0.56879311\n",
      "Iteration 88, loss = 0.56561599\n",
      "Iteration 89, loss = 0.56248307\n",
      "Iteration 90, loss = 0.55939486\n",
      "Iteration 91, loss = 0.55634964\n",
      "Iteration 92, loss = 0.55334597\n",
      "Iteration 93, loss = 0.55038436\n",
      "Iteration 94, loss = 0.54746518\n",
      "Iteration 95, loss = 0.54458790\n",
      "Iteration 96, loss = 0.54174671\n",
      "Iteration 97, loss = 0.53894309\n",
      "Iteration 98, loss = 0.53617524\n",
      "Iteration 99, loss = 0.53344149\n",
      "Iteration 100, loss = 0.53074142\n",
      "Iteration 101, loss = 0.52807485\n",
      "Iteration 102, loss = 0.52543990\n",
      "Iteration 103, loss = 0.52283724\n",
      "Iteration 104, loss = 0.52026843\n",
      "Iteration 105, loss = 0.51773006\n",
      "Iteration 106, loss = 0.51522095\n",
      "Iteration 107, loss = 0.51273946\n",
      "Iteration 108, loss = 0.51028516\n",
      "Iteration 109, loss = 0.50785855\n",
      "Iteration 110, loss = 0.50545804\n",
      "Iteration 111, loss = 0.50308276\n",
      "Iteration 112, loss = 0.50073183\n",
      "Iteration 113, loss = 0.49840473\n",
      "Iteration 114, loss = 0.49610460\n",
      "Iteration 115, loss = 0.49384012\n",
      "Iteration 116, loss = 0.49161738\n",
      "Iteration 117, loss = 0.48944087\n",
      "Iteration 118, loss = 0.48729794\n",
      "Iteration 119, loss = 0.48516976\n",
      "Iteration 120, loss = 0.48304088\n",
      "Iteration 121, loss = 0.48091798\n",
      "Iteration 122, loss = 0.47885153\n",
      "Iteration 123, loss = 0.47672685\n",
      "Iteration 124, loss = 0.47464845\n",
      "Iteration 125, loss = 0.47258905\n",
      "Iteration 126, loss = 0.47053852\n",
      "Iteration 127, loss = 0.46849469\n",
      "Iteration 128, loss = 0.46645861\n",
      "Iteration 129, loss = 0.46441612\n",
      "Iteration 130, loss = 0.46237355\n",
      "Iteration 131, loss = 0.46035196\n",
      "Iteration 132, loss = 0.45833874\n",
      "Iteration 133, loss = 0.45636254\n",
      "Iteration 134, loss = 0.45442728\n",
      "Iteration 135, loss = 0.45249905\n",
      "Iteration 136, loss = 0.45057045\n",
      "Iteration 137, loss = 0.44864174\n",
      "Iteration 138, loss = 0.44672262\n",
      "Iteration 139, loss = 0.44482381\n",
      "Iteration 140, loss = 0.44292871\n",
      "Iteration 141, loss = 0.44103812\n",
      "Iteration 142, loss = 0.43915442\n",
      "Iteration 143, loss = 0.43728526\n",
      "Iteration 144, loss = 0.43542660\n",
      "Iteration 145, loss = 0.43357202\n",
      "Iteration 146, loss = 0.43172217\n",
      "Iteration 147, loss = 0.42987752\n",
      "Iteration 148, loss = 0.42804173\n",
      "Iteration 149, loss = 0.42621652\n",
      "Iteration 150, loss = 0.42439622\n",
      "Iteration 151, loss = 0.42258008\n",
      "Iteration 152, loss = 0.42076896\n",
      "Iteration 153, loss = 0.41896203\n",
      "Iteration 154, loss = 0.41715837\n",
      "Iteration 155, loss = 0.41535393\n",
      "Iteration 156, loss = 0.41354932\n",
      "Iteration 157, loss = 0.41174674\n",
      "Iteration 158, loss = 0.40995552\n",
      "Iteration 159, loss = 0.40818092\n",
      "Iteration 160, loss = 0.40639985\n",
      "Iteration 161, loss = 0.40461554\n",
      "Iteration 162, loss = 0.40283518\n",
      "Iteration 163, loss = 0.40106617\n",
      "Iteration 164, loss = 0.39930205\n",
      "Iteration 165, loss = 0.39753870\n",
      "Iteration 166, loss = 0.39577566\n",
      "Iteration 167, loss = 0.39401456\n",
      "Iteration 168, loss = 0.39225605\n",
      "Iteration 169, loss = 0.39050412\n",
      "Iteration 170, loss = 0.38875376\n",
      "Iteration 171, loss = 0.38700422\n",
      "Iteration 172, loss = 0.38525622\n",
      "Iteration 173, loss = 0.38351017\n",
      "Iteration 174, loss = 0.38176598\n",
      "Iteration 175, loss = 0.38002410\n",
      "Iteration 176, loss = 0.37828431\n",
      "Iteration 177, loss = 0.37654686\n",
      "Iteration 178, loss = 0.37481185\n",
      "Iteration 179, loss = 0.37307760\n",
      "Iteration 180, loss = 0.37134456\n",
      "Iteration 181, loss = 0.36961386\n",
      "Iteration 182, loss = 0.36788558\n",
      "Iteration 183, loss = 0.36615927\n",
      "Iteration 184, loss = 0.36443475\n",
      "Iteration 185, loss = 0.36271240\n",
      "Iteration 186, loss = 0.36099290\n",
      "Iteration 187, loss = 0.35927572\n",
      "Iteration 188, loss = 0.35756159\n",
      "Iteration 189, loss = 0.35585034\n",
      "Iteration 190, loss = 0.35414194\n",
      "Iteration 191, loss = 0.35243713\n",
      "Iteration 192, loss = 0.35073594\n",
      "Iteration 193, loss = 0.34903817\n",
      "Iteration 194, loss = 0.34734393\n",
      "Iteration 195, loss = 0.34565336\n",
      "Iteration 196, loss = 0.34396690\n",
      "Iteration 197, loss = 0.34228474\n",
      "Iteration 198, loss = 0.34060743\n",
      "Iteration 199, loss = 0.33893334\n",
      "Iteration 200, loss = 0.33725731\n",
      "Iteration 201, loss = 0.33557478\n",
      "Iteration 202, loss = 0.33390164\n",
      "Iteration 203, loss = 0.33223833\n",
      "Iteration 204, loss = 0.33057621\n",
      "Iteration 205, loss = 0.32891378\n",
      "Iteration 206, loss = 0.32725127\n",
      "Iteration 207, loss = 0.32559019\n",
      "Iteration 208, loss = 0.32392984\n",
      "Iteration 209, loss = 0.32227023\n",
      "Iteration 210, loss = 0.32061146\n",
      "Iteration 211, loss = 0.31895367\n",
      "Iteration 212, loss = 0.31729714\n",
      "Iteration 213, loss = 0.31564210\n",
      "Iteration 214, loss = 0.31398899\n",
      "Iteration 215, loss = 0.31233774\n",
      "Iteration 216, loss = 0.31068882\n",
      "Iteration 217, loss = 0.30904221\n",
      "Iteration 218, loss = 0.30739815\n",
      "Iteration 219, loss = 0.30575730\n",
      "Iteration 220, loss = 0.30411974\n",
      "Iteration 221, loss = 0.30248554\n",
      "Iteration 222, loss = 0.30085510\n",
      "Iteration 223, loss = 0.29922864\n",
      "Iteration 224, loss = 0.29760654\n",
      "Iteration 225, loss = 0.29598896\n",
      "Iteration 226, loss = 0.29437637\n",
      "Iteration 227, loss = 0.29276905\n",
      "Iteration 228, loss = 0.29116732\n",
      "Iteration 229, loss = 0.28957130\n",
      "Iteration 230, loss = 0.28798148\n",
      "Iteration 231, loss = 0.28639788\n",
      "Iteration 232, loss = 0.28482069\n",
      "Iteration 233, loss = 0.28325033\n",
      "Iteration 234, loss = 0.28168684\n",
      "Iteration 235, loss = 0.28013042\n",
      "Iteration 236, loss = 0.27858143\n",
      "Iteration 237, loss = 0.27703998\n",
      "Iteration 238, loss = 0.27550624\n",
      "Iteration 239, loss = 0.27398071\n",
      "Iteration 240, loss = 0.27246272\n",
      "Iteration 241, loss = 0.27094873\n",
      "Iteration 242, loss = 0.26943650\n",
      "Iteration 243, loss = 0.26793998\n",
      "Iteration 244, loss = 0.26644966\n",
      "Iteration 245, loss = 0.26496533\n",
      "Iteration 246, loss = 0.26348698\n",
      "Iteration 247, loss = 0.26201497\n",
      "Iteration 248, loss = 0.26054952\n",
      "Iteration 249, loss = 0.25909063\n",
      "Iteration 250, loss = 0.25763816\n",
      "Iteration 251, loss = 0.25619192\n",
      "Iteration 252, loss = 0.25475185\n",
      "Iteration 253, loss = 0.25331796\n",
      "Iteration 254, loss = 0.25189015\n",
      "Iteration 255, loss = 0.25046807\n",
      "Iteration 256, loss = 0.24905147\n",
      "Iteration 257, loss = 0.24764043\n",
      "Iteration 258, loss = 0.24623376\n",
      "Iteration 259, loss = 0.24482811\n",
      "Iteration 260, loss = 0.24341899\n",
      "Iteration 261, loss = 0.24201973\n",
      "Iteration 262, loss = 0.24062779\n",
      "Iteration 263, loss = 0.23923931\n",
      "Iteration 264, loss = 0.23785454\n",
      "Iteration 265, loss = 0.23647367\n",
      "Iteration 266, loss = 0.23509689\n",
      "Iteration 267, loss = 0.23372450\n",
      "Iteration 268, loss = 0.23235663\n",
      "Iteration 269, loss = 0.23099345\n",
      "Iteration 270, loss = 0.22963535\n",
      "Iteration 271, loss = 0.22828221\n",
      "Iteration 272, loss = 0.22693447\n",
      "Iteration 273, loss = 0.22559241\n",
      "Iteration 274, loss = 0.22425648\n",
      "Iteration 275, loss = 0.22292660\n",
      "Iteration 276, loss = 0.22160301\n",
      "Iteration 277, loss = 0.22028608\n",
      "Iteration 278, loss = 0.21897593\n",
      "Iteration 279, loss = 0.21767357\n",
      "Iteration 280, loss = 0.21637847\n",
      "Iteration 281, loss = 0.21509092\n",
      "Iteration 282, loss = 0.21381113\n",
      "Iteration 283, loss = 0.21253873\n",
      "Iteration 284, loss = 0.21127741\n",
      "Iteration 285, loss = 0.21001994\n",
      "Iteration 286, loss = 0.20877557\n",
      "Iteration 287, loss = 0.20753922\n",
      "Iteration 288, loss = 0.20631055\n",
      "Iteration 289, loss = 0.20508916\n",
      "Iteration 290, loss = 0.20387523\n",
      "Iteration 291, loss = 0.20267282\n",
      "Iteration 292, loss = 0.20148113\n",
      "Iteration 293, loss = 0.20029563\n",
      "Iteration 294, loss = 0.19912246\n",
      "Iteration 295, loss = 0.19795637\n",
      "Iteration 296, loss = 0.19679779\n",
      "Iteration 297, loss = 0.19564793\n",
      "Iteration 298, loss = 0.19450976\n",
      "Iteration 299, loss = 0.19337767\n",
      "Iteration 300, loss = 0.19225416\n",
      "Iteration 301, loss = 0.19113994\n",
      "Iteration 302, loss = 0.19003224\n",
      "Iteration 303, loss = 0.18893136\n",
      "Iteration 304, loss = 0.18783903\n",
      "Iteration 305, loss = 0.18675346\n",
      "Iteration 306, loss = 0.18567491\n",
      "Iteration 307, loss = 0.18460457\n",
      "Iteration 308, loss = 0.18354096\n",
      "Iteration 309, loss = 0.18248483\n",
      "Iteration 310, loss = 0.18143605\n",
      "Iteration 311, loss = 0.18039447\n",
      "Iteration 312, loss = 0.17936088\n",
      "Iteration 313, loss = 0.17833520\n",
      "Iteration 314, loss = 0.17731673\n",
      "Iteration 315, loss = 0.17630609\n",
      "Iteration 316, loss = 0.17530355\n",
      "Iteration 317, loss = 0.17430911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 318, loss = 0.17332259\n",
      "Iteration 319, loss = 0.17234434\n",
      "Iteration 320, loss = 0.17137442\n",
      "Iteration 321, loss = 0.17041292\n",
      "Iteration 322, loss = 0.16945980\n",
      "Iteration 323, loss = 0.16851541\n",
      "Iteration 324, loss = 0.16757978\n",
      "Iteration 325, loss = 0.16665301\n",
      "Iteration 326, loss = 0.16573551\n",
      "Iteration 327, loss = 0.16482699\n",
      "Iteration 328, loss = 0.16392722\n",
      "Iteration 329, loss = 0.16303626\n",
      "Iteration 330, loss = 0.16215445\n",
      "Iteration 331, loss = 0.16128172\n",
      "Iteration 332, loss = 0.16041825\n",
      "Iteration 333, loss = 0.15956380\n",
      "Iteration 334, loss = 0.15871857\n",
      "Iteration 335, loss = 0.15788245\n",
      "Iteration 336, loss = 0.15705544\n",
      "Iteration 337, loss = 0.15623778\n",
      "Iteration 338, loss = 0.15542925\n",
      "Iteration 339, loss = 0.15462983\n",
      "Iteration 340, loss = 0.15383943\n",
      "Iteration 341, loss = 0.15305812\n",
      "Iteration 342, loss = 0.15228598\n",
      "Iteration 343, loss = 0.15152295\n",
      "Iteration 344, loss = 0.15076895\n",
      "Iteration 345, loss = 0.15002400\n",
      "Iteration 346, loss = 0.14928806\n",
      "Iteration 347, loss = 0.14856089\n",
      "Iteration 348, loss = 0.14784256\n",
      "Iteration 349, loss = 0.14713308\n",
      "Iteration 350, loss = 0.14643236\n",
      "Iteration 351, loss = 0.14574043\n",
      "Iteration 352, loss = 0.14505752\n",
      "Iteration 353, loss = 0.14438306\n",
      "Iteration 354, loss = 0.14371680\n",
      "Iteration 355, loss = 0.14305869\n",
      "Iteration 356, loss = 0.14240940\n",
      "Iteration 357, loss = 0.14176708\n",
      "Iteration 358, loss = 0.14113327\n",
      "Iteration 359, loss = 0.14050748\n",
      "Iteration 360, loss = 0.13988869\n",
      "Iteration 361, loss = 0.13927694\n",
      "Iteration 362, loss = 0.13867385\n",
      "Iteration 363, loss = 0.13807685\n",
      "Iteration 364, loss = 0.13748743\n",
      "Iteration 365, loss = 0.13690536\n",
      "Iteration 366, loss = 0.13632978\n",
      "Iteration 367, loss = 0.13576085\n",
      "Iteration 368, loss = 0.13519925\n",
      "Iteration 369, loss = 0.13464441\n",
      "Iteration 370, loss = 0.13409594\n",
      "Iteration 371, loss = 0.13355445\n",
      "Iteration 372, loss = 0.13301886\n",
      "Iteration 373, loss = 0.13248924\n",
      "Iteration 374, loss = 0.13196616\n",
      "Iteration 375, loss = 0.13144916\n",
      "Iteration 376, loss = 0.13093791\n",
      "Iteration 377, loss = 0.13043284\n",
      "Iteration 378, loss = 0.12993361\n",
      "Iteration 379, loss = 0.12944007\n",
      "Iteration 380, loss = 0.12895221\n",
      "Iteration 381, loss = 0.12847021\n",
      "Iteration 382, loss = 0.12799373\n",
      "Iteration 383, loss = 0.12752287\n",
      "Iteration 384, loss = 0.12705764\n",
      "Iteration 385, loss = 0.12659732\n",
      "Iteration 386, loss = 0.12614162\n",
      "Iteration 387, loss = 0.12569182\n",
      "Iteration 388, loss = 0.12524692\n",
      "Iteration 389, loss = 0.12480658\n",
      "Iteration 390, loss = 0.12437138\n",
      "Iteration 391, loss = 0.12394033\n",
      "Iteration 392, loss = 0.12351407\n",
      "Iteration 393, loss = 0.12309171\n",
      "Iteration 394, loss = 0.12267366\n",
      "Iteration 395, loss = 0.12225971\n",
      "Iteration 396, loss = 0.12184961\n",
      "Iteration 397, loss = 0.12144302\n",
      "Iteration 398, loss = 0.12104018\n",
      "Iteration 399, loss = 0.12064033\n",
      "Iteration 400, loss = 0.12024306\n",
      "Iteration 401, loss = 0.11984885\n",
      "Iteration 402, loss = 0.11945725\n",
      "Iteration 403, loss = 0.11906828\n",
      "Iteration 404, loss = 0.11868195\n",
      "Iteration 405, loss = 0.11829777\n",
      "Iteration 406, loss = 0.11791570\n",
      "Iteration 407, loss = 0.11753606\n",
      "Iteration 408, loss = 0.11715891\n",
      "Iteration 409, loss = 0.11678370\n",
      "Iteration 410, loss = 0.11641066\n",
      "Iteration 411, loss = 0.11604015\n",
      "Iteration 412, loss = 0.11567182\n",
      "Iteration 413, loss = 0.11530553\n",
      "Iteration 414, loss = 0.11494160\n",
      "Iteration 415, loss = 0.11458016\n",
      "Iteration 416, loss = 0.11422099\n",
      "Iteration 417, loss = 0.11386420\n",
      "Iteration 418, loss = 0.11350977\n",
      "Iteration 419, loss = 0.11315783\n",
      "Iteration 420, loss = 0.11280835\n",
      "Iteration 421, loss = 0.11246124\n",
      "Iteration 422, loss = 0.11211677\n",
      "Iteration 423, loss = 0.11177486\n",
      "Iteration 424, loss = 0.11143554\n",
      "Iteration 425, loss = 0.11109895\n",
      "Iteration 426, loss = 0.11076506\n",
      "Iteration 427, loss = 0.11043384\n",
      "Iteration 428, loss = 0.11010547\n",
      "Iteration 429, loss = 0.10977986\n",
      "Iteration 430, loss = 0.10945712\n",
      "Iteration 431, loss = 0.10913738\n",
      "Iteration 432, loss = 0.10882044\n",
      "Iteration 433, loss = 0.10850643\n",
      "Iteration 434, loss = 0.10819525\n",
      "Iteration 435, loss = 0.10788700\n",
      "Iteration 436, loss = 0.10758164\n",
      "Iteration 437, loss = 0.10727914\n",
      "Iteration 438, loss = 0.10697973\n",
      "Iteration 439, loss = 0.10668324\n",
      "Iteration 440, loss = 0.10638973\n",
      "Iteration 441, loss = 0.10609906\n",
      "Iteration 442, loss = 0.10581137\n",
      "Iteration 443, loss = 0.10552668\n",
      "Iteration 444, loss = 0.10524476\n",
      "Iteration 445, loss = 0.10496577\n",
      "Iteration 446, loss = 0.10468986\n",
      "Iteration 447, loss = 0.10441618\n",
      "Iteration 448, loss = 0.10414591\n",
      "Iteration 449, loss = 0.10387837\n",
      "Iteration 450, loss = 0.10361368\n",
      "Iteration 451, loss = 0.10335193\n",
      "Iteration 452, loss = 0.10309273\n",
      "Iteration 453, loss = 0.10283629\n",
      "Iteration 454, loss = 0.10258237\n",
      "Iteration 455, loss = 0.10233096\n",
      "Iteration 456, loss = 0.10208227\n",
      "Iteration 457, loss = 0.10183603\n",
      "Iteration 458, loss = 0.10159228\n",
      "Iteration 459, loss = 0.10135101\n",
      "Iteration 460, loss = 0.10111213\n",
      "Iteration 461, loss = 0.10087578\n",
      "Iteration 462, loss = 0.10064182\n",
      "Iteration 463, loss = 0.10041011\n",
      "Iteration 464, loss = 0.10018061\n",
      "Iteration 465, loss = 0.09995335\n",
      "Iteration 466, loss = 0.09972821\n",
      "Iteration 467, loss = 0.09950521\n",
      "Iteration 468, loss = 0.09928448\n",
      "Iteration 469, loss = 0.09906580\n",
      "Iteration 470, loss = 0.09884943\n",
      "Iteration 471, loss = 0.09863493\n",
      "Iteration 472, loss = 0.09842278\n",
      "Iteration 473, loss = 0.09821275\n",
      "Iteration 474, loss = 0.09800462\n",
      "Iteration 475, loss = 0.09779846\n",
      "Iteration 476, loss = 0.09759439\n",
      "Iteration 477, loss = 0.09739229\n",
      "Iteration 478, loss = 0.09719212\n",
      "Iteration 479, loss = 0.09699389\n",
      "Iteration 480, loss = 0.09679778\n",
      "Iteration 481, loss = 0.09660372\n",
      "Iteration 482, loss = 0.09641157\n",
      "Iteration 483, loss = 0.09622136\n",
      "Iteration 484, loss = 0.09603315\n",
      "Iteration 485, loss = 0.09584678\n",
      "Iteration 486, loss = 0.09566247\n",
      "Iteration 487, loss = 0.09548025\n",
      "Iteration 488, loss = 0.09530009\n",
      "Iteration 489, loss = 0.09512201\n",
      "Iteration 490, loss = 0.09494600\n",
      "Iteration 491, loss = 0.09477183\n",
      "Iteration 492, loss = 0.09459956\n",
      "Iteration 493, loss = 0.09442910\n",
      "Iteration 494, loss = 0.09426046\n",
      "Iteration 495, loss = 0.09409363\n",
      "Iteration 496, loss = 0.09392867\n",
      "Iteration 497, loss = 0.09376582\n",
      "Iteration 498, loss = 0.09360497\n",
      "Iteration 499, loss = 0.09344606\n",
      "Iteration 500, loss = 0.09328885\n",
      "Iteration 501, loss = 0.09313320\n",
      "Iteration 502, loss = 0.09297917\n",
      "Iteration 503, loss = 0.09282690\n",
      "Iteration 504, loss = 0.09267673\n",
      "Iteration 505, loss = 0.09252828\n",
      "Iteration 506, loss = 0.09238150\n",
      "Iteration 507, loss = 0.09223648\n",
      "Iteration 508, loss = 0.09209306\n",
      "Iteration 509, loss = 0.09195124\n",
      "Iteration 510, loss = 0.09181108\n",
      "Iteration 511, loss = 0.09167263\n",
      "Iteration 512, loss = 0.09153593\n",
      "Iteration 513, loss = 0.09140109\n",
      "Iteration 514, loss = 0.09126772\n",
      "Iteration 515, loss = 0.09113575\n",
      "Iteration 516, loss = 0.09100523\n",
      "Iteration 517, loss = 0.09087627\n",
      "Iteration 518, loss = 0.09074879\n",
      "Iteration 519, loss = 0.09062297\n",
      "Iteration 520, loss = 0.09049858\n",
      "Iteration 521, loss = 0.09037564\n",
      "Iteration 522, loss = 0.09025409\n",
      "Iteration 523, loss = 0.09013407\n",
      "Iteration 524, loss = 0.09001547\n",
      "Iteration 525, loss = 0.08989818\n",
      "Iteration 526, loss = 0.08978222\n",
      "Iteration 527, loss = 0.08966754\n",
      "Iteration 528, loss = 0.08955415\n",
      "Iteration 529, loss = 0.08944214\n",
      "Iteration 530, loss = 0.08933158\n",
      "Iteration 531, loss = 0.08922232\n",
      "Iteration 532, loss = 0.08911429\n",
      "Iteration 533, loss = 0.08900808\n",
      "Iteration 534, loss = 0.08890303\n",
      "Iteration 535, loss = 0.08879903\n",
      "Iteration 536, loss = 0.08869498\n",
      "Iteration 537, loss = 0.08859241\n",
      "Iteration 538, loss = 0.08849151\n",
      "Iteration 539, loss = 0.08839205\n",
      "Iteration 540, loss = 0.08829358\n",
      "Iteration 541, loss = 0.08819576\n",
      "Iteration 542, loss = 0.08809901\n",
      "Iteration 543, loss = 0.08800358\n",
      "Iteration 544, loss = 0.08790926\n",
      "Iteration 545, loss = 0.08781601\n",
      "Iteration 546, loss = 0.08772342\n",
      "Iteration 547, loss = 0.08763174\n",
      "Iteration 548, loss = 0.08754138\n",
      "Iteration 549, loss = 0.08745207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.58696255\n",
      "Iteration 3, loss = 1.52372752\n",
      "Iteration 4, loss = 1.46448076\n",
      "Iteration 5, loss = 1.40978289\n",
      "Iteration 6, loss = 1.36001658\n",
      "Iteration 7, loss = 1.31552446\n",
      "Iteration 8, loss = 1.27660874\n",
      "Iteration 9, loss = 1.24331959\n",
      "Iteration 10, loss = 1.21558815\n",
      "Iteration 11, loss = 1.19291581\n",
      "Iteration 12, loss = 1.17453185\n",
      "Iteration 13, loss = 1.15944298\n",
      "Iteration 14, loss = 1.14653470\n",
      "Iteration 15, loss = 1.13487049\n",
      "Iteration 16, loss = 1.12354390\n",
      "Iteration 17, loss = 1.11169409\n",
      "Iteration 18, loss = 1.09885015\n",
      "Iteration 19, loss = 1.08493257\n",
      "Iteration 20, loss = 1.07003044\n",
      "Iteration 21, loss = 1.05434272\n",
      "Iteration 22, loss = 1.03825691\n",
      "Iteration 23, loss = 1.02210602\n",
      "Iteration 24, loss = 1.00625894\n",
      "Iteration 25, loss = 0.99093701\n",
      "Iteration 26, loss = 0.97634915\n",
      "Iteration 27, loss = 0.96257636\n",
      "Iteration 28, loss = 0.94966909\n",
      "Iteration 29, loss = 0.93756648\n",
      "Iteration 30, loss = 0.92609988\n",
      "Iteration 31, loss = 0.91514477\n",
      "Iteration 32, loss = 0.90457131\n",
      "Iteration 33, loss = 0.89425044\n",
      "Iteration 34, loss = 0.88411157\n",
      "Iteration 35, loss = 0.87405439\n",
      "Iteration 36, loss = 0.86403765\n",
      "Iteration 37, loss = 0.85409354\n",
      "Iteration 38, loss = 0.84421078\n",
      "Iteration 39, loss = 0.83449493\n",
      "Iteration 40, loss = 0.82501681\n",
      "Iteration 41, loss = 0.81576045\n",
      "Iteration 42, loss = 0.80676571\n",
      "Iteration 43, loss = 0.79803113\n",
      "Iteration 44, loss = 0.78960213\n",
      "Iteration 45, loss = 0.78147664\n",
      "Iteration 46, loss = 0.77360338\n",
      "Iteration 47, loss = 0.76598365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.75860812\n",
      "Iteration 49, loss = 0.75144724\n",
      "Iteration 50, loss = 0.74451864\n",
      "Iteration 51, loss = 0.73776497\n",
      "Iteration 52, loss = 0.73117654\n",
      "Iteration 53, loss = 0.72476562\n",
      "Iteration 54, loss = 0.71849685\n",
      "Iteration 55, loss = 0.71234772\n",
      "Iteration 56, loss = 0.70631672\n",
      "Iteration 57, loss = 0.70039712\n",
      "Iteration 58, loss = 0.69458274\n",
      "Iteration 59, loss = 0.68884250\n",
      "Iteration 60, loss = 0.68318356\n",
      "Iteration 61, loss = 0.67765389\n",
      "Iteration 62, loss = 0.67224450\n",
      "Iteration 63, loss = 0.66693734\n",
      "Iteration 64, loss = 0.66180065\n",
      "Iteration 65, loss = 0.65688378\n",
      "Iteration 66, loss = 0.65214630\n",
      "Iteration 67, loss = 0.64760043\n",
      "Iteration 68, loss = 0.64317219\n",
      "Iteration 69, loss = 0.63883537\n",
      "Iteration 70, loss = 0.63459284\n",
      "Iteration 71, loss = 0.63043629\n",
      "Iteration 72, loss = 0.62636199\n",
      "Iteration 73, loss = 0.62234494\n",
      "Iteration 74, loss = 0.61838768\n",
      "Iteration 75, loss = 0.61449169\n",
      "Iteration 76, loss = 0.61065358\n",
      "Iteration 77, loss = 0.60687656\n",
      "Iteration 78, loss = 0.60316527\n",
      "Iteration 79, loss = 0.59951897\n",
      "Iteration 80, loss = 0.59593411\n",
      "Iteration 81, loss = 0.59241083\n",
      "Iteration 82, loss = 0.58894716\n",
      "Iteration 83, loss = 0.58553964\n",
      "Iteration 84, loss = 0.58218437\n",
      "Iteration 85, loss = 0.57888086\n",
      "Iteration 86, loss = 0.57562675\n",
      "Iteration 87, loss = 0.57241907\n",
      "Iteration 88, loss = 0.56925677\n",
      "Iteration 89, loss = 0.56613876\n",
      "Iteration 90, loss = 0.56306420\n",
      "Iteration 91, loss = 0.56003232\n",
      "Iteration 92, loss = 0.55704261\n",
      "Iteration 93, loss = 0.55409383\n",
      "Iteration 94, loss = 0.55118431\n",
      "Iteration 95, loss = 0.54831301\n",
      "Iteration 96, loss = 0.54547902\n",
      "Iteration 97, loss = 0.54267953\n",
      "Iteration 98, loss = 0.53991407\n",
      "Iteration 99, loss = 0.53718333\n",
      "Iteration 100, loss = 0.53448545\n",
      "Iteration 101, loss = 0.53181852\n",
      "Iteration 102, loss = 0.52918170\n",
      "Iteration 103, loss = 0.52657018\n",
      "Iteration 104, loss = 0.52398723\n",
      "Iteration 105, loss = 0.52143103\n",
      "Iteration 106, loss = 0.51890123\n",
      "Iteration 107, loss = 0.51639637\n",
      "Iteration 108, loss = 0.51391640\n",
      "Iteration 109, loss = 0.51145891\n",
      "Iteration 110, loss = 0.50902532\n",
      "Iteration 111, loss = 0.50661338\n",
      "Iteration 112, loss = 0.50422228\n",
      "Iteration 113, loss = 0.50185256\n",
      "Iteration 114, loss = 0.49950393\n",
      "Iteration 115, loss = 0.49717540\n",
      "Iteration 116, loss = 0.49486793\n",
      "Iteration 117, loss = 0.49258094\n",
      "Iteration 118, loss = 0.49031400\n",
      "Iteration 119, loss = 0.48806651\n",
      "Iteration 120, loss = 0.48583396\n",
      "Iteration 121, loss = 0.48360666\n",
      "Iteration 122, loss = 0.48137693\n",
      "Iteration 123, loss = 0.47918119\n",
      "Iteration 124, loss = 0.47702951\n",
      "Iteration 125, loss = 0.47485742\n",
      "Iteration 126, loss = 0.47273431\n",
      "Iteration 127, loss = 0.47061917\n",
      "Iteration 128, loss = 0.46851167\n",
      "Iteration 129, loss = 0.46641158\n",
      "Iteration 130, loss = 0.46432051\n",
      "Iteration 131, loss = 0.46223997\n",
      "Iteration 132, loss = 0.46018498\n",
      "Iteration 133, loss = 0.45814306\n",
      "Iteration 134, loss = 0.45610058\n",
      "Iteration 135, loss = 0.45407578\n",
      "Iteration 136, loss = 0.45206933\n",
      "Iteration 137, loss = 0.45007033\n",
      "Iteration 138, loss = 0.44807642\n",
      "Iteration 139, loss = 0.44609415\n",
      "Iteration 140, loss = 0.44412951\n",
      "Iteration 141, loss = 0.44217274\n",
      "Iteration 142, loss = 0.44022383\n",
      "Iteration 143, loss = 0.43828310\n",
      "Iteration 144, loss = 0.43635151\n",
      "Iteration 145, loss = 0.43442912\n",
      "Iteration 146, loss = 0.43251769\n",
      "Iteration 147, loss = 0.43061596\n",
      "Iteration 148, loss = 0.42871907\n",
      "Iteration 149, loss = 0.42683224\n",
      "Iteration 150, loss = 0.42495370\n",
      "Iteration 151, loss = 0.42310121\n",
      "Iteration 152, loss = 0.42126050\n",
      "Iteration 153, loss = 0.41943113\n",
      "Iteration 154, loss = 0.41761895\n",
      "Iteration 155, loss = 0.41582155\n",
      "Iteration 156, loss = 0.41402936\n",
      "Iteration 157, loss = 0.41225574\n",
      "Iteration 158, loss = 0.41048772\n",
      "Iteration 159, loss = 0.40870872\n",
      "Iteration 160, loss = 0.40690683\n",
      "Iteration 161, loss = 0.40510268\n",
      "Iteration 162, loss = 0.40331912\n",
      "Iteration 163, loss = 0.40154702\n",
      "Iteration 164, loss = 0.39978538\n",
      "Iteration 165, loss = 0.39802588\n",
      "Iteration 166, loss = 0.39626955\n",
      "Iteration 167, loss = 0.39452112\n",
      "Iteration 168, loss = 0.39278418\n",
      "Iteration 169, loss = 0.39105331\n",
      "Iteration 170, loss = 0.38932754\n",
      "Iteration 171, loss = 0.38760508\n",
      "Iteration 172, loss = 0.38588471\n",
      "Iteration 173, loss = 0.38416725\n",
      "Iteration 174, loss = 0.38245375\n",
      "Iteration 175, loss = 0.38074305\n",
      "Iteration 176, loss = 0.37903626\n",
      "Iteration 177, loss = 0.37733475\n",
      "Iteration 178, loss = 0.37564207\n",
      "Iteration 179, loss = 0.37395487\n",
      "Iteration 180, loss = 0.37227501\n",
      "Iteration 181, loss = 0.37059915\n",
      "Iteration 182, loss = 0.36892792\n",
      "Iteration 183, loss = 0.36726170\n",
      "Iteration 184, loss = 0.36560049\n",
      "Iteration 185, loss = 0.36394448\n",
      "Iteration 186, loss = 0.36229405\n",
      "Iteration 187, loss = 0.36064917\n",
      "Iteration 188, loss = 0.35900954\n",
      "Iteration 189, loss = 0.35737563\n",
      "Iteration 190, loss = 0.35574711\n",
      "Iteration 191, loss = 0.35412448\n",
      "Iteration 192, loss = 0.35250768\n",
      "Iteration 193, loss = 0.35089775\n",
      "Iteration 194, loss = 0.34929371\n",
      "Iteration 195, loss = 0.34769591\n",
      "Iteration 196, loss = 0.34610377\n",
      "Iteration 197, loss = 0.34451730\n",
      "Iteration 198, loss = 0.34293767\n",
      "Iteration 199, loss = 0.34136408\n",
      "Iteration 200, loss = 0.33979571\n",
      "Iteration 201, loss = 0.33822943\n",
      "Iteration 202, loss = 0.33665743\n",
      "Iteration 203, loss = 0.33509184\n",
      "Iteration 204, loss = 0.33353705\n",
      "Iteration 205, loss = 0.33198622\n",
      "Iteration 206, loss = 0.33043670\n",
      "Iteration 207, loss = 0.32888771\n",
      "Iteration 208, loss = 0.32733929\n",
      "Iteration 209, loss = 0.32579191\n",
      "Iteration 210, loss = 0.32424500\n",
      "Iteration 211, loss = 0.32269840\n",
      "Iteration 212, loss = 0.32115172\n",
      "Iteration 213, loss = 0.31960561\n",
      "Iteration 214, loss = 0.31805984\n",
      "Iteration 215, loss = 0.31651423\n",
      "Iteration 216, loss = 0.31496908\n",
      "Iteration 217, loss = 0.31342498\n",
      "Iteration 218, loss = 0.31188142\n",
      "Iteration 219, loss = 0.31033970\n",
      "Iteration 220, loss = 0.30879940\n",
      "Iteration 221, loss = 0.30726004\n",
      "Iteration 222, loss = 0.30572270\n",
      "Iteration 223, loss = 0.30418786\n",
      "Iteration 224, loss = 0.30265513\n",
      "Iteration 225, loss = 0.30112506\n",
      "Iteration 226, loss = 0.29959798\n",
      "Iteration 227, loss = 0.29807368\n",
      "Iteration 228, loss = 0.29655222\n",
      "Iteration 229, loss = 0.29503385\n",
      "Iteration 230, loss = 0.29351900\n",
      "Iteration 231, loss = 0.29200818\n",
      "Iteration 232, loss = 0.29050125\n",
      "Iteration 233, loss = 0.28899852\n",
      "Iteration 234, loss = 0.28750004\n",
      "Iteration 235, loss = 0.28600619\n",
      "Iteration 236, loss = 0.28451754\n",
      "Iteration 237, loss = 0.28303355\n",
      "Iteration 238, loss = 0.28155475\n",
      "Iteration 239, loss = 0.28008153\n",
      "Iteration 240, loss = 0.27861376\n",
      "Iteration 241, loss = 0.27715172\n",
      "Iteration 242, loss = 0.27569609\n",
      "Iteration 243, loss = 0.27424621\n",
      "Iteration 244, loss = 0.27280250\n",
      "Iteration 245, loss = 0.27136530\n",
      "Iteration 246, loss = 0.26993455\n",
      "Iteration 247, loss = 0.26850529\n",
      "Iteration 248, loss = 0.26708058\n",
      "Iteration 249, loss = 0.26566632\n",
      "Iteration 250, loss = 0.26425579\n",
      "Iteration 251, loss = 0.26284933\n",
      "Iteration 252, loss = 0.26144688\n",
      "Iteration 253, loss = 0.26004841\n",
      "Iteration 254, loss = 0.25865479\n",
      "Iteration 255, loss = 0.25726466\n",
      "Iteration 256, loss = 0.25587798\n",
      "Iteration 257, loss = 0.25449469\n",
      "Iteration 258, loss = 0.25311429\n",
      "Iteration 259, loss = 0.25173627\n",
      "Iteration 260, loss = 0.25035987\n",
      "Iteration 261, loss = 0.24898372\n",
      "Iteration 262, loss = 0.24760238\n",
      "Iteration 263, loss = 0.24622099\n",
      "Iteration 264, loss = 0.24484643\n",
      "Iteration 265, loss = 0.24347208\n",
      "Iteration 266, loss = 0.24209779\n",
      "Iteration 267, loss = 0.24072369\n",
      "Iteration 268, loss = 0.23935007\n",
      "Iteration 269, loss = 0.23797713\n",
      "Iteration 270, loss = 0.23660518\n",
      "Iteration 271, loss = 0.23523461\n",
      "Iteration 272, loss = 0.23386560\n",
      "Iteration 273, loss = 0.23249848\n",
      "Iteration 274, loss = 0.23113367\n",
      "Iteration 275, loss = 0.22977150\n",
      "Iteration 276, loss = 0.22841220\n",
      "Iteration 277, loss = 0.22705629\n",
      "Iteration 278, loss = 0.22570408\n",
      "Iteration 279, loss = 0.22435590\n",
      "Iteration 280, loss = 0.22301210\n",
      "Iteration 281, loss = 0.22167324\n",
      "Iteration 282, loss = 0.22033940\n",
      "Iteration 283, loss = 0.21901107\n",
      "Iteration 284, loss = 0.21768859\n",
      "Iteration 285, loss = 0.21637221\n",
      "Iteration 286, loss = 0.21506239\n",
      "Iteration 287, loss = 0.21375927\n",
      "Iteration 288, loss = 0.21246332\n",
      "Iteration 289, loss = 0.21117441\n",
      "Iteration 290, loss = 0.20989356\n",
      "Iteration 291, loss = 0.20861988\n",
      "Iteration 292, loss = 0.20735379\n",
      "Iteration 293, loss = 0.20609613\n",
      "Iteration 294, loss = 0.20484753\n",
      "Iteration 295, loss = 0.20360742\n",
      "Iteration 296, loss = 0.20237407\n",
      "Iteration 297, loss = 0.20114942\n",
      "Iteration 298, loss = 0.19993949\n",
      "Iteration 299, loss = 0.19873025\n",
      "Iteration 300, loss = 0.19753814\n",
      "Iteration 301, loss = 0.19635388\n",
      "Iteration 302, loss = 0.19517611\n",
      "Iteration 303, loss = 0.19400584\n",
      "Iteration 304, loss = 0.19284593\n",
      "Iteration 305, loss = 0.19170417\n",
      "Iteration 306, loss = 0.19056140\n",
      "Iteration 307, loss = 0.18943392\n",
      "Iteration 308, loss = 0.18831818\n",
      "Iteration 309, loss = 0.18721140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 310, loss = 0.18611327\n",
      "Iteration 311, loss = 0.18502380\n",
      "Iteration 312, loss = 0.18394363\n",
      "Iteration 313, loss = 0.18287754\n",
      "Iteration 314, loss = 0.18181996\n",
      "Iteration 315, loss = 0.18077043\n",
      "Iteration 316, loss = 0.17973364\n",
      "Iteration 317, loss = 0.17870688\n",
      "Iteration 318, loss = 0.17768871\n",
      "Iteration 319, loss = 0.17667994\n",
      "Iteration 320, loss = 0.17568255\n",
      "Iteration 321, loss = 0.17469537\n",
      "Iteration 322, loss = 0.17371715\n",
      "Iteration 323, loss = 0.17274915\n",
      "Iteration 324, loss = 0.17179117\n",
      "Iteration 325, loss = 0.17084343\n",
      "Iteration 326, loss = 0.16990509\n",
      "Iteration 327, loss = 0.16897602\n",
      "Iteration 328, loss = 0.16805685\n",
      "Iteration 329, loss = 0.16714728\n",
      "Iteration 330, loss = 0.16624740\n",
      "Iteration 331, loss = 0.16535697\n",
      "Iteration 332, loss = 0.16447589\n",
      "Iteration 333, loss = 0.16360409\n",
      "Iteration 334, loss = 0.16274155\n",
      "Iteration 335, loss = 0.16188825\n",
      "Iteration 336, loss = 0.16104399\n",
      "Iteration 337, loss = 0.16020806\n",
      "Iteration 338, loss = 0.15938049\n",
      "Iteration 339, loss = 0.15856106\n",
      "Iteration 340, loss = 0.15774941\n",
      "Iteration 341, loss = 0.15694543\n",
      "Iteration 342, loss = 0.15614896\n",
      "Iteration 343, loss = 0.15535963\n",
      "Iteration 344, loss = 0.15457706\n",
      "Iteration 345, loss = 0.15380107\n",
      "Iteration 346, loss = 0.15303136\n",
      "Iteration 347, loss = 0.15226796\n",
      "Iteration 348, loss = 0.15151091\n",
      "Iteration 349, loss = 0.15075968\n",
      "Iteration 350, loss = 0.15001436\n",
      "Iteration 351, loss = 0.14927498\n",
      "Iteration 352, loss = 0.14854166\n",
      "Iteration 353, loss = 0.14781447\n",
      "Iteration 354, loss = 0.14709327\n",
      "Iteration 355, loss = 0.14637830\n",
      "Iteration 356, loss = 0.14566995\n",
      "Iteration 357, loss = 0.14496660\n",
      "Iteration 358, loss = 0.14426968\n",
      "Iteration 359, loss = 0.14357864\n",
      "Iteration 360, loss = 0.14289308\n",
      "Iteration 361, loss = 0.14221357\n",
      "Iteration 362, loss = 0.14153976\n",
      "Iteration 363, loss = 0.14087146\n",
      "Iteration 364, loss = 0.14020893\n",
      "Iteration 365, loss = 0.13955196\n",
      "Iteration 366, loss = 0.13890041\n",
      "Iteration 367, loss = 0.13825492\n",
      "Iteration 368, loss = 0.13761458\n",
      "Iteration 369, loss = 0.13697995\n",
      "Iteration 370, loss = 0.13635073\n",
      "Iteration 371, loss = 0.13572695\n",
      "Iteration 372, loss = 0.13510872\n",
      "Iteration 373, loss = 0.13449645\n",
      "Iteration 374, loss = 0.13389015\n",
      "Iteration 375, loss = 0.13328958\n",
      "Iteration 376, loss = 0.13269457\n",
      "Iteration 377, loss = 0.13210573\n",
      "Iteration 378, loss = 0.13152225\n",
      "Iteration 379, loss = 0.13094514\n",
      "Iteration 380, loss = 0.13037340\n",
      "Iteration 381, loss = 0.12980713\n",
      "Iteration 382, loss = 0.12924704\n",
      "Iteration 383, loss = 0.12869295\n",
      "Iteration 384, loss = 0.12814462\n",
      "Iteration 385, loss = 0.12760202\n",
      "Iteration 386, loss = 0.12706529\n",
      "Iteration 387, loss = 0.12653423\n",
      "Iteration 388, loss = 0.12600888\n",
      "Iteration 389, loss = 0.12548933\n",
      "Iteration 390, loss = 0.12497542\n",
      "Iteration 391, loss = 0.12446725\n",
      "Iteration 392, loss = 0.12396472\n",
      "Iteration 393, loss = 0.12346786\n",
      "Iteration 394, loss = 0.12297664\n",
      "Iteration 395, loss = 0.12249105\n",
      "Iteration 396, loss = 0.12201096\n",
      "Iteration 397, loss = 0.12153654\n",
      "Iteration 398, loss = 0.12106756\n",
      "Iteration 399, loss = 0.12060400\n",
      "Iteration 400, loss = 0.12014541\n",
      "Iteration 401, loss = 0.11969196\n",
      "Iteration 402, loss = 0.11924340\n",
      "Iteration 403, loss = 0.11879986\n",
      "Iteration 404, loss = 0.11836080\n",
      "Iteration 405, loss = 0.11792668\n",
      "Iteration 406, loss = 0.11749697\n",
      "Iteration 407, loss = 0.11707153\n",
      "Iteration 408, loss = 0.11665018\n",
      "Iteration 409, loss = 0.11623269\n",
      "Iteration 410, loss = 0.11581899\n",
      "Iteration 411, loss = 0.11540894\n",
      "Iteration 412, loss = 0.11500241\n",
      "Iteration 413, loss = 0.11459936\n",
      "Iteration 414, loss = 0.11419977\n",
      "Iteration 415, loss = 0.11380356\n",
      "Iteration 416, loss = 0.11341065\n",
      "Iteration 417, loss = 0.11302109\n",
      "Iteration 418, loss = 0.11263474\n",
      "Iteration 419, loss = 0.11225165\n",
      "Iteration 420, loss = 0.11187179\n",
      "Iteration 421, loss = 0.11149523\n",
      "Iteration 422, loss = 0.11112197\n",
      "Iteration 423, loss = 0.11075191\n",
      "Iteration 424, loss = 0.11038503\n",
      "Iteration 425, loss = 0.11002125\n",
      "Iteration 426, loss = 0.10966094\n",
      "Iteration 427, loss = 0.10930385\n",
      "Iteration 428, loss = 0.10895035\n",
      "Iteration 429, loss = 0.10859998\n",
      "Iteration 430, loss = 0.10825297\n",
      "Iteration 431, loss = 0.10790912\n",
      "Iteration 432, loss = 0.10756851\n",
      "Iteration 433, loss = 0.10723096\n",
      "Iteration 434, loss = 0.10689658\n",
      "Iteration 435, loss = 0.10656551\n",
      "Iteration 436, loss = 0.10623764\n",
      "Iteration 437, loss = 0.10591273\n",
      "Iteration 438, loss = 0.10559071\n",
      "Iteration 439, loss = 0.10527178\n",
      "Iteration 440, loss = 0.10495582\n",
      "Iteration 441, loss = 0.10464259\n",
      "Iteration 442, loss = 0.10433210\n",
      "Iteration 443, loss = 0.10402425\n",
      "Iteration 444, loss = 0.10371992\n",
      "Iteration 445, loss = 0.10341861\n",
      "Iteration 446, loss = 0.10312015\n",
      "Iteration 447, loss = 0.10282447\n",
      "Iteration 448, loss = 0.10253151\n",
      "Iteration 449, loss = 0.10224151\n",
      "Iteration 450, loss = 0.10195391\n",
      "Iteration 451, loss = 0.10166852\n",
      "Iteration 452, loss = 0.10138582\n",
      "Iteration 453, loss = 0.10110635\n",
      "Iteration 454, loss = 0.10082829\n",
      "Iteration 455, loss = 0.10055305\n",
      "Iteration 456, loss = 0.10027995\n",
      "Iteration 457, loss = 0.10000913\n",
      "Iteration 458, loss = 0.09974016\n",
      "Iteration 459, loss = 0.09947333\n",
      "Iteration 460, loss = 0.09920854\n",
      "Iteration 461, loss = 0.09894551\n",
      "Iteration 462, loss = 0.09868438\n",
      "Iteration 463, loss = 0.09842512\n",
      "Iteration 464, loss = 0.09816763\n",
      "Iteration 465, loss = 0.09791204\n",
      "Iteration 466, loss = 0.09765820\n",
      "Iteration 467, loss = 0.09740626\n",
      "Iteration 468, loss = 0.09715592\n",
      "Iteration 469, loss = 0.09690736\n",
      "Iteration 470, loss = 0.09666068\n",
      "Iteration 471, loss = 0.09641593\n",
      "Iteration 472, loss = 0.09617316\n",
      "Iteration 473, loss = 0.09593212\n",
      "Iteration 474, loss = 0.09569285\n",
      "Iteration 475, loss = 0.09545545\n",
      "Iteration 476, loss = 0.09521982\n",
      "Iteration 477, loss = 0.09498613\n",
      "Iteration 478, loss = 0.09475445\n",
      "Iteration 479, loss = 0.09452475\n",
      "Iteration 480, loss = 0.09429703\n",
      "Iteration 481, loss = 0.09407115\n",
      "Iteration 482, loss = 0.09384731\n",
      "Iteration 483, loss = 0.09362532\n",
      "Iteration 484, loss = 0.09340534\n",
      "Iteration 485, loss = 0.09318736\n",
      "Iteration 486, loss = 0.09297135\n",
      "Iteration 487, loss = 0.09275731\n",
      "Iteration 488, loss = 0.09254535\n",
      "Iteration 489, loss = 0.09233543\n",
      "Iteration 490, loss = 0.09212753\n",
      "Iteration 491, loss = 0.09192172\n",
      "Iteration 492, loss = 0.09171798\n",
      "Iteration 493, loss = 0.09151636\n",
      "Iteration 494, loss = 0.09131683\n",
      "Iteration 495, loss = 0.09111938\n",
      "Iteration 496, loss = 0.09092401\n",
      "Iteration 497, loss = 0.09073070\n",
      "Iteration 498, loss = 0.09053965\n",
      "Iteration 499, loss = 0.09035069\n",
      "Iteration 500, loss = 0.09016377\n",
      "Iteration 501, loss = 0.08997904\n",
      "Iteration 502, loss = 0.08979651\n",
      "Iteration 503, loss = 0.08961611\n",
      "Iteration 504, loss = 0.08943796\n",
      "Iteration 505, loss = 0.08926190\n",
      "Iteration 506, loss = 0.08908794\n",
      "Iteration 507, loss = 0.08891604\n",
      "Iteration 508, loss = 0.08874636\n",
      "Iteration 509, loss = 0.08857884\n",
      "Iteration 510, loss = 0.08841345\n",
      "Iteration 511, loss = 0.08825018\n",
      "Iteration 512, loss = 0.08808895\n",
      "Iteration 513, loss = 0.08792974\n",
      "Iteration 514, loss = 0.08777256\n",
      "Iteration 515, loss = 0.08761749\n",
      "Iteration 516, loss = 0.08746432\n",
      "Iteration 517, loss = 0.08731311\n",
      "Iteration 518, loss = 0.08716392\n",
      "Iteration 519, loss = 0.08701669\n",
      "Iteration 520, loss = 0.08687159\n",
      "Iteration 521, loss = 0.08672844\n",
      "Iteration 522, loss = 0.08658720\n",
      "Iteration 523, loss = 0.08644783\n",
      "Iteration 524, loss = 0.08631040\n",
      "Iteration 525, loss = 0.08617488\n",
      "Iteration 526, loss = 0.08604122\n",
      "Iteration 527, loss = 0.08590944\n",
      "Iteration 528, loss = 0.08577946\n",
      "Iteration 529, loss = 0.08565119\n",
      "Iteration 530, loss = 0.08552466\n",
      "Iteration 531, loss = 0.08539993\n",
      "Iteration 532, loss = 0.08527693\n",
      "Iteration 533, loss = 0.08515555\n",
      "Iteration 534, loss = 0.08503575\n",
      "Iteration 535, loss = 0.08491756\n",
      "Iteration 536, loss = 0.08480101\n",
      "Iteration 537, loss = 0.08468606\n",
      "Iteration 538, loss = 0.08457275\n",
      "Iteration 539, loss = 0.08446090\n",
      "Iteration 540, loss = 0.08435051\n",
      "Iteration 541, loss = 0.08424158\n",
      "Iteration 542, loss = 0.08413412\n",
      "Iteration 543, loss = 0.08402810\n",
      "Iteration 544, loss = 0.08392351\n",
      "Iteration 545, loss = 0.08382031\n",
      "Iteration 546, loss = 0.08371847\n",
      "Iteration 547, loss = 0.08361794\n",
      "Iteration 548, loss = 0.08351871\n",
      "Iteration 549, loss = 0.08342080\n",
      "Iteration 550, loss = 0.08332419\n",
      "Iteration 551, loss = 0.08322899\n",
      "Iteration 552, loss = 0.08313504\n",
      "Iteration 553, loss = 0.08304234\n",
      "Iteration 554, loss = 0.08295092\n",
      "Iteration 555, loss = 0.08286059\n",
      "Iteration 556, loss = 0.08277137\n",
      "Iteration 557, loss = 0.08268326\n",
      "Iteration 558, loss = 0.08259624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.59825295\n",
      "Iteration 3, loss = 1.53451783\n",
      "Iteration 4, loss = 1.47490800\n",
      "Iteration 5, loss = 1.41984966\n",
      "Iteration 6, loss = 1.36964392\n",
      "Iteration 7, loss = 1.32459963\n",
      "Iteration 8, loss = 1.28506514\n",
      "Iteration 9, loss = 1.25102737\n",
      "Iteration 10, loss = 1.22242438\n",
      "Iteration 11, loss = 1.19875806\n",
      "Iteration 12, loss = 1.17937983\n",
      "Iteration 13, loss = 1.16337714\n",
      "Iteration 14, loss = 1.14967549\n",
      "Iteration 15, loss = 1.13733352\n",
      "Iteration 16, loss = 1.12537235\n",
      "Iteration 17, loss = 1.11299468\n",
      "Iteration 18, loss = 1.09972199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 1.08543645\n",
      "Iteration 20, loss = 1.07019443\n",
      "Iteration 21, loss = 1.05416772\n",
      "Iteration 22, loss = 1.03762675\n",
      "Iteration 23, loss = 1.02100303\n",
      "Iteration 24, loss = 1.00460144\n",
      "Iteration 25, loss = 0.98870024\n",
      "Iteration 26, loss = 0.97357882\n",
      "Iteration 27, loss = 0.95927556\n",
      "Iteration 28, loss = 0.94583076\n",
      "Iteration 29, loss = 0.93323125\n",
      "Iteration 30, loss = 0.92132930\n",
      "Iteration 31, loss = 0.91000152\n",
      "Iteration 32, loss = 0.89907659\n",
      "Iteration 33, loss = 0.88844346\n",
      "Iteration 34, loss = 0.87804372\n",
      "Iteration 35, loss = 0.86775247\n",
      "Iteration 36, loss = 0.85750552\n",
      "Iteration 37, loss = 0.84740095\n",
      "Iteration 38, loss = 0.83747924\n",
      "Iteration 39, loss = 0.82773389\n",
      "Iteration 40, loss = 0.81826689\n",
      "Iteration 41, loss = 0.80908094\n",
      "Iteration 42, loss = 0.80019198\n",
      "Iteration 43, loss = 0.79157883\n",
      "Iteration 44, loss = 0.78323907\n",
      "Iteration 45, loss = 0.77516881\n",
      "Iteration 46, loss = 0.76734769\n",
      "Iteration 47, loss = 0.75975135\n",
      "Iteration 48, loss = 0.75236305\n",
      "Iteration 49, loss = 0.74517908\n",
      "Iteration 50, loss = 0.73821368\n",
      "Iteration 51, loss = 0.73141942\n",
      "Iteration 52, loss = 0.72478488\n",
      "Iteration 53, loss = 0.71829979\n",
      "Iteration 54, loss = 0.71195729\n",
      "Iteration 55, loss = 0.70575976\n",
      "Iteration 56, loss = 0.69968902\n",
      "Iteration 57, loss = 0.69373246\n",
      "Iteration 58, loss = 0.68789632\n",
      "Iteration 59, loss = 0.68215392\n",
      "Iteration 60, loss = 0.67647396\n",
      "Iteration 61, loss = 0.67088763\n",
      "Iteration 62, loss = 0.66541938\n",
      "Iteration 63, loss = 0.66008020\n",
      "Iteration 64, loss = 0.65485289\n",
      "Iteration 65, loss = 0.64976455\n",
      "Iteration 66, loss = 0.64491272\n",
      "Iteration 67, loss = 0.64026625\n",
      "Iteration 68, loss = 0.63576461\n",
      "Iteration 69, loss = 0.63137409\n",
      "Iteration 70, loss = 0.62707242\n",
      "Iteration 71, loss = 0.62285100\n",
      "Iteration 72, loss = 0.61870675\n",
      "Iteration 73, loss = 0.61463189\n",
      "Iteration 74, loss = 0.61061719\n",
      "Iteration 75, loss = 0.60665549\n",
      "Iteration 76, loss = 0.60274138\n",
      "Iteration 77, loss = 0.59886955\n",
      "Iteration 78, loss = 0.59501563\n",
      "Iteration 79, loss = 0.59117445\n",
      "Iteration 80, loss = 0.58740716\n",
      "Iteration 81, loss = 0.58361200\n",
      "Iteration 82, loss = 0.57993664\n",
      "Iteration 83, loss = 0.57636095\n",
      "Iteration 84, loss = 0.57286672\n",
      "Iteration 85, loss = 0.56942763\n",
      "Iteration 86, loss = 0.56603280\n",
      "Iteration 87, loss = 0.56267825\n",
      "Iteration 88, loss = 0.55936338\n",
      "Iteration 89, loss = 0.55608956\n",
      "Iteration 90, loss = 0.55285861\n",
      "Iteration 91, loss = 0.54967065\n",
      "Iteration 92, loss = 0.54652715\n",
      "Iteration 93, loss = 0.54342561\n",
      "Iteration 94, loss = 0.54036296\n",
      "Iteration 95, loss = 0.53733864\n",
      "Iteration 96, loss = 0.53435180\n",
      "Iteration 97, loss = 0.53139600\n",
      "Iteration 98, loss = 0.52847404\n",
      "Iteration 99, loss = 0.52558659\n",
      "Iteration 100, loss = 0.52273426\n",
      "Iteration 101, loss = 0.51991638\n",
      "Iteration 102, loss = 0.51713139\n",
      "Iteration 103, loss = 0.51437800\n",
      "Iteration 104, loss = 0.51165471\n",
      "Iteration 105, loss = 0.50896016\n",
      "Iteration 106, loss = 0.50629299\n",
      "Iteration 107, loss = 0.50365215\n",
      "Iteration 108, loss = 0.50103752\n",
      "Iteration 109, loss = 0.49844899\n",
      "Iteration 110, loss = 0.49588592\n",
      "Iteration 111, loss = 0.49334814\n",
      "Iteration 112, loss = 0.49083512\n",
      "Iteration 113, loss = 0.48834610\n",
      "Iteration 114, loss = 0.48588000\n",
      "Iteration 115, loss = 0.48343580\n",
      "Iteration 116, loss = 0.48102560\n",
      "Iteration 117, loss = 0.47865063\n",
      "Iteration 118, loss = 0.47632259\n",
      "Iteration 119, loss = 0.47403632\n",
      "Iteration 120, loss = 0.47175369\n",
      "Iteration 121, loss = 0.46948496\n",
      "Iteration 122, loss = 0.46726840\n",
      "Iteration 123, loss = 0.46500670\n",
      "Iteration 124, loss = 0.46277094\n",
      "Iteration 125, loss = 0.46053749\n",
      "Iteration 126, loss = 0.45831465\n",
      "Iteration 127, loss = 0.45610697\n",
      "Iteration 128, loss = 0.45391270\n",
      "Iteration 129, loss = 0.45173737\n",
      "Iteration 130, loss = 0.44958000\n",
      "Iteration 131, loss = 0.44745166\n",
      "Iteration 132, loss = 0.44534888\n",
      "Iteration 133, loss = 0.44323493\n",
      "Iteration 134, loss = 0.44113546\n",
      "Iteration 135, loss = 0.43905946\n",
      "Iteration 136, loss = 0.43698816\n",
      "Iteration 137, loss = 0.43492056\n",
      "Iteration 138, loss = 0.43285608\n",
      "Iteration 139, loss = 0.43079459\n",
      "Iteration 140, loss = 0.42874546\n",
      "Iteration 141, loss = 0.42671906\n",
      "Iteration 142, loss = 0.42469566\n",
      "Iteration 143, loss = 0.42267421\n",
      "Iteration 144, loss = 0.42067040\n",
      "Iteration 145, loss = 0.41868719\n",
      "Iteration 146, loss = 0.41670937\n",
      "Iteration 147, loss = 0.41473779\n",
      "Iteration 148, loss = 0.41277130\n",
      "Iteration 149, loss = 0.41081455\n",
      "Iteration 150, loss = 0.40886368\n",
      "Iteration 151, loss = 0.40691149\n",
      "Iteration 152, loss = 0.40495925\n",
      "Iteration 153, loss = 0.40301022\n",
      "Iteration 154, loss = 0.40106380\n",
      "Iteration 155, loss = 0.39912638\n",
      "Iteration 156, loss = 0.39721337\n",
      "Iteration 157, loss = 0.39530584\n",
      "Iteration 158, loss = 0.39339266\n",
      "Iteration 159, loss = 0.39148103\n",
      "Iteration 160, loss = 0.38958187\n",
      "Iteration 161, loss = 0.38769513\n",
      "Iteration 162, loss = 0.38581108\n",
      "Iteration 163, loss = 0.38393002\n",
      "Iteration 164, loss = 0.38205100\n",
      "Iteration 165, loss = 0.38017684\n",
      "Iteration 166, loss = 0.37831141\n",
      "Iteration 167, loss = 0.37645508\n",
      "Iteration 168, loss = 0.37460459\n",
      "Iteration 169, loss = 0.37275751\n",
      "Iteration 170, loss = 0.37091435\n",
      "Iteration 171, loss = 0.36907584\n",
      "Iteration 172, loss = 0.36724400\n",
      "Iteration 173, loss = 0.36541803\n",
      "Iteration 174, loss = 0.36359865\n",
      "Iteration 175, loss = 0.36178405\n",
      "Iteration 176, loss = 0.35997347\n",
      "Iteration 177, loss = 0.35816820\n",
      "Iteration 178, loss = 0.35636984\n",
      "Iteration 179, loss = 0.35457673\n",
      "Iteration 180, loss = 0.35278919\n",
      "Iteration 181, loss = 0.35100774\n",
      "Iteration 182, loss = 0.34923202\n",
      "Iteration 183, loss = 0.34746130\n",
      "Iteration 184, loss = 0.34569700\n",
      "Iteration 185, loss = 0.34393920\n",
      "Iteration 186, loss = 0.34218701\n",
      "Iteration 187, loss = 0.34044035\n",
      "Iteration 188, loss = 0.33869989\n",
      "Iteration 189, loss = 0.33696586\n",
      "Iteration 190, loss = 0.33523806\n",
      "Iteration 191, loss = 0.33351626\n",
      "Iteration 192, loss = 0.33180020\n",
      "Iteration 193, loss = 0.33009005\n",
      "Iteration 194, loss = 0.32838641\n",
      "Iteration 195, loss = 0.32668941\n",
      "Iteration 196, loss = 0.32499839\n",
      "Iteration 197, loss = 0.32331346\n",
      "Iteration 198, loss = 0.32163478\n",
      "Iteration 199, loss = 0.31996270\n",
      "Iteration 200, loss = 0.31829622\n",
      "Iteration 201, loss = 0.31663067\n",
      "Iteration 202, loss = 0.31496015\n",
      "Iteration 203, loss = 0.31329801\n",
      "Iteration 204, loss = 0.31164617\n",
      "Iteration 205, loss = 0.30999653\n",
      "Iteration 206, loss = 0.30834781\n",
      "Iteration 207, loss = 0.30669956\n",
      "Iteration 208, loss = 0.30505187\n",
      "Iteration 209, loss = 0.30340503\n",
      "Iteration 210, loss = 0.30175883\n",
      "Iteration 211, loss = 0.30011314\n",
      "Iteration 212, loss = 0.29846799\n",
      "Iteration 213, loss = 0.29682353\n",
      "Iteration 214, loss = 0.29517982\n",
      "Iteration 215, loss = 0.29353688\n",
      "Iteration 216, loss = 0.29189491\n",
      "Iteration 217, loss = 0.29025459\n",
      "Iteration 218, loss = 0.28861543\n",
      "Iteration 219, loss = 0.28697758\n",
      "Iteration 220, loss = 0.28534146\n",
      "Iteration 221, loss = 0.28370782\n",
      "Iteration 222, loss = 0.28207655\n",
      "Iteration 223, loss = 0.28044748\n",
      "Iteration 224, loss = 0.27882110\n",
      "Iteration 225, loss = 0.27719809\n",
      "Iteration 226, loss = 0.27557937\n",
      "Iteration 227, loss = 0.27396410\n",
      "Iteration 228, loss = 0.27235270\n",
      "Iteration 229, loss = 0.27074557\n",
      "Iteration 230, loss = 0.26914295\n",
      "Iteration 231, loss = 0.26754483\n",
      "Iteration 232, loss = 0.26595170\n",
      "Iteration 233, loss = 0.26436374\n",
      "Iteration 234, loss = 0.26278189\n",
      "Iteration 235, loss = 0.26120539\n",
      "Iteration 236, loss = 0.25963444\n",
      "Iteration 237, loss = 0.25807028\n",
      "Iteration 238, loss = 0.25651237\n",
      "Iteration 239, loss = 0.25496085\n",
      "Iteration 240, loss = 0.25341561\n",
      "Iteration 241, loss = 0.25187265\n",
      "Iteration 242, loss = 0.25033058\n",
      "Iteration 243, loss = 0.24880202\n",
      "Iteration 244, loss = 0.24727834\n",
      "Iteration 245, loss = 0.24575843\n",
      "Iteration 246, loss = 0.24424237\n",
      "Iteration 247, loss = 0.24273088\n",
      "Iteration 248, loss = 0.24122384\n",
      "Iteration 249, loss = 0.23972103\n",
      "Iteration 250, loss = 0.23822226\n",
      "Iteration 251, loss = 0.23672768\n",
      "Iteration 252, loss = 0.23523717\n",
      "Iteration 253, loss = 0.23375072\n",
      "Iteration 254, loss = 0.23226857\n",
      "Iteration 255, loss = 0.23079057\n",
      "Iteration 256, loss = 0.22931665\n",
      "Iteration 257, loss = 0.22784682\n",
      "Iteration 258, loss = 0.22638058\n",
      "Iteration 259, loss = 0.22491349\n",
      "Iteration 260, loss = 0.22344143\n",
      "Iteration 261, loss = 0.22197858\n",
      "Iteration 262, loss = 0.22052087\n",
      "Iteration 263, loss = 0.21906561\n",
      "Iteration 264, loss = 0.21761291\n",
      "Iteration 265, loss = 0.21616276\n",
      "Iteration 266, loss = 0.21471533\n",
      "Iteration 267, loss = 0.21327084\n",
      "Iteration 268, loss = 0.21182972\n",
      "Iteration 269, loss = 0.21039262\n",
      "Iteration 270, loss = 0.20895922\n",
      "Iteration 271, loss = 0.20752978\n",
      "Iteration 272, loss = 0.20610489\n",
      "Iteration 273, loss = 0.20468499\n",
      "Iteration 274, loss = 0.20327043\n",
      "Iteration 275, loss = 0.20186138\n",
      "Iteration 276, loss = 0.20045795\n",
      "Iteration 277, loss = 0.19906072\n",
      "Iteration 278, loss = 0.19767010\n",
      "Iteration 279, loss = 0.19628641\n",
      "Iteration 280, loss = 0.19490984\n",
      "Iteration 281, loss = 0.19354095\n",
      "Iteration 282, loss = 0.19217960\n",
      "Iteration 283, loss = 0.19082660\n",
      "Iteration 284, loss = 0.18948223\n",
      "Iteration 285, loss = 0.18814675\n",
      "Iteration 286, loss = 0.18682037\n",
      "Iteration 287, loss = 0.18550316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 288, loss = 0.18419542\n",
      "Iteration 289, loss = 0.18289759\n",
      "Iteration 290, loss = 0.18160949\n",
      "Iteration 291, loss = 0.18033139\n",
      "Iteration 292, loss = 0.17906325\n",
      "Iteration 293, loss = 0.17780521\n",
      "Iteration 294, loss = 0.17655741\n",
      "Iteration 295, loss = 0.17531976\n",
      "Iteration 296, loss = 0.17409193\n",
      "Iteration 297, loss = 0.17287443\n",
      "Iteration 298, loss = 0.17166742\n",
      "Iteration 299, loss = 0.17047080\n",
      "Iteration 300, loss = 0.16928442\n",
      "Iteration 301, loss = 0.16810849\n",
      "Iteration 302, loss = 0.16694303\n",
      "Iteration 303, loss = 0.16578798\n",
      "Iteration 304, loss = 0.16464325\n",
      "Iteration 305, loss = 0.16350991\n",
      "Iteration 306, loss = 0.16238669\n",
      "Iteration 307, loss = 0.16127438\n",
      "Iteration 308, loss = 0.16017293\n",
      "Iteration 309, loss = 0.15908218\n",
      "Iteration 310, loss = 0.15800295\n",
      "Iteration 311, loss = 0.15693374\n",
      "Iteration 312, loss = 0.15587555\n",
      "Iteration 313, loss = 0.15482844\n",
      "Iteration 314, loss = 0.15379203\n",
      "Iteration 315, loss = 0.15276646\n",
      "Iteration 316, loss = 0.15175190\n",
      "Iteration 317, loss = 0.15074824\n",
      "Iteration 318, loss = 0.14975530\n",
      "Iteration 319, loss = 0.14877344\n",
      "Iteration 320, loss = 0.14780285\n",
      "Iteration 321, loss = 0.14684311\n",
      "Iteration 322, loss = 0.14589403\n",
      "Iteration 323, loss = 0.14495563\n",
      "Iteration 324, loss = 0.14402797\n",
      "Iteration 325, loss = 0.14311092\n",
      "Iteration 326, loss = 0.14220448\n",
      "Iteration 327, loss = 0.14130862\n",
      "Iteration 328, loss = 0.14042346\n",
      "Iteration 329, loss = 0.13954881\n",
      "Iteration 330, loss = 0.13868442\n",
      "Iteration 331, loss = 0.13783049\n",
      "Iteration 332, loss = 0.13698702\n",
      "Iteration 333, loss = 0.13615361\n",
      "Iteration 334, loss = 0.13533025\n",
      "Iteration 335, loss = 0.13451647\n",
      "Iteration 336, loss = 0.13371195\n",
      "Iteration 337, loss = 0.13291651\n",
      "Iteration 338, loss = 0.13212997\n",
      "Iteration 339, loss = 0.13135217\n",
      "Iteration 340, loss = 0.13058270\n",
      "Iteration 341, loss = 0.12982134\n",
      "Iteration 342, loss = 0.12906787\n",
      "Iteration 343, loss = 0.12832230\n",
      "Iteration 344, loss = 0.12758379\n",
      "Iteration 345, loss = 0.12685275\n",
      "Iteration 346, loss = 0.12612871\n",
      "Iteration 347, loss = 0.12541121\n",
      "Iteration 348, loss = 0.12470020\n",
      "Iteration 349, loss = 0.12399623\n",
      "Iteration 350, loss = 0.12329861\n",
      "Iteration 351, loss = 0.12260725\n",
      "Iteration 352, loss = 0.12192247\n",
      "Iteration 353, loss = 0.12124396\n",
      "Iteration 354, loss = 0.12057168\n",
      "Iteration 355, loss = 0.11990583\n",
      "Iteration 356, loss = 0.11924680\n",
      "Iteration 357, loss = 0.11859306\n",
      "Iteration 358, loss = 0.11794645\n",
      "Iteration 359, loss = 0.11730585\n",
      "Iteration 360, loss = 0.11667112\n",
      "Iteration 361, loss = 0.11604219\n",
      "Iteration 362, loss = 0.11541968\n",
      "Iteration 363, loss = 0.11480356\n",
      "Iteration 364, loss = 0.11419260\n",
      "Iteration 365, loss = 0.11358832\n",
      "Iteration 366, loss = 0.11298975\n",
      "Iteration 367, loss = 0.11239662\n",
      "Iteration 368, loss = 0.11180969\n",
      "Iteration 369, loss = 0.11122897\n",
      "Iteration 370, loss = 0.11065355\n",
      "Iteration 371, loss = 0.11008483\n",
      "Iteration 372, loss = 0.10952146\n",
      "Iteration 373, loss = 0.10896362\n",
      "Iteration 374, loss = 0.10841164\n",
      "Iteration 375, loss = 0.10786584\n",
      "Iteration 376, loss = 0.10732549\n",
      "Iteration 377, loss = 0.10679114\n",
      "Iteration 378, loss = 0.10626249\n",
      "Iteration 379, loss = 0.10573933\n",
      "Iteration 380, loss = 0.10522201\n",
      "Iteration 381, loss = 0.10471047\n",
      "Iteration 382, loss = 0.10420444\n",
      "Iteration 383, loss = 0.10370409\n",
      "Iteration 384, loss = 0.10320942\n",
      "Iteration 385, loss = 0.10272028\n",
      "Iteration 386, loss = 0.10223675\n",
      "Iteration 387, loss = 0.10175885\n",
      "Iteration 388, loss = 0.10128622\n",
      "Iteration 389, loss = 0.10081903\n",
      "Iteration 390, loss = 0.10035705\n",
      "Iteration 391, loss = 0.09990025\n",
      "Iteration 392, loss = 0.09944826\n",
      "Iteration 393, loss = 0.09900151\n",
      "Iteration 394, loss = 0.09855945\n",
      "Iteration 395, loss = 0.09812270\n",
      "Iteration 396, loss = 0.09769012\n",
      "Iteration 397, loss = 0.09726181\n",
      "Iteration 398, loss = 0.09683777\n",
      "Iteration 399, loss = 0.09641807\n",
      "Iteration 400, loss = 0.09600220\n",
      "Iteration 401, loss = 0.09559004\n",
      "Iteration 402, loss = 0.09518183\n",
      "Iteration 403, loss = 0.09477714\n",
      "Iteration 404, loss = 0.09437610\n",
      "Iteration 405, loss = 0.09397864\n",
      "Iteration 406, loss = 0.09358484\n",
      "Iteration 407, loss = 0.09319456\n",
      "Iteration 408, loss = 0.09280773\n",
      "Iteration 409, loss = 0.09242462\n",
      "Iteration 410, loss = 0.09204513\n",
      "Iteration 411, loss = 0.09166877\n",
      "Iteration 412, loss = 0.09129548\n",
      "Iteration 413, loss = 0.09092559\n",
      "Iteration 414, loss = 0.09055895\n",
      "Iteration 415, loss = 0.09019559\n",
      "Iteration 416, loss = 0.08983556\n",
      "Iteration 417, loss = 0.08947860\n",
      "Iteration 418, loss = 0.08912479\n",
      "Iteration 419, loss = 0.08877418\n",
      "Iteration 420, loss = 0.08842663\n",
      "Iteration 421, loss = 0.08808208\n",
      "Iteration 422, loss = 0.08774051\n",
      "Iteration 423, loss = 0.08740193\n",
      "Iteration 424, loss = 0.08706630\n",
      "Iteration 425, loss = 0.08673353\n",
      "Iteration 426, loss = 0.08640365\n",
      "Iteration 427, loss = 0.08607657\n",
      "Iteration 428, loss = 0.08575235\n",
      "Iteration 429, loss = 0.08543093\n",
      "Iteration 430, loss = 0.08511227\n",
      "Iteration 431, loss = 0.08479647\n",
      "Iteration 432, loss = 0.08448334\n",
      "Iteration 433, loss = 0.08417301\n",
      "Iteration 434, loss = 0.08386546\n",
      "Iteration 435, loss = 0.08356068\n",
      "Iteration 436, loss = 0.08325867\n",
      "Iteration 437, loss = 0.08295947\n",
      "Iteration 438, loss = 0.08266293\n",
      "Iteration 439, loss = 0.08236929\n",
      "Iteration 440, loss = 0.08207842\n",
      "Iteration 441, loss = 0.08179023\n",
      "Iteration 442, loss = 0.08150489\n",
      "Iteration 443, loss = 0.08122224\n",
      "Iteration 444, loss = 0.08094247\n",
      "Iteration 445, loss = 0.08066553\n",
      "Iteration 446, loss = 0.08039129\n",
      "Iteration 447, loss = 0.08011985\n",
      "Iteration 448, loss = 0.07985125\n",
      "Iteration 449, loss = 0.07958541\n",
      "Iteration 450, loss = 0.07932240\n",
      "Iteration 451, loss = 0.07906223\n",
      "Iteration 452, loss = 0.07880478\n",
      "Iteration 453, loss = 0.07855016\n",
      "Iteration 454, loss = 0.07829835\n",
      "Iteration 455, loss = 0.07804929\n",
      "Iteration 456, loss = 0.07780303\n",
      "Iteration 457, loss = 0.07755952\n",
      "Iteration 458, loss = 0.07731879\n",
      "Iteration 459, loss = 0.07708085\n",
      "Iteration 460, loss = 0.07684561\n",
      "Iteration 461, loss = 0.07661312\n",
      "Iteration 462, loss = 0.07638339\n",
      "Iteration 463, loss = 0.07615637\n",
      "Iteration 464, loss = 0.07593203\n",
      "Iteration 465, loss = 0.07571032\n",
      "Iteration 466, loss = 0.07549142\n",
      "Iteration 467, loss = 0.07527510\n",
      "Iteration 468, loss = 0.07506134\n",
      "Iteration 469, loss = 0.07485026\n",
      "Iteration 470, loss = 0.07464179\n",
      "Iteration 471, loss = 0.07443589\n",
      "Iteration 472, loss = 0.07423250\n",
      "Iteration 473, loss = 0.07403166\n",
      "Iteration 474, loss = 0.07383333\n",
      "Iteration 475, loss = 0.07363754\n",
      "Iteration 476, loss = 0.07344415\n",
      "Iteration 477, loss = 0.07325325\n",
      "Iteration 478, loss = 0.07306484\n",
      "Iteration 479, loss = 0.07287876\n",
      "Iteration 480, loss = 0.07269503\n",
      "Iteration 481, loss = 0.07251367\n",
      "Iteration 482, loss = 0.07233469\n",
      "Iteration 483, loss = 0.07215793\n",
      "Iteration 484, loss = 0.07198345\n",
      "Iteration 485, loss = 0.07181117\n",
      "Iteration 486, loss = 0.07164116\n",
      "Iteration 487, loss = 0.07147334\n",
      "Iteration 488, loss = 0.07130779\n",
      "Iteration 489, loss = 0.07114427\n",
      "Iteration 490, loss = 0.07098294\n",
      "Iteration 491, loss = 0.07082363\n",
      "Iteration 492, loss = 0.07066631\n",
      "Iteration 493, loss = 0.07051109\n",
      "Iteration 494, loss = 0.07035780\n",
      "Iteration 495, loss = 0.07020639\n",
      "Iteration 496, loss = 0.07005694\n",
      "Iteration 497, loss = 0.06990945\n",
      "Iteration 498, loss = 0.06976376\n",
      "Iteration 499, loss = 0.06961996\n",
      "Iteration 500, loss = 0.06947800\n",
      "Iteration 501, loss = 0.06933778\n",
      "Iteration 502, loss = 0.06919930\n",
      "Iteration 503, loss = 0.06906270\n",
      "Iteration 504, loss = 0.06892775\n",
      "Iteration 505, loss = 0.06879444\n",
      "Iteration 506, loss = 0.06866289\n",
      "Iteration 507, loss = 0.06853301\n",
      "Iteration 508, loss = 0.06840475\n",
      "Iteration 509, loss = 0.06827809\n",
      "Iteration 510, loss = 0.06815297\n",
      "Iteration 511, loss = 0.06802952\n",
      "Iteration 512, loss = 0.06790764\n",
      "Iteration 513, loss = 0.06778723\n",
      "Iteration 514, loss = 0.06766847\n",
      "Iteration 515, loss = 0.06755119\n",
      "Iteration 516, loss = 0.06743542\n",
      "Iteration 517, loss = 0.06732105\n",
      "Iteration 518, loss = 0.06720811\n",
      "Iteration 519, loss = 0.06709656\n",
      "Iteration 520, loss = 0.06698645\n",
      "Iteration 521, loss = 0.06687779\n",
      "Iteration 522, loss = 0.06677053\n",
      "Iteration 523, loss = 0.06666451\n",
      "Iteration 524, loss = 0.06655983\n",
      "Iteration 525, loss = 0.06645646\n",
      "Iteration 526, loss = 0.06635447\n",
      "Iteration 527, loss = 0.06625373\n",
      "Iteration 528, loss = 0.06615421\n",
      "Iteration 529, loss = 0.06605591\n",
      "Iteration 530, loss = 0.06595883\n",
      "Iteration 531, loss = 0.06586294\n",
      "Iteration 532, loss = 0.06576827\n",
      "Iteration 533, loss = 0.06567477\n",
      "Iteration 534, loss = 0.06558239\n",
      "Iteration 535, loss = 0.06549109\n",
      "Iteration 536, loss = 0.06540093\n",
      "Iteration 537, loss = 0.06531186\n",
      "Iteration 538, loss = 0.06522384\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.59105998\n",
      "Iteration 3, loss = 1.52761329\n",
      "Iteration 4, loss = 1.46829729\n",
      "Iteration 5, loss = 1.41357353\n",
      "Iteration 6, loss = 1.36368588\n",
      "Iteration 7, loss = 1.31892456\n",
      "Iteration 8, loss = 1.27967027\n",
      "Iteration 9, loss = 1.24595667\n",
      "Iteration 10, loss = 1.21767594\n",
      "Iteration 11, loss = 1.19436912\n",
      "Iteration 12, loss = 1.17537946\n",
      "Iteration 13, loss = 1.15983258\n",
      "Iteration 14, loss = 1.14662798\n",
      "Iteration 15, loss = 1.13465487\n",
      "Iteration 16, loss = 1.12296630\n",
      "Iteration 17, loss = 1.11078759\n",
      "Iteration 18, loss = 1.09770079\n",
      "Iteration 19, loss = 1.08357305\n",
      "Iteration 20, loss = 1.06847195\n",
      "Iteration 21, loss = 1.05257437\n",
      "Iteration 22, loss = 1.03621946\n",
      "Iteration 23, loss = 1.01977578\n",
      "Iteration 24, loss = 1.00361646\n",
      "Iteration 25, loss = 0.98801309\n",
      "Iteration 26, loss = 0.97318221\n",
      "Iteration 27, loss = 0.95916162\n",
      "Iteration 28, loss = 0.94597746\n",
      "Iteration 29, loss = 0.93360437\n",
      "Iteration 30, loss = 0.92190161\n",
      "Iteration 31, loss = 0.91075504\n",
      "Iteration 32, loss = 0.90002309\n",
      "Iteration 33, loss = 0.88952409\n",
      "Iteration 34, loss = 0.87918719\n",
      "Iteration 35, loss = 0.86890362\n",
      "Iteration 36, loss = 0.85871423\n",
      "Iteration 37, loss = 0.84865032\n",
      "Iteration 38, loss = 0.83872211\n",
      "Iteration 39, loss = 0.82896636\n",
      "Iteration 40, loss = 0.81942163\n",
      "Iteration 41, loss = 0.81014193\n",
      "Iteration 42, loss = 0.80118486\n",
      "Iteration 43, loss = 0.79251394\n",
      "Iteration 44, loss = 0.78415596\n",
      "Iteration 45, loss = 0.77605777\n",
      "Iteration 46, loss = 0.76819193\n",
      "Iteration 47, loss = 0.76058534\n",
      "Iteration 48, loss = 0.75320912\n",
      "Iteration 49, loss = 0.74602919\n",
      "Iteration 50, loss = 0.73906526\n",
      "Iteration 51, loss = 0.73228876\n",
      "Iteration 52, loss = 0.72568519\n",
      "Iteration 53, loss = 0.71924685\n",
      "Iteration 54, loss = 0.71296505\n",
      "Iteration 55, loss = 0.70682212\n",
      "Iteration 56, loss = 0.70079522\n",
      "Iteration 57, loss = 0.69486355\n",
      "Iteration 58, loss = 0.68903213\n",
      "Iteration 59, loss = 0.68327920\n",
      "Iteration 60, loss = 0.67758492\n",
      "Iteration 61, loss = 0.67196664\n",
      "Iteration 62, loss = 0.66648108\n",
      "Iteration 63, loss = 0.66111348\n",
      "Iteration 64, loss = 0.65587256\n",
      "Iteration 65, loss = 0.65080361\n",
      "Iteration 66, loss = 0.64595591\n",
      "Iteration 67, loss = 0.64130063\n",
      "Iteration 68, loss = 0.63681240\n",
      "Iteration 69, loss = 0.63244393\n",
      "Iteration 70, loss = 0.62816815\n",
      "Iteration 71, loss = 0.62398063\n",
      "Iteration 72, loss = 0.61985721\n",
      "Iteration 73, loss = 0.61580452\n",
      "Iteration 74, loss = 0.61180383\n",
      "Iteration 75, loss = 0.60785408\n",
      "Iteration 76, loss = 0.60395201\n",
      "Iteration 77, loss = 0.60010552\n",
      "Iteration 78, loss = 0.59630614\n",
      "Iteration 79, loss = 0.59254547\n",
      "Iteration 80, loss = 0.58879424\n",
      "Iteration 81, loss = 0.58508830\n",
      "Iteration 82, loss = 0.58148458\n",
      "Iteration 83, loss = 0.57798538\n",
      "Iteration 84, loss = 0.57456128\n",
      "Iteration 85, loss = 0.57118775\n",
      "Iteration 86, loss = 0.56785929\n",
      "Iteration 87, loss = 0.56457285\n",
      "Iteration 88, loss = 0.56132774\n",
      "Iteration 89, loss = 0.55812454\n",
      "Iteration 90, loss = 0.55496418\n",
      "Iteration 91, loss = 0.55184452\n",
      "Iteration 92, loss = 0.54876435\n",
      "Iteration 93, loss = 0.54572246\n",
      "Iteration 94, loss = 0.54271738\n",
      "Iteration 95, loss = 0.53974790\n",
      "Iteration 96, loss = 0.53681483\n",
      "Iteration 97, loss = 0.53391699\n",
      "Iteration 98, loss = 0.53105529\n",
      "Iteration 99, loss = 0.52822784\n",
      "Iteration 100, loss = 0.52543457\n",
      "Iteration 101, loss = 0.52267325\n",
      "Iteration 102, loss = 0.51994177\n",
      "Iteration 103, loss = 0.51723913\n",
      "Iteration 104, loss = 0.51456213\n",
      "Iteration 105, loss = 0.51190930\n",
      "Iteration 106, loss = 0.50927916\n",
      "Iteration 107, loss = 0.50667418\n",
      "Iteration 108, loss = 0.50409129\n",
      "Iteration 109, loss = 0.50153013\n",
      "Iteration 110, loss = 0.49899086\n",
      "Iteration 111, loss = 0.49647413\n",
      "Iteration 112, loss = 0.49397966\n",
      "Iteration 113, loss = 0.49150367\n",
      "Iteration 114, loss = 0.48904731\n",
      "Iteration 115, loss = 0.48660930\n",
      "Iteration 116, loss = 0.48418973\n",
      "Iteration 117, loss = 0.48178778\n",
      "Iteration 118, loss = 0.47940310\n",
      "Iteration 119, loss = 0.47702771\n",
      "Iteration 120, loss = 0.47465523\n",
      "Iteration 121, loss = 0.47228863\n",
      "Iteration 122, loss = 0.46996640\n",
      "Iteration 123, loss = 0.46765295\n",
      "Iteration 124, loss = 0.46535882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 125, loss = 0.46307948\n",
      "Iteration 126, loss = 0.46081231\n",
      "Iteration 127, loss = 0.45855638\n",
      "Iteration 128, loss = 0.45631074\n",
      "Iteration 129, loss = 0.45408118\n",
      "Iteration 130, loss = 0.45185914\n",
      "Iteration 131, loss = 0.44965146\n",
      "Iteration 132, loss = 0.44745501\n",
      "Iteration 133, loss = 0.44526943\n",
      "Iteration 134, loss = 0.44309593\n",
      "Iteration 135, loss = 0.44093300\n",
      "Iteration 136, loss = 0.43878048\n",
      "Iteration 137, loss = 0.43663842\n",
      "Iteration 138, loss = 0.43450655\n",
      "Iteration 139, loss = 0.43238509\n",
      "Iteration 140, loss = 0.43027436\n",
      "Iteration 141, loss = 0.42817315\n",
      "Iteration 142, loss = 0.42608171\n",
      "Iteration 143, loss = 0.42399995\n",
      "Iteration 144, loss = 0.42192791\n",
      "Iteration 145, loss = 0.41986559\n",
      "Iteration 146, loss = 0.41781342\n",
      "Iteration 147, loss = 0.41577040\n",
      "Iteration 148, loss = 0.41373510\n",
      "Iteration 149, loss = 0.41170555\n",
      "Iteration 150, loss = 0.40967975\n",
      "Iteration 151, loss = 0.40765691\n",
      "Iteration 152, loss = 0.40563606\n",
      "Iteration 153, loss = 0.40362670\n",
      "Iteration 154, loss = 0.40165415\n",
      "Iteration 155, loss = 0.39970558\n",
      "Iteration 156, loss = 0.39777558\n",
      "Iteration 157, loss = 0.39585451\n",
      "Iteration 158, loss = 0.39394112\n",
      "Iteration 159, loss = 0.39203866\n",
      "Iteration 160, loss = 0.39014921\n",
      "Iteration 161, loss = 0.38825662\n",
      "Iteration 162, loss = 0.38635780\n",
      "Iteration 163, loss = 0.38445350\n",
      "Iteration 164, loss = 0.38255068\n",
      "Iteration 165, loss = 0.38065685\n",
      "Iteration 166, loss = 0.37877254\n",
      "Iteration 167, loss = 0.37689242\n",
      "Iteration 168, loss = 0.37501983\n",
      "Iteration 169, loss = 0.37315656\n",
      "Iteration 170, loss = 0.37130423\n",
      "Iteration 171, loss = 0.36946113\n",
      "Iteration 172, loss = 0.36762485\n",
      "Iteration 173, loss = 0.36579392\n",
      "Iteration 174, loss = 0.36396835\n",
      "Iteration 175, loss = 0.36214766\n",
      "Iteration 176, loss = 0.36033144\n",
      "Iteration 177, loss = 0.35852004\n",
      "Iteration 178, loss = 0.35671430\n",
      "Iteration 179, loss = 0.35491348\n",
      "Iteration 180, loss = 0.35311965\n",
      "Iteration 181, loss = 0.35133206\n",
      "Iteration 182, loss = 0.34955128\n",
      "Iteration 183, loss = 0.34777689\n",
      "Iteration 184, loss = 0.34600829\n",
      "Iteration 185, loss = 0.34424570\n",
      "Iteration 186, loss = 0.34249054\n",
      "Iteration 187, loss = 0.34074382\n",
      "Iteration 188, loss = 0.33900409\n",
      "Iteration 189, loss = 0.33727367\n",
      "Iteration 190, loss = 0.33554951\n",
      "Iteration 191, loss = 0.33383024\n",
      "Iteration 192, loss = 0.33211727\n",
      "Iteration 193, loss = 0.33041120\n",
      "Iteration 194, loss = 0.32871004\n",
      "Iteration 195, loss = 0.32701671\n",
      "Iteration 196, loss = 0.32533088\n",
      "Iteration 197, loss = 0.32365186\n",
      "Iteration 198, loss = 0.32198060\n",
      "Iteration 199, loss = 0.32031506\n",
      "Iteration 200, loss = 0.31865030\n",
      "Iteration 201, loss = 0.31698459\n",
      "Iteration 202, loss = 0.31532925\n",
      "Iteration 203, loss = 0.31368128\n",
      "Iteration 204, loss = 0.31203493\n",
      "Iteration 205, loss = 0.31039034\n",
      "Iteration 206, loss = 0.30874674\n",
      "Iteration 207, loss = 0.30710416\n",
      "Iteration 208, loss = 0.30546242\n",
      "Iteration 209, loss = 0.30382163\n",
      "Iteration 210, loss = 0.30218158\n",
      "Iteration 211, loss = 0.30054297\n",
      "Iteration 212, loss = 0.29890502\n",
      "Iteration 213, loss = 0.29726917\n",
      "Iteration 214, loss = 0.29563424\n",
      "Iteration 215, loss = 0.29400046\n",
      "Iteration 216, loss = 0.29236845\n",
      "Iteration 217, loss = 0.29073808\n",
      "Iteration 218, loss = 0.28910924\n",
      "Iteration 219, loss = 0.28748245\n",
      "Iteration 220, loss = 0.28585807\n",
      "Iteration 221, loss = 0.28423585\n",
      "Iteration 222, loss = 0.28261607\n",
      "Iteration 223, loss = 0.28099928\n",
      "Iteration 224, loss = 0.27938549\n",
      "Iteration 225, loss = 0.27777523\n",
      "Iteration 226, loss = 0.27616830\n",
      "Iteration 227, loss = 0.27456556\n",
      "Iteration 228, loss = 0.27296676\n",
      "Iteration 229, loss = 0.27137204\n",
      "Iteration 230, loss = 0.26978215\n",
      "Iteration 231, loss = 0.26819703\n",
      "Iteration 232, loss = 0.26661674\n",
      "Iteration 233, loss = 0.26504170\n",
      "Iteration 234, loss = 0.26347236\n",
      "Iteration 235, loss = 0.26190867\n",
      "Iteration 236, loss = 0.26035079\n",
      "Iteration 237, loss = 0.25879895\n",
      "Iteration 238, loss = 0.25725382\n",
      "Iteration 239, loss = 0.25571506\n",
      "Iteration 240, loss = 0.25418234\n",
      "Iteration 241, loss = 0.25265268\n",
      "Iteration 242, loss = 0.25112480\n",
      "Iteration 243, loss = 0.24960865\n",
      "Iteration 244, loss = 0.24809627\n",
      "Iteration 245, loss = 0.24658769\n",
      "Iteration 246, loss = 0.24508332\n",
      "Iteration 247, loss = 0.24358275\n",
      "Iteration 248, loss = 0.24208634\n",
      "Iteration 249, loss = 0.24059418\n",
      "Iteration 250, loss = 0.23910566\n",
      "Iteration 251, loss = 0.23762093\n",
      "Iteration 252, loss = 0.23614084\n",
      "Iteration 253, loss = 0.23466425\n",
      "Iteration 254, loss = 0.23319212\n",
      "Iteration 255, loss = 0.23172402\n",
      "Iteration 256, loss = 0.23025952\n",
      "Iteration 257, loss = 0.22879921\n",
      "Iteration 258, loss = 0.22734206\n",
      "Iteration 259, loss = 0.22588376\n",
      "Iteration 260, loss = 0.22442142\n",
      "Iteration 261, loss = 0.22296958\n",
      "Iteration 262, loss = 0.22152156\n",
      "Iteration 263, loss = 0.22007570\n",
      "Iteration 264, loss = 0.21863192\n",
      "Iteration 265, loss = 0.21719066\n",
      "Iteration 266, loss = 0.21575190\n",
      "Iteration 267, loss = 0.21431598\n",
      "Iteration 268, loss = 0.21288303\n",
      "Iteration 269, loss = 0.21145328\n",
      "Iteration 270, loss = 0.21002720\n",
      "Iteration 271, loss = 0.20860505\n",
      "Iteration 272, loss = 0.20718695\n",
      "Iteration 273, loss = 0.20577326\n",
      "Iteration 274, loss = 0.20436436\n",
      "Iteration 275, loss = 0.20296044\n",
      "Iteration 276, loss = 0.20156198\n",
      "Iteration 277, loss = 0.20016935\n",
      "Iteration 278, loss = 0.19878286\n",
      "Iteration 279, loss = 0.19740299\n",
      "Iteration 280, loss = 0.19602995\n",
      "Iteration 281, loss = 0.19466390\n",
      "Iteration 282, loss = 0.19330520\n",
      "Iteration 283, loss = 0.19195433\n",
      "Iteration 284, loss = 0.19061161\n",
      "Iteration 285, loss = 0.18927685\n",
      "Iteration 286, loss = 0.18795104\n",
      "Iteration 287, loss = 0.18663409\n",
      "Iteration 288, loss = 0.18532614\n",
      "Iteration 289, loss = 0.18402738\n",
      "Iteration 290, loss = 0.18273810\n",
      "Iteration 291, loss = 0.18145854\n",
      "Iteration 292, loss = 0.18018885\n",
      "Iteration 293, loss = 0.17892907\n",
      "Iteration 294, loss = 0.17767942\n",
      "Iteration 295, loss = 0.17644037\n",
      "Iteration 296, loss = 0.17521161\n",
      "Iteration 297, loss = 0.17399335\n",
      "Iteration 298, loss = 0.17278529\n",
      "Iteration 299, loss = 0.17158982\n",
      "Iteration 300, loss = 0.17040389\n",
      "Iteration 301, loss = 0.16923019\n",
      "Iteration 302, loss = 0.16806605\n",
      "Iteration 303, loss = 0.16691146\n",
      "Iteration 304, loss = 0.16576723\n",
      "Iteration 305, loss = 0.16464283\n",
      "Iteration 306, loss = 0.16351584\n",
      "Iteration 307, loss = 0.16241004\n",
      "Iteration 308, loss = 0.16131477\n",
      "Iteration 309, loss = 0.16022873\n",
      "Iteration 310, loss = 0.15915173\n",
      "Iteration 311, loss = 0.15808357\n",
      "Iteration 312, loss = 0.15702690\n",
      "Iteration 313, loss = 0.15599016\n",
      "Iteration 314, loss = 0.15495189\n",
      "Iteration 315, loss = 0.15393089\n",
      "Iteration 316, loss = 0.15292187\n",
      "Iteration 317, loss = 0.15192284\n",
      "Iteration 318, loss = 0.15093226\n",
      "Iteration 319, loss = 0.14995150\n",
      "Iteration 320, loss = 0.14898089\n",
      "Iteration 321, loss = 0.14802332\n",
      "Iteration 322, loss = 0.14707564\n",
      "Iteration 323, loss = 0.14613561\n",
      "Iteration 324, loss = 0.14520818\n",
      "Iteration 325, loss = 0.14429132\n",
      "Iteration 326, loss = 0.14338401\n",
      "Iteration 327, loss = 0.14248597\n",
      "Iteration 328, loss = 0.14159828\n",
      "Iteration 329, loss = 0.14072037\n",
      "Iteration 330, loss = 0.13985180\n",
      "Iteration 331, loss = 0.13899202\n",
      "Iteration 332, loss = 0.13814281\n",
      "Iteration 333, loss = 0.13730306\n",
      "Iteration 334, loss = 0.13647242\n",
      "Iteration 335, loss = 0.13565082\n",
      "Iteration 336, loss = 0.13483826\n",
      "Iteration 337, loss = 0.13403465\n",
      "Iteration 338, loss = 0.13323983\n",
      "Iteration 339, loss = 0.13245366\n",
      "Iteration 340, loss = 0.13167603\n",
      "Iteration 341, loss = 0.13090752\n",
      "Iteration 342, loss = 0.13014784\n",
      "Iteration 343, loss = 0.12939620\n",
      "Iteration 344, loss = 0.12865225\n",
      "Iteration 345, loss = 0.12791591\n",
      "Iteration 346, loss = 0.12718715\n",
      "Iteration 347, loss = 0.12646566\n",
      "Iteration 348, loss = 0.12575137\n",
      "Iteration 349, loss = 0.12504400\n",
      "Iteration 350, loss = 0.12434299\n",
      "Iteration 351, loss = 0.12364797\n",
      "Iteration 352, loss = 0.12295876\n",
      "Iteration 353, loss = 0.12227547\n",
      "Iteration 354, loss = 0.12159776\n",
      "Iteration 355, loss = 0.12092531\n",
      "Iteration 356, loss = 0.12025841\n",
      "Iteration 357, loss = 0.11959593\n",
      "Iteration 358, loss = 0.11893887\n",
      "Iteration 359, loss = 0.11828683\n",
      "Iteration 360, loss = 0.11763939\n",
      "Iteration 361, loss = 0.11699673\n",
      "Iteration 362, loss = 0.11635892\n",
      "Iteration 363, loss = 0.11572556\n",
      "Iteration 364, loss = 0.11509684\n",
      "Iteration 365, loss = 0.11447272\n",
      "Iteration 366, loss = 0.11385300\n",
      "Iteration 367, loss = 0.11323779\n",
      "Iteration 368, loss = 0.11262709\n",
      "Iteration 369, loss = 0.11202119\n",
      "Iteration 370, loss = 0.11141978\n",
      "Iteration 371, loss = 0.11082301\n",
      "Iteration 372, loss = 0.11023073\n",
      "Iteration 373, loss = 0.10964334\n",
      "Iteration 374, loss = 0.10906041\n",
      "Iteration 375, loss = 0.10848245\n",
      "Iteration 376, loss = 0.10790910\n",
      "Iteration 377, loss = 0.10734070\n",
      "Iteration 378, loss = 0.10677723\n",
      "Iteration 379, loss = 0.10621833\n",
      "Iteration 380, loss = 0.10566437\n",
      "Iteration 381, loss = 0.10511545\n",
      "Iteration 382, loss = 0.10457138\n",
      "Iteration 383, loss = 0.10403214\n",
      "Iteration 384, loss = 0.10349792\n",
      "Iteration 385, loss = 0.10296870\n",
      "Iteration 386, loss = 0.10244447\n",
      "Iteration 387, loss = 0.10192514\n",
      "Iteration 388, loss = 0.10141068\n",
      "Iteration 389, loss = 0.10090123\n",
      "Iteration 390, loss = 0.10039668\n",
      "Iteration 391, loss = 0.09989693\n",
      "Iteration 392, loss = 0.09940189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 393, loss = 0.09891146\n",
      "Iteration 394, loss = 0.09842547\n",
      "Iteration 395, loss = 0.09794365\n",
      "Iteration 396, loss = 0.09746613\n",
      "Iteration 397, loss = 0.09699228\n",
      "Iteration 398, loss = 0.09652248\n",
      "Iteration 399, loss = 0.09605637\n",
      "Iteration 400, loss = 0.09559402\n",
      "Iteration 401, loss = 0.09513518\n",
      "Iteration 402, loss = 0.09467977\n",
      "Iteration 403, loss = 0.09422780\n",
      "Iteration 404, loss = 0.09377912\n",
      "Iteration 405, loss = 0.09333385\n",
      "Iteration 406, loss = 0.09289193\n",
      "Iteration 407, loss = 0.09245334\n",
      "Iteration 408, loss = 0.09201813\n",
      "Iteration 409, loss = 0.09158621\n",
      "Iteration 410, loss = 0.09115752\n",
      "Iteration 411, loss = 0.09073200\n",
      "Iteration 412, loss = 0.09030981\n",
      "Iteration 413, loss = 0.08989086\n",
      "Iteration 414, loss = 0.08947520\n",
      "Iteration 415, loss = 0.08906284\n",
      "Iteration 416, loss = 0.08865376\n",
      "Iteration 417, loss = 0.08824785\n",
      "Iteration 418, loss = 0.08784528\n",
      "Iteration 419, loss = 0.08744580\n",
      "Iteration 420, loss = 0.08704944\n",
      "Iteration 421, loss = 0.08665620\n",
      "Iteration 422, loss = 0.08626596\n",
      "Iteration 423, loss = 0.08587866\n",
      "Iteration 424, loss = 0.08549427\n",
      "Iteration 425, loss = 0.08511279\n",
      "Iteration 426, loss = 0.08473421\n",
      "Iteration 427, loss = 0.08435847\n",
      "Iteration 428, loss = 0.08398546\n",
      "Iteration 429, loss = 0.08361523\n",
      "Iteration 430, loss = 0.08324772\n",
      "Iteration 431, loss = 0.08288287\n",
      "Iteration 432, loss = 0.08252071\n",
      "Iteration 433, loss = 0.08216129\n",
      "Iteration 434, loss = 0.08180447\n",
      "Iteration 435, loss = 0.08145033\n",
      "Iteration 436, loss = 0.08109885\n",
      "Iteration 437, loss = 0.08075004\n",
      "Iteration 438, loss = 0.08040393\n",
      "Iteration 439, loss = 0.08006052\n",
      "Iteration 440, loss = 0.07971978\n",
      "Iteration 441, loss = 0.07938176\n",
      "Iteration 442, loss = 0.07904642\n",
      "Iteration 443, loss = 0.07871386\n",
      "Iteration 444, loss = 0.07838400\n",
      "Iteration 445, loss = 0.07805691\n",
      "Iteration 446, loss = 0.07773257\n",
      "Iteration 447, loss = 0.07741101\n",
      "Iteration 448, loss = 0.07709222\n",
      "Iteration 449, loss = 0.07677611\n",
      "Iteration 450, loss = 0.07646268\n",
      "Iteration 451, loss = 0.07615190\n",
      "Iteration 452, loss = 0.07584376\n",
      "Iteration 453, loss = 0.07553821\n",
      "Iteration 454, loss = 0.07523521\n",
      "Iteration 455, loss = 0.07493476\n",
      "Iteration 456, loss = 0.07463692\n",
      "Iteration 457, loss = 0.07434137\n",
      "Iteration 458, loss = 0.07404833\n",
      "Iteration 459, loss = 0.07375756\n",
      "Iteration 460, loss = 0.07346902\n",
      "Iteration 461, loss = 0.07318267\n",
      "Iteration 462, loss = 0.07289845\n",
      "Iteration 463, loss = 0.07261640\n",
      "Iteration 464, loss = 0.07233653\n",
      "Iteration 465, loss = 0.07205864\n",
      "Iteration 466, loss = 0.07178285\n",
      "Iteration 467, loss = 0.07150920\n",
      "Iteration 468, loss = 0.07123755\n",
      "Iteration 469, loss = 0.07096792\n",
      "Iteration 470, loss = 0.07070040\n",
      "Iteration 471, loss = 0.07043492\n",
      "Iteration 472, loss = 0.07017151\n",
      "Iteration 473, loss = 0.06991022\n",
      "Iteration 474, loss = 0.06965100\n",
      "Iteration 475, loss = 0.06939390\n",
      "Iteration 476, loss = 0.06913896\n",
      "Iteration 477, loss = 0.06888615\n",
      "Iteration 478, loss = 0.06863551\n",
      "Iteration 479, loss = 0.06838710\n",
      "Iteration 480, loss = 0.06814089\n",
      "Iteration 481, loss = 0.06789687\n",
      "Iteration 482, loss = 0.06765508\n",
      "Iteration 483, loss = 0.06741554\n",
      "Iteration 484, loss = 0.06717846\n",
      "Iteration 485, loss = 0.06694363\n",
      "Iteration 486, loss = 0.06671105\n",
      "Iteration 487, loss = 0.06648081\n",
      "Iteration 488, loss = 0.06625288\n",
      "Iteration 489, loss = 0.06602725\n",
      "Iteration 490, loss = 0.06580390\n",
      "Iteration 491, loss = 0.06558283\n",
      "Iteration 492, loss = 0.06536406\n",
      "Iteration 493, loss = 0.06514757\n",
      "Iteration 494, loss = 0.06493337\n",
      "Iteration 495, loss = 0.06472145\n",
      "Iteration 496, loss = 0.06451179\n",
      "Iteration 497, loss = 0.06430442\n",
      "Iteration 498, loss = 0.06409928\n",
      "Iteration 499, loss = 0.06389637\n",
      "Iteration 500, loss = 0.06369574\n",
      "Iteration 501, loss = 0.06349738\n",
      "Iteration 502, loss = 0.06330126\n",
      "Iteration 503, loss = 0.06310733\n",
      "Iteration 504, loss = 0.06291559\n",
      "Iteration 505, loss = 0.06272604\n",
      "Iteration 506, loss = 0.06253866\n",
      "Iteration 507, loss = 0.06235341\n",
      "Iteration 508, loss = 0.06217026\n",
      "Iteration 509, loss = 0.06198920\n",
      "Iteration 510, loss = 0.06181022\n",
      "Iteration 511, loss = 0.06163331\n",
      "Iteration 512, loss = 0.06145852\n",
      "Iteration 513, loss = 0.06128573\n",
      "Iteration 514, loss = 0.06111497\n",
      "Iteration 515, loss = 0.06094618\n",
      "Iteration 516, loss = 0.06077936\n",
      "Iteration 517, loss = 0.06061448\n",
      "Iteration 518, loss = 0.06045154\n",
      "Iteration 519, loss = 0.06029051\n",
      "Iteration 520, loss = 0.06013135\n",
      "Iteration 521, loss = 0.05997409\n",
      "Iteration 522, loss = 0.05981870\n",
      "Iteration 523, loss = 0.05966510\n",
      "Iteration 524, loss = 0.05951329\n",
      "Iteration 525, loss = 0.05936327\n",
      "Iteration 526, loss = 0.05921503\n",
      "Iteration 527, loss = 0.05906852\n",
      "Iteration 528, loss = 0.05892372\n",
      "Iteration 529, loss = 0.05878060\n",
      "Iteration 530, loss = 0.05863915\n",
      "Iteration 531, loss = 0.05849938\n",
      "Iteration 532, loss = 0.05836123\n",
      "Iteration 533, loss = 0.05822464\n",
      "Iteration 534, loss = 0.05808965\n",
      "Iteration 535, loss = 0.05795623\n",
      "Iteration 536, loss = 0.05782436\n",
      "Iteration 537, loss = 0.05769402\n",
      "Iteration 538, loss = 0.05756516\n",
      "Iteration 539, loss = 0.05743777\n",
      "Iteration 540, loss = 0.05731184\n",
      "Iteration 541, loss = 0.05718735\n",
      "Iteration 542, loss = 0.05706432\n",
      "Iteration 543, loss = 0.05694267\n",
      "Iteration 544, loss = 0.05682237\n",
      "Iteration 545, loss = 0.05670345\n",
      "Iteration 546, loss = 0.05658587\n",
      "Iteration 547, loss = 0.05646963\n",
      "Iteration 548, loss = 0.05635468\n",
      "Iteration 549, loss = 0.05624102\n",
      "Iteration 550, loss = 0.05612862\n",
      "Iteration 551, loss = 0.05601746\n",
      "Iteration 552, loss = 0.05590753\n",
      "Iteration 553, loss = 0.05579887\n",
      "Iteration 554, loss = 0.05569140\n",
      "Iteration 555, loss = 0.05558509\n",
      "Iteration 556, loss = 0.05547998\n",
      "Iteration 557, loss = 0.05537601\n",
      "Iteration 558, loss = 0.05527317\n",
      "Iteration 559, loss = 0.05517144\n",
      "Iteration 560, loss = 0.05507081\n",
      "Iteration 561, loss = 0.05497126\n",
      "Iteration 562, loss = 0.05487278\n",
      "Iteration 563, loss = 0.05477536\n",
      "Iteration 564, loss = 0.05467896\n",
      "Iteration 565, loss = 0.05458364\n",
      "Iteration 566, loss = 0.05448931\n",
      "Iteration 567, loss = 0.05439595\n",
      "Iteration 568, loss = 0.05430358\n",
      "Iteration 569, loss = 0.05421219\n",
      "Iteration 570, loss = 0.05412175\n",
      "Iteration 571, loss = 0.05403226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.59315210\n",
      "Iteration 3, loss = 1.52935741\n",
      "Iteration 4, loss = 1.46970156\n",
      "Iteration 5, loss = 1.41467199\n",
      "Iteration 6, loss = 1.36462002\n",
      "Iteration 7, loss = 1.31996670\n",
      "Iteration 8, loss = 1.28110265\n",
      "Iteration 9, loss = 1.24804242\n",
      "Iteration 10, loss = 1.22074220\n",
      "Iteration 11, loss = 1.19859545\n",
      "Iteration 12, loss = 1.18071736\n",
      "Iteration 13, loss = 1.16603817\n",
      "Iteration 14, loss = 1.15333194\n",
      "Iteration 15, loss = 1.14147628\n",
      "Iteration 16, loss = 1.12955391\n",
      "Iteration 17, loss = 1.11693662\n",
      "Iteration 18, loss = 1.10318909\n",
      "Iteration 19, loss = 1.08828424\n",
      "Iteration 20, loss = 1.07242468\n",
      "Iteration 21, loss = 1.05582859\n",
      "Iteration 22, loss = 1.03894541\n",
      "Iteration 23, loss = 1.02212031\n",
      "Iteration 24, loss = 1.00574841\n",
      "Iteration 25, loss = 0.99004872\n",
      "Iteration 26, loss = 0.97519812\n",
      "Iteration 27, loss = 0.96121692\n",
      "Iteration 28, loss = 0.94810998\n",
      "Iteration 29, loss = 0.93583933\n",
      "Iteration 30, loss = 0.92422379\n",
      "Iteration 31, loss = 0.91307830\n",
      "Iteration 32, loss = 0.90221454\n",
      "Iteration 33, loss = 0.89151592\n",
      "Iteration 34, loss = 0.88094174\n",
      "Iteration 35, loss = 0.87036349\n",
      "Iteration 36, loss = 0.85982921\n",
      "Iteration 37, loss = 0.84936639\n",
      "Iteration 38, loss = 0.83898784\n",
      "Iteration 39, loss = 0.82870429\n",
      "Iteration 40, loss = 0.81862341\n",
      "Iteration 41, loss = 0.80882910\n",
      "Iteration 42, loss = 0.79936616\n",
      "Iteration 43, loss = 0.79020018\n",
      "Iteration 44, loss = 0.78133543\n",
      "Iteration 45, loss = 0.77274335\n",
      "Iteration 46, loss = 0.76445496\n",
      "Iteration 47, loss = 0.75648221\n",
      "Iteration 48, loss = 0.74880618\n",
      "Iteration 49, loss = 0.74137555\n",
      "Iteration 50, loss = 0.73420888\n",
      "Iteration 51, loss = 0.72729455\n",
      "Iteration 52, loss = 0.72058130\n",
      "Iteration 53, loss = 0.71404257\n",
      "Iteration 54, loss = 0.70768536\n",
      "Iteration 55, loss = 0.70146060\n",
      "Iteration 56, loss = 0.69535818\n",
      "Iteration 57, loss = 0.68936639\n",
      "Iteration 58, loss = 0.68347666\n",
      "Iteration 59, loss = 0.67768912\n",
      "Iteration 60, loss = 0.67197401\n",
      "Iteration 61, loss = 0.66636454\n",
      "Iteration 62, loss = 0.66092476\n",
      "Iteration 63, loss = 0.65562017\n",
      "Iteration 64, loss = 0.65046701\n",
      "Iteration 65, loss = 0.64552286\n",
      "Iteration 66, loss = 0.64077901\n",
      "Iteration 67, loss = 0.63620946\n",
      "Iteration 68, loss = 0.63177164\n",
      "Iteration 69, loss = 0.62746187\n",
      "Iteration 70, loss = 0.62324459\n",
      "Iteration 71, loss = 0.61912767\n",
      "Iteration 72, loss = 0.61509326\n",
      "Iteration 73, loss = 0.61113097\n",
      "Iteration 74, loss = 0.60722702\n",
      "Iteration 75, loss = 0.60336880\n",
      "Iteration 76, loss = 0.59954537\n",
      "Iteration 77, loss = 0.59576788\n",
      "Iteration 78, loss = 0.59202536\n",
      "Iteration 79, loss = 0.58826508\n",
      "Iteration 80, loss = 0.58457789\n",
      "Iteration 81, loss = 0.58101389\n",
      "Iteration 82, loss = 0.57754688\n",
      "Iteration 83, loss = 0.57414408\n",
      "Iteration 84, loss = 0.57079726\n",
      "Iteration 85, loss = 0.56749522\n",
      "Iteration 86, loss = 0.56423510\n",
      "Iteration 87, loss = 0.56101839\n",
      "Iteration 88, loss = 0.55784610\n",
      "Iteration 89, loss = 0.55471772\n",
      "Iteration 90, loss = 0.55163254\n",
      "Iteration 91, loss = 0.54858993\n",
      "Iteration 92, loss = 0.54558691\n",
      "Iteration 93, loss = 0.54262218\n",
      "Iteration 94, loss = 0.53969437\n",
      "Iteration 95, loss = 0.53680276\n",
      "Iteration 96, loss = 0.53394742\n",
      "Iteration 97, loss = 0.53112750\n",
      "Iteration 98, loss = 0.52834327\n",
      "Iteration 99, loss = 0.52559423\n",
      "Iteration 100, loss = 0.52287897\n",
      "Iteration 101, loss = 0.52019543\n",
      "Iteration 102, loss = 0.51754429\n",
      "Iteration 103, loss = 0.51492382\n",
      "Iteration 104, loss = 0.51233209\n",
      "Iteration 105, loss = 0.50976767\n",
      "Iteration 106, loss = 0.50723018\n",
      "Iteration 107, loss = 0.50471879\n",
      "Iteration 108, loss = 0.50223296\n",
      "Iteration 109, loss = 0.49977215\n",
      "Iteration 110, loss = 0.49733556\n",
      "Iteration 111, loss = 0.49492500\n",
      "Iteration 112, loss = 0.49253843\n",
      "Iteration 113, loss = 0.49017395\n",
      "Iteration 114, loss = 0.48783074\n",
      "Iteration 115, loss = 0.48551123\n",
      "Iteration 116, loss = 0.48326127\n",
      "Iteration 117, loss = 0.48105951\n",
      "Iteration 118, loss = 0.47888228\n",
      "Iteration 119, loss = 0.47671463\n",
      "Iteration 120, loss = 0.47454361\n",
      "Iteration 121, loss = 0.47236937\n",
      "Iteration 122, loss = 0.47023180\n",
      "Iteration 123, loss = 0.46807190\n",
      "Iteration 124, loss = 0.46595492\n",
      "Iteration 125, loss = 0.46384559\n",
      "Iteration 126, loss = 0.46174228\n",
      "Iteration 127, loss = 0.45964785\n",
      "Iteration 128, loss = 0.45758024\n",
      "Iteration 129, loss = 0.45553224\n",
      "Iteration 130, loss = 0.45350434\n",
      "Iteration 131, loss = 0.45149739\n",
      "Iteration 132, loss = 0.44949655\n",
      "Iteration 133, loss = 0.44750294\n",
      "Iteration 134, loss = 0.44551965\n",
      "Iteration 135, loss = 0.44355006\n",
      "Iteration 136, loss = 0.44158611\n",
      "Iteration 137, loss = 0.43963340\n",
      "Iteration 138, loss = 0.43769058\n",
      "Iteration 139, loss = 0.43575590\n",
      "Iteration 140, loss = 0.43382885\n",
      "Iteration 141, loss = 0.43191008\n",
      "Iteration 142, loss = 0.43000531\n",
      "Iteration 143, loss = 0.42811560\n",
      "Iteration 144, loss = 0.42623595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 145, loss = 0.42436626\n",
      "Iteration 146, loss = 0.42250658\n",
      "Iteration 147, loss = 0.42065404\n",
      "Iteration 148, loss = 0.41880890\n",
      "Iteration 149, loss = 0.41697387\n",
      "Iteration 150, loss = 0.41514448\n",
      "Iteration 151, loss = 0.41331960\n",
      "Iteration 152, loss = 0.41150047\n",
      "Iteration 153, loss = 0.40968418\n",
      "Iteration 154, loss = 0.40787012\n",
      "Iteration 155, loss = 0.40605808\n",
      "Iteration 156, loss = 0.40424928\n",
      "Iteration 157, loss = 0.40245304\n",
      "Iteration 158, loss = 0.40067549\n",
      "Iteration 159, loss = 0.39889706\n",
      "Iteration 160, loss = 0.39711681\n",
      "Iteration 161, loss = 0.39534332\n",
      "Iteration 162, loss = 0.39357998\n",
      "Iteration 163, loss = 0.39182502\n",
      "Iteration 164, loss = 0.39007321\n",
      "Iteration 165, loss = 0.38832305\n",
      "Iteration 166, loss = 0.38657591\n",
      "Iteration 167, loss = 0.38483342\n",
      "Iteration 168, loss = 0.38310094\n",
      "Iteration 169, loss = 0.38137334\n",
      "Iteration 170, loss = 0.37965073\n",
      "Iteration 171, loss = 0.37793207\n",
      "Iteration 172, loss = 0.37621716\n",
      "Iteration 173, loss = 0.37450706\n",
      "Iteration 174, loss = 0.37280208\n",
      "Iteration 175, loss = 0.37110210\n",
      "Iteration 176, loss = 0.36940753\n",
      "Iteration 177, loss = 0.36771857\n",
      "Iteration 178, loss = 0.36603446\n",
      "Iteration 179, loss = 0.36435563\n",
      "Iteration 180, loss = 0.36268146\n",
      "Iteration 181, loss = 0.36101301\n",
      "Iteration 182, loss = 0.35935021\n",
      "Iteration 183, loss = 0.35769264\n",
      "Iteration 184, loss = 0.35604147\n",
      "Iteration 185, loss = 0.35439593\n",
      "Iteration 186, loss = 0.35275562\n",
      "Iteration 187, loss = 0.35112065\n",
      "Iteration 188, loss = 0.34949176\n",
      "Iteration 189, loss = 0.34786911\n",
      "Iteration 190, loss = 0.34625200\n",
      "Iteration 191, loss = 0.34464045\n",
      "Iteration 192, loss = 0.34303534\n",
      "Iteration 193, loss = 0.34143628\n",
      "Iteration 194, loss = 0.33984280\n",
      "Iteration 195, loss = 0.33825561\n",
      "Iteration 196, loss = 0.33667427\n",
      "Iteration 197, loss = 0.33509886\n",
      "Iteration 198, loss = 0.33352982\n",
      "Iteration 199, loss = 0.33196661\n",
      "Iteration 200, loss = 0.33040459\n",
      "Iteration 201, loss = 0.32884049\n",
      "Iteration 202, loss = 0.32728244\n",
      "Iteration 203, loss = 0.32573515\n",
      "Iteration 204, loss = 0.32419010\n",
      "Iteration 205, loss = 0.32264638\n",
      "Iteration 206, loss = 0.32110371\n",
      "Iteration 207, loss = 0.31956183\n",
      "Iteration 208, loss = 0.31802067\n",
      "Iteration 209, loss = 0.31648048\n",
      "Iteration 210, loss = 0.31494109\n",
      "Iteration 211, loss = 0.31340270\n",
      "Iteration 212, loss = 0.31186517\n",
      "Iteration 213, loss = 0.31032922\n",
      "Iteration 214, loss = 0.30879430\n",
      "Iteration 215, loss = 0.30726055\n",
      "Iteration 216, loss = 0.30572838\n",
      "Iteration 217, loss = 0.30419780\n",
      "Iteration 218, loss = 0.30266930\n",
      "Iteration 219, loss = 0.30114229\n",
      "Iteration 220, loss = 0.29961700\n",
      "Iteration 221, loss = 0.29809565\n",
      "Iteration 222, loss = 0.29657664\n",
      "Iteration 223, loss = 0.29506013\n",
      "Iteration 224, loss = 0.29354657\n",
      "Iteration 225, loss = 0.29203636\n",
      "Iteration 226, loss = 0.29052965\n",
      "Iteration 227, loss = 0.28902732\n",
      "Iteration 228, loss = 0.28752909\n",
      "Iteration 229, loss = 0.28603511\n",
      "Iteration 230, loss = 0.28454579\n",
      "Iteration 231, loss = 0.28306092\n",
      "Iteration 232, loss = 0.28158106\n",
      "Iteration 233, loss = 0.28010631\n",
      "Iteration 234, loss = 0.27863697\n",
      "Iteration 235, loss = 0.27717321\n",
      "Iteration 236, loss = 0.27571530\n",
      "Iteration 237, loss = 0.27426360\n",
      "Iteration 238, loss = 0.27281800\n",
      "Iteration 239, loss = 0.27137868\n",
      "Iteration 240, loss = 0.26994554\n",
      "Iteration 241, loss = 0.26851399\n",
      "Iteration 242, loss = 0.26708590\n",
      "Iteration 243, loss = 0.26566848\n",
      "Iteration 244, loss = 0.26425562\n",
      "Iteration 245, loss = 0.26284663\n",
      "Iteration 246, loss = 0.26144163\n",
      "Iteration 247, loss = 0.26004095\n",
      "Iteration 248, loss = 0.25864425\n",
      "Iteration 249, loss = 0.25725184\n",
      "Iteration 250, loss = 0.25586361\n",
      "Iteration 251, loss = 0.25447929\n",
      "Iteration 252, loss = 0.25309881\n",
      "Iteration 253, loss = 0.25172224\n",
      "Iteration 254, loss = 0.25034940\n",
      "Iteration 255, loss = 0.24898030\n",
      "Iteration 256, loss = 0.24761516\n",
      "Iteration 257, loss = 0.24625443\n",
      "Iteration 258, loss = 0.24489813\n",
      "Iteration 259, loss = 0.24354654\n",
      "Iteration 260, loss = 0.24219939\n",
      "Iteration 261, loss = 0.24085688\n",
      "Iteration 262, loss = 0.23951940\n",
      "Iteration 263, loss = 0.23818705\n",
      "Iteration 264, loss = 0.23685999\n",
      "Iteration 265, loss = 0.23553837\n",
      "Iteration 266, loss = 0.23422212\n",
      "Iteration 267, loss = 0.23291148\n",
      "Iteration 268, loss = 0.23160633\n",
      "Iteration 269, loss = 0.23030669\n",
      "Iteration 270, loss = 0.22901256\n",
      "Iteration 271, loss = 0.22772366\n",
      "Iteration 272, loss = 0.22643968\n",
      "Iteration 273, loss = 0.22516053\n",
      "Iteration 274, loss = 0.22388535\n",
      "Iteration 275, loss = 0.22261352\n",
      "Iteration 276, loss = 0.22134478\n",
      "Iteration 277, loss = 0.22007915\n",
      "Iteration 278, loss = 0.21881969\n",
      "Iteration 279, loss = 0.21756175\n",
      "Iteration 280, loss = 0.21630612\n",
      "Iteration 281, loss = 0.21505360\n",
      "Iteration 282, loss = 0.21380659\n",
      "Iteration 283, loss = 0.21256298\n",
      "Iteration 284, loss = 0.21131773\n",
      "Iteration 285, loss = 0.21007858\n",
      "Iteration 286, loss = 0.20884487\n",
      "Iteration 287, loss = 0.20761928\n",
      "Iteration 288, loss = 0.20640390\n",
      "Iteration 289, loss = 0.20519783\n",
      "Iteration 290, loss = 0.20400422\n",
      "Iteration 291, loss = 0.20282363\n",
      "Iteration 292, loss = 0.20165118\n",
      "Iteration 293, loss = 0.20048683\n",
      "Iteration 294, loss = 0.19932833\n",
      "Iteration 295, loss = 0.19818042\n",
      "Iteration 296, loss = 0.19704187\n",
      "Iteration 297, loss = 0.19591212\n",
      "Iteration 298, loss = 0.19479098\n",
      "Iteration 299, loss = 0.19367894\n",
      "Iteration 300, loss = 0.19257576\n",
      "Iteration 301, loss = 0.19148215\n",
      "Iteration 302, loss = 0.19039718\n",
      "Iteration 303, loss = 0.18932124\n",
      "Iteration 304, loss = 0.18825354\n",
      "Iteration 305, loss = 0.18719434\n",
      "Iteration 306, loss = 0.18614336\n",
      "Iteration 307, loss = 0.18509950\n",
      "Iteration 308, loss = 0.18406303\n",
      "Iteration 309, loss = 0.18303689\n",
      "Iteration 310, loss = 0.18201867\n",
      "Iteration 311, loss = 0.18100976\n",
      "Iteration 312, loss = 0.18000761\n",
      "Iteration 313, loss = 0.17901275\n",
      "Iteration 314, loss = 0.17802812\n",
      "Iteration 315, loss = 0.17705009\n",
      "Iteration 316, loss = 0.17608154\n",
      "Iteration 317, loss = 0.17512038\n",
      "Iteration 318, loss = 0.17416819\n",
      "Iteration 319, loss = 0.17322376\n",
      "Iteration 320, loss = 0.17228694\n",
      "Iteration 321, loss = 0.17135910\n",
      "Iteration 322, loss = 0.17043905\n",
      "Iteration 323, loss = 0.16952711\n",
      "Iteration 324, loss = 0.16862395\n",
      "Iteration 325, loss = 0.16772857\n",
      "Iteration 326, loss = 0.16684135\n",
      "Iteration 327, loss = 0.16596280\n",
      "Iteration 328, loss = 0.16509214\n",
      "Iteration 329, loss = 0.16422948\n",
      "Iteration 330, loss = 0.16337547\n",
      "Iteration 331, loss = 0.16252961\n",
      "Iteration 332, loss = 0.16169183\n",
      "Iteration 333, loss = 0.16086220\n",
      "Iteration 334, loss = 0.16004093\n",
      "Iteration 335, loss = 0.15922771\n",
      "Iteration 336, loss = 0.15842273\n",
      "Iteration 337, loss = 0.15762601\n",
      "Iteration 338, loss = 0.15683774\n",
      "Iteration 339, loss = 0.15605758\n",
      "Iteration 340, loss = 0.15528509\n",
      "Iteration 341, loss = 0.15452003\n",
      "Iteration 342, loss = 0.15376250\n",
      "Iteration 343, loss = 0.15301246\n",
      "Iteration 344, loss = 0.15226952\n",
      "Iteration 345, loss = 0.15153362\n",
      "Iteration 346, loss = 0.15080469\n",
      "Iteration 347, loss = 0.15008222\n",
      "Iteration 348, loss = 0.14936612\n",
      "Iteration 349, loss = 0.14865648\n",
      "Iteration 350, loss = 0.14795301\n",
      "Iteration 351, loss = 0.14725564\n",
      "Iteration 352, loss = 0.14656421\n",
      "Iteration 353, loss = 0.14587855\n",
      "Iteration 354, loss = 0.14519866\n",
      "Iteration 355, loss = 0.14452459\n",
      "Iteration 356, loss = 0.14385616\n",
      "Iteration 357, loss = 0.14319361\n",
      "Iteration 358, loss = 0.14253667\n",
      "Iteration 359, loss = 0.14188556\n",
      "Iteration 360, loss = 0.14124046\n",
      "Iteration 361, loss = 0.14060091\n",
      "Iteration 362, loss = 0.13996735\n",
      "Iteration 363, loss = 0.13934005\n",
      "Iteration 364, loss = 0.13871834\n",
      "Iteration 365, loss = 0.13810095\n",
      "Iteration 366, loss = 0.13748728\n",
      "Iteration 367, loss = 0.13688029\n",
      "Iteration 368, loss = 0.13627851\n",
      "Iteration 369, loss = 0.13568132\n",
      "Iteration 370, loss = 0.13508867\n",
      "Iteration 371, loss = 0.13450048\n",
      "Iteration 372, loss = 0.13391692\n",
      "Iteration 373, loss = 0.13333824\n",
      "Iteration 374, loss = 0.13276415\n",
      "Iteration 375, loss = 0.13219465\n",
      "Iteration 376, loss = 0.13163030\n",
      "Iteration 377, loss = 0.13107107\n",
      "Iteration 378, loss = 0.13051570\n",
      "Iteration 379, loss = 0.12996472\n",
      "Iteration 380, loss = 0.12941895\n",
      "Iteration 381, loss = 0.12887798\n",
      "Iteration 382, loss = 0.12834185\n",
      "Iteration 383, loss = 0.12781054\n",
      "Iteration 384, loss = 0.12728387\n",
      "Iteration 385, loss = 0.12676202\n",
      "Iteration 386, loss = 0.12624516\n",
      "Iteration 387, loss = 0.12573302\n",
      "Iteration 388, loss = 0.12522568\n",
      "Iteration 389, loss = 0.12472336\n",
      "Iteration 390, loss = 0.12422604\n",
      "Iteration 391, loss = 0.12373348\n",
      "Iteration 392, loss = 0.12324571\n",
      "Iteration 393, loss = 0.12276277\n",
      "Iteration 394, loss = 0.12228450\n",
      "Iteration 395, loss = 0.12181068\n",
      "Iteration 396, loss = 0.12134166\n",
      "Iteration 397, loss = 0.12087707\n",
      "Iteration 398, loss = 0.12041707\n",
      "Iteration 399, loss = 0.11996130\n",
      "Iteration 400, loss = 0.11950971\n",
      "Iteration 401, loss = 0.11906224\n",
      "Iteration 402, loss = 0.11861880\n",
      "Iteration 403, loss = 0.11817933\n",
      "Iteration 404, loss = 0.11774388\n",
      "Iteration 405, loss = 0.11731247\n",
      "Iteration 406, loss = 0.11688498\n",
      "Iteration 407, loss = 0.11646145\n",
      "Iteration 408, loss = 0.11604192\n",
      "Iteration 409, loss = 0.11562633\n",
      "Iteration 410, loss = 0.11521472\n",
      "Iteration 411, loss = 0.11480710\n",
      "Iteration 412, loss = 0.11440356\n",
      "Iteration 413, loss = 0.11400398\n",
      "Iteration 414, loss = 0.11360860\n",
      "Iteration 415, loss = 0.11321734\n",
      "Iteration 416, loss = 0.11283008\n",
      "Iteration 417, loss = 0.11244699\n",
      "Iteration 418, loss = 0.11206807\n",
      "Iteration 419, loss = 0.11169320\n",
      "Iteration 420, loss = 0.11132232\n",
      "Iteration 421, loss = 0.11095542\n",
      "Iteration 422, loss = 0.11059244\n",
      "Iteration 423, loss = 0.11023307\n",
      "Iteration 424, loss = 0.10987765\n",
      "Iteration 425, loss = 0.10952614\n",
      "Iteration 426, loss = 0.10917840\n",
      "Iteration 427, loss = 0.10883440\n",
      "Iteration 428, loss = 0.10849419\n",
      "Iteration 429, loss = 0.10815774\n",
      "Iteration 430, loss = 0.10782494\n",
      "Iteration 431, loss = 0.10749582\n",
      "Iteration 432, loss = 0.10717025\n",
      "Iteration 433, loss = 0.10684821\n",
      "Iteration 434, loss = 0.10652962\n",
      "Iteration 435, loss = 0.10621442\n",
      "Iteration 436, loss = 0.10590257\n",
      "Iteration 437, loss = 0.10559402\n",
      "Iteration 438, loss = 0.10528879\n",
      "Iteration 439, loss = 0.10498670\n",
      "Iteration 440, loss = 0.10468776\n",
      "Iteration 441, loss = 0.10439203\n",
      "Iteration 442, loss = 0.10409936\n",
      "Iteration 443, loss = 0.10380968\n",
      "Iteration 444, loss = 0.10352292\n",
      "Iteration 445, loss = 0.10323917\n",
      "Iteration 446, loss = 0.10295829\n",
      "Iteration 447, loss = 0.10268030\n",
      "Iteration 448, loss = 0.10240513\n",
      "Iteration 449, loss = 0.10213288\n",
      "Iteration 450, loss = 0.10186329\n",
      "Iteration 451, loss = 0.10159660\n",
      "Iteration 452, loss = 0.10133261\n",
      "Iteration 453, loss = 0.10107136\n",
      "Iteration 454, loss = 0.10081270\n",
      "Iteration 455, loss = 0.10055687\n",
      "Iteration 456, loss = 0.10030370\n",
      "Iteration 457, loss = 0.10005320\n",
      "Iteration 458, loss = 0.09980528\n",
      "Iteration 459, loss = 0.09955992\n",
      "Iteration 460, loss = 0.09931714\n",
      "Iteration 461, loss = 0.09907692\n",
      "Iteration 462, loss = 0.09883915\n",
      "Iteration 463, loss = 0.09860383\n",
      "Iteration 464, loss = 0.09837111\n",
      "Iteration 465, loss = 0.09814090\n",
      "Iteration 466, loss = 0.09791303\n",
      "Iteration 467, loss = 0.09768753\n",
      "Iteration 468, loss = 0.09746455\n",
      "Iteration 469, loss = 0.09724389\n",
      "Iteration 470, loss = 0.09702547\n",
      "Iteration 471, loss = 0.09680926\n",
      "Iteration 472, loss = 0.09659529\n",
      "Iteration 473, loss = 0.09638352\n",
      "Iteration 474, loss = 0.09617404\n",
      "Iteration 475, loss = 0.09596655\n",
      "Iteration 476, loss = 0.09576143\n",
      "Iteration 477, loss = 0.09555840\n",
      "Iteration 478, loss = 0.09535746\n",
      "Iteration 479, loss = 0.09515852\n",
      "Iteration 480, loss = 0.09496167\n",
      "Iteration 481, loss = 0.09476683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 482, loss = 0.09457406\n",
      "Iteration 483, loss = 0.09438345\n",
      "Iteration 484, loss = 0.09419488\n",
      "Iteration 485, loss = 0.09400827\n",
      "Iteration 486, loss = 0.09382372\n",
      "Iteration 487, loss = 0.09364127\n",
      "Iteration 488, loss = 0.09346081\n",
      "Iteration 489, loss = 0.09328232\n",
      "Iteration 490, loss = 0.09310583\n",
      "Iteration 491, loss = 0.09293132\n",
      "Iteration 492, loss = 0.09275883\n",
      "Iteration 493, loss = 0.09258831\n",
      "Iteration 494, loss = 0.09241980\n",
      "Iteration 495, loss = 0.09225331\n",
      "Iteration 496, loss = 0.09208876\n",
      "Iteration 497, loss = 0.09192613\n",
      "Iteration 498, loss = 0.09176546\n",
      "Iteration 499, loss = 0.09160673\n",
      "Iteration 500, loss = 0.09144995\n",
      "Iteration 501, loss = 0.09129515\n",
      "Iteration 502, loss = 0.09114230\n",
      "Iteration 503, loss = 0.09099135\n",
      "Iteration 504, loss = 0.09084230\n",
      "Iteration 505, loss = 0.09069516\n",
      "Iteration 506, loss = 0.09054995\n",
      "Iteration 507, loss = 0.09040657\n",
      "Iteration 508, loss = 0.09026504\n",
      "Iteration 509, loss = 0.09012540\n",
      "Iteration 510, loss = 0.08998762\n",
      "Iteration 511, loss = 0.08985162\n",
      "Iteration 512, loss = 0.08971736\n",
      "Iteration 513, loss = 0.08958483\n",
      "Iteration 514, loss = 0.08945406\n",
      "Iteration 515, loss = 0.08932508\n",
      "Iteration 516, loss = 0.08919792\n",
      "Iteration 517, loss = 0.08907242\n",
      "Iteration 518, loss = 0.08894854\n",
      "Iteration 519, loss = 0.08882632\n",
      "Iteration 520, loss = 0.08870571\n",
      "Iteration 521, loss = 0.08858677\n",
      "Iteration 522, loss = 0.08846935\n",
      "Iteration 523, loss = 0.08835347\n",
      "Iteration 524, loss = 0.08823915\n",
      "Iteration 525, loss = 0.08812641\n",
      "Iteration 526, loss = 0.08801508\n",
      "Iteration 527, loss = 0.08790524\n",
      "Iteration 528, loss = 0.08779709\n",
      "Iteration 529, loss = 0.08769067\n",
      "Iteration 530, loss = 0.08758614\n",
      "Iteration 531, loss = 0.08748257\n",
      "Iteration 532, loss = 0.08737924\n",
      "Iteration 533, loss = 0.08727671\n",
      "Iteration 534, loss = 0.08717686\n",
      "Iteration 535, loss = 0.08707907\n",
      "Iteration 536, loss = 0.08698137\n",
      "Iteration 537, loss = 0.08688435\n",
      "Iteration 538, loss = 0.08678941\n",
      "Iteration 539, loss = 0.08669609\n",
      "Iteration 540, loss = 0.08660313\n",
      "Iteration 541, loss = 0.08651089\n",
      "Iteration 542, loss = 0.08642051\n",
      "Iteration 543, loss = 0.08633143\n",
      "Iteration 544, loss = 0.08624262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66212118\n",
      "Iteration 2, loss = 1.62570402\n",
      "Iteration 3, loss = 1.57682010\n",
      "Iteration 4, loss = 1.51974280\n",
      "Iteration 5, loss = 1.45893919\n",
      "Iteration 6, loss = 1.39889203\n",
      "Iteration 7, loss = 1.34331287\n",
      "Iteration 8, loss = 1.29530610\n",
      "Iteration 9, loss = 1.25675812\n",
      "Iteration 10, loss = 1.22789841\n",
      "Iteration 11, loss = 1.20744254\n",
      "Iteration 12, loss = 1.19295399\n",
      "Iteration 13, loss = 1.18160684\n",
      "Iteration 14, loss = 1.17076766\n",
      "Iteration 15, loss = 1.15849353\n",
      "Iteration 16, loss = 1.14370805\n",
      "Iteration 17, loss = 1.12652765\n",
      "Iteration 18, loss = 1.10732691\n",
      "Iteration 19, loss = 1.08645030\n",
      "Iteration 20, loss = 1.06487701\n",
      "Iteration 21, loss = 1.04359787\n",
      "Iteration 22, loss = 1.02349592\n",
      "Iteration 23, loss = 1.00519789\n",
      "Iteration 24, loss = 0.98866827\n",
      "Iteration 25, loss = 0.97394585\n",
      "Iteration 26, loss = 0.96078301\n",
      "Iteration 27, loss = 0.94887654\n",
      "Iteration 28, loss = 0.93788412\n",
      "Iteration 29, loss = 0.92744302\n",
      "Iteration 30, loss = 0.91732945\n",
      "Iteration 31, loss = 0.90739655\n",
      "Iteration 32, loss = 0.89758373\n",
      "Iteration 33, loss = 0.88798730\n",
      "Iteration 34, loss = 0.87873773\n",
      "Iteration 35, loss = 0.86979518\n",
      "Iteration 36, loss = 0.86119824\n",
      "Iteration 37, loss = 0.85301655\n",
      "Iteration 38, loss = 0.84513816\n",
      "Iteration 39, loss = 0.83756595\n",
      "Iteration 40, loss = 0.83030391\n",
      "Iteration 41, loss = 0.82330530\n",
      "Iteration 42, loss = 0.81654807\n",
      "Iteration 43, loss = 0.80999720\n",
      "Iteration 44, loss = 0.80359172\n",
      "Iteration 45, loss = 0.79729105\n",
      "Iteration 46, loss = 0.79109317\n",
      "Iteration 47, loss = 0.78503159\n",
      "Iteration 48, loss = 0.77904732\n",
      "Iteration 49, loss = 0.77313854\n",
      "Iteration 50, loss = 0.76729171\n",
      "Iteration 51, loss = 0.76151905\n",
      "Iteration 52, loss = 0.75584695\n",
      "Iteration 53, loss = 0.75031049\n",
      "Iteration 54, loss = 0.74490187\n",
      "Iteration 55, loss = 0.73961615\n",
      "Iteration 56, loss = 0.73447619\n",
      "Iteration 57, loss = 0.72947140\n",
      "Iteration 58, loss = 0.72460764\n",
      "Iteration 59, loss = 0.71988514\n",
      "Iteration 60, loss = 0.71529304\n",
      "Iteration 61, loss = 0.71082389\n",
      "Iteration 62, loss = 0.70648466\n",
      "Iteration 63, loss = 0.70223810\n",
      "Iteration 64, loss = 0.69809809\n",
      "Iteration 65, loss = 0.69404592\n",
      "Iteration 66, loss = 0.69008470\n",
      "Iteration 67, loss = 0.68621105\n",
      "Iteration 68, loss = 0.68241794\n",
      "Iteration 69, loss = 0.67870252\n",
      "Iteration 70, loss = 0.67506124\n",
      "Iteration 71, loss = 0.67149221\n",
      "Iteration 72, loss = 0.66799380\n",
      "Iteration 73, loss = 0.66456247\n",
      "Iteration 74, loss = 0.66119579\n",
      "Iteration 75, loss = 0.65789286\n",
      "Iteration 76, loss = 0.65465153\n",
      "Iteration 77, loss = 0.65146547\n",
      "Iteration 78, loss = 0.64833382\n",
      "Iteration 79, loss = 0.64525788\n",
      "Iteration 80, loss = 0.64223721\n",
      "Iteration 81, loss = 0.63926853\n",
      "Iteration 82, loss = 0.63635327\n",
      "Iteration 83, loss = 0.63348978\n",
      "Iteration 84, loss = 0.63067609\n",
      "Iteration 85, loss = 0.62790389\n",
      "Iteration 86, loss = 0.62517316\n",
      "Iteration 87, loss = 0.62248592\n",
      "Iteration 88, loss = 0.61984330\n",
      "Iteration 89, loss = 0.61724461\n",
      "Iteration 90, loss = 0.61468565\n",
      "Iteration 91, loss = 0.61216694\n",
      "Iteration 92, loss = 0.60968813\n",
      "Iteration 93, loss = 0.60724706\n",
      "Iteration 94, loss = 0.60484221\n",
      "Iteration 95, loss = 0.60247533\n",
      "Iteration 96, loss = 0.60014616\n",
      "Iteration 97, loss = 0.59785337\n",
      "Iteration 98, loss = 0.59559398\n",
      "Iteration 99, loss = 0.59336685\n",
      "Iteration 100, loss = 0.59117249\n",
      "Iteration 101, loss = 0.58901143\n",
      "Iteration 102, loss = 0.58688267\n",
      "Iteration 103, loss = 0.58478515\n",
      "Iteration 104, loss = 0.58271892\n",
      "Iteration 105, loss = 0.58068307\n",
      "Iteration 106, loss = 0.57867674\n",
      "Iteration 107, loss = 0.57669889\n",
      "Iteration 108, loss = 0.57474933\n",
      "Iteration 109, loss = 0.57282735\n",
      "Iteration 110, loss = 0.57093234\n",
      "Iteration 111, loss = 0.56906382\n",
      "Iteration 112, loss = 0.56722118\n",
      "Iteration 113, loss = 0.56540460\n",
      "Iteration 114, loss = 0.56361308\n",
      "Iteration 115, loss = 0.56184300\n",
      "Iteration 116, loss = 0.56009632\n",
      "Iteration 117, loss = 0.55837262\n",
      "Iteration 118, loss = 0.55667132\n",
      "Iteration 119, loss = 0.55499194\n",
      "Iteration 120, loss = 0.55333433\n",
      "Iteration 121, loss = 0.55169762\n",
      "Iteration 122, loss = 0.55008136\n",
      "Iteration 123, loss = 0.54848509\n",
      "Iteration 124, loss = 0.54690849\n",
      "Iteration 125, loss = 0.54535120\n",
      "Iteration 126, loss = 0.54381277\n",
      "Iteration 127, loss = 0.54229272\n",
      "Iteration 128, loss = 0.54079061\n",
      "Iteration 129, loss = 0.53930615\n",
      "Iteration 130, loss = 0.53783882\n",
      "Iteration 131, loss = 0.53638820\n",
      "Iteration 132, loss = 0.53495422\n",
      "Iteration 133, loss = 0.53353630\n",
      "Iteration 134, loss = 0.53213408\n",
      "Iteration 135, loss = 0.53074761\n",
      "Iteration 136, loss = 0.52937624\n",
      "Iteration 137, loss = 0.52801916\n",
      "Iteration 138, loss = 0.52667577\n",
      "Iteration 139, loss = 0.52534642\n",
      "Iteration 140, loss = 0.52403078\n",
      "Iteration 141, loss = 0.52272857\n",
      "Iteration 142, loss = 0.52143953\n",
      "Iteration 143, loss = 0.52016348\n",
      "Iteration 144, loss = 0.51890166\n",
      "Iteration 145, loss = 0.51765224\n",
      "Iteration 146, loss = 0.51641499\n",
      "Iteration 147, loss = 0.51518872\n",
      "Iteration 148, loss = 0.51397404\n",
      "Iteration 149, loss = 0.51277050\n",
      "Iteration 150, loss = 0.51157827\n",
      "Iteration 151, loss = 0.51039709\n",
      "Iteration 152, loss = 0.50922634\n",
      "Iteration 153, loss = 0.50806526\n",
      "Iteration 154, loss = 0.50691457\n",
      "Iteration 155, loss = 0.50577417\n",
      "Iteration 156, loss = 0.50464377\n",
      "Iteration 157, loss = 0.50352313\n",
      "Iteration 158, loss = 0.50241137\n",
      "Iteration 159, loss = 0.50130927\n",
      "Iteration 160, loss = 0.50021724\n",
      "Iteration 161, loss = 0.49913401\n",
      "Iteration 162, loss = 0.49805953\n",
      "Iteration 163, loss = 0.49699393\n",
      "Iteration 164, loss = 0.49593585\n",
      "Iteration 165, loss = 0.49488371\n",
      "Iteration 166, loss = 0.49383953\n",
      "Iteration 167, loss = 0.49280336\n",
      "Iteration 168, loss = 0.49177540\n",
      "Iteration 169, loss = 0.49075656\n",
      "Iteration 170, loss = 0.48974603\n",
      "Iteration 171, loss = 0.48874162\n",
      "Iteration 172, loss = 0.48774254\n",
      "Iteration 173, loss = 0.48675049\n",
      "Iteration 174, loss = 0.48576605\n",
      "Iteration 175, loss = 0.48478763\n",
      "Iteration 176, loss = 0.48381544\n",
      "Iteration 177, loss = 0.48284788\n",
      "Iteration 178, loss = 0.48188203\n",
      "Iteration 179, loss = 0.48092327\n",
      "Iteration 180, loss = 0.47997032\n",
      "Iteration 181, loss = 0.47902210\n",
      "Iteration 182, loss = 0.47807762\n",
      "Iteration 183, loss = 0.47713583\n",
      "Iteration 184, loss = 0.47619737\n",
      "Iteration 185, loss = 0.47526141\n",
      "Iteration 186, loss = 0.47432547\n",
      "Iteration 187, loss = 0.47339066\n",
      "Iteration 188, loss = 0.47245931\n",
      "Iteration 189, loss = 0.47153017\n",
      "Iteration 190, loss = 0.47061191\n",
      "Iteration 191, loss = 0.46969445\n",
      "Iteration 192, loss = 0.46878099\n",
      "Iteration 193, loss = 0.46787769\n",
      "Iteration 194, loss = 0.46697875\n",
      "Iteration 195, loss = 0.46608788\n",
      "Iteration 196, loss = 0.46520866\n",
      "Iteration 197, loss = 0.46433977\n",
      "Iteration 198, loss = 0.46347709\n",
      "Iteration 199, loss = 0.46262525\n",
      "Iteration 200, loss = 0.46178176\n",
      "Iteration 201, loss = 0.46095206\n",
      "Iteration 202, loss = 0.46013137\n",
      "Iteration 203, loss = 0.45931913\n",
      "Iteration 204, loss = 0.45851177\n",
      "Iteration 205, loss = 0.45770945\n",
      "Iteration 206, loss = 0.45691469\n",
      "Iteration 207, loss = 0.45612499\n",
      "Iteration 208, loss = 0.45534085\n",
      "Iteration 209, loss = 0.45456073\n",
      "Iteration 210, loss = 0.45378459\n",
      "Iteration 211, loss = 0.45301237\n",
      "Iteration 212, loss = 0.45224350\n",
      "Iteration 213, loss = 0.45147904\n",
      "Iteration 214, loss = 0.45071817\n",
      "Iteration 215, loss = 0.44996138\n",
      "Iteration 216, loss = 0.44920805\n",
      "Iteration 217, loss = 0.44845803\n",
      "Iteration 218, loss = 0.44771157\n",
      "Iteration 219, loss = 0.44696823\n",
      "Iteration 220, loss = 0.44622809\n",
      "Iteration 221, loss = 0.44548968\n",
      "Iteration 222, loss = 0.44475493\n",
      "Iteration 223, loss = 0.44402329\n",
      "Iteration 224, loss = 0.44329485\n",
      "Iteration 225, loss = 0.44256955\n",
      "Iteration 226, loss = 0.44184738\n",
      "Iteration 227, loss = 0.44112827\n",
      "Iteration 228, loss = 0.44041196\n",
      "Iteration 229, loss = 0.43969868\n",
      "Iteration 230, loss = 0.43898843\n",
      "Iteration 231, loss = 0.43828117\n",
      "Iteration 232, loss = 0.43757752\n",
      "Iteration 233, loss = 0.43687530\n",
      "Iteration 234, loss = 0.43617097\n",
      "Iteration 235, loss = 0.43546953\n",
      "Iteration 236, loss = 0.43476694\n",
      "Iteration 237, loss = 0.43406636\n",
      "Iteration 238, loss = 0.43336783\n",
      "Iteration 239, loss = 0.43267142\n",
      "Iteration 240, loss = 0.43197628\n",
      "Iteration 241, loss = 0.43128200\n",
      "Iteration 242, loss = 0.43058952\n",
      "Iteration 243, loss = 0.42989897\n",
      "Iteration 244, loss = 0.42921067\n",
      "Iteration 245, loss = 0.42852452\n",
      "Iteration 246, loss = 0.42783675\n",
      "Iteration 247, loss = 0.42714823\n",
      "Iteration 248, loss = 0.42645895\n",
      "Iteration 249, loss = 0.42576471\n",
      "Iteration 250, loss = 0.42506508\n",
      "Iteration 251, loss = 0.42436104\n",
      "Iteration 252, loss = 0.42364829\n",
      "Iteration 253, loss = 0.42292781\n",
      "Iteration 254, loss = 0.42220226\n",
      "Iteration 255, loss = 0.42146031\n",
      "Iteration 256, loss = 0.42069426\n",
      "Iteration 257, loss = 0.41991332\n",
      "Iteration 258, loss = 0.41911859\n",
      "Iteration 259, loss = 0.41830568\n",
      "Iteration 260, loss = 0.41747449\n",
      "Iteration 261, loss = 0.41661485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 262, loss = 0.41574421\n",
      "Iteration 263, loss = 0.41487229\n",
      "Iteration 264, loss = 0.41401236\n",
      "Iteration 265, loss = 0.41316504\n",
      "Iteration 266, loss = 0.41229745\n",
      "Iteration 267, loss = 0.41144295\n",
      "Iteration 268, loss = 0.41062536\n",
      "Iteration 269, loss = 0.40984575\n",
      "Iteration 270, loss = 0.40910055\n",
      "Iteration 271, loss = 0.40836580\n",
      "Iteration 272, loss = 0.40765527\n",
      "Iteration 273, loss = 0.40697637\n",
      "Iteration 274, loss = 0.40630910\n",
      "Iteration 275, loss = 0.40566002\n",
      "Iteration 276, loss = 0.40501993\n",
      "Iteration 277, loss = 0.40439097\n",
      "Iteration 278, loss = 0.40376333\n",
      "Iteration 279, loss = 0.40314390\n",
      "Iteration 280, loss = 0.40253692\n",
      "Iteration 281, loss = 0.40193508\n",
      "Iteration 282, loss = 0.40133911\n",
      "Iteration 283, loss = 0.40074597\n",
      "Iteration 284, loss = 0.40015652\n",
      "Iteration 285, loss = 0.39957013\n",
      "Iteration 286, loss = 0.39898943\n",
      "Iteration 287, loss = 0.39841452\n",
      "Iteration 288, loss = 0.39784224\n",
      "Iteration 289, loss = 0.39727521\n",
      "Iteration 290, loss = 0.39671015\n",
      "Iteration 291, loss = 0.39614743\n",
      "Iteration 292, loss = 0.39558709\n",
      "Iteration 293, loss = 0.39502908\n",
      "Iteration 294, loss = 0.39447304\n",
      "Iteration 295, loss = 0.39391890\n",
      "Iteration 296, loss = 0.39336653\n",
      "Iteration 297, loss = 0.39281600\n",
      "Iteration 298, loss = 0.39226718\n",
      "Iteration 299, loss = 0.39171988\n",
      "Iteration 300, loss = 0.39117418\n",
      "Iteration 301, loss = 0.39063056\n",
      "Iteration 302, loss = 0.39008795\n",
      "Iteration 303, loss = 0.38954681\n",
      "Iteration 304, loss = 0.38900740\n",
      "Iteration 305, loss = 0.38846933\n",
      "Iteration 306, loss = 0.38793284\n",
      "Iteration 307, loss = 0.38739791\n",
      "Iteration 308, loss = 0.38686481\n",
      "Iteration 309, loss = 0.38633252\n",
      "Iteration 310, loss = 0.38580201\n",
      "Iteration 311, loss = 0.38527286\n",
      "Iteration 312, loss = 0.38474506\n",
      "Iteration 313, loss = 0.38421870\n",
      "Iteration 314, loss = 0.38369375\n",
      "Iteration 315, loss = 0.38317020\n",
      "Iteration 316, loss = 0.38264810\n",
      "Iteration 317, loss = 0.38212766\n",
      "Iteration 318, loss = 0.38160881\n",
      "Iteration 319, loss = 0.38109112\n",
      "Iteration 320, loss = 0.38057469\n",
      "Iteration 321, loss = 0.38005952\n",
      "Iteration 322, loss = 0.37954578\n",
      "Iteration 323, loss = 0.37903340\n",
      "Iteration 324, loss = 0.37852238\n",
      "Iteration 325, loss = 0.37801260\n",
      "Iteration 326, loss = 0.37750413\n",
      "Iteration 327, loss = 0.37699698\n",
      "Iteration 328, loss = 0.37649076\n",
      "Iteration 329, loss = 0.37598578\n",
      "Iteration 330, loss = 0.37548200\n",
      "Iteration 331, loss = 0.37497945\n",
      "Iteration 332, loss = 0.37447823\n",
      "Iteration 333, loss = 0.37397816\n",
      "Iteration 334, loss = 0.37347934\n",
      "Iteration 335, loss = 0.37298189\n",
      "Iteration 336, loss = 0.37248558\n",
      "Iteration 337, loss = 0.37199056\n",
      "Iteration 338, loss = 0.37149676\n",
      "Iteration 339, loss = 0.37100438\n",
      "Iteration 340, loss = 0.37051327\n",
      "Iteration 341, loss = 0.37002340\n",
      "Iteration 342, loss = 0.36953493\n",
      "Iteration 343, loss = 0.36904794\n",
      "Iteration 344, loss = 0.36856212\n",
      "Iteration 345, loss = 0.36807752\n",
      "Iteration 346, loss = 0.36759397\n",
      "Iteration 347, loss = 0.36711170\n",
      "Iteration 348, loss = 0.36663092\n",
      "Iteration 349, loss = 0.36615112\n",
      "Iteration 350, loss = 0.36567253\n",
      "Iteration 351, loss = 0.36519502\n",
      "Iteration 352, loss = 0.36471864\n",
      "Iteration 353, loss = 0.36424350\n",
      "Iteration 354, loss = 0.36376944\n",
      "Iteration 355, loss = 0.36329655\n",
      "Iteration 356, loss = 0.36282480\n",
      "Iteration 357, loss = 0.36235405\n",
      "Iteration 358, loss = 0.36188437\n",
      "Iteration 359, loss = 0.36141611\n",
      "Iteration 360, loss = 0.36094893\n",
      "Iteration 361, loss = 0.36048275\n",
      "Iteration 362, loss = 0.36001765\n",
      "Iteration 363, loss = 0.35955354\n",
      "Iteration 364, loss = 0.35909060\n",
      "Iteration 365, loss = 0.35862874\n",
      "Iteration 366, loss = 0.35816798\n",
      "Iteration 367, loss = 0.35770826\n",
      "Iteration 368, loss = 0.35724975\n",
      "Iteration 369, loss = 0.35679231\n",
      "Iteration 370, loss = 0.35633587\n",
      "Iteration 371, loss = 0.35588037\n",
      "Iteration 372, loss = 0.35542591\n",
      "Iteration 373, loss = 0.35497254\n",
      "Iteration 374, loss = 0.35452005\n",
      "Iteration 375, loss = 0.35406862\n",
      "Iteration 376, loss = 0.35361830\n",
      "Iteration 377, loss = 0.35316904\n",
      "Iteration 378, loss = 0.35272089\n",
      "Iteration 379, loss = 0.35227364\n",
      "Iteration 380, loss = 0.35182742\n",
      "Iteration 381, loss = 0.35138218\n",
      "Iteration 382, loss = 0.35093789\n",
      "Iteration 383, loss = 0.35049436\n",
      "Iteration 384, loss = 0.35005193\n",
      "Iteration 385, loss = 0.34961024\n",
      "Iteration 386, loss = 0.34916963\n",
      "Iteration 387, loss = 0.34872993\n",
      "Iteration 388, loss = 0.34829119\n",
      "Iteration 389, loss = 0.34785353\n",
      "Iteration 390, loss = 0.34741683\n",
      "Iteration 391, loss = 0.34698102\n",
      "Iteration 392, loss = 0.34654615\n",
      "Iteration 393, loss = 0.34611228\n",
      "Iteration 394, loss = 0.34567924\n",
      "Iteration 395, loss = 0.34524715\n",
      "Iteration 396, loss = 0.34481601\n",
      "Iteration 397, loss = 0.34438574\n",
      "Iteration 398, loss = 0.34395629\n",
      "Iteration 399, loss = 0.34352791\n",
      "Iteration 400, loss = 0.34310027\n",
      "Iteration 401, loss = 0.34267386\n",
      "Iteration 402, loss = 0.34224824\n",
      "Iteration 403, loss = 0.34182372\n",
      "Iteration 404, loss = 0.34140019\n",
      "Iteration 405, loss = 0.34097769\n",
      "Iteration 406, loss = 0.34055591\n",
      "Iteration 407, loss = 0.34013513\n",
      "Iteration 408, loss = 0.33971521\n",
      "Iteration 409, loss = 0.33929609\n",
      "Iteration 410, loss = 0.33887792\n",
      "Iteration 411, loss = 0.33846063\n",
      "Iteration 412, loss = 0.33804426\n",
      "Iteration 413, loss = 0.33762879\n",
      "Iteration 414, loss = 0.33721420\n",
      "Iteration 415, loss = 0.33680041\n",
      "Iteration 416, loss = 0.33638759\n",
      "Iteration 417, loss = 0.33597546\n",
      "Iteration 418, loss = 0.33556438\n",
      "Iteration 419, loss = 0.33515425\n",
      "Iteration 420, loss = 0.33474505\n",
      "Iteration 421, loss = 0.33433662\n",
      "Iteration 422, loss = 0.33392914\n",
      "Iteration 423, loss = 0.33352238\n",
      "Iteration 424, loss = 0.33311650\n",
      "Iteration 425, loss = 0.33271143\n",
      "Iteration 426, loss = 0.33230711\n",
      "Iteration 427, loss = 0.33190369\n",
      "Iteration 428, loss = 0.33150108\n",
      "Iteration 429, loss = 0.33109929\n",
      "Iteration 430, loss = 0.33069827\n",
      "Iteration 431, loss = 0.33029809\n",
      "Iteration 432, loss = 0.32989868\n",
      "Iteration 433, loss = 0.32950011\n",
      "Iteration 434, loss = 0.32910233\n",
      "Iteration 435, loss = 0.32870529\n",
      "Iteration 436, loss = 0.32830921\n",
      "Iteration 437, loss = 0.32791376\n",
      "Iteration 438, loss = 0.32751925\n",
      "Iteration 439, loss = 0.32712553\n",
      "Iteration 440, loss = 0.32673262\n",
      "Iteration 441, loss = 0.32634040\n",
      "Iteration 442, loss = 0.32594915\n",
      "Iteration 443, loss = 0.32555845\n",
      "Iteration 444, loss = 0.32516874\n",
      "Iteration 445, loss = 0.32477975\n",
      "Iteration 446, loss = 0.32439150\n",
      "Iteration 447, loss = 0.32400409\n",
      "Iteration 448, loss = 0.32361752\n",
      "Iteration 449, loss = 0.32323164\n",
      "Iteration 450, loss = 0.32284659\n",
      "Iteration 451, loss = 0.32246232\n",
      "Iteration 452, loss = 0.32207881\n",
      "Iteration 453, loss = 0.32169610\n",
      "Iteration 454, loss = 0.32131415\n",
      "Iteration 455, loss = 0.32093292\n",
      "Iteration 456, loss = 0.32055257\n",
      "Iteration 457, loss = 0.32017290\n",
      "Iteration 458, loss = 0.31979399\n",
      "Iteration 459, loss = 0.31941593\n",
      "Iteration 460, loss = 0.31903851\n",
      "Iteration 461, loss = 0.31866197\n",
      "Iteration 462, loss = 0.31828608\n",
      "Iteration 463, loss = 0.31791103\n",
      "Iteration 464, loss = 0.31753670\n",
      "Iteration 465, loss = 0.31716311\n",
      "Iteration 466, loss = 0.31679028\n",
      "Iteration 467, loss = 0.31641833\n",
      "Iteration 468, loss = 0.31604713\n",
      "Iteration 469, loss = 0.31567662\n",
      "Iteration 470, loss = 0.31530695\n",
      "Iteration 471, loss = 0.31493791\n",
      "Iteration 472, loss = 0.31456972\n",
      "Iteration 473, loss = 0.31420216\n",
      "Iteration 474, loss = 0.31383542\n",
      "Iteration 475, loss = 0.31346942\n",
      "Iteration 476, loss = 0.31310405\n",
      "Iteration 477, loss = 0.31273939\n",
      "Iteration 478, loss = 0.31237567\n",
      "Iteration 479, loss = 0.31201243\n",
      "Iteration 480, loss = 0.31164999\n",
      "Iteration 481, loss = 0.31128826\n",
      "Iteration 482, loss = 0.31092729\n",
      "Iteration 483, loss = 0.31056714\n",
      "Iteration 484, loss = 0.31020782\n",
      "Iteration 485, loss = 0.30984931\n",
      "Iteration 486, loss = 0.30949144\n",
      "Iteration 487, loss = 0.30913432\n",
      "Iteration 488, loss = 0.30877792\n",
      "Iteration 489, loss = 0.30842221\n",
      "Iteration 490, loss = 0.30806725\n",
      "Iteration 491, loss = 0.30771295\n",
      "Iteration 492, loss = 0.30735939\n",
      "Iteration 493, loss = 0.30700655\n",
      "Iteration 494, loss = 0.30665444\n",
      "Iteration 495, loss = 0.30630302\n",
      "Iteration 496, loss = 0.30595234\n",
      "Iteration 497, loss = 0.30560235\n",
      "Iteration 498, loss = 0.30525306\n",
      "Iteration 499, loss = 0.30490445\n",
      "Iteration 500, loss = 0.30455659\n",
      "Iteration 501, loss = 0.30420931\n",
      "Iteration 502, loss = 0.30386286\n",
      "Iteration 503, loss = 0.30351703\n",
      "Iteration 504, loss = 0.30317189\n",
      "Iteration 505, loss = 0.30282740\n",
      "Iteration 506, loss = 0.30248364\n",
      "Iteration 507, loss = 0.30214053\n",
      "Iteration 508, loss = 0.30179814\n",
      "Iteration 509, loss = 0.30145638\n",
      "Iteration 510, loss = 0.30111533\n",
      "Iteration 511, loss = 0.30077496\n",
      "Iteration 512, loss = 0.30043517\n",
      "Iteration 513, loss = 0.30009617\n",
      "Iteration 514, loss = 0.29975775\n",
      "Iteration 515, loss = 0.29942010\n",
      "Iteration 516, loss = 0.29908302\n",
      "Iteration 517, loss = 0.29874666\n",
      "Iteration 518, loss = 0.29841094\n",
      "Iteration 519, loss = 0.29807588\n",
      "Iteration 520, loss = 0.29774153\n",
      "Iteration 521, loss = 0.29740773\n",
      "Iteration 522, loss = 0.29707472\n",
      "Iteration 523, loss = 0.29674228\n",
      "Iteration 524, loss = 0.29641048\n",
      "Iteration 525, loss = 0.29607935\n",
      "Iteration 526, loss = 0.29574887\n",
      "Iteration 527, loss = 0.29541909\n",
      "Iteration 528, loss = 0.29508990\n",
      "Iteration 529, loss = 0.29476142\n",
      "Iteration 530, loss = 0.29443353\n",
      "Iteration 531, loss = 0.29410634\n",
      "Iteration 532, loss = 0.29377979\n",
      "Iteration 533, loss = 0.29345381\n",
      "Iteration 534, loss = 0.29312856\n",
      "Iteration 535, loss = 0.29280386\n",
      "Iteration 536, loss = 0.29247990\n",
      "Iteration 537, loss = 0.29215650\n",
      "Iteration 538, loss = 0.29183379\n",
      "Iteration 539, loss = 0.29151175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 540, loss = 0.29119026\n",
      "Iteration 541, loss = 0.29086951\n",
      "Iteration 542, loss = 0.29054927\n",
      "Iteration 543, loss = 0.29022994\n",
      "Iteration 544, loss = 0.28991113\n",
      "Iteration 545, loss = 0.28959301\n",
      "Iteration 546, loss = 0.28927558\n",
      "Iteration 547, loss = 0.28895868\n",
      "Iteration 548, loss = 0.28864252\n",
      "Iteration 549, loss = 0.28832688\n",
      "Iteration 550, loss = 0.28801195\n",
      "Iteration 551, loss = 0.28769757\n",
      "Iteration 552, loss = 0.28738397\n",
      "Iteration 553, loss = 0.28707097\n",
      "Iteration 554, loss = 0.28675872\n",
      "Iteration 555, loss = 0.28644703\n",
      "Iteration 556, loss = 0.28613596\n",
      "Iteration 557, loss = 0.28582562\n",
      "Iteration 558, loss = 0.28551582\n",
      "Iteration 559, loss = 0.28520663\n",
      "Iteration 560, loss = 0.28489819\n",
      "Iteration 561, loss = 0.28459031\n",
      "Iteration 562, loss = 0.28428308\n",
      "Iteration 563, loss = 0.28397648\n",
      "Iteration 564, loss = 0.28367040\n",
      "Iteration 565, loss = 0.28336504\n",
      "Iteration 566, loss = 0.28306018\n",
      "Iteration 567, loss = 0.28275602\n",
      "Iteration 568, loss = 0.28245236\n",
      "Iteration 569, loss = 0.28214942\n",
      "Iteration 570, loss = 0.28184697\n",
      "Iteration 571, loss = 0.28154521\n",
      "Iteration 572, loss = 0.28124401\n",
      "Iteration 573, loss = 0.28094335\n",
      "Iteration 574, loss = 0.28064336\n",
      "Iteration 575, loss = 0.28034391\n",
      "Iteration 576, loss = 0.28004509\n",
      "Iteration 577, loss = 0.27974683\n",
      "Iteration 578, loss = 0.27944916\n",
      "Iteration 579, loss = 0.27915211\n",
      "Iteration 580, loss = 0.27885557\n",
      "Iteration 581, loss = 0.27855976\n",
      "Iteration 582, loss = 0.27826437\n",
      "Iteration 583, loss = 0.27796972\n",
      "Iteration 584, loss = 0.27767553\n",
      "Iteration 585, loss = 0.27738197\n",
      "Iteration 586, loss = 0.27708898\n",
      "Iteration 587, loss = 0.27679657\n",
      "Iteration 588, loss = 0.27650471\n",
      "Iteration 589, loss = 0.27621349\n",
      "Iteration 590, loss = 0.27592278\n",
      "Iteration 591, loss = 0.27563263\n",
      "Iteration 592, loss = 0.27534309\n",
      "Iteration 593, loss = 0.27505413\n",
      "Iteration 594, loss = 0.27476570\n",
      "Iteration 595, loss = 0.27447798\n",
      "Iteration 596, loss = 0.27419068\n",
      "Iteration 597, loss = 0.27390402\n",
      "Iteration 598, loss = 0.27361786\n",
      "Iteration 599, loss = 0.27333239\n",
      "Iteration 600, loss = 0.27304734\n",
      "Iteration 601, loss = 0.27276301\n",
      "Iteration 602, loss = 0.27247913\n",
      "Iteration 603, loss = 0.27219591\n",
      "Iteration 604, loss = 0.27191337\n",
      "Iteration 605, loss = 0.27163135\n",
      "Iteration 606, loss = 0.27134994\n",
      "Iteration 607, loss = 0.27106905\n",
      "Iteration 608, loss = 0.27078882\n",
      "Iteration 609, loss = 0.27050903\n",
      "Iteration 610, loss = 0.27022998\n",
      "Iteration 611, loss = 0.26995142\n",
      "Iteration 612, loss = 0.26967349\n",
      "Iteration 613, loss = 0.26939608\n",
      "Iteration 614, loss = 0.26911925\n",
      "Iteration 615, loss = 0.26884298\n",
      "Iteration 616, loss = 0.26856725\n",
      "Iteration 617, loss = 0.26829211\n",
      "Iteration 618, loss = 0.26801756\n",
      "Iteration 619, loss = 0.26774370\n",
      "Iteration 620, loss = 0.26747031\n",
      "Iteration 621, loss = 0.26719756\n",
      "Iteration 622, loss = 0.26692527\n",
      "Iteration 623, loss = 0.26665363\n",
      "Iteration 624, loss = 0.26638246\n",
      "Iteration 625, loss = 0.26611186\n",
      "Iteration 626, loss = 0.26584184\n",
      "Iteration 627, loss = 0.26557228\n",
      "Iteration 628, loss = 0.26530341\n",
      "Iteration 629, loss = 0.26503488\n",
      "Iteration 630, loss = 0.26476702\n",
      "Iteration 631, loss = 0.26449958\n",
      "Iteration 632, loss = 0.26423283\n",
      "Iteration 633, loss = 0.26396655\n",
      "Iteration 634, loss = 0.26370082\n",
      "Iteration 635, loss = 0.26343559\n",
      "Iteration 636, loss = 0.26317088\n",
      "Iteration 637, loss = 0.26290669\n",
      "Iteration 638, loss = 0.26264309\n",
      "Iteration 639, loss = 0.26237994\n",
      "Iteration 640, loss = 0.26211736\n",
      "Iteration 641, loss = 0.26185532\n",
      "Iteration 642, loss = 0.26159387\n",
      "Iteration 643, loss = 0.26133298\n",
      "Iteration 644, loss = 0.26107257\n",
      "Iteration 645, loss = 0.26081276\n",
      "Iteration 646, loss = 0.26055348\n",
      "Iteration 647, loss = 0.26029478\n",
      "Iteration 648, loss = 0.26003658\n",
      "Iteration 649, loss = 0.25977893\n",
      "Iteration 650, loss = 0.25952176\n",
      "Iteration 651, loss = 0.25926516\n",
      "Iteration 652, loss = 0.25900902\n",
      "Iteration 653, loss = 0.25875349\n",
      "Iteration 654, loss = 0.25849834\n",
      "Iteration 655, loss = 0.25824382\n",
      "Iteration 656, loss = 0.25798973\n",
      "Iteration 657, loss = 0.25773616\n",
      "Iteration 658, loss = 0.25748307\n",
      "Iteration 659, loss = 0.25723054\n",
      "Iteration 660, loss = 0.25697842\n",
      "Iteration 661, loss = 0.25672690\n",
      "Iteration 662, loss = 0.25647579\n",
      "Iteration 663, loss = 0.25622521\n",
      "Iteration 664, loss = 0.25597517\n",
      "Iteration 665, loss = 0.25572564\n",
      "Iteration 666, loss = 0.25547653\n",
      "Iteration 667, loss = 0.25522804\n",
      "Iteration 668, loss = 0.25497991\n",
      "Iteration 669, loss = 0.25473236\n",
      "Iteration 670, loss = 0.25448527\n",
      "Iteration 671, loss = 0.25423871\n",
      "Iteration 672, loss = 0.25399261\n",
      "Iteration 673, loss = 0.25374705\n",
      "Iteration 674, loss = 0.25350191\n",
      "Iteration 675, loss = 0.25325735\n",
      "Iteration 676, loss = 0.25301321\n",
      "Iteration 677, loss = 0.25276954\n",
      "Iteration 678, loss = 0.25252643\n",
      "Iteration 679, loss = 0.25228375\n",
      "Iteration 680, loss = 0.25204158\n",
      "Iteration 681, loss = 0.25179997\n",
      "Iteration 682, loss = 0.25155881\n",
      "Iteration 683, loss = 0.25131812\n",
      "Iteration 684, loss = 0.25107795\n",
      "Iteration 685, loss = 0.25083818\n",
      "Iteration 686, loss = 0.25059892\n",
      "Iteration 687, loss = 0.25036011\n",
      "Iteration 688, loss = 0.25012184\n",
      "Iteration 689, loss = 0.24988396\n",
      "Iteration 690, loss = 0.24964658\n",
      "Iteration 691, loss = 0.24940971\n",
      "Iteration 692, loss = 0.24917325\n",
      "Iteration 693, loss = 0.24893725\n",
      "Iteration 694, loss = 0.24870179\n",
      "Iteration 695, loss = 0.24846673\n",
      "Iteration 696, loss = 0.24823213\n",
      "Iteration 697, loss = 0.24799803\n",
      "Iteration 698, loss = 0.24776440\n",
      "Iteration 699, loss = 0.24753122\n",
      "Iteration 700, loss = 0.24729848\n",
      "Iteration 701, loss = 0.24706631\n",
      "Iteration 702, loss = 0.24683444\n",
      "Iteration 703, loss = 0.24660317\n",
      "Iteration 704, loss = 0.24637229\n",
      "Iteration 705, loss = 0.24614189\n",
      "Iteration 706, loss = 0.24591191\n",
      "Iteration 707, loss = 0.24568242\n",
      "Iteration 708, loss = 0.24545340\n",
      "Iteration 709, loss = 0.24522483\n",
      "Iteration 710, loss = 0.24499668\n",
      "Iteration 711, loss = 0.24476907\n",
      "Iteration 712, loss = 0.24454186\n",
      "Iteration 713, loss = 0.24431509\n",
      "Iteration 714, loss = 0.24408879\n",
      "Iteration 715, loss = 0.24386291\n",
      "Iteration 716, loss = 0.24363751\n",
      "Iteration 717, loss = 0.24341250\n",
      "Iteration 718, loss = 0.24318800\n",
      "Iteration 719, loss = 0.24296392\n",
      "Iteration 720, loss = 0.24274028\n",
      "Iteration 721, loss = 0.24251710\n",
      "Iteration 722, loss = 0.24229433\n",
      "Iteration 723, loss = 0.24207210\n",
      "Iteration 724, loss = 0.24185024\n",
      "Iteration 725, loss = 0.24162887\n",
      "Iteration 726, loss = 0.24140789\n",
      "Iteration 727, loss = 0.24118742\n",
      "Iteration 728, loss = 0.24096732\n",
      "Iteration 729, loss = 0.24074765\n",
      "Iteration 730, loss = 0.24052850\n",
      "Iteration 731, loss = 0.24030973\n",
      "Iteration 732, loss = 0.24009146\n",
      "Iteration 733, loss = 0.23987355\n",
      "Iteration 734, loss = 0.23965612\n",
      "Iteration 735, loss = 0.23943907\n",
      "Iteration 736, loss = 0.23922231\n",
      "Iteration 737, loss = 0.23900595\n",
      "Iteration 738, loss = 0.23879000\n",
      "Iteration 739, loss = 0.23857455\n",
      "Iteration 740, loss = 0.23835942\n",
      "Iteration 741, loss = 0.23814476\n",
      "Iteration 742, loss = 0.23793047\n",
      "Iteration 743, loss = 0.23771665\n",
      "Iteration 744, loss = 0.23750316\n",
      "Iteration 745, loss = 0.23729015\n",
      "Iteration 746, loss = 0.23707752\n",
      "Iteration 747, loss = 0.23686531\n",
      "Iteration 748, loss = 0.23665360\n",
      "Iteration 749, loss = 0.23644222\n",
      "Iteration 750, loss = 0.23623128\n",
      "Iteration 751, loss = 0.23602074\n",
      "Iteration 752, loss = 0.23581068\n",
      "Iteration 753, loss = 0.23560096\n",
      "Iteration 754, loss = 0.23539167\n",
      "Iteration 755, loss = 0.23518283\n",
      "Iteration 756, loss = 0.23497439\n",
      "Iteration 757, loss = 0.23476634\n",
      "Iteration 758, loss = 0.23455872\n",
      "Iteration 759, loss = 0.23435149\n",
      "Iteration 760, loss = 0.23414463\n",
      "Iteration 761, loss = 0.23393820\n",
      "Iteration 762, loss = 0.23373219\n",
      "Iteration 763, loss = 0.23352652\n",
      "Iteration 764, loss = 0.23332133\n",
      "Iteration 765, loss = 0.23311650\n",
      "Iteration 766, loss = 0.23291210\n",
      "Iteration 767, loss = 0.23270810\n",
      "Iteration 768, loss = 0.23250447\n",
      "Iteration 769, loss = 0.23230130\n",
      "Iteration 770, loss = 0.23209841\n",
      "Iteration 771, loss = 0.23189593\n",
      "Iteration 772, loss = 0.23169386\n",
      "Iteration 773, loss = 0.23149209\n",
      "Iteration 774, loss = 0.23129074\n",
      "Iteration 775, loss = 0.23108976\n",
      "Iteration 776, loss = 0.23088918\n",
      "Iteration 777, loss = 0.23068902\n",
      "Iteration 778, loss = 0.23048923\n",
      "Iteration 779, loss = 0.23028983\n",
      "Iteration 780, loss = 0.23009079\n",
      "Iteration 781, loss = 0.22989215\n",
      "Iteration 782, loss = 0.22969390\n",
      "Iteration 783, loss = 0.22949604\n",
      "Iteration 784, loss = 0.22929859\n",
      "Iteration 785, loss = 0.22910150\n",
      "Iteration 786, loss = 0.22890481\n",
      "Iteration 787, loss = 0.22870849\n",
      "Iteration 788, loss = 0.22851256\n",
      "Iteration 789, loss = 0.22831701\n",
      "Iteration 790, loss = 0.22812181\n",
      "Iteration 791, loss = 0.22792701\n",
      "Iteration 792, loss = 0.22773258\n",
      "Iteration 793, loss = 0.22753853\n",
      "Iteration 794, loss = 0.22734484\n",
      "Iteration 795, loss = 0.22715153\n",
      "Iteration 796, loss = 0.22695855\n",
      "Iteration 797, loss = 0.22676599\n",
      "Iteration 798, loss = 0.22657377\n",
      "Iteration 799, loss = 0.22638193\n",
      "Iteration 800, loss = 0.22619041\n",
      "Iteration 801, loss = 0.22599933\n",
      "Iteration 802, loss = 0.22580857\n",
      "Iteration 803, loss = 0.22561820\n",
      "Iteration 804, loss = 0.22542824\n",
      "Iteration 805, loss = 0.22523862\n",
      "Iteration 806, loss = 0.22504932\n",
      "Iteration 807, loss = 0.22486045\n",
      "Iteration 808, loss = 0.22467187\n",
      "Iteration 809, loss = 0.22448372\n",
      "Iteration 810, loss = 0.22429588\n",
      "Iteration 811, loss = 0.22410834\n",
      "Iteration 812, loss = 0.22392121\n",
      "Iteration 813, loss = 0.22373434\n",
      "Iteration 814, loss = 0.22354779\n",
      "Iteration 815, loss = 0.22336161\n",
      "Iteration 816, loss = 0.22317577\n",
      "Iteration 817, loss = 0.22299041\n",
      "Iteration 818, loss = 0.22280523\n",
      "Iteration 819, loss = 0.22262059\n",
      "Iteration 820, loss = 0.22243619\n",
      "Iteration 821, loss = 0.22225216\n",
      "Iteration 822, loss = 0.22206850\n",
      "Iteration 823, loss = 0.22188513\n",
      "Iteration 824, loss = 0.22170208\n",
      "Iteration 825, loss = 0.22151944\n",
      "Iteration 826, loss = 0.22133708\n",
      "Iteration 827, loss = 0.22115515\n",
      "Iteration 828, loss = 0.22097353\n",
      "Iteration 829, loss = 0.22079225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 830, loss = 0.22061135\n",
      "Iteration 831, loss = 0.22043081\n",
      "Iteration 832, loss = 0.22025062\n",
      "Iteration 833, loss = 0.22007080\n",
      "Iteration 834, loss = 0.21989131\n",
      "Iteration 835, loss = 0.21971218\n",
      "Iteration 836, loss = 0.21953334\n",
      "Iteration 837, loss = 0.21935484\n",
      "Iteration 838, loss = 0.21917675\n",
      "Iteration 839, loss = 0.21899897\n",
      "Iteration 840, loss = 0.21882152\n",
      "Iteration 841, loss = 0.21864447\n",
      "Iteration 842, loss = 0.21846770\n",
      "Iteration 843, loss = 0.21829115\n",
      "Iteration 844, loss = 0.21811494\n",
      "Iteration 845, loss = 0.21793908\n",
      "Iteration 846, loss = 0.21776355\n",
      "Iteration 847, loss = 0.21758836\n",
      "Iteration 848, loss = 0.21741350\n",
      "Iteration 849, loss = 0.21723905\n",
      "Iteration 850, loss = 0.21706476\n",
      "Iteration 851, loss = 0.21689074\n",
      "Iteration 852, loss = 0.21671694\n",
      "Iteration 853, loss = 0.21654338\n",
      "Iteration 854, loss = 0.21637013\n",
      "Iteration 855, loss = 0.21619725\n",
      "Iteration 856, loss = 0.21602466\n",
      "Iteration 857, loss = 0.21585237\n",
      "Iteration 858, loss = 0.21568041\n",
      "Iteration 859, loss = 0.21550879\n",
      "Iteration 860, loss = 0.21533751\n",
      "Iteration 861, loss = 0.21516652\n",
      "Iteration 862, loss = 0.21499581\n",
      "Iteration 863, loss = 0.21482542\n",
      "Iteration 864, loss = 0.21465530\n",
      "Iteration 865, loss = 0.21448551\n",
      "Iteration 866, loss = 0.21431606\n",
      "Iteration 867, loss = 0.21414691\n",
      "Iteration 868, loss = 0.21397814\n",
      "Iteration 869, loss = 0.21380962\n",
      "Iteration 870, loss = 0.21364150\n",
      "Iteration 871, loss = 0.21347366\n",
      "Iteration 872, loss = 0.21330615\n",
      "Iteration 873, loss = 0.21313902\n",
      "Iteration 874, loss = 0.21297209\n",
      "Iteration 875, loss = 0.21280543\n",
      "Iteration 876, loss = 0.21263919\n",
      "Iteration 877, loss = 0.21247314\n",
      "Iteration 878, loss = 0.21230736\n",
      "Iteration 879, loss = 0.21214183\n",
      "Iteration 880, loss = 0.21197654\n",
      "Iteration 881, loss = 0.21181155\n",
      "Iteration 882, loss = 0.21164689\n",
      "Iteration 883, loss = 0.21148251\n",
      "Iteration 884, loss = 0.21131844\n",
      "Iteration 885, loss = 0.21115469\n",
      "Iteration 886, loss = 0.21099123\n",
      "Iteration 887, loss = 0.21082810\n",
      "Iteration 888, loss = 0.21066520\n",
      "Iteration 889, loss = 0.21050242\n",
      "Iteration 890, loss = 0.21033993\n",
      "Iteration 891, loss = 0.21017774\n",
      "Iteration 892, loss = 0.21001585\n",
      "Iteration 893, loss = 0.20985424\n",
      "Iteration 894, loss = 0.20969282\n",
      "Iteration 895, loss = 0.20953162\n",
      "Iteration 896, loss = 0.20937066\n",
      "Iteration 897, loss = 0.20920991\n",
      "Iteration 898, loss = 0.20904950\n",
      "Iteration 899, loss = 0.20888937\n",
      "Iteration 900, loss = 0.20872953\n",
      "Iteration 901, loss = 0.20857000\n",
      "Iteration 902, loss = 0.20841079\n",
      "Iteration 903, loss = 0.20825186\n",
      "Iteration 904, loss = 0.20809323\n",
      "Iteration 905, loss = 0.20793492\n",
      "Iteration 906, loss = 0.20777688\n",
      "Iteration 907, loss = 0.20761920\n",
      "Iteration 908, loss = 0.20746177\n",
      "Iteration 909, loss = 0.20730467\n",
      "Iteration 910, loss = 0.20714785\n",
      "Iteration 911, loss = 0.20699116\n",
      "Iteration 912, loss = 0.20683477\n",
      "Iteration 913, loss = 0.20667865\n",
      "Iteration 914, loss = 0.20652277\n",
      "Iteration 915, loss = 0.20636721\n",
      "Iteration 916, loss = 0.20621187\n",
      "Iteration 917, loss = 0.20605669\n",
      "Iteration 918, loss = 0.20590179\n",
      "Iteration 919, loss = 0.20574722\n",
      "Iteration 920, loss = 0.20559285\n",
      "Iteration 921, loss = 0.20543879\n",
      "Iteration 922, loss = 0.20528505\n",
      "Iteration 923, loss = 0.20513158\n",
      "Iteration 924, loss = 0.20497843\n",
      "Iteration 925, loss = 0.20482556\n",
      "Iteration 926, loss = 0.20467297\n",
      "Iteration 927, loss = 0.20452071\n",
      "Iteration 928, loss = 0.20436871\n",
      "Iteration 929, loss = 0.20421702\n",
      "Iteration 930, loss = 0.20406565\n",
      "Iteration 931, loss = 0.20391456\n",
      "Iteration 932, loss = 0.20376375\n",
      "Iteration 933, loss = 0.20361328\n",
      "Iteration 934, loss = 0.20346303\n",
      "Iteration 935, loss = 0.20331311\n",
      "Iteration 936, loss = 0.20316346\n",
      "Iteration 937, loss = 0.20301412\n",
      "Iteration 938, loss = 0.20286502\n",
      "Iteration 939, loss = 0.20271617\n",
      "Iteration 940, loss = 0.20256760\n",
      "Iteration 941, loss = 0.20241924\n",
      "Iteration 942, loss = 0.20227091\n",
      "Iteration 943, loss = 0.20212282\n",
      "Iteration 944, loss = 0.20197502\n",
      "Iteration 945, loss = 0.20182750\n",
      "Iteration 946, loss = 0.20168023\n",
      "Iteration 947, loss = 0.20153326\n",
      "Iteration 948, loss = 0.20138650\n",
      "Iteration 949, loss = 0.20123990\n",
      "Iteration 950, loss = 0.20109361\n",
      "Iteration 951, loss = 0.20094754\n",
      "Iteration 952, loss = 0.20080176\n",
      "Iteration 953, loss = 0.20065628\n",
      "Iteration 954, loss = 0.20051106\n",
      "Iteration 955, loss = 0.20036612\n",
      "Iteration 956, loss = 0.20022150\n",
      "Iteration 957, loss = 0.20007709\n",
      "Iteration 958, loss = 0.19993302\n",
      "Iteration 959, loss = 0.19978924\n",
      "Iteration 960, loss = 0.19964571\n",
      "Iteration 961, loss = 0.19950245\n",
      "Iteration 962, loss = 0.19935946\n",
      "Iteration 963, loss = 0.19921665\n",
      "Iteration 964, loss = 0.19907410\n",
      "Iteration 965, loss = 0.19893183\n",
      "Iteration 966, loss = 0.19878981\n",
      "Iteration 967, loss = 0.19864808\n",
      "Iteration 968, loss = 0.19850665\n",
      "Iteration 969, loss = 0.19836546\n",
      "Iteration 970, loss = 0.19822455\n",
      "Iteration 971, loss = 0.19808392\n",
      "Iteration 972, loss = 0.19794355\n",
      "Iteration 973, loss = 0.19780349\n",
      "Iteration 974, loss = 0.19766366\n",
      "Iteration 975, loss = 0.19752411\n",
      "Iteration 976, loss = 0.19738487\n",
      "Iteration 977, loss = 0.19724586\n",
      "Iteration 978, loss = 0.19710708\n",
      "Iteration 979, loss = 0.19696859\n",
      "Iteration 980, loss = 0.19683035\n",
      "Iteration 981, loss = 0.19669237\n",
      "Iteration 982, loss = 0.19655470\n",
      "Iteration 983, loss = 0.19641724\n",
      "Iteration 984, loss = 0.19628010\n",
      "Iteration 985, loss = 0.19614321\n",
      "Iteration 986, loss = 0.19600655\n",
      "Iteration 987, loss = 0.19587012\n",
      "Iteration 988, loss = 0.19573399\n",
      "Iteration 989, loss = 0.19559808\n",
      "Iteration 990, loss = 0.19546243\n",
      "Iteration 991, loss = 0.19532698\n",
      "Iteration 992, loss = 0.19519175\n",
      "Iteration 993, loss = 0.19505679\n",
      "Iteration 994, loss = 0.19492212\n",
      "Iteration 995, loss = 0.19478763\n",
      "Iteration 996, loss = 0.19465340\n",
      "Iteration 997, loss = 0.19451941\n",
      "Iteration 998, loss = 0.19438564\n",
      "Iteration 999, loss = 0.19425205\n",
      "Iteration 1000, loss = 0.19411876\n",
      "Iteration 1, loss = 1.65392470\n",
      "Iteration 2, loss = 1.61776763\n",
      "Iteration 3, loss = 1.56920198\n",
      "Iteration 4, loss = 1.51241495\n",
      "Iteration 5, loss = 1.45169214\n",
      "Iteration 6, loss = 1.39154924\n",
      "Iteration 7, loss = 1.33582163\n",
      "Iteration 8, loss = 1.28766035\n",
      "Iteration 9, loss = 1.24911375\n",
      "Iteration 10, loss = 1.22041673\n",
      "Iteration 11, loss = 1.20026356\n",
      "Iteration 12, loss = 1.18626736\n",
      "Iteration 13, loss = 1.17550480\n",
      "Iteration 14, loss = 1.16538653\n",
      "Iteration 15, loss = 1.15392846\n",
      "Iteration 16, loss = 1.14038920\n",
      "Iteration 17, loss = 1.12423663\n",
      "Iteration 18, loss = 1.10573181\n",
      "Iteration 19, loss = 1.08546969\n",
      "Iteration 20, loss = 1.06443475\n",
      "Iteration 21, loss = 1.04370726\n",
      "Iteration 22, loss = 1.02407341\n",
      "Iteration 23, loss = 1.00617819\n",
      "Iteration 24, loss = 0.99015988\n",
      "Iteration 25, loss = 0.97587491\n",
      "Iteration 26, loss = 0.96312515\n",
      "Iteration 27, loss = 0.95156739\n",
      "Iteration 28, loss = 0.94087417\n",
      "Iteration 29, loss = 0.93073015\n",
      "Iteration 30, loss = 0.92090839\n",
      "Iteration 31, loss = 0.91130916\n",
      "Iteration 32, loss = 0.90187801\n",
      "Iteration 33, loss = 0.89267182\n",
      "Iteration 34, loss = 0.88366284\n",
      "Iteration 35, loss = 0.87483807\n",
      "Iteration 36, loss = 0.86624399\n",
      "Iteration 37, loss = 0.85792753\n",
      "Iteration 38, loss = 0.84994140\n",
      "Iteration 39, loss = 0.84228789\n",
      "Iteration 40, loss = 0.83502119\n",
      "Iteration 41, loss = 0.82812628\n",
      "Iteration 42, loss = 0.82157732\n",
      "Iteration 43, loss = 0.81536965\n",
      "Iteration 44, loss = 0.80933158\n",
      "Iteration 45, loss = 0.80340290\n",
      "Iteration 46, loss = 0.79755129\n",
      "Iteration 47, loss = 0.79176411\n",
      "Iteration 48, loss = 0.78604239\n",
      "Iteration 49, loss = 0.78037985\n",
      "Iteration 50, loss = 0.77479035\n",
      "Iteration 51, loss = 0.76929853\n",
      "Iteration 52, loss = 0.76395257\n",
      "Iteration 53, loss = 0.75875816\n",
      "Iteration 54, loss = 0.75370210\n",
      "Iteration 55, loss = 0.74876430\n",
      "Iteration 56, loss = 0.74393839\n",
      "Iteration 57, loss = 0.73922103\n",
      "Iteration 58, loss = 0.73460559\n",
      "Iteration 59, loss = 0.73008728\n",
      "Iteration 60, loss = 0.72566065\n",
      "Iteration 61, loss = 0.72132149\n",
      "Iteration 62, loss = 0.71706618\n",
      "Iteration 63, loss = 0.71289151\n",
      "Iteration 64, loss = 0.70879378\n",
      "Iteration 65, loss = 0.70477324\n",
      "Iteration 66, loss = 0.70082957\n",
      "Iteration 67, loss = 0.69696144\n",
      "Iteration 68, loss = 0.69316763\n",
      "Iteration 69, loss = 0.68944641\n",
      "Iteration 70, loss = 0.68579705\n",
      "Iteration 71, loss = 0.68221826\n",
      "Iteration 72, loss = 0.67870689\n",
      "Iteration 73, loss = 0.67526233\n",
      "Iteration 74, loss = 0.67188245\n",
      "Iteration 75, loss = 0.66856492\n",
      "Iteration 76, loss = 0.66530784\n",
      "Iteration 77, loss = 0.66211055\n",
      "Iteration 78, loss = 0.65897302\n",
      "Iteration 79, loss = 0.65588724\n",
      "Iteration 80, loss = 0.65285259\n",
      "Iteration 81, loss = 0.64987070\n",
      "Iteration 82, loss = 0.64693953\n",
      "Iteration 83, loss = 0.64405668\n",
      "Iteration 84, loss = 0.64122071\n",
      "Iteration 85, loss = 0.63843179\n",
      "Iteration 86, loss = 0.63568867\n",
      "Iteration 87, loss = 0.63299031\n",
      "Iteration 88, loss = 0.63033526\n",
      "Iteration 89, loss = 0.62772291\n",
      "Iteration 90, loss = 0.62515221\n",
      "Iteration 91, loss = 0.62262157\n",
      "Iteration 92, loss = 0.62013026\n",
      "Iteration 93, loss = 0.61767738\n",
      "Iteration 94, loss = 0.61526148\n",
      "Iteration 95, loss = 0.61288170\n",
      "Iteration 96, loss = 0.61053712\n",
      "Iteration 97, loss = 0.60822759\n",
      "Iteration 98, loss = 0.60595183\n",
      "Iteration 99, loss = 0.60370855\n",
      "Iteration 100, loss = 0.60149680\n",
      "Iteration 101, loss = 0.59931421\n",
      "Iteration 102, loss = 0.59716099\n",
      "Iteration 103, loss = 0.59503690\n",
      "Iteration 104, loss = 0.59294151\n",
      "Iteration 105, loss = 0.59087304\n",
      "Iteration 106, loss = 0.58883119\n",
      "Iteration 107, loss = 0.58681682\n",
      "Iteration 108, loss = 0.58482912\n",
      "Iteration 109, loss = 0.58286755\n",
      "Iteration 110, loss = 0.58093134\n",
      "Iteration 111, loss = 0.57901835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 112, loss = 0.57712789\n",
      "Iteration 113, loss = 0.57525794\n",
      "Iteration 114, loss = 0.57341049\n",
      "Iteration 115, loss = 0.57158611\n",
      "Iteration 116, loss = 0.56978482\n",
      "Iteration 117, loss = 0.56800495\n",
      "Iteration 118, loss = 0.56624505\n",
      "Iteration 119, loss = 0.56450360\n",
      "Iteration 120, loss = 0.56278272\n",
      "Iteration 121, loss = 0.56108110\n",
      "Iteration 122, loss = 0.55939883\n",
      "Iteration 123, loss = 0.55773456\n",
      "Iteration 124, loss = 0.55608941\n",
      "Iteration 125, loss = 0.55446379\n",
      "Iteration 126, loss = 0.55285661\n",
      "Iteration 127, loss = 0.55126755\n",
      "Iteration 128, loss = 0.54969541\n",
      "Iteration 129, loss = 0.54813981\n",
      "Iteration 130, loss = 0.54660016\n",
      "Iteration 131, loss = 0.54507658\n",
      "Iteration 132, loss = 0.54356934\n",
      "Iteration 133, loss = 0.54207814\n",
      "Iteration 134, loss = 0.54060271\n",
      "Iteration 135, loss = 0.53914278\n",
      "Iteration 136, loss = 0.53769842\n",
      "Iteration 137, loss = 0.53626969\n",
      "Iteration 138, loss = 0.53485570\n",
      "Iteration 139, loss = 0.53345617\n",
      "Iteration 140, loss = 0.53207088\n",
      "Iteration 141, loss = 0.53069954\n",
      "Iteration 142, loss = 0.52934192\n",
      "Iteration 143, loss = 0.52799775\n",
      "Iteration 144, loss = 0.52666682\n",
      "Iteration 145, loss = 0.52534888\n",
      "Iteration 146, loss = 0.52404369\n",
      "Iteration 147, loss = 0.52275101\n",
      "Iteration 148, loss = 0.52147051\n",
      "Iteration 149, loss = 0.52020204\n",
      "Iteration 150, loss = 0.51894548\n",
      "Iteration 151, loss = 0.51769933\n",
      "Iteration 152, loss = 0.51646421\n",
      "Iteration 153, loss = 0.51524022\n",
      "Iteration 154, loss = 0.51402737\n",
      "Iteration 155, loss = 0.51282552\n",
      "Iteration 156, loss = 0.51163429\n",
      "Iteration 157, loss = 0.51045337\n",
      "Iteration 158, loss = 0.50928259\n",
      "Iteration 159, loss = 0.50812177\n",
      "Iteration 160, loss = 0.50697076\n",
      "Iteration 161, loss = 0.50582935\n",
      "Iteration 162, loss = 0.50469741\n",
      "Iteration 163, loss = 0.50357473\n",
      "Iteration 164, loss = 0.50246116\n",
      "Iteration 165, loss = 0.50135654\n",
      "Iteration 166, loss = 0.50026072\n",
      "Iteration 167, loss = 0.49917340\n",
      "Iteration 168, loss = 0.49809446\n",
      "Iteration 169, loss = 0.49702385\n",
      "Iteration 170, loss = 0.49596141\n",
      "Iteration 171, loss = 0.49490697\n",
      "Iteration 172, loss = 0.49386041\n",
      "Iteration 173, loss = 0.49282158\n",
      "Iteration 174, loss = 0.49179025\n",
      "Iteration 175, loss = 0.49076638\n",
      "Iteration 176, loss = 0.48974989\n",
      "Iteration 177, loss = 0.48874070\n",
      "Iteration 178, loss = 0.48773900\n",
      "Iteration 179, loss = 0.48674473\n",
      "Iteration 180, loss = 0.48575721\n",
      "Iteration 181, loss = 0.48477638\n",
      "Iteration 182, loss = 0.48380224\n",
      "Iteration 183, loss = 0.48283463\n",
      "Iteration 184, loss = 0.48187340\n",
      "Iteration 185, loss = 0.48091844\n",
      "Iteration 186, loss = 0.47996955\n",
      "Iteration 187, loss = 0.47902676\n",
      "Iteration 188, loss = 0.47808998\n",
      "Iteration 189, loss = 0.47715919\n",
      "Iteration 190, loss = 0.47623424\n",
      "Iteration 191, loss = 0.47531506\n",
      "Iteration 192, loss = 0.47440163\n",
      "Iteration 193, loss = 0.47349423\n",
      "Iteration 194, loss = 0.47259229\n",
      "Iteration 195, loss = 0.47169571\n",
      "Iteration 196, loss = 0.47080440\n",
      "Iteration 197, loss = 0.46991836\n",
      "Iteration 198, loss = 0.46903761\n",
      "Iteration 199, loss = 0.46816217\n",
      "Iteration 200, loss = 0.46729146\n",
      "Iteration 201, loss = 0.46642556\n",
      "Iteration 202, loss = 0.46556446\n",
      "Iteration 203, loss = 0.46470780\n",
      "Iteration 204, loss = 0.46385578\n",
      "Iteration 205, loss = 0.46300870\n",
      "Iteration 206, loss = 0.46216595\n",
      "Iteration 207, loss = 0.46132797\n",
      "Iteration 208, loss = 0.46049442\n",
      "Iteration 209, loss = 0.45966503\n",
      "Iteration 210, loss = 0.45883992\n",
      "Iteration 211, loss = 0.45801894\n",
      "Iteration 212, loss = 0.45720212\n",
      "Iteration 213, loss = 0.45638952\n",
      "Iteration 214, loss = 0.45558103\n",
      "Iteration 215, loss = 0.45477701\n",
      "Iteration 216, loss = 0.45397632\n",
      "Iteration 217, loss = 0.45317934\n",
      "Iteration 218, loss = 0.45238627\n",
      "Iteration 219, loss = 0.45159653\n",
      "Iteration 220, loss = 0.45081035\n",
      "Iteration 221, loss = 0.45002782\n",
      "Iteration 222, loss = 0.44924807\n",
      "Iteration 223, loss = 0.44847162\n",
      "Iteration 224, loss = 0.44769781\n",
      "Iteration 225, loss = 0.44692728\n",
      "Iteration 226, loss = 0.44616001\n",
      "Iteration 227, loss = 0.44539556\n",
      "Iteration 228, loss = 0.44463485\n",
      "Iteration 229, loss = 0.44387758\n",
      "Iteration 230, loss = 0.44312295\n",
      "Iteration 231, loss = 0.44237151\n",
      "Iteration 232, loss = 0.44162320\n",
      "Iteration 233, loss = 0.44087780\n",
      "Iteration 234, loss = 0.44013513\n",
      "Iteration 235, loss = 0.43939521\n",
      "Iteration 236, loss = 0.43865780\n",
      "Iteration 237, loss = 0.43792311\n",
      "Iteration 238, loss = 0.43719122\n",
      "Iteration 239, loss = 0.43646205\n",
      "Iteration 240, loss = 0.43573558\n",
      "Iteration 241, loss = 0.43501230\n",
      "Iteration 242, loss = 0.43429216\n",
      "Iteration 243, loss = 0.43357441\n",
      "Iteration 244, loss = 0.43285864\n",
      "Iteration 245, loss = 0.43214542\n",
      "Iteration 246, loss = 0.43143539\n",
      "Iteration 247, loss = 0.43072802\n",
      "Iteration 248, loss = 0.43002380\n",
      "Iteration 249, loss = 0.42932205\n",
      "Iteration 250, loss = 0.42862296\n",
      "Iteration 251, loss = 0.42792593\n",
      "Iteration 252, loss = 0.42723224\n",
      "Iteration 253, loss = 0.42654219\n",
      "Iteration 254, loss = 0.42585554\n",
      "Iteration 255, loss = 0.42517121\n",
      "Iteration 256, loss = 0.42448961\n",
      "Iteration 257, loss = 0.42381059\n",
      "Iteration 258, loss = 0.42313423\n",
      "Iteration 259, loss = 0.42246049\n",
      "Iteration 260, loss = 0.42178909\n",
      "Iteration 261, loss = 0.42111994\n",
      "Iteration 262, loss = 0.42045341\n",
      "Iteration 263, loss = 0.41979027\n",
      "Iteration 264, loss = 0.41912995\n",
      "Iteration 265, loss = 0.41847231\n",
      "Iteration 266, loss = 0.41781698\n",
      "Iteration 267, loss = 0.41716430\n",
      "Iteration 268, loss = 0.41651490\n",
      "Iteration 269, loss = 0.41586810\n",
      "Iteration 270, loss = 0.41522347\n",
      "Iteration 271, loss = 0.41458131\n",
      "Iteration 272, loss = 0.41394140\n",
      "Iteration 273, loss = 0.41330365\n",
      "Iteration 274, loss = 0.41266797\n",
      "Iteration 275, loss = 0.41203423\n",
      "Iteration 276, loss = 0.41140246\n",
      "Iteration 277, loss = 0.41077261\n",
      "Iteration 278, loss = 0.41014464\n",
      "Iteration 279, loss = 0.40951863\n",
      "Iteration 280, loss = 0.40889455\n",
      "Iteration 281, loss = 0.40827252\n",
      "Iteration 282, loss = 0.40765272\n",
      "Iteration 283, loss = 0.40703482\n",
      "Iteration 284, loss = 0.40641878\n",
      "Iteration 285, loss = 0.40580480\n",
      "Iteration 286, loss = 0.40519285\n",
      "Iteration 287, loss = 0.40458277\n",
      "Iteration 288, loss = 0.40397447\n",
      "Iteration 289, loss = 0.40336796\n",
      "Iteration 290, loss = 0.40276336\n",
      "Iteration 291, loss = 0.40216008\n",
      "Iteration 292, loss = 0.40155860\n",
      "Iteration 293, loss = 0.40095903\n",
      "Iteration 294, loss = 0.40036111\n",
      "Iteration 295, loss = 0.39976491\n",
      "Iteration 296, loss = 0.39917038\n",
      "Iteration 297, loss = 0.39857757\n",
      "Iteration 298, loss = 0.39798657\n",
      "Iteration 299, loss = 0.39739742\n",
      "Iteration 300, loss = 0.39681027\n",
      "Iteration 301, loss = 0.39622494\n",
      "Iteration 302, loss = 0.39564136\n",
      "Iteration 303, loss = 0.39505931\n",
      "Iteration 304, loss = 0.39447886\n",
      "Iteration 305, loss = 0.39390001\n",
      "Iteration 306, loss = 0.39332284\n",
      "Iteration 307, loss = 0.39274731\n",
      "Iteration 308, loss = 0.39217355\n",
      "Iteration 309, loss = 0.39160129\n",
      "Iteration 310, loss = 0.39103060\n",
      "Iteration 311, loss = 0.39046160\n",
      "Iteration 312, loss = 0.38989430\n",
      "Iteration 313, loss = 0.38932866\n",
      "Iteration 314, loss = 0.38876447\n",
      "Iteration 315, loss = 0.38820131\n",
      "Iteration 316, loss = 0.38763901\n",
      "Iteration 317, loss = 0.38707821\n",
      "Iteration 318, loss = 0.38651867\n",
      "Iteration 319, loss = 0.38596026\n",
      "Iteration 320, loss = 0.38540331\n",
      "Iteration 321, loss = 0.38484764\n",
      "Iteration 322, loss = 0.38429171\n",
      "Iteration 323, loss = 0.38373492\n",
      "Iteration 324, loss = 0.38317886\n",
      "Iteration 325, loss = 0.38262338\n",
      "Iteration 326, loss = 0.38206760\n",
      "Iteration 327, loss = 0.38151203\n",
      "Iteration 328, loss = 0.38095735\n",
      "Iteration 329, loss = 0.38040362\n",
      "Iteration 330, loss = 0.37985085\n",
      "Iteration 331, loss = 0.37929852\n",
      "Iteration 332, loss = 0.37874651\n",
      "Iteration 333, loss = 0.37819557\n",
      "Iteration 334, loss = 0.37764575\n",
      "Iteration 335, loss = 0.37709713\n",
      "Iteration 336, loss = 0.37654970\n",
      "Iteration 337, loss = 0.37600336\n",
      "Iteration 338, loss = 0.37545727\n",
      "Iteration 339, loss = 0.37491001\n",
      "Iteration 340, loss = 0.37436191\n",
      "Iteration 341, loss = 0.37381319\n",
      "Iteration 342, loss = 0.37326114\n",
      "Iteration 343, loss = 0.37270355\n",
      "Iteration 344, loss = 0.37213983\n",
      "Iteration 345, loss = 0.37157490\n",
      "Iteration 346, loss = 0.37100860\n",
      "Iteration 347, loss = 0.37043813\n",
      "Iteration 348, loss = 0.36986541\n",
      "Iteration 349, loss = 0.36928745\n",
      "Iteration 350, loss = 0.36869485\n",
      "Iteration 351, loss = 0.36808780\n",
      "Iteration 352, loss = 0.36746990\n",
      "Iteration 353, loss = 0.36683751\n",
      "Iteration 354, loss = 0.36619033\n",
      "Iteration 355, loss = 0.36552992\n",
      "Iteration 356, loss = 0.36484915\n",
      "Iteration 357, loss = 0.36415031\n",
      "Iteration 358, loss = 0.36344936\n",
      "Iteration 359, loss = 0.36274803\n",
      "Iteration 360, loss = 0.36204749\n",
      "Iteration 361, loss = 0.36134173\n",
      "Iteration 362, loss = 0.36062062\n",
      "Iteration 363, loss = 0.35991065\n",
      "Iteration 364, loss = 0.35922915\n",
      "Iteration 365, loss = 0.35857069\n",
      "Iteration 366, loss = 0.35793694\n",
      "Iteration 367, loss = 0.35731116\n",
      "Iteration 368, loss = 0.35669608\n",
      "Iteration 369, loss = 0.35611275\n",
      "Iteration 370, loss = 0.35554814\n",
      "Iteration 371, loss = 0.35499428\n",
      "Iteration 372, loss = 0.35444956\n",
      "Iteration 373, loss = 0.35390864\n",
      "Iteration 374, loss = 0.35337651\n",
      "Iteration 375, loss = 0.35284819\n",
      "Iteration 376, loss = 0.35232880\n",
      "Iteration 377, loss = 0.35181714\n",
      "Iteration 378, loss = 0.35130924\n",
      "Iteration 379, loss = 0.35080487\n",
      "Iteration 380, loss = 0.35030211\n",
      "Iteration 381, loss = 0.34980093\n",
      "Iteration 382, loss = 0.34930207\n",
      "Iteration 383, loss = 0.34880735\n",
      "Iteration 384, loss = 0.34831662\n",
      "Iteration 385, loss = 0.34782755\n",
      "Iteration 386, loss = 0.34734213\n",
      "Iteration 387, loss = 0.34685746\n",
      "Iteration 388, loss = 0.34637338\n",
      "Iteration 389, loss = 0.34589007\n",
      "Iteration 390, loss = 0.34540779\n",
      "Iteration 391, loss = 0.34492731\n",
      "Iteration 392, loss = 0.34444583\n",
      "Iteration 393, loss = 0.34396349\n",
      "Iteration 394, loss = 0.34348217\n",
      "Iteration 395, loss = 0.34300113\n",
      "Iteration 396, loss = 0.34252142\n",
      "Iteration 397, loss = 0.34204181\n",
      "Iteration 398, loss = 0.34156587\n",
      "Iteration 399, loss = 0.34109035\n",
      "Iteration 400, loss = 0.34061376\n",
      "Iteration 401, loss = 0.34013877\n",
      "Iteration 402, loss = 0.33966617\n",
      "Iteration 403, loss = 0.33919438\n",
      "Iteration 404, loss = 0.33872415\n",
      "Iteration 405, loss = 0.33825697\n",
      "Iteration 406, loss = 0.33779142\n",
      "Iteration 407, loss = 0.33733043\n",
      "Iteration 408, loss = 0.33687178\n",
      "Iteration 409, loss = 0.33641426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 410, loss = 0.33595893\n",
      "Iteration 411, loss = 0.33550546\n",
      "Iteration 412, loss = 0.33505441\n",
      "Iteration 413, loss = 0.33460645\n",
      "Iteration 414, loss = 0.33416308\n",
      "Iteration 415, loss = 0.33372209\n",
      "Iteration 416, loss = 0.33328348\n",
      "Iteration 417, loss = 0.33284671\n",
      "Iteration 418, loss = 0.33241176\n",
      "Iteration 419, loss = 0.33197796\n",
      "Iteration 420, loss = 0.33154530\n",
      "Iteration 421, loss = 0.33111354\n",
      "Iteration 422, loss = 0.33068296\n",
      "Iteration 423, loss = 0.33025360\n",
      "Iteration 424, loss = 0.32982535\n",
      "Iteration 425, loss = 0.32939845\n",
      "Iteration 426, loss = 0.32897263\n",
      "Iteration 427, loss = 0.32854761\n",
      "Iteration 428, loss = 0.32812361\n",
      "Iteration 429, loss = 0.32770042\n",
      "Iteration 430, loss = 0.32727800\n",
      "Iteration 431, loss = 0.32685645\n",
      "Iteration 432, loss = 0.32643570\n",
      "Iteration 433, loss = 0.32601600\n",
      "Iteration 434, loss = 0.32559698\n",
      "Iteration 435, loss = 0.32517883\n",
      "Iteration 436, loss = 0.32476148\n",
      "Iteration 437, loss = 0.32434506\n",
      "Iteration 438, loss = 0.32392930\n",
      "Iteration 439, loss = 0.32351454\n",
      "Iteration 440, loss = 0.32310045\n",
      "Iteration 441, loss = 0.32268731\n",
      "Iteration 442, loss = 0.32227486\n",
      "Iteration 443, loss = 0.32186342\n",
      "Iteration 444, loss = 0.32145262\n",
      "Iteration 445, loss = 0.32104274\n",
      "Iteration 446, loss = 0.32063359\n",
      "Iteration 447, loss = 0.32022537\n",
      "Iteration 448, loss = 0.31981784\n",
      "Iteration 449, loss = 0.31941138\n",
      "Iteration 450, loss = 0.31900570\n",
      "Iteration 451, loss = 0.31860103\n",
      "Iteration 452, loss = 0.31819705\n",
      "Iteration 453, loss = 0.31779401\n",
      "Iteration 454, loss = 0.31739167\n",
      "Iteration 455, loss = 0.31699026\n",
      "Iteration 456, loss = 0.31658959\n",
      "Iteration 457, loss = 0.31619004\n",
      "Iteration 458, loss = 0.31579126\n",
      "Iteration 459, loss = 0.31539330\n",
      "Iteration 460, loss = 0.31499615\n",
      "Iteration 461, loss = 0.31459990\n",
      "Iteration 462, loss = 0.31420442\n",
      "Iteration 463, loss = 0.31380973\n",
      "Iteration 464, loss = 0.31341587\n",
      "Iteration 465, loss = 0.31302284\n",
      "Iteration 466, loss = 0.31263048\n",
      "Iteration 467, loss = 0.31223910\n",
      "Iteration 468, loss = 0.31184828\n",
      "Iteration 469, loss = 0.31145843\n",
      "Iteration 470, loss = 0.31106920\n",
      "Iteration 471, loss = 0.31068079\n",
      "Iteration 472, loss = 0.31029321\n",
      "Iteration 473, loss = 0.30990636\n",
      "Iteration 474, loss = 0.30952034\n",
      "Iteration 475, loss = 0.30913510\n",
      "Iteration 476, loss = 0.30875066\n",
      "Iteration 477, loss = 0.30836694\n",
      "Iteration 478, loss = 0.30798409\n",
      "Iteration 479, loss = 0.30760191\n",
      "Iteration 480, loss = 0.30722046\n",
      "Iteration 481, loss = 0.30683994\n",
      "Iteration 482, loss = 0.30646025\n",
      "Iteration 483, loss = 0.30608127\n",
      "Iteration 484, loss = 0.30570312\n",
      "Iteration 485, loss = 0.30532581\n",
      "Iteration 486, loss = 0.30494920\n",
      "Iteration 487, loss = 0.30457343\n",
      "Iteration 488, loss = 0.30419836\n",
      "Iteration 489, loss = 0.30382409\n",
      "Iteration 490, loss = 0.30345054\n",
      "Iteration 491, loss = 0.30307780\n",
      "Iteration 492, loss = 0.30270573\n",
      "Iteration 493, loss = 0.30233465\n",
      "Iteration 494, loss = 0.30196432\n",
      "Iteration 495, loss = 0.30159473\n",
      "Iteration 496, loss = 0.30122602\n",
      "Iteration 497, loss = 0.30085793\n",
      "Iteration 498, loss = 0.30049062\n",
      "Iteration 499, loss = 0.30012409\n",
      "Iteration 500, loss = 0.29975833\n",
      "Iteration 501, loss = 0.29939342\n",
      "Iteration 502, loss = 0.29902931\n",
      "Iteration 503, loss = 0.29866591\n",
      "Iteration 504, loss = 0.29830336\n",
      "Iteration 505, loss = 0.29794154\n",
      "Iteration 506, loss = 0.29758054\n",
      "Iteration 507, loss = 0.29722025\n",
      "Iteration 508, loss = 0.29686079\n",
      "Iteration 509, loss = 0.29650207\n",
      "Iteration 510, loss = 0.29614409\n",
      "Iteration 511, loss = 0.29578684\n",
      "Iteration 512, loss = 0.29543030\n",
      "Iteration 513, loss = 0.29507452\n",
      "Iteration 514, loss = 0.29471941\n",
      "Iteration 515, loss = 0.29436511\n",
      "Iteration 516, loss = 0.29401145\n",
      "Iteration 517, loss = 0.29365856\n",
      "Iteration 518, loss = 0.29330634\n",
      "Iteration 519, loss = 0.29295487\n",
      "Iteration 520, loss = 0.29260409\n",
      "Iteration 521, loss = 0.29225403\n",
      "Iteration 522, loss = 0.29190465\n",
      "Iteration 523, loss = 0.29155600\n",
      "Iteration 524, loss = 0.29120804\n",
      "Iteration 525, loss = 0.29086076\n",
      "Iteration 526, loss = 0.29051422\n",
      "Iteration 527, loss = 0.29016833\n",
      "Iteration 528, loss = 0.28982319\n",
      "Iteration 529, loss = 0.28947864\n",
      "Iteration 530, loss = 0.28913481\n",
      "Iteration 531, loss = 0.28879165\n",
      "Iteration 532, loss = 0.28844917\n",
      "Iteration 533, loss = 0.28810744\n",
      "Iteration 534, loss = 0.28776636\n",
      "Iteration 535, loss = 0.28742597\n",
      "Iteration 536, loss = 0.28708630\n",
      "Iteration 537, loss = 0.28674731\n",
      "Iteration 538, loss = 0.28640916\n",
      "Iteration 539, loss = 0.28607169\n",
      "Iteration 540, loss = 0.28573496\n",
      "Iteration 541, loss = 0.28539886\n",
      "Iteration 542, loss = 0.28506354\n",
      "Iteration 543, loss = 0.28472885\n",
      "Iteration 544, loss = 0.28439488\n",
      "Iteration 545, loss = 0.28406171\n",
      "Iteration 546, loss = 0.28372920\n",
      "Iteration 547, loss = 0.28339754\n",
      "Iteration 548, loss = 0.28306653\n",
      "Iteration 549, loss = 0.28273629\n",
      "Iteration 550, loss = 0.28240690\n",
      "Iteration 551, loss = 0.28207823\n",
      "Iteration 552, loss = 0.28175015\n",
      "Iteration 553, loss = 0.28142275\n",
      "Iteration 554, loss = 0.28109611\n",
      "Iteration 555, loss = 0.28077004\n",
      "Iteration 556, loss = 0.28044465\n",
      "Iteration 557, loss = 0.28011996\n",
      "Iteration 558, loss = 0.27979592\n",
      "Iteration 559, loss = 0.27947256\n",
      "Iteration 560, loss = 0.27914988\n",
      "Iteration 561, loss = 0.27882792\n",
      "Iteration 562, loss = 0.27850663\n",
      "Iteration 563, loss = 0.27818603\n",
      "Iteration 564, loss = 0.27786605\n",
      "Iteration 565, loss = 0.27754675\n",
      "Iteration 566, loss = 0.27722814\n",
      "Iteration 567, loss = 0.27691023\n",
      "Iteration 568, loss = 0.27659301\n",
      "Iteration 569, loss = 0.27627652\n",
      "Iteration 570, loss = 0.27596071\n",
      "Iteration 571, loss = 0.27564552\n",
      "Iteration 572, loss = 0.27533098\n",
      "Iteration 573, loss = 0.27501708\n",
      "Iteration 574, loss = 0.27470389\n",
      "Iteration 575, loss = 0.27439132\n",
      "Iteration 576, loss = 0.27407937\n",
      "Iteration 577, loss = 0.27376808\n",
      "Iteration 578, loss = 0.27345743\n",
      "Iteration 579, loss = 0.27314742\n",
      "Iteration 580, loss = 0.27283804\n",
      "Iteration 581, loss = 0.27252933\n",
      "Iteration 582, loss = 0.27222119\n",
      "Iteration 583, loss = 0.27191370\n",
      "Iteration 584, loss = 0.27160684\n",
      "Iteration 585, loss = 0.27130061\n",
      "Iteration 586, loss = 0.27099518\n",
      "Iteration 587, loss = 0.27069036\n",
      "Iteration 588, loss = 0.27038616\n",
      "Iteration 589, loss = 0.27008261\n",
      "Iteration 590, loss = 0.26977968\n",
      "Iteration 591, loss = 0.26947737\n",
      "Iteration 592, loss = 0.26917568\n",
      "Iteration 593, loss = 0.26887463\n",
      "Iteration 594, loss = 0.26857416\n",
      "Iteration 595, loss = 0.26827430\n",
      "Iteration 596, loss = 0.26797505\n",
      "Iteration 597, loss = 0.26767641\n",
      "Iteration 598, loss = 0.26737841\n",
      "Iteration 599, loss = 0.26708099\n",
      "Iteration 600, loss = 0.26678417\n",
      "Iteration 601, loss = 0.26648795\n",
      "Iteration 602, loss = 0.26619234\n",
      "Iteration 603, loss = 0.26589736\n",
      "Iteration 604, loss = 0.26560291\n",
      "Iteration 605, loss = 0.26530909\n",
      "Iteration 606, loss = 0.26501587\n",
      "Iteration 607, loss = 0.26472327\n",
      "Iteration 608, loss = 0.26443123\n",
      "Iteration 609, loss = 0.26413981\n",
      "Iteration 610, loss = 0.26384897\n",
      "Iteration 611, loss = 0.26355874\n",
      "Iteration 612, loss = 0.26326911\n",
      "Iteration 613, loss = 0.26298010\n",
      "Iteration 614, loss = 0.26269164\n",
      "Iteration 615, loss = 0.26240378\n",
      "Iteration 616, loss = 0.26211654\n",
      "Iteration 617, loss = 0.26182989\n",
      "Iteration 618, loss = 0.26154382\n",
      "Iteration 619, loss = 0.26125833\n",
      "Iteration 620, loss = 0.26097341\n",
      "Iteration 621, loss = 0.26068908\n",
      "Iteration 622, loss = 0.26040534\n",
      "Iteration 623, loss = 0.26012219\n",
      "Iteration 624, loss = 0.25983965\n",
      "Iteration 625, loss = 0.25955763\n",
      "Iteration 626, loss = 0.25927622\n",
      "Iteration 627, loss = 0.25899539\n",
      "Iteration 628, loss = 0.25871513\n",
      "Iteration 629, loss = 0.25843540\n",
      "Iteration 630, loss = 0.25815623\n",
      "Iteration 631, loss = 0.25787761\n",
      "Iteration 632, loss = 0.25759956\n",
      "Iteration 633, loss = 0.25732207\n",
      "Iteration 634, loss = 0.25704514\n",
      "Iteration 635, loss = 0.25676878\n",
      "Iteration 636, loss = 0.25649299\n",
      "Iteration 637, loss = 0.25621774\n",
      "Iteration 638, loss = 0.25594306\n",
      "Iteration 639, loss = 0.25566894\n",
      "Iteration 640, loss = 0.25539534\n",
      "Iteration 641, loss = 0.25512229\n",
      "Iteration 642, loss = 0.25484981\n",
      "Iteration 643, loss = 0.25457788\n",
      "Iteration 644, loss = 0.25430648\n",
      "Iteration 645, loss = 0.25403561\n",
      "Iteration 646, loss = 0.25376530\n",
      "Iteration 647, loss = 0.25349540\n",
      "Iteration 648, loss = 0.25322599\n",
      "Iteration 649, loss = 0.25295711\n",
      "Iteration 650, loss = 0.25268875\n",
      "Iteration 651, loss = 0.25242080\n",
      "Iteration 652, loss = 0.25215340\n",
      "Iteration 653, loss = 0.25188652\n",
      "Iteration 654, loss = 0.25162015\n",
      "Iteration 655, loss = 0.25135432\n",
      "Iteration 656, loss = 0.25108897\n",
      "Iteration 657, loss = 0.25082415\n",
      "Iteration 658, loss = 0.25055983\n",
      "Iteration 659, loss = 0.25029605\n",
      "Iteration 660, loss = 0.25003279\n",
      "Iteration 661, loss = 0.24977003\n",
      "Iteration 662, loss = 0.24950779\n",
      "Iteration 663, loss = 0.24924606\n",
      "Iteration 664, loss = 0.24898487\n",
      "Iteration 665, loss = 0.24872421\n",
      "Iteration 666, loss = 0.24846406\n",
      "Iteration 667, loss = 0.24820443\n",
      "Iteration 668, loss = 0.24794534\n",
      "Iteration 669, loss = 0.24768677\n",
      "Iteration 670, loss = 0.24742869\n",
      "Iteration 671, loss = 0.24717111\n",
      "Iteration 672, loss = 0.24691406\n",
      "Iteration 673, loss = 0.24665753\n",
      "Iteration 674, loss = 0.24640155\n",
      "Iteration 675, loss = 0.24614606\n",
      "Iteration 676, loss = 0.24589111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 677, loss = 0.24563668\n",
      "Iteration 678, loss = 0.24538278\n",
      "Iteration 679, loss = 0.24512930\n",
      "Iteration 680, loss = 0.24487622\n",
      "Iteration 681, loss = 0.24462363\n",
      "Iteration 682, loss = 0.24437145\n",
      "Iteration 683, loss = 0.24411977\n",
      "Iteration 684, loss = 0.24386860\n",
      "Iteration 685, loss = 0.24361792\n",
      "Iteration 686, loss = 0.24336772\n",
      "Iteration 687, loss = 0.24311797\n",
      "Iteration 688, loss = 0.24286873\n",
      "Iteration 689, loss = 0.24261995\n",
      "Iteration 690, loss = 0.24237163\n",
      "Iteration 691, loss = 0.24212381\n",
      "Iteration 692, loss = 0.24187650\n",
      "Iteration 693, loss = 0.24162970\n",
      "Iteration 694, loss = 0.24138331\n",
      "Iteration 695, loss = 0.24113730\n",
      "Iteration 696, loss = 0.24089173\n",
      "Iteration 697, loss = 0.24064664\n",
      "Iteration 698, loss = 0.24040203\n",
      "Iteration 699, loss = 0.24015791\n",
      "Iteration 700, loss = 0.23991427\n",
      "Iteration 701, loss = 0.23967112\n",
      "Iteration 702, loss = 0.23942846\n",
      "Iteration 703, loss = 0.23918628\n",
      "Iteration 704, loss = 0.23894449\n",
      "Iteration 705, loss = 0.23870319\n",
      "Iteration 706, loss = 0.23846238\n",
      "Iteration 707, loss = 0.23822206\n",
      "Iteration 708, loss = 0.23798223\n",
      "Iteration 709, loss = 0.23774290\n",
      "Iteration 710, loss = 0.23750404\n",
      "Iteration 711, loss = 0.23726557\n",
      "Iteration 712, loss = 0.23702757\n",
      "Iteration 713, loss = 0.23679008\n",
      "Iteration 714, loss = 0.23655301\n",
      "Iteration 715, loss = 0.23631638\n",
      "Iteration 716, loss = 0.23608023\n",
      "Iteration 717, loss = 0.23584453\n",
      "Iteration 718, loss = 0.23560928\n",
      "Iteration 719, loss = 0.23537450\n",
      "Iteration 720, loss = 0.23514020\n",
      "Iteration 721, loss = 0.23490614\n",
      "Iteration 722, loss = 0.23467235\n",
      "Iteration 723, loss = 0.23443901\n",
      "Iteration 724, loss = 0.23420612\n",
      "Iteration 725, loss = 0.23397359\n",
      "Iteration 726, loss = 0.23374146\n",
      "Iteration 727, loss = 0.23350979\n",
      "Iteration 728, loss = 0.23327854\n",
      "Iteration 729, loss = 0.23304761\n",
      "Iteration 730, loss = 0.23281713\n",
      "Iteration 731, loss = 0.23258711\n",
      "Iteration 732, loss = 0.23235753\n",
      "Iteration 733, loss = 0.23212842\n",
      "Iteration 734, loss = 0.23189980\n",
      "Iteration 735, loss = 0.23167165\n",
      "Iteration 736, loss = 0.23144373\n",
      "Iteration 737, loss = 0.23121604\n",
      "Iteration 738, loss = 0.23098875\n",
      "Iteration 739, loss = 0.23076189\n",
      "Iteration 740, loss = 0.23053547\n",
      "Iteration 741, loss = 0.23030937\n",
      "Iteration 742, loss = 0.23008365\n",
      "Iteration 743, loss = 0.22985810\n",
      "Iteration 744, loss = 0.22963288\n",
      "Iteration 745, loss = 0.22940802\n",
      "Iteration 746, loss = 0.22918348\n",
      "Iteration 747, loss = 0.22895936\n",
      "Iteration 748, loss = 0.22873567\n",
      "Iteration 749, loss = 0.22851234\n",
      "Iteration 750, loss = 0.22828927\n",
      "Iteration 751, loss = 0.22806662\n",
      "Iteration 752, loss = 0.22784439\n",
      "Iteration 753, loss = 0.22762260\n",
      "Iteration 754, loss = 0.22740124\n",
      "Iteration 755, loss = 0.22718032\n",
      "Iteration 756, loss = 0.22695966\n",
      "Iteration 757, loss = 0.22673935\n",
      "Iteration 758, loss = 0.22651946\n",
      "Iteration 759, loss = 0.22630001\n",
      "Iteration 760, loss = 0.22608102\n",
      "Iteration 761, loss = 0.22586247\n",
      "Iteration 762, loss = 0.22564436\n",
      "Iteration 763, loss = 0.22542671\n",
      "Iteration 764, loss = 0.22520951\n",
      "Iteration 765, loss = 0.22499258\n",
      "Iteration 766, loss = 0.22477599\n",
      "Iteration 767, loss = 0.22455964\n",
      "Iteration 768, loss = 0.22434365\n",
      "Iteration 769, loss = 0.22412809\n",
      "Iteration 770, loss = 0.22391296\n",
      "Iteration 771, loss = 0.22369827\n",
      "Iteration 772, loss = 0.22348402\n",
      "Iteration 773, loss = 0.22327020\n",
      "Iteration 774, loss = 0.22305683\n",
      "Iteration 775, loss = 0.22284389\n",
      "Iteration 776, loss = 0.22263140\n",
      "Iteration 777, loss = 0.22241936\n",
      "Iteration 778, loss = 0.22220777\n",
      "Iteration 779, loss = 0.22199662\n",
      "Iteration 780, loss = 0.22178596\n",
      "Iteration 781, loss = 0.22157569\n",
      "Iteration 782, loss = 0.22136588\n",
      "Iteration 783, loss = 0.22115653\n",
      "Iteration 784, loss = 0.22094765\n",
      "Iteration 785, loss = 0.22073922\n",
      "Iteration 786, loss = 0.22053124\n",
      "Iteration 787, loss = 0.22032370\n",
      "Iteration 788, loss = 0.22011661\n",
      "Iteration 789, loss = 0.21990997\n",
      "Iteration 790, loss = 0.21970378\n",
      "Iteration 791, loss = 0.21949804\n",
      "Iteration 792, loss = 0.21929274\n",
      "Iteration 793, loss = 0.21908776\n",
      "Iteration 794, loss = 0.21888289\n",
      "Iteration 795, loss = 0.21867845\n",
      "Iteration 796, loss = 0.21847443\n",
      "Iteration 797, loss = 0.21827084\n",
      "Iteration 798, loss = 0.21806764\n",
      "Iteration 799, loss = 0.21786484\n",
      "Iteration 800, loss = 0.21766245\n",
      "Iteration 801, loss = 0.21746042\n",
      "Iteration 802, loss = 0.21725882\n",
      "Iteration 803, loss = 0.21705765\n",
      "Iteration 804, loss = 0.21685691\n",
      "Iteration 805, loss = 0.21665661\n",
      "Iteration 806, loss = 0.21645674\n",
      "Iteration 807, loss = 0.21625729\n",
      "Iteration 808, loss = 0.21605830\n",
      "Iteration 809, loss = 0.21585969\n",
      "Iteration 810, loss = 0.21566150\n",
      "Iteration 811, loss = 0.21546372\n",
      "Iteration 812, loss = 0.21526637\n",
      "Iteration 813, loss = 0.21506944\n",
      "Iteration 814, loss = 0.21487293\n",
      "Iteration 815, loss = 0.21467684\n",
      "Iteration 816, loss = 0.21448120\n",
      "Iteration 817, loss = 0.21428594\n",
      "Iteration 818, loss = 0.21409112\n",
      "Iteration 819, loss = 0.21389671\n",
      "Iteration 820, loss = 0.21370273\n",
      "Iteration 821, loss = 0.21350917\n",
      "Iteration 822, loss = 0.21331602\n",
      "Iteration 823, loss = 0.21312329\n",
      "Iteration 824, loss = 0.21293087\n",
      "Iteration 825, loss = 0.21273887\n",
      "Iteration 826, loss = 0.21254729\n",
      "Iteration 827, loss = 0.21235611\n",
      "Iteration 828, loss = 0.21216534\n",
      "Iteration 829, loss = 0.21197498\n",
      "Iteration 830, loss = 0.21178504\n",
      "Iteration 831, loss = 0.21159549\n",
      "Iteration 832, loss = 0.21140609\n",
      "Iteration 833, loss = 0.21121708\n",
      "Iteration 834, loss = 0.21102845\n",
      "Iteration 835, loss = 0.21084021\n",
      "Iteration 836, loss = 0.21065240\n",
      "Iteration 837, loss = 0.21046496\n",
      "Iteration 838, loss = 0.21027793\n",
      "Iteration 839, loss = 0.21009123\n",
      "Iteration 840, loss = 0.20990489\n",
      "Iteration 841, loss = 0.20971895\n",
      "Iteration 842, loss = 0.20953339\n",
      "Iteration 843, loss = 0.20934824\n",
      "Iteration 844, loss = 0.20916346\n",
      "Iteration 845, loss = 0.20897908\n",
      "Iteration 846, loss = 0.20879508\n",
      "Iteration 847, loss = 0.20861147\n",
      "Iteration 848, loss = 0.20842826\n",
      "Iteration 849, loss = 0.20824543\n",
      "Iteration 850, loss = 0.20806302\n",
      "Iteration 851, loss = 0.20788094\n",
      "Iteration 852, loss = 0.20769923\n",
      "Iteration 853, loss = 0.20751791\n",
      "Iteration 854, loss = 0.20733698\n",
      "Iteration 855, loss = 0.20715645\n",
      "Iteration 856, loss = 0.20697629\n",
      "Iteration 857, loss = 0.20679653\n",
      "Iteration 858, loss = 0.20661714\n",
      "Iteration 859, loss = 0.20643813\n",
      "Iteration 860, loss = 0.20625945\n",
      "Iteration 861, loss = 0.20608115\n",
      "Iteration 862, loss = 0.20590324\n",
      "Iteration 863, loss = 0.20572570\n",
      "Iteration 864, loss = 0.20554854\n",
      "Iteration 865, loss = 0.20537160\n",
      "Iteration 866, loss = 0.20519498\n",
      "Iteration 867, loss = 0.20501863\n",
      "Iteration 868, loss = 0.20484256\n",
      "Iteration 869, loss = 0.20466684\n",
      "Iteration 870, loss = 0.20449145\n",
      "Iteration 871, loss = 0.20431632\n",
      "Iteration 872, loss = 0.20414154\n",
      "Iteration 873, loss = 0.20396715\n",
      "Iteration 874, loss = 0.20379305\n",
      "Iteration 875, loss = 0.20361934\n",
      "Iteration 876, loss = 0.20344596\n",
      "Iteration 877, loss = 0.20327280\n",
      "Iteration 878, loss = 0.20309996\n",
      "Iteration 879, loss = 0.20292741\n",
      "Iteration 880, loss = 0.20275520\n",
      "Iteration 881, loss = 0.20258334\n",
      "Iteration 882, loss = 0.20241179\n",
      "Iteration 883, loss = 0.20224047\n",
      "Iteration 884, loss = 0.20206949\n",
      "Iteration 885, loss = 0.20189886\n",
      "Iteration 886, loss = 0.20172857\n",
      "Iteration 887, loss = 0.20155863\n",
      "Iteration 888, loss = 0.20138905\n",
      "Iteration 889, loss = 0.20121969\n",
      "Iteration 890, loss = 0.20105059\n",
      "Iteration 891, loss = 0.20088179\n",
      "Iteration 892, loss = 0.20071336\n",
      "Iteration 893, loss = 0.20054515\n",
      "Iteration 894, loss = 0.20037722\n",
      "Iteration 895, loss = 0.20020964\n",
      "Iteration 896, loss = 0.20004239\n",
      "Iteration 897, loss = 0.19987549\n",
      "Iteration 898, loss = 0.19970897\n",
      "Iteration 899, loss = 0.19954275\n",
      "Iteration 900, loss = 0.19937680\n",
      "Iteration 901, loss = 0.19921129\n",
      "Iteration 902, loss = 0.19904599\n",
      "Iteration 903, loss = 0.19888112\n",
      "Iteration 904, loss = 0.19871639\n",
      "Iteration 905, loss = 0.19855191\n",
      "Iteration 906, loss = 0.19838775\n",
      "Iteration 907, loss = 0.19822388\n",
      "Iteration 908, loss = 0.19806025\n",
      "Iteration 909, loss = 0.19789696\n",
      "Iteration 910, loss = 0.19773397\n",
      "Iteration 911, loss = 0.19757133\n",
      "Iteration 912, loss = 0.19740902\n",
      "Iteration 913, loss = 0.19724703\n",
      "Iteration 914, loss = 0.19708537\n",
      "Iteration 915, loss = 0.19692386\n",
      "Iteration 916, loss = 0.19676265\n",
      "Iteration 917, loss = 0.19660175\n",
      "Iteration 918, loss = 0.19644114\n",
      "Iteration 919, loss = 0.19628073\n",
      "Iteration 920, loss = 0.19612064\n",
      "Iteration 921, loss = 0.19596085\n",
      "Iteration 922, loss = 0.19580139\n",
      "Iteration 923, loss = 0.19564228\n",
      "Iteration 924, loss = 0.19548346\n",
      "Iteration 925, loss = 0.19532498\n",
      "Iteration 926, loss = 0.19516682\n",
      "Iteration 927, loss = 0.19500902\n",
      "Iteration 928, loss = 0.19485151\n",
      "Iteration 929, loss = 0.19469431\n",
      "Iteration 930, loss = 0.19453735\n",
      "Iteration 931, loss = 0.19438067\n",
      "Iteration 932, loss = 0.19422431\n",
      "Iteration 933, loss = 0.19406831\n",
      "Iteration 934, loss = 0.19391258\n",
      "Iteration 935, loss = 0.19375715\n",
      "Iteration 936, loss = 0.19360199\n",
      "Iteration 937, loss = 0.19344718\n",
      "Iteration 938, loss = 0.19329261\n",
      "Iteration 939, loss = 0.19313833\n",
      "Iteration 940, loss = 0.19298438\n",
      "Iteration 941, loss = 0.19283074\n",
      "Iteration 942, loss = 0.19267741\n",
      "Iteration 943, loss = 0.19252442\n",
      "Iteration 944, loss = 0.19237171\n",
      "Iteration 945, loss = 0.19221929\n",
      "Iteration 946, loss = 0.19206694\n",
      "Iteration 947, loss = 0.19191468\n",
      "Iteration 948, loss = 0.19176265\n",
      "Iteration 949, loss = 0.19161091\n",
      "Iteration 950, loss = 0.19145947\n",
      "Iteration 951, loss = 0.19130829\n",
      "Iteration 952, loss = 0.19115741\n",
      "Iteration 953, loss = 0.19100686\n",
      "Iteration 954, loss = 0.19085655\n",
      "Iteration 955, loss = 0.19070657\n",
      "Iteration 956, loss = 0.19055688\n",
      "Iteration 957, loss = 0.19040751\n",
      "Iteration 958, loss = 0.19025844\n",
      "Iteration 959, loss = 0.19010966\n",
      "Iteration 960, loss = 0.18996120\n",
      "Iteration 961, loss = 0.18981304\n",
      "Iteration 962, loss = 0.18966518\n",
      "Iteration 963, loss = 0.18951765\n",
      "Iteration 964, loss = 0.18937040\n",
      "Iteration 965, loss = 0.18922346\n",
      "Iteration 966, loss = 0.18907684\n",
      "Iteration 967, loss = 0.18893052\n",
      "Iteration 968, loss = 0.18878452\n",
      "Iteration 969, loss = 0.18863881\n",
      "Iteration 970, loss = 0.18849339\n",
      "Iteration 971, loss = 0.18834827\n",
      "Iteration 972, loss = 0.18820347\n",
      "Iteration 973, loss = 0.18805897\n",
      "Iteration 974, loss = 0.18791491\n",
      "Iteration 975, loss = 0.18777161\n",
      "Iteration 976, loss = 0.18762854\n",
      "Iteration 977, loss = 0.18748578\n",
      "Iteration 978, loss = 0.18734333\n",
      "Iteration 979, loss = 0.18720119\n",
      "Iteration 980, loss = 0.18705936\n",
      "Iteration 981, loss = 0.18691784\n",
      "Iteration 982, loss = 0.18677663\n",
      "Iteration 983, loss = 0.18663575\n",
      "Iteration 984, loss = 0.18649515\n",
      "Iteration 985, loss = 0.18635627\n",
      "Iteration 986, loss = 0.18621787\n",
      "Iteration 987, loss = 0.18607985\n",
      "Iteration 988, loss = 0.18594215\n",
      "Iteration 989, loss = 0.18580532\n",
      "Iteration 990, loss = 0.18566935\n",
      "Iteration 991, loss = 0.18553369\n",
      "Iteration 992, loss = 0.18539837\n",
      "Iteration 993, loss = 0.18526334\n",
      "Iteration 994, loss = 0.18512860\n",
      "Iteration 995, loss = 0.18499419\n",
      "Iteration 996, loss = 0.18486005\n",
      "Iteration 997, loss = 0.18472622\n",
      "Iteration 998, loss = 0.18459269\n",
      "Iteration 999, loss = 0.18445970\n",
      "Iteration 1000, loss = 0.18432704\n",
      "Iteration 1, loss = 1.66585275\n",
      "Iteration 2, loss = 1.62912514\n",
      "Iteration 3, loss = 1.57983559\n",
      "Iteration 4, loss = 1.52226751\n",
      "Iteration 5, loss = 1.46080908\n",
      "Iteration 6, loss = 1.39990779\n",
      "Iteration 7, loss = 1.34341118\n",
      "Iteration 8, loss = 1.29438294\n",
      "Iteration 9, loss = 1.25481799\n",
      "Iteration 10, loss = 1.22501209\n",
      "Iteration 11, loss = 1.20368430\n",
      "Iteration 12, loss = 1.18843672\n",
      "Iteration 13, loss = 1.17651553\n",
      "Iteration 14, loss = 1.16538625\n",
      "Iteration 15, loss = 1.15314176\n",
      "Iteration 16, loss = 1.13889774\n",
      "Iteration 17, loss = 1.12203322\n",
      "Iteration 18, loss = 1.10283474\n",
      "Iteration 19, loss = 1.08177142\n",
      "Iteration 20, loss = 1.05990638\n",
      "Iteration 21, loss = 1.03833776\n",
      "Iteration 22, loss = 1.01791930\n",
      "Iteration 23, loss = 0.99930274\n",
      "Iteration 24, loss = 0.98260565\n",
      "Iteration 25, loss = 0.96774938\n",
      "Iteration 26, loss = 0.95445292\n",
      "Iteration 27, loss = 0.94243325\n",
      "Iteration 28, loss = 0.93130953\n",
      "Iteration 29, loss = 0.92074937\n",
      "Iteration 30, loss = 0.91050598\n",
      "Iteration 31, loss = 0.90045733\n",
      "Iteration 32, loss = 0.89059965\n",
      "Iteration 33, loss = 0.88108193\n",
      "Iteration 34, loss = 0.87189368\n",
      "Iteration 35, loss = 0.86293818\n",
      "Iteration 36, loss = 0.85427372\n",
      "Iteration 37, loss = 0.84585682\n",
      "Iteration 38, loss = 0.83771309\n",
      "Iteration 39, loss = 0.82984559\n",
      "Iteration 40, loss = 0.82225928\n",
      "Iteration 41, loss = 0.81494155\n",
      "Iteration 42, loss = 0.80782838\n",
      "Iteration 43, loss = 0.80091809\n",
      "Iteration 44, loss = 0.79421374\n",
      "Iteration 45, loss = 0.78770388\n",
      "Iteration 46, loss = 0.78134704"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 47, loss = 0.77511591\n",
      "Iteration 48, loss = 0.76900163\n",
      "Iteration 49, loss = 0.76303755\n",
      "Iteration 50, loss = 0.75725460\n",
      "Iteration 51, loss = 0.75162858\n",
      "Iteration 52, loss = 0.74616704\n",
      "Iteration 53, loss = 0.74084629\n",
      "Iteration 54, loss = 0.73568476\n",
      "Iteration 55, loss = 0.73065207\n",
      "Iteration 56, loss = 0.72574492\n",
      "Iteration 57, loss = 0.72095590\n",
      "Iteration 58, loss = 0.71627337\n",
      "Iteration 59, loss = 0.71168795\n",
      "Iteration 60, loss = 0.70719534\n",
      "Iteration 61, loss = 0.70279262\n",
      "Iteration 62, loss = 0.69847839\n",
      "Iteration 63, loss = 0.69425099\n",
      "Iteration 64, loss = 0.69010931\n",
      "Iteration 65, loss = 0.68605211\n",
      "Iteration 66, loss = 0.68207789\n",
      "Iteration 67, loss = 0.67818490\n",
      "Iteration 68, loss = 0.67437162\n",
      "Iteration 69, loss = 0.67063617\n",
      "Iteration 70, loss = 0.66697577\n",
      "Iteration 71, loss = 0.66338678\n",
      "Iteration 72, loss = 0.65986625\n",
      "Iteration 73, loss = 0.65641188\n",
      "Iteration 74, loss = 0.65302223\n",
      "Iteration 75, loss = 0.64969525\n",
      "Iteration 76, loss = 0.64642940\n",
      "Iteration 77, loss = 0.64322294\n",
      "Iteration 78, loss = 0.64007430\n",
      "Iteration 79, loss = 0.63698141\n",
      "Iteration 80, loss = 0.63394365\n",
      "Iteration 81, loss = 0.63095942\n",
      "Iteration 82, loss = 0.62802721\n",
      "Iteration 83, loss = 0.62514553\n",
      "Iteration 84, loss = 0.62231290\n",
      "Iteration 85, loss = 0.61952809\n",
      "Iteration 86, loss = 0.61678963\n",
      "Iteration 87, loss = 0.61409630\n",
      "Iteration 88, loss = 0.61144691\n",
      "Iteration 89, loss = 0.60884242\n",
      "Iteration 90, loss = 0.60628078\n",
      "Iteration 91, loss = 0.60375993\n",
      "Iteration 92, loss = 0.60128151\n",
      "Iteration 93, loss = 0.59884169\n",
      "Iteration 94, loss = 0.59643985\n",
      "Iteration 95, loss = 0.59407517\n",
      "Iteration 96, loss = 0.59174677\n",
      "Iteration 97, loss = 0.58945299\n",
      "Iteration 98, loss = 0.58719301\n",
      "Iteration 99, loss = 0.58496685\n",
      "Iteration 100, loss = 0.58277320\n",
      "Iteration 101, loss = 0.58061100\n",
      "Iteration 102, loss = 0.57847946\n",
      "Iteration 103, loss = 0.57637791\n",
      "Iteration 104, loss = 0.57430505\n",
      "Iteration 105, loss = 0.57226001\n",
      "Iteration 106, loss = 0.57024221\n",
      "Iteration 107, loss = 0.56825118\n",
      "Iteration 108, loss = 0.56628666\n",
      "Iteration 109, loss = 0.56434806\n",
      "Iteration 110, loss = 0.56243474\n",
      "Iteration 111, loss = 0.56054624\n",
      "Iteration 112, loss = 0.55868193\n",
      "Iteration 113, loss = 0.55684043\n",
      "Iteration 114, loss = 0.55502153\n",
      "Iteration 115, loss = 0.55322510\n",
      "Iteration 116, loss = 0.55145070\n",
      "Iteration 117, loss = 0.54969776\n",
      "Iteration 118, loss = 0.54796524\n",
      "Iteration 119, loss = 0.54625279\n",
      "Iteration 120, loss = 0.54455986\n",
      "Iteration 121, loss = 0.54288659\n",
      "Iteration 122, loss = 0.54123261\n",
      "Iteration 123, loss = 0.53959709\n",
      "Iteration 124, loss = 0.53797880\n",
      "Iteration 125, loss = 0.53637707\n",
      "Iteration 126, loss = 0.53479151\n",
      "Iteration 127, loss = 0.53322272\n",
      "Iteration 128, loss = 0.53167017\n",
      "Iteration 129, loss = 0.53013359\n",
      "Iteration 130, loss = 0.52861245\n",
      "Iteration 131, loss = 0.52710602\n",
      "Iteration 132, loss = 0.52561483\n",
      "Iteration 133, loss = 0.52413877\n",
      "Iteration 134, loss = 0.52267776\n",
      "Iteration 135, loss = 0.52123120\n",
      "Iteration 136, loss = 0.51979967\n",
      "Iteration 137, loss = 0.51838115\n",
      "Iteration 138, loss = 0.51697451\n",
      "Iteration 139, loss = 0.51557854\n",
      "Iteration 140, loss = 0.51419559\n",
      "Iteration 141, loss = 0.51282552\n",
      "Iteration 142, loss = 0.51146807\n",
      "Iteration 143, loss = 0.51012325\n",
      "Iteration 144, loss = 0.50879074\n",
      "Iteration 145, loss = 0.50746894\n",
      "Iteration 146, loss = 0.50615689\n",
      "Iteration 147, loss = 0.50485506\n",
      "Iteration 148, loss = 0.50356295\n",
      "Iteration 149, loss = 0.50228059\n",
      "Iteration 150, loss = 0.50100516\n",
      "Iteration 151, loss = 0.49973942\n",
      "Iteration 152, loss = 0.49848045\n",
      "Iteration 153, loss = 0.49722992\n",
      "Iteration 154, loss = 0.49598862\n",
      "Iteration 155, loss = 0.49475904\n",
      "Iteration 156, loss = 0.49353937\n",
      "Iteration 157, loss = 0.49232882\n",
      "Iteration 158, loss = 0.49112444\n",
      "Iteration 159, loss = 0.48992819\n",
      "Iteration 160, loss = 0.48873880\n",
      "Iteration 161, loss = 0.48755453\n",
      "Iteration 162, loss = 0.48638142\n",
      "Iteration 163, loss = 0.48521745\n",
      "Iteration 164, loss = 0.48405884\n",
      "Iteration 165, loss = 0.48290532\n",
      "Iteration 166, loss = 0.48175631\n",
      "Iteration 167, loss = 0.48061256\n",
      "Iteration 168, loss = 0.47947297\n",
      "Iteration 169, loss = 0.47833535\n",
      "Iteration 170, loss = 0.47720470\n",
      "Iteration 171, loss = 0.47608845\n",
      "Iteration 172, loss = 0.47497257\n",
      "Iteration 173, loss = 0.47386680\n",
      "Iteration 174, loss = 0.47277458\n",
      "Iteration 175, loss = 0.47168963\n",
      "Iteration 176, loss = 0.47061787\n",
      "Iteration 177, loss = 0.46956309\n",
      "Iteration 178, loss = 0.46851933\n",
      "Iteration 179, loss = 0.46748516\n",
      "Iteration 180, loss = 0.46646055\n",
      "Iteration 181, loss = 0.46545180\n",
      "Iteration 182, loss = 0.46445693\n",
      "Iteration 183, loss = 0.46347074\n",
      "Iteration 184, loss = 0.46249439\n",
      "Iteration 185, loss = 0.46152486\n",
      "Iteration 186, loss = 0.46056242\n",
      "Iteration 187, loss = 0.45960861\n",
      "Iteration 188, loss = 0.45866158\n",
      "Iteration 189, loss = 0.45772041\n",
      "Iteration 190, loss = 0.45678621\n",
      "Iteration 191, loss = 0.45585680\n",
      "Iteration 192, loss = 0.45493312\n",
      "Iteration 193, loss = 0.45401510\n",
      "Iteration 194, loss = 0.45310241\n",
      "Iteration 195, loss = 0.45219477\n",
      "Iteration 196, loss = 0.45129210\n",
      "Iteration 197, loss = 0.45039419\n",
      "Iteration 198, loss = 0.44950126\n",
      "Iteration 199, loss = 0.44861294\n",
      "Iteration 200, loss = 0.44772897\n",
      "Iteration 201, loss = 0.44684954\n",
      "Iteration 202, loss = 0.44597458\n",
      "Iteration 203, loss = 0.44510430\n",
      "Iteration 204, loss = 0.44423863\n",
      "Iteration 205, loss = 0.44337709\n",
      "Iteration 206, loss = 0.44251956\n",
      "Iteration 207, loss = 0.44166610\n",
      "Iteration 208, loss = 0.44081687\n",
      "Iteration 209, loss = 0.43997233\n",
      "Iteration 210, loss = 0.43913189\n",
      "Iteration 211, loss = 0.43829558\n",
      "Iteration 212, loss = 0.43746322\n",
      "Iteration 213, loss = 0.43663486\n",
      "Iteration 214, loss = 0.43581049\n",
      "Iteration 215, loss = 0.43498978\n",
      "Iteration 216, loss = 0.43417295\n",
      "Iteration 217, loss = 0.43335981\n",
      "Iteration 218, loss = 0.43255051\n",
      "Iteration 219, loss = 0.43174495\n",
      "Iteration 220, loss = 0.43094251\n",
      "Iteration 221, loss = 0.43014322\n",
      "Iteration 222, loss = 0.42934664\n",
      "Iteration 223, loss = 0.42855306\n",
      "Iteration 224, loss = 0.42776282\n",
      "Iteration 225, loss = 0.42697540\n",
      "Iteration 226, loss = 0.42619063\n",
      "Iteration 227, loss = 0.42540851\n",
      "Iteration 228, loss = 0.42462929\n",
      "Iteration 229, loss = 0.42385306\n",
      "Iteration 230, loss = 0.42307976\n",
      "Iteration 231, loss = 0.42231044\n",
      "Iteration 232, loss = 0.42154389\n",
      "Iteration 233, loss = 0.42077996\n",
      "Iteration 234, loss = 0.42001900\n",
      "Iteration 235, loss = 0.41926080\n",
      "Iteration 236, loss = 0.41850525\n",
      "Iteration 237, loss = 0.41775266\n",
      "Iteration 238, loss = 0.41700280\n",
      "Iteration 239, loss = 0.41625629\n",
      "Iteration 240, loss = 0.41551262\n",
      "Iteration 241, loss = 0.41477153\n",
      "Iteration 242, loss = 0.41403307\n",
      "Iteration 243, loss = 0.41329800\n",
      "Iteration 244, loss = 0.41256564\n",
      "Iteration 245, loss = 0.41183560\n",
      "Iteration 246, loss = 0.41110784\n",
      "Iteration 247, loss = 0.41038256\n",
      "Iteration 248, loss = 0.40966047\n",
      "Iteration 249, loss = 0.40894084\n",
      "Iteration 250, loss = 0.40822372\n",
      "Iteration 251, loss = 0.40750898\n",
      "Iteration 252, loss = 0.40679657\n",
      "Iteration 253, loss = 0.40608656\n",
      "Iteration 254, loss = 0.40537910\n",
      "Iteration 255, loss = 0.40467502\n",
      "Iteration 256, loss = 0.40397435\n",
      "Iteration 257, loss = 0.40327604\n",
      "Iteration 258, loss = 0.40258048\n",
      "Iteration 259, loss = 0.40188764\n",
      "Iteration 260, loss = 0.40119791\n",
      "Iteration 261, loss = 0.40051069\n",
      "Iteration 262, loss = 0.39982579\n",
      "Iteration 263, loss = 0.39914305\n",
      "Iteration 264, loss = 0.39846338\n",
      "Iteration 265, loss = 0.39778699\n",
      "Iteration 266, loss = 0.39711322\n",
      "Iteration 267, loss = 0.39644164\n",
      "Iteration 268, loss = 0.39577220\n",
      "Iteration 269, loss = 0.39510583\n",
      "Iteration 270, loss = 0.39444234\n",
      "Iteration 271, loss = 0.39378114\n",
      "Iteration 272, loss = 0.39312277\n",
      "Iteration 273, loss = 0.39246668\n",
      "Iteration 274, loss = 0.39181309\n",
      "Iteration 275, loss = 0.39116174\n",
      "Iteration 276, loss = 0.39051241\n",
      "Iteration 277, loss = 0.38986547\n",
      "Iteration 278, loss = 0.38922048\n",
      "Iteration 279, loss = 0.38857752\n",
      "Iteration 280, loss = 0.38793648\n",
      "Iteration 281, loss = 0.38729756\n",
      "Iteration 282, loss = 0.38666052\n",
      "Iteration 283, loss = 0.38602549\n",
      "Iteration 284, loss = 0.38539223\n",
      "Iteration 285, loss = 0.38476072\n",
      "Iteration 286, loss = 0.38413119\n",
      "Iteration 287, loss = 0.38350347\n",
      "Iteration 288, loss = 0.38287751\n",
      "Iteration 289, loss = 0.38225333\n",
      "Iteration 290, loss = 0.38163097\n",
      "Iteration 291, loss = 0.38101036\n",
      "Iteration 292, loss = 0.38039181\n",
      "Iteration 293, loss = 0.37977497\n",
      "Iteration 294, loss = 0.37916014\n",
      "Iteration 295, loss = 0.37854704\n",
      "Iteration 296, loss = 0.37793566\n",
      "Iteration 297, loss = 0.37732601\n",
      "Iteration 298, loss = 0.37671809\n",
      "Iteration 299, loss = 0.37611182\n",
      "Iteration 300, loss = 0.37550728\n",
      "Iteration 301, loss = 0.37490441\n",
      "Iteration 302, loss = 0.37430318\n",
      "Iteration 303, loss = 0.37370353\n",
      "Iteration 304, loss = 0.37310530\n",
      "Iteration 305, loss = 0.37250865\n",
      "Iteration 306, loss = 0.37191351\n",
      "Iteration 307, loss = 0.37132002\n",
      "Iteration 308, loss = 0.37072803\n",
      "Iteration 309, loss = 0.37013769\n",
      "Iteration 310, loss = 0.36954875\n",
      "Iteration 311, loss = 0.36896146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 312, loss = 0.36837589\n",
      "Iteration 313, loss = 0.36779187\n",
      "Iteration 314, loss = 0.36720924\n",
      "Iteration 315, loss = 0.36662815\n",
      "Iteration 316, loss = 0.36604835\n",
      "Iteration 317, loss = 0.36547038\n",
      "Iteration 318, loss = 0.36489394\n",
      "Iteration 319, loss = 0.36431902\n",
      "Iteration 320, loss = 0.36374561\n",
      "Iteration 321, loss = 0.36317358\n",
      "Iteration 322, loss = 0.36260306\n",
      "Iteration 323, loss = 0.36203397\n",
      "Iteration 324, loss = 0.36146645\n",
      "Iteration 325, loss = 0.36090017\n",
      "Iteration 326, loss = 0.36033551\n",
      "Iteration 327, loss = 0.35977206\n",
      "Iteration 328, loss = 0.35920987\n",
      "Iteration 329, loss = 0.35864901\n",
      "Iteration 330, loss = 0.35808940\n",
      "Iteration 331, loss = 0.35753127\n",
      "Iteration 332, loss = 0.35697466\n",
      "Iteration 333, loss = 0.35641976\n",
      "Iteration 334, loss = 0.35586599\n",
      "Iteration 335, loss = 0.35531365\n",
      "Iteration 336, loss = 0.35476246\n",
      "Iteration 337, loss = 0.35421259\n",
      "Iteration 338, loss = 0.35366400\n",
      "Iteration 339, loss = 0.35311678\n",
      "Iteration 340, loss = 0.35257075\n",
      "Iteration 341, loss = 0.35202643\n",
      "Iteration 342, loss = 0.35148330\n",
      "Iteration 343, loss = 0.35094139\n",
      "Iteration 344, loss = 0.35040077\n",
      "Iteration 345, loss = 0.34986131\n",
      "Iteration 346, loss = 0.34932324\n",
      "Iteration 347, loss = 0.34878650\n",
      "Iteration 348, loss = 0.34825117\n",
      "Iteration 349, loss = 0.34771688\n",
      "Iteration 350, loss = 0.34718387\n",
      "Iteration 351, loss = 0.34665215\n",
      "Iteration 352, loss = 0.34612171\n",
      "Iteration 353, loss = 0.34559245\n",
      "Iteration 354, loss = 0.34506460\n",
      "Iteration 355, loss = 0.34453797\n",
      "Iteration 356, loss = 0.34401255\n",
      "Iteration 357, loss = 0.34348834\n",
      "Iteration 358, loss = 0.34296526\n",
      "Iteration 359, loss = 0.34244344\n",
      "Iteration 360, loss = 0.34192284\n",
      "Iteration 361, loss = 0.34140366\n",
      "Iteration 362, loss = 0.34088559\n",
      "Iteration 363, loss = 0.34036879\n",
      "Iteration 364, loss = 0.33985313\n",
      "Iteration 365, loss = 0.33933864\n",
      "Iteration 366, loss = 0.33882531\n",
      "Iteration 367, loss = 0.33831328\n",
      "Iteration 368, loss = 0.33780265\n",
      "Iteration 369, loss = 0.33729306\n",
      "Iteration 370, loss = 0.33678488\n",
      "Iteration 371, loss = 0.33627774\n",
      "Iteration 372, loss = 0.33577204\n",
      "Iteration 373, loss = 0.33526741\n",
      "Iteration 374, loss = 0.33476388\n",
      "Iteration 375, loss = 0.33426152\n",
      "Iteration 376, loss = 0.33376027\n",
      "Iteration 377, loss = 0.33326027\n",
      "Iteration 378, loss = 0.33276133\n",
      "Iteration 379, loss = 0.33226364\n",
      "Iteration 380, loss = 0.33176697\n",
      "Iteration 381, loss = 0.33127140\n",
      "Iteration 382, loss = 0.33077697\n",
      "Iteration 383, loss = 0.33028360\n",
      "Iteration 384, loss = 0.32979143\n",
      "Iteration 385, loss = 0.32930033\n",
      "Iteration 386, loss = 0.32881052\n",
      "Iteration 387, loss = 0.32832170\n",
      "Iteration 388, loss = 0.32783401\n",
      "Iteration 389, loss = 0.32734735\n",
      "Iteration 390, loss = 0.32686191\n",
      "Iteration 391, loss = 0.32637729\n",
      "Iteration 392, loss = 0.32589389\n",
      "Iteration 393, loss = 0.32541142\n",
      "Iteration 394, loss = 0.32492997\n",
      "Iteration 395, loss = 0.32444964\n",
      "Iteration 396, loss = 0.32397025\n",
      "Iteration 397, loss = 0.32349202\n",
      "Iteration 398, loss = 0.32301477\n",
      "Iteration 399, loss = 0.32253864\n",
      "Iteration 400, loss = 0.32206348\n",
      "Iteration 401, loss = 0.32158955\n",
      "Iteration 402, loss = 0.32111654\n",
      "Iteration 403, loss = 0.32064457\n",
      "Iteration 404, loss = 0.32017361\n",
      "Iteration 405, loss = 0.31970370\n",
      "Iteration 406, loss = 0.31923487\n",
      "Iteration 407, loss = 0.31876707\n",
      "Iteration 408, loss = 0.31830032\n",
      "Iteration 409, loss = 0.31783451\n",
      "Iteration 410, loss = 0.31736984\n",
      "Iteration 411, loss = 0.31690602\n",
      "Iteration 412, loss = 0.31644324\n",
      "Iteration 413, loss = 0.31598133\n",
      "Iteration 414, loss = 0.31552046\n",
      "Iteration 415, loss = 0.31506049\n",
      "Iteration 416, loss = 0.31460164\n",
      "Iteration 417, loss = 0.31414369\n",
      "Iteration 418, loss = 0.31368683\n",
      "Iteration 419, loss = 0.31323094\n",
      "Iteration 420, loss = 0.31277612\n",
      "Iteration 421, loss = 0.31232229\n",
      "Iteration 422, loss = 0.31186945\n",
      "Iteration 423, loss = 0.31141757\n",
      "Iteration 424, loss = 0.31096661\n",
      "Iteration 425, loss = 0.31051675\n",
      "Iteration 426, loss = 0.31006770\n",
      "Iteration 427, loss = 0.30961971\n",
      "Iteration 428, loss = 0.30917261\n",
      "Iteration 429, loss = 0.30872651\n",
      "Iteration 430, loss = 0.30828130\n",
      "Iteration 431, loss = 0.30783716\n",
      "Iteration 432, loss = 0.30739357\n",
      "Iteration 433, loss = 0.30695100\n",
      "Iteration 434, loss = 0.30650928\n",
      "Iteration 435, loss = 0.30606854\n",
      "Iteration 436, loss = 0.30562869\n",
      "Iteration 437, loss = 0.30518991\n",
      "Iteration 438, loss = 0.30475202\n",
      "Iteration 439, loss = 0.30431519\n",
      "Iteration 440, loss = 0.30387922\n",
      "Iteration 441, loss = 0.30344424\n",
      "Iteration 442, loss = 0.30301024\n",
      "Iteration 443, loss = 0.30257711\n",
      "Iteration 444, loss = 0.30214499\n",
      "Iteration 445, loss = 0.30171403\n",
      "Iteration 446, loss = 0.30128397\n",
      "Iteration 447, loss = 0.30085490\n",
      "Iteration 448, loss = 0.30042666\n",
      "Iteration 449, loss = 0.29999906\n",
      "Iteration 450, loss = 0.29957225\n",
      "Iteration 451, loss = 0.29914648\n",
      "Iteration 452, loss = 0.29872146\n",
      "Iteration 453, loss = 0.29829748\n",
      "Iteration 454, loss = 0.29787434\n",
      "Iteration 455, loss = 0.29745222\n",
      "Iteration 456, loss = 0.29703094\n",
      "Iteration 457, loss = 0.29661067\n",
      "Iteration 458, loss = 0.29619057\n",
      "Iteration 459, loss = 0.29577016\n",
      "Iteration 460, loss = 0.29535041\n",
      "Iteration 461, loss = 0.29493153\n",
      "Iteration 462, loss = 0.29451328\n",
      "Iteration 463, loss = 0.29409488\n",
      "Iteration 464, loss = 0.29367717\n",
      "Iteration 465, loss = 0.29326024\n",
      "Iteration 466, loss = 0.29284396\n",
      "Iteration 467, loss = 0.29242856\n",
      "Iteration 468, loss = 0.29201382\n",
      "Iteration 469, loss = 0.29159994\n",
      "Iteration 470, loss = 0.29118674\n",
      "Iteration 471, loss = 0.29077385\n",
      "Iteration 472, loss = 0.29036173\n",
      "Iteration 473, loss = 0.28995040\n",
      "Iteration 474, loss = 0.28953993\n",
      "Iteration 475, loss = 0.28913020\n",
      "Iteration 476, loss = 0.28872136\n",
      "Iteration 477, loss = 0.28831325\n",
      "Iteration 478, loss = 0.28790611\n",
      "Iteration 479, loss = 0.28749970\n",
      "Iteration 480, loss = 0.28709369\n",
      "Iteration 481, loss = 0.28668769\n",
      "Iteration 482, loss = 0.28628066\n",
      "Iteration 483, loss = 0.28587311\n",
      "Iteration 484, loss = 0.28546562\n",
      "Iteration 485, loss = 0.28505672\n",
      "Iteration 486, loss = 0.28464516\n",
      "Iteration 487, loss = 0.28423128\n",
      "Iteration 488, loss = 0.28381406\n",
      "Iteration 489, loss = 0.28339633\n",
      "Iteration 490, loss = 0.28297545\n",
      "Iteration 491, loss = 0.28255128\n",
      "Iteration 492, loss = 0.28212393\n",
      "Iteration 493, loss = 0.28169580\n",
      "Iteration 494, loss = 0.28126610\n",
      "Iteration 495, loss = 0.28083002\n",
      "Iteration 496, loss = 0.28038641\n",
      "Iteration 497, loss = 0.27993901\n",
      "Iteration 498, loss = 0.27948742\n",
      "Iteration 499, loss = 0.27902972\n",
      "Iteration 500, loss = 0.27856781\n",
      "Iteration 501, loss = 0.27810055\n",
      "Iteration 502, loss = 0.27762144\n",
      "Iteration 503, loss = 0.27713427\n",
      "Iteration 504, loss = 0.27664019\n",
      "Iteration 505, loss = 0.27614470\n",
      "Iteration 506, loss = 0.27565012\n",
      "Iteration 507, loss = 0.27515335\n",
      "Iteration 508, loss = 0.27466147\n",
      "Iteration 509, loss = 0.27416967\n",
      "Iteration 510, loss = 0.27367651\n",
      "Iteration 511, loss = 0.27319198\n",
      "Iteration 512, loss = 0.27271428\n",
      "Iteration 513, loss = 0.27225324\n",
      "Iteration 514, loss = 0.27179816\n",
      "Iteration 515, loss = 0.27135931\n",
      "Iteration 516, loss = 0.27093160\n",
      "Iteration 517, loss = 0.27050476\n",
      "Iteration 518, loss = 0.27007952\n",
      "Iteration 519, loss = 0.26966249\n",
      "Iteration 520, loss = 0.26925553\n",
      "Iteration 521, loss = 0.26885782\n",
      "Iteration 522, loss = 0.26846547\n",
      "Iteration 523, loss = 0.26807831\n",
      "Iteration 524, loss = 0.26769483\n",
      "Iteration 525, loss = 0.26731436\n",
      "Iteration 526, loss = 0.26693908\n",
      "Iteration 527, loss = 0.26656360\n",
      "Iteration 528, loss = 0.26619036\n",
      "Iteration 529, loss = 0.26582202\n",
      "Iteration 530, loss = 0.26545761\n",
      "Iteration 531, loss = 0.26509477\n",
      "Iteration 532, loss = 0.26473332\n",
      "Iteration 533, loss = 0.26437405\n",
      "Iteration 534, loss = 0.26401625\n",
      "Iteration 535, loss = 0.26365974\n",
      "Iteration 536, loss = 0.26330430\n",
      "Iteration 537, loss = 0.26295007\n",
      "Iteration 538, loss = 0.26259725\n",
      "Iteration 539, loss = 0.26224636\n",
      "Iteration 540, loss = 0.26189704\n",
      "Iteration 541, loss = 0.26155028\n",
      "Iteration 542, loss = 0.26120501\n",
      "Iteration 543, loss = 0.26086077\n",
      "Iteration 544, loss = 0.26051744\n",
      "Iteration 545, loss = 0.26017569\n",
      "Iteration 546, loss = 0.25983507\n",
      "Iteration 547, loss = 0.25949526\n",
      "Iteration 548, loss = 0.25915642\n",
      "Iteration 549, loss = 0.25881825\n",
      "Iteration 550, loss = 0.25848100\n",
      "Iteration 551, loss = 0.25814462\n",
      "Iteration 552, loss = 0.25780927\n",
      "Iteration 553, loss = 0.25747465\n",
      "Iteration 554, loss = 0.25714084\n",
      "Iteration 555, loss = 0.25680788\n",
      "Iteration 556, loss = 0.25647559\n",
      "Iteration 557, loss = 0.25614419\n",
      "Iteration 558, loss = 0.25581377\n",
      "Iteration 559, loss = 0.25548411\n",
      "Iteration 560, loss = 0.25515526\n",
      "Iteration 561, loss = 0.25482718\n",
      "Iteration 562, loss = 0.25449987\n",
      "Iteration 563, loss = 0.25417331\n",
      "Iteration 564, loss = 0.25384756\n",
      "Iteration 565, loss = 0.25352252\n",
      "Iteration 566, loss = 0.25319827\n",
      "Iteration 567, loss = 0.25287467\n",
      "Iteration 568, loss = 0.25255181\n",
      "Iteration 569, loss = 0.25222972\n",
      "Iteration 570, loss = 0.25190846\n",
      "Iteration 571, loss = 0.25158779\n",
      "Iteration 572, loss = 0.25126785\n",
      "Iteration 573, loss = 0.25094862\n",
      "Iteration 574, loss = 0.25063012\n",
      "Iteration 575, loss = 0.25031229\n",
      "Iteration 576, loss = 0.24999518\n",
      "Iteration 577, loss = 0.24967873\n",
      "Iteration 578, loss = 0.24936298\n",
      "Iteration 579, loss = 0.24904803\n",
      "Iteration 580, loss = 0.24873390\n",
      "Iteration 581, loss = 0.24842041\n",
      "Iteration 582, loss = 0.24810770\n",
      "Iteration 583, loss = 0.24779567\n",
      "Iteration 584, loss = 0.24748437\n",
      "Iteration 585, loss = 0.24717382\n",
      "Iteration 586, loss = 0.24686389\n",
      "Iteration 587, loss = 0.24655468\n",
      "Iteration 588, loss = 0.24624614\n",
      "Iteration 589, loss = 0.24593828\n",
      "Iteration 590, loss = 0.24563108\n",
      "Iteration 591, loss = 0.24532458\n",
      "Iteration 592, loss = 0.24501871\n",
      "Iteration 593, loss = 0.24471353\n",
      "Iteration 594, loss = 0.24440913\n",
      "Iteration 595, loss = 0.24410533\n",
      "Iteration 596, loss = 0.24380221\n",
      "Iteration 597, loss = 0.24349977\n",
      "Iteration 598, loss = 0.24319794\n",
      "Iteration 599, loss = 0.24289683\n",
      "Iteration 600, loss = 0.24259637\n",
      "Iteration 601, loss = 0.24229661\n",
      "Iteration 602, loss = 0.24199749\n",
      "Iteration 603, loss = 0.24169901\n",
      "Iteration 604, loss = 0.24140125\n",
      "Iteration 605, loss = 0.24110411\n",
      "Iteration 606, loss = 0.24080764\n",
      "Iteration 607, loss = 0.24051183\n",
      "Iteration 608, loss = 0.24021665\n",
      "Iteration 609, loss = 0.23992211\n",
      "Iteration 610, loss = 0.23962820\n",
      "Iteration 611, loss = 0.23933495\n",
      "Iteration 612, loss = 0.23904237\n",
      "Iteration 613, loss = 0.23875036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 614, loss = 0.23845901\n",
      "Iteration 615, loss = 0.23816831\n",
      "Iteration 616, loss = 0.23787825\n",
      "Iteration 617, loss = 0.23758883\n",
      "Iteration 618, loss = 0.23730009\n",
      "Iteration 619, loss = 0.23701189\n",
      "Iteration 620, loss = 0.23672432\n",
      "Iteration 621, loss = 0.23643740\n",
      "Iteration 622, loss = 0.23615108\n",
      "Iteration 623, loss = 0.23586538\n",
      "Iteration 624, loss = 0.23558029\n",
      "Iteration 625, loss = 0.23529590\n",
      "Iteration 626, loss = 0.23501204\n",
      "Iteration 627, loss = 0.23472884\n",
      "Iteration 628, loss = 0.23444627\n",
      "Iteration 629, loss = 0.23416429\n",
      "Iteration 630, loss = 0.23388297\n",
      "Iteration 631, loss = 0.23360222\n",
      "Iteration 632, loss = 0.23332210\n",
      "Iteration 633, loss = 0.23304257\n",
      "Iteration 634, loss = 0.23276365\n",
      "Iteration 635, loss = 0.23248533\n",
      "Iteration 636, loss = 0.23220767\n",
      "Iteration 637, loss = 0.23193055\n",
      "Iteration 638, loss = 0.23165405\n",
      "Iteration 639, loss = 0.23137817\n",
      "Iteration 640, loss = 0.23110286\n",
      "Iteration 641, loss = 0.23082819\n",
      "Iteration 642, loss = 0.23055408\n",
      "Iteration 643, loss = 0.23028057\n",
      "Iteration 644, loss = 0.23000764\n",
      "Iteration 645, loss = 0.22973530\n",
      "Iteration 646, loss = 0.22946362\n",
      "Iteration 647, loss = 0.22919243\n",
      "Iteration 648, loss = 0.22892187\n",
      "Iteration 649, loss = 0.22865187\n",
      "Iteration 650, loss = 0.22838247\n",
      "Iteration 651, loss = 0.22811361\n",
      "Iteration 652, loss = 0.22784535\n",
      "Iteration 653, loss = 0.22757766\n",
      "Iteration 654, loss = 0.22731057\n",
      "Iteration 655, loss = 0.22704397\n",
      "Iteration 656, loss = 0.22677781\n",
      "Iteration 657, loss = 0.22651223\n",
      "Iteration 658, loss = 0.22624720\n",
      "Iteration 659, loss = 0.22598274\n",
      "Iteration 660, loss = 0.22571888\n",
      "Iteration 661, loss = 0.22545559\n",
      "Iteration 662, loss = 0.22519284\n",
      "Iteration 663, loss = 0.22493061\n",
      "Iteration 664, loss = 0.22466891\n",
      "Iteration 665, loss = 0.22440772\n",
      "Iteration 666, loss = 0.22414710\n",
      "Iteration 667, loss = 0.22388703\n",
      "Iteration 668, loss = 0.22362754\n",
      "Iteration 669, loss = 0.22336855\n",
      "Iteration 670, loss = 0.22311010\n",
      "Iteration 671, loss = 0.22285220\n",
      "Iteration 672, loss = 0.22259480\n",
      "Iteration 673, loss = 0.22233784\n",
      "Iteration 674, loss = 0.22208140\n",
      "Iteration 675, loss = 0.22182549\n",
      "Iteration 676, loss = 0.22157013\n",
      "Iteration 677, loss = 0.22131523\n",
      "Iteration 678, loss = 0.22106090\n",
      "Iteration 679, loss = 0.22080708\n",
      "Iteration 680, loss = 0.22055377\n",
      "Iteration 681, loss = 0.22030101\n",
      "Iteration 682, loss = 0.22004878\n",
      "Iteration 683, loss = 0.21979711\n",
      "Iteration 684, loss = 0.21954599\n",
      "Iteration 685, loss = 0.21929534\n",
      "Iteration 686, loss = 0.21904523\n",
      "Iteration 687, loss = 0.21879569\n",
      "Iteration 688, loss = 0.21854665\n",
      "Iteration 689, loss = 0.21829815\n",
      "Iteration 690, loss = 0.21805018\n",
      "Iteration 691, loss = 0.21780279\n",
      "Iteration 692, loss = 0.21755590\n",
      "Iteration 693, loss = 0.21730953\n",
      "Iteration 694, loss = 0.21706362\n",
      "Iteration 695, loss = 0.21681829\n",
      "Iteration 696, loss = 0.21657348\n",
      "Iteration 697, loss = 0.21632918\n",
      "Iteration 698, loss = 0.21608544\n",
      "Iteration 699, loss = 0.21584217\n",
      "Iteration 700, loss = 0.21559942\n",
      "Iteration 701, loss = 0.21535717\n",
      "Iteration 702, loss = 0.21511539\n",
      "Iteration 703, loss = 0.21487413\n",
      "Iteration 704, loss = 0.21463334\n",
      "Iteration 705, loss = 0.21439303\n",
      "Iteration 706, loss = 0.21415325\n",
      "Iteration 707, loss = 0.21391393\n",
      "Iteration 708, loss = 0.21367513\n",
      "Iteration 709, loss = 0.21343681\n",
      "Iteration 710, loss = 0.21319899\n",
      "Iteration 711, loss = 0.21296174\n",
      "Iteration 712, loss = 0.21272490\n",
      "Iteration 713, loss = 0.21248857\n",
      "Iteration 714, loss = 0.21225274\n",
      "Iteration 715, loss = 0.21201725\n",
      "Iteration 716, loss = 0.21178219\n",
      "Iteration 717, loss = 0.21154762\n",
      "Iteration 718, loss = 0.21131345\n",
      "Iteration 719, loss = 0.21107973\n",
      "Iteration 720, loss = 0.21084653\n",
      "Iteration 721, loss = 0.21061380\n",
      "Iteration 722, loss = 0.21038156\n",
      "Iteration 723, loss = 0.21014985\n",
      "Iteration 724, loss = 0.20991860\n",
      "Iteration 725, loss = 0.20968773\n",
      "Iteration 726, loss = 0.20945738\n",
      "Iteration 727, loss = 0.20922751\n",
      "Iteration 728, loss = 0.20899814\n",
      "Iteration 729, loss = 0.20876925\n",
      "Iteration 730, loss = 0.20854088\n",
      "Iteration 731, loss = 0.20831292\n",
      "Iteration 732, loss = 0.20808539\n",
      "Iteration 733, loss = 0.20785834\n",
      "Iteration 734, loss = 0.20763162\n",
      "Iteration 735, loss = 0.20740541\n",
      "Iteration 736, loss = 0.20717953\n",
      "Iteration 737, loss = 0.20695411\n",
      "Iteration 738, loss = 0.20672915\n",
      "Iteration 739, loss = 0.20650455\n",
      "Iteration 740, loss = 0.20628003\n",
      "Iteration 741, loss = 0.20605591\n",
      "Iteration 742, loss = 0.20583215\n",
      "Iteration 743, loss = 0.20560886\n",
      "Iteration 744, loss = 0.20538588\n",
      "Iteration 745, loss = 0.20516329\n",
      "Iteration 746, loss = 0.20494110\n",
      "Iteration 747, loss = 0.20471943\n",
      "Iteration 748, loss = 0.20449814\n",
      "Iteration 749, loss = 0.20427741\n",
      "Iteration 750, loss = 0.20405705\n",
      "Iteration 751, loss = 0.20383723\n",
      "Iteration 752, loss = 0.20361787\n",
      "Iteration 753, loss = 0.20339898\n",
      "Iteration 754, loss = 0.20318054\n",
      "Iteration 755, loss = 0.20296238\n",
      "Iteration 756, loss = 0.20274456\n",
      "Iteration 757, loss = 0.20252713\n",
      "Iteration 758, loss = 0.20231011\n",
      "Iteration 759, loss = 0.20209354\n",
      "Iteration 760, loss = 0.20187742\n",
      "Iteration 761, loss = 0.20166157\n",
      "Iteration 762, loss = 0.20144615\n",
      "Iteration 763, loss = 0.20123115\n",
      "Iteration 764, loss = 0.20101650\n",
      "Iteration 765, loss = 0.20080221\n",
      "Iteration 766, loss = 0.20058838\n",
      "Iteration 767, loss = 0.20037480\n",
      "Iteration 768, loss = 0.20016140\n",
      "Iteration 769, loss = 0.19994841\n",
      "Iteration 770, loss = 0.19973589\n",
      "Iteration 771, loss = 0.19952372\n",
      "Iteration 772, loss = 0.19931206\n",
      "Iteration 773, loss = 0.19910078\n",
      "Iteration 774, loss = 0.19888993\n",
      "Iteration 775, loss = 0.19867959\n",
      "Iteration 776, loss = 0.19846964\n",
      "Iteration 777, loss = 0.19826013\n",
      "Iteration 778, loss = 0.19805111\n",
      "Iteration 779, loss = 0.19784251\n",
      "Iteration 780, loss = 0.19763436\n",
      "Iteration 781, loss = 0.19742674\n",
      "Iteration 782, loss = 0.19721950\n",
      "Iteration 783, loss = 0.19701270\n",
      "Iteration 784, loss = 0.19680644\n",
      "Iteration 785, loss = 0.19660054\n",
      "Iteration 786, loss = 0.19639517\n",
      "Iteration 787, loss = 0.19619024\n",
      "Iteration 788, loss = 0.19598574\n",
      "Iteration 789, loss = 0.19578170\n",
      "Iteration 790, loss = 0.19557817\n",
      "Iteration 791, loss = 0.19537502\n",
      "Iteration 792, loss = 0.19517239\n",
      "Iteration 793, loss = 0.19497019\n",
      "Iteration 794, loss = 0.19476847\n",
      "Iteration 795, loss = 0.19456717\n",
      "Iteration 796, loss = 0.19436639\n",
      "Iteration 797, loss = 0.19416600\n",
      "Iteration 798, loss = 0.19396608\n",
      "Iteration 799, loss = 0.19376665\n",
      "Iteration 800, loss = 0.19356761\n",
      "Iteration 801, loss = 0.19336907\n",
      "Iteration 802, loss = 0.19317100\n",
      "Iteration 803, loss = 0.19297331\n",
      "Iteration 804, loss = 0.19277616\n",
      "Iteration 805, loss = 0.19257944\n",
      "Iteration 806, loss = 0.19238316\n",
      "Iteration 807, loss = 0.19218735\n",
      "Iteration 808, loss = 0.19199198\n",
      "Iteration 809, loss = 0.19179707\n",
      "Iteration 810, loss = 0.19160256\n",
      "Iteration 811, loss = 0.19140855\n",
      "Iteration 812, loss = 0.19121465\n",
      "Iteration 813, loss = 0.19102100\n",
      "Iteration 814, loss = 0.19082773\n",
      "Iteration 815, loss = 0.19063466\n",
      "Iteration 816, loss = 0.19044204\n",
      "Iteration 817, loss = 0.19024979\n",
      "Iteration 818, loss = 0.19005796\n",
      "Iteration 819, loss = 0.18986652\n",
      "Iteration 820, loss = 0.18967552\n",
      "Iteration 821, loss = 0.18948490\n",
      "Iteration 822, loss = 0.18929471\n",
      "Iteration 823, loss = 0.18910493\n",
      "Iteration 824, loss = 0.18891556\n",
      "Iteration 825, loss = 0.18872662\n",
      "Iteration 826, loss = 0.18853805\n",
      "Iteration 827, loss = 0.18834992\n",
      "Iteration 828, loss = 0.18816213\n",
      "Iteration 829, loss = 0.18797476\n",
      "Iteration 830, loss = 0.18778787\n",
      "Iteration 831, loss = 0.18760131\n",
      "Iteration 832, loss = 0.18741522\n",
      "Iteration 833, loss = 0.18722956\n",
      "Iteration 834, loss = 0.18704433\n",
      "Iteration 835, loss = 0.18685950\n",
      "Iteration 836, loss = 0.18667511\n",
      "Iteration 837, loss = 0.18649111\n",
      "Iteration 838, loss = 0.18630752\n",
      "Iteration 839, loss = 0.18612438\n",
      "Iteration 840, loss = 0.18594163\n",
      "Iteration 841, loss = 0.18575928\n",
      "Iteration 842, loss = 0.18557740\n",
      "Iteration 843, loss = 0.18539586\n",
      "Iteration 844, loss = 0.18521467\n",
      "Iteration 845, loss = 0.18503371\n",
      "Iteration 846, loss = 0.18485313\n",
      "Iteration 847, loss = 0.18467293\n",
      "Iteration 848, loss = 0.18449311\n",
      "Iteration 849, loss = 0.18431371\n",
      "Iteration 850, loss = 0.18413467\n",
      "Iteration 851, loss = 0.18395601\n",
      "Iteration 852, loss = 0.18377772\n",
      "Iteration 853, loss = 0.18359979\n",
      "Iteration 854, loss = 0.18342221\n",
      "Iteration 855, loss = 0.18324502\n",
      "Iteration 856, loss = 0.18306814\n",
      "Iteration 857, loss = 0.18289167\n",
      "Iteration 858, loss = 0.18271557\n",
      "Iteration 859, loss = 0.18253985\n",
      "Iteration 860, loss = 0.18236450\n",
      "Iteration 861, loss = 0.18218957\n",
      "Iteration 862, loss = 0.18201496\n",
      "Iteration 863, loss = 0.18184077\n",
      "Iteration 864, loss = 0.18166695\n",
      "Iteration 865, loss = 0.18149350\n",
      "Iteration 866, loss = 0.18132044\n",
      "Iteration 867, loss = 0.18114780\n",
      "Iteration 868, loss = 0.18097550\n",
      "Iteration 869, loss = 0.18080358\n",
      "Iteration 870, loss = 0.18063208\n",
      "Iteration 871, loss = 0.18046093\n",
      "Iteration 872, loss = 0.18029016\n",
      "Iteration 873, loss = 0.18011976\n",
      "Iteration 874, loss = 0.17994974\n",
      "Iteration 875, loss = 0.17978001\n",
      "Iteration 876, loss = 0.17961065\n",
      "Iteration 877, loss = 0.17944170\n",
      "Iteration 878, loss = 0.17927306\n",
      "Iteration 879, loss = 0.17910470\n",
      "Iteration 880, loss = 0.17893672\n",
      "Iteration 881, loss = 0.17876902\n",
      "Iteration 882, loss = 0.17860164\n",
      "Iteration 883, loss = 0.17843458\n",
      "Iteration 884, loss = 0.17826782\n",
      "Iteration 885, loss = 0.17810136\n",
      "Iteration 886, loss = 0.17793523\n",
      "Iteration 887, loss = 0.17776943\n",
      "Iteration 888, loss = 0.17760396\n",
      "Iteration 889, loss = 0.17743887\n",
      "Iteration 890, loss = 0.17727402\n",
      "Iteration 891, loss = 0.17710942\n",
      "Iteration 892, loss = 0.17694521\n",
      "Iteration 893, loss = 0.17678124\n",
      "Iteration 894, loss = 0.17661759\n",
      "Iteration 895, loss = 0.17645430\n",
      "Iteration 896, loss = 0.17629130\n",
      "Iteration 897, loss = 0.17612866\n",
      "Iteration 898, loss = 0.17596634\n",
      "Iteration 899, loss = 0.17580429\n",
      "Iteration 900, loss = 0.17564259\n",
      "Iteration 901, loss = 0.17548126\n",
      "Iteration 902, loss = 0.17532023\n",
      "Iteration 903, loss = 0.17515946\n",
      "Iteration 904, loss = 0.17499897\n",
      "Iteration 905, loss = 0.17483870\n",
      "Iteration 906, loss = 0.17467870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 907, loss = 0.17451902\n",
      "Iteration 908, loss = 0.17435959\n",
      "Iteration 909, loss = 0.17420037\n",
      "Iteration 910, loss = 0.17404138\n",
      "Iteration 911, loss = 0.17388271\n",
      "Iteration 912, loss = 0.17372433\n",
      "Iteration 913, loss = 0.17356627\n",
      "Iteration 914, loss = 0.17340855\n",
      "Iteration 915, loss = 0.17325113\n",
      "Iteration 916, loss = 0.17309397\n",
      "Iteration 917, loss = 0.17293716\n",
      "Iteration 918, loss = 0.17278060\n",
      "Iteration 919, loss = 0.17262432\n",
      "Iteration 920, loss = 0.17246830\n",
      "Iteration 921, loss = 0.17231255\n",
      "Iteration 922, loss = 0.17215710\n",
      "Iteration 923, loss = 0.17200193\n",
      "Iteration 924, loss = 0.17184705\n",
      "Iteration 925, loss = 0.17169248\n",
      "Iteration 926, loss = 0.17153825\n",
      "Iteration 927, loss = 0.17138431\n",
      "Iteration 928, loss = 0.17123071\n",
      "Iteration 929, loss = 0.17107743\n",
      "Iteration 930, loss = 0.17092447\n",
      "Iteration 931, loss = 0.17077183\n",
      "Iteration 932, loss = 0.17061950\n",
      "Iteration 933, loss = 0.17046745\n",
      "Iteration 934, loss = 0.17031574\n",
      "Iteration 935, loss = 0.17016423\n",
      "Iteration 936, loss = 0.17001301\n",
      "Iteration 937, loss = 0.16986211\n",
      "Iteration 938, loss = 0.16971152\n",
      "Iteration 939, loss = 0.16956124\n",
      "Iteration 940, loss = 0.16941130\n",
      "Iteration 941, loss = 0.16926165\n",
      "Iteration 942, loss = 0.16911232\n",
      "Iteration 943, loss = 0.16896334\n",
      "Iteration 944, loss = 0.16881464\n",
      "Iteration 945, loss = 0.16866627\n",
      "Iteration 946, loss = 0.16851820\n",
      "Iteration 947, loss = 0.16837036\n",
      "Iteration 948, loss = 0.16822280\n",
      "Iteration 949, loss = 0.16807556\n",
      "Iteration 950, loss = 0.16792864\n",
      "Iteration 951, loss = 0.16778201\n",
      "Iteration 952, loss = 0.16763573\n",
      "Iteration 953, loss = 0.16748973\n",
      "Iteration 954, loss = 0.16734404\n",
      "Iteration 955, loss = 0.16719859\n",
      "Iteration 956, loss = 0.16705339\n",
      "Iteration 957, loss = 0.16690850\n",
      "Iteration 958, loss = 0.16676441\n",
      "Iteration 959, loss = 0.16662087\n",
      "Iteration 960, loss = 0.16647769\n",
      "Iteration 961, loss = 0.16633488\n",
      "Iteration 962, loss = 0.16619234\n",
      "Iteration 963, loss = 0.16605013\n",
      "Iteration 964, loss = 0.16590823\n",
      "Iteration 965, loss = 0.16576659\n",
      "Iteration 966, loss = 0.16562514\n",
      "Iteration 967, loss = 0.16548398\n",
      "Iteration 968, loss = 0.16534313\n",
      "Iteration 969, loss = 0.16520258\n",
      "Iteration 970, loss = 0.16506233\n",
      "Iteration 971, loss = 0.16492239\n",
      "Iteration 972, loss = 0.16478275\n",
      "Iteration 973, loss = 0.16464339\n",
      "Iteration 974, loss = 0.16450438\n",
      "Iteration 975, loss = 0.16436561\n",
      "Iteration 976, loss = 0.16422715\n",
      "Iteration 977, loss = 0.16408898\n",
      "Iteration 978, loss = 0.16395110\n",
      "Iteration 979, loss = 0.16381350\n",
      "Iteration 980, loss = 0.16367621\n",
      "Iteration 981, loss = 0.16353921\n",
      "Iteration 982, loss = 0.16340249\n",
      "Iteration 983, loss = 0.16326609\n",
      "Iteration 984, loss = 0.16312995\n",
      "Iteration 985, loss = 0.16299411\n",
      "Iteration 986, loss = 0.16285859\n",
      "Iteration 987, loss = 0.16272333\n",
      "Iteration 988, loss = 0.16258836\n",
      "Iteration 989, loss = 0.16245370\n",
      "Iteration 990, loss = 0.16231932\n",
      "Iteration 991, loss = 0.16218523\n",
      "Iteration 992, loss = 0.16205143\n",
      "Iteration 993, loss = 0.16191791\n",
      "Iteration 994, loss = 0.16178466\n",
      "Iteration 995, loss = 0.16165213\n",
      "Iteration 996, loss = 0.16152034\n",
      "Iteration 997, loss = 0.16138886\n",
      "Iteration 998, loss = 0.16125769\n",
      "Iteration 999, loss = 0.16112686\n",
      "Iteration 1000, loss = 0.16099625\n",
      "Iteration 1, loss = 1.65830114\n",
      "Iteration 2, loss = 1.62211189\n",
      "Iteration 3, loss = 1.57349632\n",
      "Iteration 4, loss = 1.51659112\n",
      "Iteration 5, loss = 1.45574514\n",
      "Iteration 6, loss = 1.39546428\n",
      "Iteration 7, loss = 1.33949488\n",
      "Iteration 8, loss = 1.29090254\n",
      "Iteration 9, loss = 1.25160218\n",
      "Iteration 10, loss = 1.22193554\n",
      "Iteration 11, loss = 1.20067602\n",
      "Iteration 12, loss = 1.18548685\n",
      "Iteration 13, loss = 1.17370058\n",
      "Iteration 14, loss = 1.16283085\n",
      "Iteration 15, loss = 1.15110710\n",
      "Iteration 16, loss = 1.13715699\n",
      "Iteration 17, loss = 1.12058367\n",
      "Iteration 18, loss = 1.10164507\n",
      "Iteration 19, loss = 1.08094332\n",
      "Iteration 20, loss = 1.05946966\n",
      "Iteration 21, loss = 1.03828325\n",
      "Iteration 22, loss = 1.01826693\n",
      "Iteration 23, loss = 0.99998905\n",
      "Iteration 24, loss = 0.98362861\n",
      "Iteration 25, loss = 0.96917524\n",
      "Iteration 26, loss = 0.95622154\n",
      "Iteration 27, loss = 0.94448588\n",
      "Iteration 28, loss = 0.93362703\n",
      "Iteration 29, loss = 0.92329511\n",
      "Iteration 30, loss = 0.91328850\n",
      "Iteration 31, loss = 0.90355694"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 32, loss = 0.89412824\n",
      "Iteration 33, loss = 0.88496435\n",
      "Iteration 34, loss = 0.87612091\n",
      "Iteration 35, loss = 0.86757613\n",
      "Iteration 36, loss = 0.85923582\n",
      "Iteration 37, loss = 0.85112397\n",
      "Iteration 38, loss = 0.84332205\n",
      "Iteration 39, loss = 0.83581651\n",
      "Iteration 40, loss = 0.82860608\n",
      "Iteration 41, loss = 0.82167597\n",
      "Iteration 42, loss = 0.81499998\n",
      "Iteration 43, loss = 0.80852805\n",
      "Iteration 44, loss = 0.80226352\n",
      "Iteration 45, loss = 0.79615101\n",
      "Iteration 46, loss = 0.79016807\n",
      "Iteration 47, loss = 0.78427672\n",
      "Iteration 48, loss = 0.77847534\n",
      "Iteration 49, loss = 0.77277321\n",
      "Iteration 50, loss = 0.76718265\n",
      "Iteration 51, loss = 0.76171230\n",
      "Iteration 52, loss = 0.75637081\n",
      "Iteration 53, loss = 0.75115857\n",
      "Iteration 54, loss = 0.74607349\n",
      "Iteration 55, loss = 0.74110961\n",
      "Iteration 56, loss = 0.73625973\n",
      "Iteration 57, loss = 0.73151462\n",
      "Iteration 58, loss = 0.72686917\n",
      "Iteration 59, loss = 0.72231790\n",
      "Iteration 60, loss = 0.71785555\n",
      "Iteration 61, loss = 0.71347779\n",
      "Iteration 62, loss = 0.70918117\n",
      "Iteration 63, loss = 0.70496688\n",
      "Iteration 64, loss = 0.70083681\n",
      "Iteration 65, loss = 0.69679105\n",
      "Iteration 66, loss = 0.69282732\n",
      "Iteration 67, loss = 0.68894377\n",
      "Iteration 68, loss = 0.68513726\n",
      "Iteration 69, loss = 0.68140482\n",
      "Iteration 70, loss = 0.67774413\n",
      "Iteration 71, loss = 0.67415301\n",
      "Iteration 72, loss = 0.67062972\n",
      "Iteration 73, loss = 0.66717199\n",
      "Iteration 74, loss = 0.66377770\n",
      "Iteration 75, loss = 0.66044421\n",
      "Iteration 76, loss = 0.65717001\n",
      "Iteration 77, loss = 0.65395382\n",
      "Iteration 78, loss = 0.65079434\n",
      "Iteration 79, loss = 0.64769024\n",
      "Iteration 80, loss = 0.64464020\n",
      "Iteration 81, loss = 0.64164273\n",
      "Iteration 82, loss = 0.63869698\n",
      "Iteration 83, loss = 0.63580053\n",
      "Iteration 84, loss = 0.63295248\n",
      "Iteration 85, loss = 0.63015107\n",
      "Iteration 86, loss = 0.62739504\n",
      "Iteration 87, loss = 0.62468275\n",
      "Iteration 88, loss = 0.62201289\n",
      "Iteration 89, loss = 0.61938499\n",
      "Iteration 90, loss = 0.61679783\n",
      "Iteration 91, loss = 0.61425060\n",
      "Iteration 92, loss = 0.61174216\n",
      "Iteration 93, loss = 0.60927163\n",
      "Iteration 94, loss = 0.60683809\n",
      "Iteration 95, loss = 0.60444128\n",
      "Iteration 96, loss = 0.60208079\n",
      "Iteration 97, loss = 0.59975519\n",
      "Iteration 98, loss = 0.59746307\n",
      "Iteration 99, loss = 0.59520359\n",
      "Iteration 100, loss = 0.59297615\n",
      "Iteration 101, loss = 0.59078073\n",
      "Iteration 102, loss = 0.58861594\n",
      "Iteration 103, loss = 0.58648102\n",
      "Iteration 104, loss = 0.58437534\n",
      "Iteration 105, loss = 0.58229835\n",
      "Iteration 106, loss = 0.58024938\n",
      "Iteration 107, loss = 0.57822840\n",
      "Iteration 108, loss = 0.57623394\n",
      "Iteration 109, loss = 0.57426512\n",
      "Iteration 110, loss = 0.57232148\n",
      "Iteration 111, loss = 0.57040156\n",
      "Iteration 112, loss = 0.56850476\n",
      "Iteration 113, loss = 0.56663060\n",
      "Iteration 114, loss = 0.56477954\n",
      "Iteration 115, loss = 0.56295098\n",
      "Iteration 116, loss = 0.56114417\n",
      "Iteration 117, loss = 0.55935865\n",
      "Iteration 118, loss = 0.55759400\n",
      "Iteration 119, loss = 0.55584990\n",
      "Iteration 120, loss = 0.55412584\n",
      "Iteration 121, loss = 0.55242128\n",
      "Iteration 122, loss = 0.55073582\n",
      "Iteration 123, loss = 0.54906904\n",
      "Iteration 124, loss = 0.54742051\n",
      "Iteration 125, loss = 0.54578866\n",
      "Iteration 126, loss = 0.54417428\n",
      "Iteration 127, loss = 0.54257698\n",
      "Iteration 128, loss = 0.54099640\n",
      "Iteration 129, loss = 0.53943174\n",
      "Iteration 130, loss = 0.53788272\n",
      "Iteration 131, loss = 0.53635073\n",
      "Iteration 132, loss = 0.53483347\n",
      "Iteration 133, loss = 0.53333128\n",
      "Iteration 134, loss = 0.53184320\n",
      "Iteration 135, loss = 0.53036945\n",
      "Iteration 136, loss = 0.52890719\n",
      "Iteration 137, loss = 0.52745478\n",
      "Iteration 138, loss = 0.52601486\n",
      "Iteration 139, loss = 0.52458506\n",
      "Iteration 140, loss = 0.52316647\n",
      "Iteration 141, loss = 0.52175977\n",
      "Iteration 142, loss = 0.52036482\n",
      "Iteration 143, loss = 0.51898166\n",
      "Iteration 144, loss = 0.51760952\n",
      "Iteration 145, loss = 0.51624608\n",
      "Iteration 146, loss = 0.51489325\n",
      "Iteration 147, loss = 0.51354878\n",
      "Iteration 148, loss = 0.51221063\n",
      "Iteration 149, loss = 0.51088489\n",
      "Iteration 150, loss = 0.50956848\n",
      "Iteration 151, loss = 0.50825856\n",
      "Iteration 152, loss = 0.50695345\n",
      "Iteration 153, loss = 0.50565547\n",
      "Iteration 154, loss = 0.50435756\n",
      "Iteration 155, loss = 0.50306656\n",
      "Iteration 156, loss = 0.50178493\n",
      "Iteration 157, loss = 0.50051269\n",
      "Iteration 158, loss = 0.49924831\n",
      "Iteration 159, loss = 0.49799783\n",
      "Iteration 160, loss = 0.49676188\n",
      "Iteration 161, loss = 0.49553698\n",
      "Iteration 162, loss = 0.49432776\n",
      "Iteration 163, loss = 0.49313862\n",
      "Iteration 164, loss = 0.49196126\n",
      "Iteration 165, loss = 0.49079881\n",
      "Iteration 166, loss = 0.48965192\n",
      "Iteration 167, loss = 0.48852189\n",
      "Iteration 168, loss = 0.48740510\n",
      "Iteration 169, loss = 0.48629965\n",
      "Iteration 170, loss = 0.48520292\n",
      "Iteration 171, loss = 0.48411780\n",
      "Iteration 172, loss = 0.48304467\n",
      "Iteration 173, loss = 0.48197988\n",
      "Iteration 174, loss = 0.48092303\n",
      "Iteration 175, loss = 0.47987374\n",
      "Iteration 176, loss = 0.47883141\n",
      "Iteration 177, loss = 0.47779603\n",
      "Iteration 178, loss = 0.47676703\n",
      "Iteration 179, loss = 0.47574403\n",
      "Iteration 180, loss = 0.47472729\n",
      "Iteration 181, loss = 0.47371735\n",
      "Iteration 182, loss = 0.47271279\n",
      "Iteration 183, loss = 0.47171378\n",
      "Iteration 184, loss = 0.47072032\n",
      "Iteration 185, loss = 0.46973253\n",
      "Iteration 186, loss = 0.46875077\n",
      "Iteration 187, loss = 0.46777529\n",
      "Iteration 188, loss = 0.46680558\n",
      "Iteration 189, loss = 0.46584169\n",
      "Iteration 190, loss = 0.46488345\n",
      "Iteration 191, loss = 0.46393074\n",
      "Iteration 192, loss = 0.46298340\n",
      "Iteration 193, loss = 0.46204142\n",
      "Iteration 194, loss = 0.46110489\n",
      "Iteration 195, loss = 0.46017408\n",
      "Iteration 196, loss = 0.45924848\n",
      "Iteration 197, loss = 0.45832799\n",
      "Iteration 198, loss = 0.45741249\n",
      "Iteration 199, loss = 0.45650193\n",
      "Iteration 200, loss = 0.45559655\n",
      "Iteration 201, loss = 0.45469655\n",
      "Iteration 202, loss = 0.45380128\n",
      "Iteration 203, loss = 0.45291065\n",
      "Iteration 204, loss = 0.45202451\n",
      "Iteration 205, loss = 0.45114294\n",
      "Iteration 206, loss = 0.45026589\n",
      "Iteration 207, loss = 0.44939328\n",
      "Iteration 208, loss = 0.44852513\n",
      "Iteration 209, loss = 0.44766122\n",
      "Iteration 210, loss = 0.44680140\n",
      "Iteration 211, loss = 0.44594573\n",
      "Iteration 212, loss = 0.44509417\n",
      "Iteration 213, loss = 0.44424668\n",
      "Iteration 214, loss = 0.44340318\n",
      "Iteration 215, loss = 0.44256365\n",
      "Iteration 216, loss = 0.44172802\n",
      "Iteration 217, loss = 0.44089628\n",
      "Iteration 218, loss = 0.44006840\n",
      "Iteration 219, loss = 0.43924424\n",
      "Iteration 220, loss = 0.43842383\n",
      "Iteration 221, loss = 0.43760711\n",
      "Iteration 222, loss = 0.43679391\n",
      "Iteration 223, loss = 0.43598404\n",
      "Iteration 224, loss = 0.43517741\n",
      "Iteration 225, loss = 0.43437409\n",
      "Iteration 226, loss = 0.43357417\n",
      "Iteration 227, loss = 0.43277761\n",
      "Iteration 228, loss = 0.43198441\n",
      "Iteration 229, loss = 0.43119441\n",
      "Iteration 230, loss = 0.43040774\n",
      "Iteration 231, loss = 0.42962381\n",
      "Iteration 232, loss = 0.42884309\n",
      "Iteration 233, loss = 0.42806549\n",
      "Iteration 234, loss = 0.42729094\n",
      "Iteration 235, loss = 0.42651961\n",
      "Iteration 236, loss = 0.42575128\n",
      "Iteration 237, loss = 0.42498575\n",
      "Iteration 238, loss = 0.42422223\n",
      "Iteration 239, loss = 0.42346149\n",
      "Iteration 240, loss = 0.42270378\n",
      "Iteration 241, loss = 0.42194894\n",
      "Iteration 242, loss = 0.42119670\n",
      "Iteration 243, loss = 0.42044690\n",
      "Iteration 244, loss = 0.41969935\n",
      "Iteration 245, loss = 0.41895438\n",
      "Iteration 246, loss = 0.41821181\n",
      "Iteration 247, loss = 0.41747164\n",
      "Iteration 248, loss = 0.41673393\n",
      "Iteration 249, loss = 0.41599846\n",
      "Iteration 250, loss = 0.41526541\n",
      "Iteration 251, loss = 0.41453477\n",
      "Iteration 252, loss = 0.41380656\n",
      "Iteration 253, loss = 0.41308076\n",
      "Iteration 254, loss = 0.41235714\n",
      "Iteration 255, loss = 0.41163555\n",
      "Iteration 256, loss = 0.41091674\n",
      "Iteration 257, loss = 0.41020020\n",
      "Iteration 258, loss = 0.40948606\n",
      "Iteration 259, loss = 0.40877409\n",
      "Iteration 260, loss = 0.40806437\n",
      "Iteration 261, loss = 0.40735694\n",
      "Iteration 262, loss = 0.40665208\n",
      "Iteration 263, loss = 0.40594882\n",
      "Iteration 264, loss = 0.40524753\n",
      "Iteration 265, loss = 0.40454891\n",
      "Iteration 266, loss = 0.40385267\n",
      "Iteration 267, loss = 0.40315887\n",
      "Iteration 268, loss = 0.40246714\n",
      "Iteration 269, loss = 0.40177751\n",
      "Iteration 270, loss = 0.40109009\n",
      "Iteration 271, loss = 0.40040546\n",
      "Iteration 272, loss = 0.39972490\n",
      "Iteration 273, loss = 0.39904709\n",
      "Iteration 274, loss = 0.39837180\n",
      "Iteration 275, loss = 0.39769940\n",
      "Iteration 276, loss = 0.39702960\n",
      "Iteration 277, loss = 0.39636275\n",
      "Iteration 278, loss = 0.39569809\n",
      "Iteration 279, loss = 0.39503532\n",
      "Iteration 280, loss = 0.39437483\n",
      "Iteration 281, loss = 0.39371719\n",
      "Iteration 282, loss = 0.39306199\n",
      "Iteration 283, loss = 0.39240914\n",
      "Iteration 284, loss = 0.39175839\n",
      "Iteration 285, loss = 0.39110966\n",
      "Iteration 286, loss = 0.39046362\n",
      "Iteration 287, loss = 0.38981994\n",
      "Iteration 288, loss = 0.38917822\n",
      "Iteration 289, loss = 0.38853898\n",
      "Iteration 290, loss = 0.38790195\n",
      "Iteration 291, loss = 0.38726693\n",
      "Iteration 292, loss = 0.38663371\n",
      "Iteration 293, loss = 0.38600237\n",
      "Iteration 294, loss = 0.38537290\n",
      "Iteration 295, loss = 0.38474511\n",
      "Iteration 296, loss = 0.38411897\n",
      "Iteration 297, loss = 0.38349472\n",
      "Iteration 298, loss = 0.38287225\n",
      "Iteration 299, loss = 0.38225164\n",
      "Iteration 300, loss = 0.38163295\n",
      "Iteration 301, loss = 0.38101578\n",
      "Iteration 302, loss = 0.38040046\n",
      "Iteration 303, loss = 0.37978695\n",
      "Iteration 304, loss = 0.37917511\n",
      "Iteration 305, loss = 0.37856510\n",
      "Iteration 306, loss = 0.37795663\n",
      "Iteration 307, loss = 0.37734983\n",
      "Iteration 308, loss = 0.37674465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 309, loss = 0.37614109\n",
      "Iteration 310, loss = 0.37553909\n",
      "Iteration 311, loss = 0.37493859\n",
      "Iteration 312, loss = 0.37433977\n",
      "Iteration 313, loss = 0.37374257\n",
      "Iteration 314, loss = 0.37314690\n",
      "Iteration 315, loss = 0.37255304\n",
      "Iteration 316, loss = 0.37196078\n",
      "Iteration 317, loss = 0.37137024\n",
      "Iteration 318, loss = 0.37078132\n",
      "Iteration 319, loss = 0.37019410\n",
      "Iteration 320, loss = 0.36960843\n",
      "Iteration 321, loss = 0.36902429\n",
      "Iteration 322, loss = 0.36844169\n",
      "Iteration 323, loss = 0.36786043\n",
      "Iteration 324, loss = 0.36728062\n",
      "Iteration 325, loss = 0.36670225\n",
      "Iteration 326, loss = 0.36612531\n",
      "Iteration 327, loss = 0.36554962\n",
      "Iteration 328, loss = 0.36497520\n",
      "Iteration 329, loss = 0.36440243\n",
      "Iteration 330, loss = 0.36383109\n",
      "Iteration 331, loss = 0.36326122\n",
      "Iteration 332, loss = 0.36269281\n",
      "Iteration 333, loss = 0.36212585\n",
      "Iteration 334, loss = 0.36156028\n",
      "Iteration 335, loss = 0.36099610\n",
      "Iteration 336, loss = 0.36043337\n",
      "Iteration 337, loss = 0.35987207\n",
      "Iteration 338, loss = 0.35931235\n",
      "Iteration 339, loss = 0.35875397\n",
      "Iteration 340, loss = 0.35819688\n",
      "Iteration 341, loss = 0.35764126\n",
      "Iteration 342, loss = 0.35708689\n",
      "Iteration 343, loss = 0.35653392\n",
      "Iteration 344, loss = 0.35598221\n",
      "Iteration 345, loss = 0.35543177\n",
      "Iteration 346, loss = 0.35488295\n",
      "Iteration 347, loss = 0.35433549\n",
      "Iteration 348, loss = 0.35378938\n",
      "Iteration 349, loss = 0.35324474\n",
      "Iteration 350, loss = 0.35270136\n",
      "Iteration 351, loss = 0.35215935\n",
      "Iteration 352, loss = 0.35161864\n",
      "Iteration 353, loss = 0.35107913\n",
      "Iteration 354, loss = 0.35054094\n",
      "Iteration 355, loss = 0.35000425\n",
      "Iteration 356, loss = 0.34946903\n",
      "Iteration 357, loss = 0.34893504\n",
      "Iteration 358, loss = 0.34840236\n",
      "Iteration 359, loss = 0.34787093\n",
      "Iteration 360, loss = 0.34734076\n",
      "Iteration 361, loss = 0.34681198\n",
      "Iteration 362, loss = 0.34628457\n",
      "Iteration 363, loss = 0.34575854\n",
      "Iteration 364, loss = 0.34523382\n",
      "Iteration 365, loss = 0.34471034\n",
      "Iteration 366, loss = 0.34418808\n",
      "Iteration 367, loss = 0.34366715\n",
      "Iteration 368, loss = 0.34314751\n",
      "Iteration 369, loss = 0.34262914\n",
      "Iteration 370, loss = 0.34211195\n",
      "Iteration 371, loss = 0.34159596\n",
      "Iteration 372, loss = 0.34108122\n",
      "Iteration 373, loss = 0.34056759\n",
      "Iteration 374, loss = 0.34005516\n",
      "Iteration 375, loss = 0.33954390\n",
      "Iteration 376, loss = 0.33903384\n",
      "Iteration 377, loss = 0.33852495\n",
      "Iteration 378, loss = 0.33801724\n",
      "Iteration 379, loss = 0.33751066\n",
      "Iteration 380, loss = 0.33700532\n",
      "Iteration 381, loss = 0.33650116\n",
      "Iteration 382, loss = 0.33599813\n",
      "Iteration 383, loss = 0.33549622\n",
      "Iteration 384, loss = 0.33499545\n",
      "Iteration 385, loss = 0.33449586\n",
      "Iteration 386, loss = 0.33399729\n",
      "Iteration 387, loss = 0.33349994\n",
      "Iteration 388, loss = 0.33300370\n",
      "Iteration 389, loss = 0.33250856\n",
      "Iteration 390, loss = 0.33201452\n",
      "Iteration 391, loss = 0.33152162\n",
      "Iteration 392, loss = 0.33102981\n",
      "Iteration 393, loss = 0.33053914\n",
      "Iteration 394, loss = 0.33004955\n",
      "Iteration 395, loss = 0.32956104\n",
      "Iteration 396, loss = 0.32907364\n",
      "Iteration 397, loss = 0.32858728\n",
      "Iteration 398, loss = 0.32810212\n",
      "Iteration 399, loss = 0.32761804\n",
      "Iteration 400, loss = 0.32713505\n",
      "Iteration 401, loss = 0.32665311\n",
      "Iteration 402, loss = 0.32617222\n",
      "Iteration 403, loss = 0.32569240\n",
      "Iteration 404, loss = 0.32521363\n",
      "Iteration 405, loss = 0.32473592\n",
      "Iteration 406, loss = 0.32425931\n",
      "Iteration 407, loss = 0.32378369\n",
      "Iteration 408, loss = 0.32330913\n",
      "Iteration 409, loss = 0.32283559\n",
      "Iteration 410, loss = 0.32236312\n",
      "Iteration 411, loss = 0.32189184\n",
      "Iteration 412, loss = 0.32142158\n",
      "Iteration 413, loss = 0.32095234\n",
      "Iteration 414, loss = 0.32048414\n",
      "Iteration 415, loss = 0.32001698\n",
      "Iteration 416, loss = 0.31955077\n",
      "Iteration 417, loss = 0.31908562\n",
      "Iteration 418, loss = 0.31862149\n",
      "Iteration 419, loss = 0.31815838\n",
      "Iteration 420, loss = 0.31769630\n",
      "Iteration 421, loss = 0.31723522\n",
      "Iteration 422, loss = 0.31677512\n",
      "Iteration 423, loss = 0.31631627\n",
      "Iteration 424, loss = 0.31585856\n",
      "Iteration 425, loss = 0.31540198\n",
      "Iteration 426, loss = 0.31494633\n",
      "Iteration 427, loss = 0.31449173\n",
      "Iteration 428, loss = 0.31403813\n",
      "Iteration 429, loss = 0.31358552\n",
      "Iteration 430, loss = 0.31313387\n",
      "Iteration 431, loss = 0.31268319\n",
      "Iteration 432, loss = 0.31223351\n",
      "Iteration 433, loss = 0.31178494\n",
      "Iteration 434, loss = 0.31133737\n",
      "Iteration 435, loss = 0.31089075\n",
      "Iteration 436, loss = 0.31044509\n",
      "Iteration 437, loss = 0.31000040\n",
      "Iteration 438, loss = 0.30955668\n",
      "Iteration 439, loss = 0.30911387\n",
      "Iteration 440, loss = 0.30867204\n",
      "Iteration 441, loss = 0.30823114\n",
      "Iteration 442, loss = 0.30779118\n",
      "Iteration 443, loss = 0.30735218\n",
      "Iteration 444, loss = 0.30691409\n",
      "Iteration 445, loss = 0.30647694\n",
      "Iteration 446, loss = 0.30604072\n",
      "Iteration 447, loss = 0.30560542\n",
      "Iteration 448, loss = 0.30517104\n",
      "Iteration 449, loss = 0.30473760\n",
      "Iteration 450, loss = 0.30430505\n",
      "Iteration 451, loss = 0.30387344\n",
      "Iteration 452, loss = 0.30344273\n",
      "Iteration 453, loss = 0.30301294\n",
      "Iteration 454, loss = 0.30258407\n",
      "Iteration 455, loss = 0.30215612\n",
      "Iteration 456, loss = 0.30172905\n",
      "Iteration 457, loss = 0.30130289\n",
      "Iteration 458, loss = 0.30087760\n",
      "Iteration 459, loss = 0.30045324\n",
      "Iteration 460, loss = 0.30002975\n",
      "Iteration 461, loss = 0.29960718\n",
      "Iteration 462, loss = 0.29918552\n",
      "Iteration 463, loss = 0.29876474\n",
      "Iteration 464, loss = 0.29834484\n",
      "Iteration 465, loss = 0.29792584\n",
      "Iteration 466, loss = 0.29750772\n",
      "Iteration 467, loss = 0.29709048\n",
      "Iteration 468, loss = 0.29667414\n",
      "Iteration 469, loss = 0.29625869\n",
      "Iteration 470, loss = 0.29584412\n",
      "Iteration 471, loss = 0.29543042\n",
      "Iteration 472, loss = 0.29501758\n",
      "Iteration 473, loss = 0.29460562\n",
      "Iteration 474, loss = 0.29419454\n",
      "Iteration 475, loss = 0.29378433\n",
      "Iteration 476, loss = 0.29337498\n",
      "Iteration 477, loss = 0.29296648\n",
      "Iteration 478, loss = 0.29255886\n",
      "Iteration 479, loss = 0.29215208\n",
      "Iteration 480, loss = 0.29174616\n",
      "Iteration 481, loss = 0.29134109\n",
      "Iteration 482, loss = 0.29093687\n",
      "Iteration 483, loss = 0.29053351\n",
      "Iteration 484, loss = 0.29013097\n",
      "Iteration 485, loss = 0.28972929\n",
      "Iteration 486, loss = 0.28932844\n",
      "Iteration 487, loss = 0.28892843\n",
      "Iteration 488, loss = 0.28852927\n",
      "Iteration 489, loss = 0.28813093\n",
      "Iteration 490, loss = 0.28773345\n",
      "Iteration 491, loss = 0.28733684\n",
      "Iteration 492, loss = 0.28694105\n",
      "Iteration 493, loss = 0.28654613\n",
      "Iteration 494, loss = 0.28615203\n",
      "Iteration 495, loss = 0.28575877\n",
      "Iteration 496, loss = 0.28536636\n",
      "Iteration 497, loss = 0.28497478\n",
      "Iteration 498, loss = 0.28458404\n",
      "Iteration 499, loss = 0.28419411\n",
      "Iteration 500, loss = 0.28380502\n",
      "Iteration 501, loss = 0.28341670\n",
      "Iteration 502, loss = 0.28302917\n",
      "Iteration 503, loss = 0.28264243\n",
      "Iteration 504, loss = 0.28225647\n",
      "Iteration 505, loss = 0.28187131\n",
      "Iteration 506, loss = 0.28148696\n",
      "Iteration 507, loss = 0.28110341\n",
      "Iteration 508, loss = 0.28072066\n",
      "Iteration 509, loss = 0.28033871\n",
      "Iteration 510, loss = 0.27995791\n",
      "Iteration 511, loss = 0.27957806\n",
      "Iteration 512, loss = 0.27919905\n",
      "Iteration 513, loss = 0.27882083\n",
      "Iteration 514, loss = 0.27844344\n",
      "Iteration 515, loss = 0.27806691\n",
      "Iteration 516, loss = 0.27769128\n",
      "Iteration 517, loss = 0.27731650\n",
      "Iteration 518, loss = 0.27694265\n",
      "Iteration 519, loss = 0.27656963\n",
      "Iteration 520, loss = 0.27619758\n",
      "Iteration 521, loss = 0.27582646\n",
      "Iteration 522, loss = 0.27545616\n",
      "Iteration 523, loss = 0.27508665\n",
      "Iteration 524, loss = 0.27471795\n",
      "Iteration 525, loss = 0.27435004\n",
      "Iteration 526, loss = 0.27398294\n",
      "Iteration 527, loss = 0.27361662\n",
      "Iteration 528, loss = 0.27325110\n",
      "Iteration 529, loss = 0.27288636\n",
      "Iteration 530, loss = 0.27252241\n",
      "Iteration 531, loss = 0.27215930\n",
      "Iteration 532, loss = 0.27179700\n",
      "Iteration 533, loss = 0.27143547\n",
      "Iteration 534, loss = 0.27107472\n",
      "Iteration 535, loss = 0.27071474\n",
      "Iteration 536, loss = 0.27035552\n",
      "Iteration 537, loss = 0.26999721\n",
      "Iteration 538, loss = 0.26963971\n",
      "Iteration 539, loss = 0.26928298\n",
      "Iteration 540, loss = 0.26892701\n",
      "Iteration 541, loss = 0.26857180\n",
      "Iteration 542, loss = 0.26821735\n",
      "Iteration 543, loss = 0.26786372\n",
      "Iteration 544, loss = 0.26751085\n",
      "Iteration 545, loss = 0.26715873\n",
      "Iteration 546, loss = 0.26680736\n",
      "Iteration 547, loss = 0.26645674\n",
      "Iteration 548, loss = 0.26610689\n",
      "Iteration 549, loss = 0.26575778\n",
      "Iteration 550, loss = 0.26540946\n",
      "Iteration 551, loss = 0.26506180\n",
      "Iteration 552, loss = 0.26471492\n",
      "Iteration 553, loss = 0.26436877\n",
      "Iteration 554, loss = 0.26402335\n",
      "Iteration 555, loss = 0.26367873\n",
      "Iteration 556, loss = 0.26333484\n",
      "Iteration 557, loss = 0.26299168\n",
      "Iteration 558, loss = 0.26264925\n",
      "Iteration 559, loss = 0.26230753\n",
      "Iteration 560, loss = 0.26196654\n",
      "Iteration 561, loss = 0.26162626\n",
      "Iteration 562, loss = 0.26128666\n",
      "Iteration 563, loss = 0.26094777\n",
      "Iteration 564, loss = 0.26060961\n",
      "Iteration 565, loss = 0.26027216\n",
      "Iteration 566, loss = 0.25993542\n",
      "Iteration 567, loss = 0.25959939\n",
      "Iteration 568, loss = 0.25926407\n",
      "Iteration 569, loss = 0.25892946\n",
      "Iteration 570, loss = 0.25859555\n",
      "Iteration 571, loss = 0.25826233\n",
      "Iteration 572, loss = 0.25792984\n",
      "Iteration 573, loss = 0.25759801\n",
      "Iteration 574, loss = 0.25726690\n",
      "Iteration 575, loss = 0.25693649\n",
      "Iteration 576, loss = 0.25660679\n",
      "Iteration 577, loss = 0.25627780\n",
      "Iteration 578, loss = 0.25594950\n",
      "Iteration 579, loss = 0.25562190\n",
      "Iteration 580, loss = 0.25529498\n",
      "Iteration 581, loss = 0.25496875\n",
      "Iteration 582, loss = 0.25464320\n",
      "Iteration 583, loss = 0.25431833\n",
      "Iteration 584, loss = 0.25399416\n",
      "Iteration 585, loss = 0.25367068\n",
      "Iteration 586, loss = 0.25334788\n",
      "Iteration 587, loss = 0.25302576\n",
      "Iteration 588, loss = 0.25270432\n",
      "Iteration 589, loss = 0.25238355\n",
      "Iteration 590, loss = 0.25206346\n",
      "Iteration 591, loss = 0.25174405\n",
      "Iteration 592, loss = 0.25142536\n",
      "Iteration 593, loss = 0.25110749\n",
      "Iteration 594, loss = 0.25079027\n",
      "Iteration 595, loss = 0.25047371\n",
      "Iteration 596, loss = 0.25015780\n",
      "Iteration 597, loss = 0.24984234\n",
      "Iteration 598, loss = 0.24952747\n",
      "Iteration 599, loss = 0.24921323\n",
      "Iteration 600, loss = 0.24889965\n",
      "Iteration 601, loss = 0.24858677\n",
      "Iteration 602, loss = 0.24827445\n",
      "Iteration 603, loss = 0.24796272\n",
      "Iteration 604, loss = 0.24765167\n",
      "Iteration 605, loss = 0.24734125\n",
      "Iteration 606, loss = 0.24703139\n",
      "Iteration 607, loss = 0.24672220\n",
      "Iteration 608, loss = 0.24641357\n",
      "Iteration 609, loss = 0.24610555\n",
      "Iteration 610, loss = 0.24579811\n",
      "Iteration 611, loss = 0.24549131\n",
      "Iteration 612, loss = 0.24518514\n",
      "Iteration 613, loss = 0.24487953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 614, loss = 0.24457451\n",
      "Iteration 615, loss = 0.24427007\n",
      "Iteration 616, loss = 0.24396628\n",
      "Iteration 617, loss = 0.24366309\n",
      "Iteration 618, loss = 0.24336056\n",
      "Iteration 619, loss = 0.24305862\n",
      "Iteration 620, loss = 0.24275735\n",
      "Iteration 621, loss = 0.24245668\n",
      "Iteration 622, loss = 0.24215664\n",
      "Iteration 623, loss = 0.24185720\n",
      "Iteration 624, loss = 0.24155841\n",
      "Iteration 625, loss = 0.24126017\n",
      "Iteration 626, loss = 0.24096254\n",
      "Iteration 627, loss = 0.24066546\n",
      "Iteration 628, loss = 0.24036896\n",
      "Iteration 629, loss = 0.24007302\n",
      "Iteration 630, loss = 0.23977772\n",
      "Iteration 631, loss = 0.23948303\n",
      "Iteration 632, loss = 0.23918894\n",
      "Iteration 633, loss = 0.23889550\n",
      "Iteration 634, loss = 0.23860258\n",
      "Iteration 635, loss = 0.23831016\n",
      "Iteration 636, loss = 0.23801831\n",
      "Iteration 637, loss = 0.23772707\n",
      "Iteration 638, loss = 0.23743640\n",
      "Iteration 639, loss = 0.23714622\n",
      "Iteration 640, loss = 0.23685649\n",
      "Iteration 641, loss = 0.23656733\n",
      "Iteration 642, loss = 0.23627880\n",
      "Iteration 643, loss = 0.23599083\n",
      "Iteration 644, loss = 0.23570350\n",
      "Iteration 645, loss = 0.23541665\n",
      "Iteration 646, loss = 0.23513036\n",
      "Iteration 647, loss = 0.23484455\n",
      "Iteration 648, loss = 0.23455926\n",
      "Iteration 649, loss = 0.23427452\n",
      "Iteration 650, loss = 0.23399027\n",
      "Iteration 651, loss = 0.23370659\n",
      "Iteration 652, loss = 0.23342349\n",
      "Iteration 653, loss = 0.23314097\n",
      "Iteration 654, loss = 0.23285907\n",
      "Iteration 655, loss = 0.23257773\n",
      "Iteration 656, loss = 0.23229693\n",
      "Iteration 657, loss = 0.23201665\n",
      "Iteration 658, loss = 0.23173690\n",
      "Iteration 659, loss = 0.23145778\n",
      "Iteration 660, loss = 0.23117923\n",
      "Iteration 661, loss = 0.23090127\n",
      "Iteration 662, loss = 0.23062382\n",
      "Iteration 663, loss = 0.23034679\n",
      "Iteration 664, loss = 0.23007036\n",
      "Iteration 665, loss = 0.22979450\n",
      "Iteration 666, loss = 0.22951921\n",
      "Iteration 667, loss = 0.22924450\n",
      "Iteration 668, loss = 0.22897025\n",
      "Iteration 669, loss = 0.22869631\n",
      "Iteration 670, loss = 0.22842297\n",
      "Iteration 671, loss = 0.22815008\n",
      "Iteration 672, loss = 0.22787763\n",
      "Iteration 673, loss = 0.22760575\n",
      "Iteration 674, loss = 0.22733441\n",
      "Iteration 675, loss = 0.22706346\n",
      "Iteration 676, loss = 0.22679304\n",
      "Iteration 677, loss = 0.22652312\n",
      "Iteration 678, loss = 0.22625377\n",
      "Iteration 679, loss = 0.22598495\n",
      "Iteration 680, loss = 0.22571667\n",
      "Iteration 681, loss = 0.22544894\n",
      "Iteration 682, loss = 0.22518181\n",
      "Iteration 683, loss = 0.22491504\n",
      "Iteration 684, loss = 0.22464857\n",
      "Iteration 685, loss = 0.22438252\n",
      "Iteration 686, loss = 0.22411691\n",
      "Iteration 687, loss = 0.22385186\n",
      "Iteration 688, loss = 0.22358726\n",
      "Iteration 689, loss = 0.22332295\n",
      "Iteration 690, loss = 0.22305898\n",
      "Iteration 691, loss = 0.22279554\n",
      "Iteration 692, loss = 0.22253259\n",
      "Iteration 693, loss = 0.22227004\n",
      "Iteration 694, loss = 0.22200790\n",
      "Iteration 695, loss = 0.22174611\n",
      "Iteration 696, loss = 0.22148482\n",
      "Iteration 697, loss = 0.22122405\n",
      "Iteration 698, loss = 0.22096379\n",
      "Iteration 699, loss = 0.22070386\n",
      "Iteration 700, loss = 0.22044429\n",
      "Iteration 701, loss = 0.22018522\n",
      "Iteration 702, loss = 0.21992666\n",
      "Iteration 703, loss = 0.21966862\n",
      "Iteration 704, loss = 0.21941113\n",
      "Iteration 705, loss = 0.21915413\n",
      "Iteration 706, loss = 0.21889768\n",
      "Iteration 707, loss = 0.21864180\n",
      "Iteration 708, loss = 0.21838643\n",
      "Iteration 709, loss = 0.21813160\n",
      "Iteration 710, loss = 0.21787733\n",
      "Iteration 711, loss = 0.21762335\n",
      "Iteration 712, loss = 0.21736956\n",
      "Iteration 713, loss = 0.21711616\n",
      "Iteration 714, loss = 0.21686328\n",
      "Iteration 715, loss = 0.21661090\n",
      "Iteration 716, loss = 0.21635904\n",
      "Iteration 717, loss = 0.21610770\n",
      "Iteration 718, loss = 0.21585691\n",
      "Iteration 719, loss = 0.21560663\n",
      "Iteration 720, loss = 0.21535688\n",
      "Iteration 721, loss = 0.21510766\n",
      "Iteration 722, loss = 0.21485900\n",
      "Iteration 723, loss = 0.21461085\n",
      "Iteration 724, loss = 0.21436324\n",
      "Iteration 725, loss = 0.21411617\n",
      "Iteration 726, loss = 0.21386965\n",
      "Iteration 727, loss = 0.21362368\n",
      "Iteration 728, loss = 0.21337826\n",
      "Iteration 729, loss = 0.21313337\n",
      "Iteration 730, loss = 0.21288903\n",
      "Iteration 731, loss = 0.21264519\n",
      "Iteration 732, loss = 0.21240131\n",
      "Iteration 733, loss = 0.21215792\n",
      "Iteration 734, loss = 0.21191505\n",
      "Iteration 735, loss = 0.21167267\n",
      "Iteration 736, loss = 0.21143080\n",
      "Iteration 737, loss = 0.21118929\n",
      "Iteration 738, loss = 0.21094824\n",
      "Iteration 739, loss = 0.21070770\n",
      "Iteration 740, loss = 0.21046767\n",
      "Iteration 741, loss = 0.21022816\n",
      "Iteration 742, loss = 0.20998915\n",
      "Iteration 743, loss = 0.20975067\n",
      "Iteration 744, loss = 0.20951269\n",
      "Iteration 745, loss = 0.20927524\n",
      "Iteration 746, loss = 0.20903830\n",
      "Iteration 747, loss = 0.20880189\n",
      "Iteration 748, loss = 0.20856600\n",
      "Iteration 749, loss = 0.20833065\n",
      "Iteration 750, loss = 0.20809583\n",
      "Iteration 751, loss = 0.20786153\n",
      "Iteration 752, loss = 0.20762777\n",
      "Iteration 753, loss = 0.20739453\n",
      "Iteration 754, loss = 0.20716179\n",
      "Iteration 755, loss = 0.20692958\n",
      "Iteration 756, loss = 0.20669778\n",
      "Iteration 757, loss = 0.20646645\n",
      "Iteration 758, loss = 0.20623563\n",
      "Iteration 759, loss = 0.20600533\n",
      "Iteration 760, loss = 0.20577554\n",
      "Iteration 761, loss = 0.20554627\n",
      "Iteration 762, loss = 0.20531751\n",
      "Iteration 763, loss = 0.20508928\n",
      "Iteration 764, loss = 0.20486150\n",
      "Iteration 765, loss = 0.20463414\n",
      "Iteration 766, loss = 0.20440727\n",
      "Iteration 767, loss = 0.20418090\n",
      "Iteration 768, loss = 0.20395502\n",
      "Iteration 769, loss = 0.20372964\n",
      "Iteration 770, loss = 0.20350476\n",
      "Iteration 771, loss = 0.20328037\n",
      "Iteration 772, loss = 0.20305648\n",
      "Iteration 773, loss = 0.20283301\n",
      "Iteration 774, loss = 0.20260995\n",
      "Iteration 775, loss = 0.20238732\n",
      "Iteration 776, loss = 0.20216518\n",
      "Iteration 777, loss = 0.20194352\n",
      "Iteration 778, loss = 0.20172233\n",
      "Iteration 779, loss = 0.20150163\n",
      "Iteration 780, loss = 0.20128141\n",
      "Iteration 781, loss = 0.20106176\n",
      "Iteration 782, loss = 0.20084246\n",
      "Iteration 783, loss = 0.20062372\n",
      "Iteration 784, loss = 0.20040546\n",
      "Iteration 785, loss = 0.20018760\n",
      "Iteration 786, loss = 0.19997026\n",
      "Iteration 787, loss = 0.19975337\n",
      "Iteration 788, loss = 0.19953695\n",
      "Iteration 789, loss = 0.19932100\n",
      "Iteration 790, loss = 0.19910554\n",
      "Iteration 791, loss = 0.19889053\n",
      "Iteration 792, loss = 0.19867599\n",
      "Iteration 793, loss = 0.19846184\n",
      "Iteration 794, loss = 0.19824813\n",
      "Iteration 795, loss = 0.19803488\n",
      "Iteration 796, loss = 0.19782212\n",
      "Iteration 797, loss = 0.19760973\n",
      "Iteration 798, loss = 0.19739777\n",
      "Iteration 799, loss = 0.19718611\n",
      "Iteration 800, loss = 0.19697486\n",
      "Iteration 801, loss = 0.19676406\n",
      "Iteration 802, loss = 0.19655362\n",
      "Iteration 803, loss = 0.19634360\n",
      "Iteration 804, loss = 0.19613403\n",
      "Iteration 805, loss = 0.19592489\n",
      "Iteration 806, loss = 0.19571622\n",
      "Iteration 807, loss = 0.19550797\n",
      "Iteration 808, loss = 0.19530019\n",
      "Iteration 809, loss = 0.19509286\n",
      "Iteration 810, loss = 0.19488598\n",
      "Iteration 811, loss = 0.19467956\n",
      "Iteration 812, loss = 0.19447358\n",
      "Iteration 813, loss = 0.19426804\n",
      "Iteration 814, loss = 0.19406296\n",
      "Iteration 815, loss = 0.19385826\n",
      "Iteration 816, loss = 0.19365391\n",
      "Iteration 817, loss = 0.19344997\n",
      "Iteration 818, loss = 0.19324647\n",
      "Iteration 819, loss = 0.19304341\n",
      "Iteration 820, loss = 0.19284078\n",
      "Iteration 821, loss = 0.19263859\n",
      "Iteration 822, loss = 0.19243681\n",
      "Iteration 823, loss = 0.19223515\n",
      "Iteration 824, loss = 0.19203388\n",
      "Iteration 825, loss = 0.19183303\n",
      "Iteration 826, loss = 0.19163258\n",
      "Iteration 827, loss = 0.19143243\n",
      "Iteration 828, loss = 0.19123253\n",
      "Iteration 829, loss = 0.19103300\n",
      "Iteration 830, loss = 0.19083388\n",
      "Iteration 831, loss = 0.19063515\n",
      "Iteration 832, loss = 0.19043683\n",
      "Iteration 833, loss = 0.19023891\n",
      "Iteration 834, loss = 0.19004135\n",
      "Iteration 835, loss = 0.18984405\n",
      "Iteration 836, loss = 0.18964718\n",
      "Iteration 837, loss = 0.18945064\n",
      "Iteration 838, loss = 0.18925437\n",
      "Iteration 839, loss = 0.18905852\n",
      "Iteration 840, loss = 0.18886298\n",
      "Iteration 841, loss = 0.18866776\n",
      "Iteration 842, loss = 0.18847288\n",
      "Iteration 843, loss = 0.18827838\n",
      "Iteration 844, loss = 0.18808427\n",
      "Iteration 845, loss = 0.18789053\n",
      "Iteration 846, loss = 0.18769713\n",
      "Iteration 847, loss = 0.18750415\n",
      "Iteration 848, loss = 0.18731145\n",
      "Iteration 849, loss = 0.18711894\n",
      "Iteration 850, loss = 0.18692681\n",
      "Iteration 851, loss = 0.18673503\n",
      "Iteration 852, loss = 0.18654358\n",
      "Iteration 853, loss = 0.18635236\n",
      "Iteration 854, loss = 0.18616136\n",
      "Iteration 855, loss = 0.18597075\n",
      "Iteration 856, loss = 0.18578049\n",
      "Iteration 857, loss = 0.18559061\n",
      "Iteration 858, loss = 0.18540111\n",
      "Iteration 859, loss = 0.18521199\n",
      "Iteration 860, loss = 0.18502326\n",
      "Iteration 861, loss = 0.18483493\n",
      "Iteration 862, loss = 0.18464698\n",
      "Iteration 863, loss = 0.18445942\n",
      "Iteration 864, loss = 0.18427222\n",
      "Iteration 865, loss = 0.18408531\n",
      "Iteration 866, loss = 0.18389880\n",
      "Iteration 867, loss = 0.18371267\n",
      "Iteration 868, loss = 0.18352693\n",
      "Iteration 869, loss = 0.18334151\n",
      "Iteration 870, loss = 0.18315650\n",
      "Iteration 871, loss = 0.18297270\n",
      "Iteration 872, loss = 0.18278922\n",
      "Iteration 873, loss = 0.18260616\n",
      "Iteration 874, loss = 0.18242353\n",
      "Iteration 875, loss = 0.18224132\n",
      "Iteration 876, loss = 0.18205955\n",
      "Iteration 877, loss = 0.18187819\n",
      "Iteration 878, loss = 0.18169715\n",
      "Iteration 879, loss = 0.18151624\n",
      "Iteration 880, loss = 0.18133567\n",
      "Iteration 881, loss = 0.18115538\n",
      "Iteration 882, loss = 0.18097538\n",
      "Iteration 883, loss = 0.18079575\n",
      "Iteration 884, loss = 0.18061649\n",
      "Iteration 885, loss = 0.18043760\n",
      "Iteration 886, loss = 0.18025911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 887, loss = 0.18008099\n",
      "Iteration 888, loss = 0.17990323\n",
      "Iteration 889, loss = 0.17972582\n",
      "Iteration 890, loss = 0.17954875\n",
      "Iteration 891, loss = 0.17937205\n",
      "Iteration 892, loss = 0.17919575\n",
      "Iteration 893, loss = 0.17901977\n",
      "Iteration 894, loss = 0.17884416\n",
      "Iteration 895, loss = 0.17866892\n",
      "Iteration 896, loss = 0.17849407\n",
      "Iteration 897, loss = 0.17831958\n",
      "Iteration 898, loss = 0.17814548\n",
      "Iteration 899, loss = 0.17797175\n",
      "Iteration 900, loss = 0.17779838\n",
      "Iteration 901, loss = 0.17762538\n",
      "Iteration 902, loss = 0.17745278\n",
      "Iteration 903, loss = 0.17728073\n",
      "Iteration 904, loss = 0.17710943\n",
      "Iteration 905, loss = 0.17693856\n",
      "Iteration 906, loss = 0.17676918\n",
      "Iteration 907, loss = 0.17660028\n",
      "Iteration 908, loss = 0.17643174\n",
      "Iteration 909, loss = 0.17626363\n",
      "Iteration 910, loss = 0.17609592\n",
      "Iteration 911, loss = 0.17592862\n",
      "Iteration 912, loss = 0.17576164\n",
      "Iteration 913, loss = 0.17559498\n",
      "Iteration 914, loss = 0.17542871\n",
      "Iteration 915, loss = 0.17526280\n",
      "Iteration 916, loss = 0.17509728\n",
      "Iteration 917, loss = 0.17493213\n",
      "Iteration 918, loss = 0.17476735\n",
      "Iteration 919, loss = 0.17460293\n",
      "Iteration 920, loss = 0.17443891\n",
      "Iteration 921, loss = 0.17427517\n",
      "Iteration 922, loss = 0.17411182\n",
      "Iteration 923, loss = 0.17394882\n",
      "Iteration 924, loss = 0.17378616\n",
      "Iteration 925, loss = 0.17362387\n",
      "Iteration 926, loss = 0.17346224\n",
      "Iteration 927, loss = 0.17330113\n",
      "Iteration 928, loss = 0.17314038\n",
      "Iteration 929, loss = 0.17297998\n",
      "Iteration 930, loss = 0.17281994\n",
      "Iteration 931, loss = 0.17266025\n",
      "Iteration 932, loss = 0.17250090\n",
      "Iteration 933, loss = 0.17234194\n",
      "Iteration 934, loss = 0.17218347\n",
      "Iteration 935, loss = 0.17202552\n",
      "Iteration 936, loss = 0.17186792\n",
      "Iteration 937, loss = 0.17171066\n",
      "Iteration 938, loss = 0.17155374\n",
      "Iteration 939, loss = 0.17139719\n",
      "Iteration 940, loss = 0.17124097\n",
      "Iteration 941, loss = 0.17108509\n",
      "Iteration 942, loss = 0.17092956\n",
      "Iteration 943, loss = 0.17077436\n",
      "Iteration 944, loss = 0.17061950\n",
      "Iteration 945, loss = 0.17046496\n",
      "Iteration 946, loss = 0.17031074\n",
      "Iteration 947, loss = 0.17015685\n",
      "Iteration 948, loss = 0.17000328\n",
      "Iteration 949, loss = 0.16985004\n",
      "Iteration 950, loss = 0.16969710\n",
      "Iteration 951, loss = 0.16954448\n",
      "Iteration 952, loss = 0.16939218\n",
      "Iteration 953, loss = 0.16924018\n",
      "Iteration 954, loss = 0.16908850\n",
      "Iteration 955, loss = 0.16893713\n",
      "Iteration 956, loss = 0.16878608\n",
      "Iteration 957, loss = 0.16863534\n",
      "Iteration 958, loss = 0.16848493\n",
      "Iteration 959, loss = 0.16833479\n",
      "Iteration 960, loss = 0.16818498\n",
      "Iteration 961, loss = 0.16803546\n",
      "Iteration 962, loss = 0.16788625\n",
      "Iteration 963, loss = 0.16773733\n",
      "Iteration 964, loss = 0.16758872\n",
      "Iteration 965, loss = 0.16744042\n",
      "Iteration 966, loss = 0.16729244\n",
      "Iteration 967, loss = 0.16714475\n",
      "Iteration 968, loss = 0.16699737\n",
      "Iteration 969, loss = 0.16685028\n",
      "Iteration 970, loss = 0.16670349\n",
      "Iteration 971, loss = 0.16655699\n",
      "Iteration 972, loss = 0.16641079\n",
      "Iteration 973, loss = 0.16626489\n",
      "Iteration 974, loss = 0.16611927\n",
      "Iteration 975, loss = 0.16597395\n",
      "Iteration 976, loss = 0.16582893\n",
      "Iteration 977, loss = 0.16568419\n",
      "Iteration 978, loss = 0.16553975\n",
      "Iteration 979, loss = 0.16539560\n",
      "Iteration 980, loss = 0.16525174\n",
      "Iteration 981, loss = 0.16510818\n",
      "Iteration 982, loss = 0.16496511\n",
      "Iteration 983, loss = 0.16482223\n",
      "Iteration 984, loss = 0.16467971\n",
      "Iteration 985, loss = 0.16453745\n",
      "Iteration 986, loss = 0.16439547\n",
      "Iteration 987, loss = 0.16425384\n",
      "Iteration 988, loss = 0.16411241\n",
      "Iteration 989, loss = 0.16397127\n",
      "Iteration 990, loss = 0.16383041\n",
      "Iteration 991, loss = 0.16368994\n",
      "Iteration 992, loss = 0.16354958\n",
      "Iteration 993, loss = 0.16340956\n",
      "Iteration 994, loss = 0.16326986\n",
      "Iteration 995, loss = 0.16313042\n",
      "Iteration 996, loss = 0.16299125\n",
      "Iteration 997, loss = 0.16285234\n",
      "Iteration 998, loss = 0.16271369\n",
      "Iteration 999, loss = 0.16257545\n",
      "Iteration 1000, loss = 0.16243730\n",
      "Iteration 1, loss = 1.66075849\n",
      "Iteration 2, loss = 1.62388596\n",
      "Iteration 3, loss = 1.57434597\n",
      "Iteration 4, loss = 1.51652159\n",
      "Iteration 5, loss = 1.45485615\n",
      "Iteration 6, loss = 1.39392087\n",
      "Iteration 7, loss = 1.33762662\n",
      "Iteration 8, loss = 1.28919141\n",
      "Iteration 9, loss = 1.25059449\n",
      "Iteration 10, loss = 1.22200674\n",
      "Iteration 11, loss = 1.20197852\n",
      "Iteration 12, loss = 1.18788886\n",
      "Iteration 13, loss = 1.17669170\n",
      "Iteration 14, loss = 1.16574471\n",
      "Iteration 15, loss = 1.15300393\n",
      "Iteration 16, loss = 1.13744268\n",
      "Iteration 17, loss = 1.11901001\n",
      "Iteration 18, loss = 1.09866789\n",
      "Iteration 19, loss = 1.07688439\n",
      "Iteration 20, loss = 1.05470220\n",
      "Iteration 21, loss = 1.03305136\n",
      "Iteration 22, loss = 1.01266515\n",
      "Iteration 23, loss = 0.99413781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 24, loss = 0.97754961\n",
      "Iteration 25, loss = 0.96282171\n",
      "Iteration 26, loss = 0.94958846\n",
      "Iteration 27, loss = 0.93758139\n",
      "Iteration 28, loss = 0.92638356\n",
      "Iteration 29, loss = 0.91567730\n",
      "Iteration 30, loss = 0.90527386\n",
      "Iteration 31, loss = 0.89504738\n",
      "Iteration 32, loss = 0.88503865\n",
      "Iteration 33, loss = 0.87534103\n",
      "Iteration 34, loss = 0.86606897\n",
      "Iteration 35, loss = 0.85726890\n",
      "Iteration 36, loss = 0.84881851\n",
      "Iteration 37, loss = 0.84066406\n",
      "Iteration 38, loss = 0.83284230\n",
      "Iteration 39, loss = 0.82537415\n",
      "Iteration 40, loss = 0.81822893\n",
      "Iteration 41, loss = 0.81135424\n",
      "Iteration 42, loss = 0.80468422\n",
      "Iteration 43, loss = 0.79821176\n",
      "Iteration 44, loss = 0.79190750\n",
      "Iteration 45, loss = 0.78576647\n",
      "Iteration 46, loss = 0.77976610\n",
      "Iteration 47, loss = 0.77387221\n",
      "Iteration 48, loss = 0.76808781\n",
      "Iteration 49, loss = 0.76241777\n",
      "Iteration 50, loss = 0.75686946\n",
      "Iteration 51, loss = 0.75145200\n",
      "Iteration 52, loss = 0.74617179\n",
      "Iteration 53, loss = 0.74102654\n",
      "Iteration 54, loss = 0.73601291\n",
      "Iteration 55, loss = 0.73112303\n",
      "Iteration 56, loss = 0.72634884\n",
      "Iteration 57, loss = 0.72168226\n",
      "Iteration 58, loss = 0.71711613\n",
      "Iteration 59, loss = 0.71264486\n",
      "Iteration 60, loss = 0.70826467\n",
      "Iteration 61, loss = 0.70397281\n",
      "Iteration 62, loss = 0.69976757\n",
      "Iteration 63, loss = 0.69564778\n",
      "Iteration 64, loss = 0.69161407\n",
      "Iteration 65, loss = 0.68766380\n",
      "Iteration 66, loss = 0.68379564\n",
      "Iteration 67, loss = 0.68000731\n",
      "Iteration 68, loss = 0.67629442\n",
      "Iteration 69, loss = 0.67265737\n",
      "Iteration 70, loss = 0.66909244\n",
      "Iteration 71, loss = 0.66559681\n",
      "Iteration 72, loss = 0.66216598\n",
      "Iteration 73, loss = 0.65879859\n",
      "Iteration 74, loss = 0.65549453\n",
      "Iteration 75, loss = 0.65225207\n",
      "Iteration 76, loss = 0.64906687\n",
      "Iteration 77, loss = 0.64593804\n",
      "Iteration 78, loss = 0.64286508\n",
      "Iteration 79, loss = 0.63984658\n",
      "Iteration 80, loss = 0.63688115\n",
      "Iteration 81, loss = 0.63396513\n",
      "Iteration 82, loss = 0.63109808\n",
      "Iteration 83, loss = 0.62827891\n",
      "Iteration 84, loss = 0.62550601\n",
      "Iteration 85, loss = 0.62277950\n",
      "Iteration 86, loss = 0.62009857\n",
      "Iteration 87, loss = 0.61746215\n",
      "Iteration 88, loss = 0.61486583\n",
      "Iteration 89, loss = 0.61230601\n",
      "Iteration 90, loss = 0.60977871\n",
      "Iteration 91, loss = 0.60728919\n",
      "Iteration 92, loss = 0.60483815\n",
      "Iteration 93, loss = 0.60242469\n",
      "Iteration 94, loss = 0.60004573\n",
      "Iteration 95, loss = 0.59769995\n",
      "Iteration 96, loss = 0.59538755\n",
      "Iteration 97, loss = 0.59310831\n",
      "Iteration 98, loss = 0.59086110\n",
      "Iteration 99, loss = 0.58864671\n",
      "Iteration 100, loss = 0.58646382\n",
      "Iteration 101, loss = 0.58431184\n",
      "Iteration 102, loss = 0.58218826\n",
      "Iteration 103, loss = 0.58009333\n",
      "Iteration 104, loss = 0.57802543\n",
      "Iteration 105, loss = 0.57598433\n",
      "Iteration 106, loss = 0.57397210\n",
      "Iteration 107, loss = 0.57198652\n",
      "Iteration 108, loss = 0.57002847\n",
      "Iteration 109, loss = 0.56809647\n",
      "Iteration 110, loss = 0.56618844\n",
      "Iteration 111, loss = 0.56430495\n",
      "Iteration 112, loss = 0.56244058\n",
      "Iteration 113, loss = 0.56059714\n",
      "Iteration 114, loss = 0.55877675\n",
      "Iteration 115, loss = 0.55698246\n",
      "Iteration 116, loss = 0.55520687\n",
      "Iteration 117, loss = 0.55344732\n",
      "Iteration 118, loss = 0.55170693\n",
      "Iteration 119, loss = 0.54998810\n",
      "Iteration 120, loss = 0.54827901\n",
      "Iteration 121, loss = 0.54659049\n",
      "Iteration 122, loss = 0.54492043\n",
      "Iteration 123, loss = 0.54326629\n",
      "Iteration 124, loss = 0.54162510\n",
      "Iteration 125, loss = 0.53999864\n",
      "Iteration 126, loss = 0.53838432\n",
      "Iteration 127, loss = 0.53679324\n",
      "Iteration 128, loss = 0.53522170\n",
      "Iteration 129, loss = 0.53366660\n",
      "Iteration 130, loss = 0.53213036\n",
      "Iteration 131, loss = 0.53061516\n",
      "Iteration 132, loss = 0.52912059\n",
      "Iteration 133, loss = 0.52765639\n",
      "Iteration 134, loss = 0.52621776\n",
      "Iteration 135, loss = 0.52479634\n",
      "Iteration 136, loss = 0.52339530\n",
      "Iteration 137, loss = 0.52201447\n",
      "Iteration 138, loss = 0.52065154\n",
      "Iteration 139, loss = 0.51930866\n",
      "Iteration 140, loss = 0.51798445\n",
      "Iteration 141, loss = 0.51667485\n",
      "Iteration 142, loss = 0.51538137\n",
      "Iteration 143, loss = 0.51410476\n",
      "Iteration 144, loss = 0.51284247\n",
      "Iteration 145, loss = 0.51159338\n",
      "Iteration 146, loss = 0.51035741\n",
      "Iteration 147, loss = 0.50913373\n",
      "Iteration 148, loss = 0.50792160\n",
      "Iteration 149, loss = 0.50672130\n",
      "Iteration 150, loss = 0.50553154\n",
      "Iteration 151, loss = 0.50435169\n",
      "Iteration 152, loss = 0.50318161\n",
      "Iteration 153, loss = 0.50202140\n",
      "Iteration 154, loss = 0.50087069\n",
      "Iteration 155, loss = 0.49972933\n",
      "Iteration 156, loss = 0.49859722\n",
      "Iteration 157, loss = 0.49747481\n",
      "Iteration 158, loss = 0.49636137\n",
      "Iteration 159, loss = 0.49525680\n",
      "Iteration 160, loss = 0.49416128\n",
      "Iteration 161, loss = 0.49307507\n",
      "Iteration 162, loss = 0.49199808\n",
      "Iteration 163, loss = 0.49092967\n",
      "Iteration 164, loss = 0.48986968\n",
      "Iteration 165, loss = 0.48881855\n",
      "Iteration 166, loss = 0.48777556\n",
      "Iteration 167, loss = 0.48674043\n",
      "Iteration 168, loss = 0.48571312\n",
      "Iteration 169, loss = 0.48469403\n",
      "Iteration 170, loss = 0.48368259\n",
      "Iteration 171, loss = 0.48267859\n",
      "Iteration 172, loss = 0.48168182\n",
      "Iteration 173, loss = 0.48069216\n",
      "Iteration 174, loss = 0.47970947\n",
      "Iteration 175, loss = 0.47873372\n",
      "Iteration 176, loss = 0.47776525\n",
      "Iteration 177, loss = 0.47680317\n",
      "Iteration 178, loss = 0.47584756\n",
      "Iteration 179, loss = 0.47489813\n",
      "Iteration 180, loss = 0.47395483\n",
      "Iteration 181, loss = 0.47301801\n",
      "Iteration 182, loss = 0.47208751\n",
      "Iteration 183, loss = 0.47116294\n",
      "Iteration 184, loss = 0.47024421\n",
      "Iteration 185, loss = 0.46933136\n",
      "Iteration 186, loss = 0.46842401\n",
      "Iteration 187, loss = 0.46752241\n",
      "Iteration 188, loss = 0.46662628\n",
      "Iteration 189, loss = 0.46573583\n",
      "Iteration 190, loss = 0.46485063\n",
      "Iteration 191, loss = 0.46397080\n",
      "Iteration 192, loss = 0.46309626\n",
      "Iteration 193, loss = 0.46222697\n",
      "Iteration 194, loss = 0.46136267\n",
      "Iteration 195, loss = 0.46050352\n",
      "Iteration 196, loss = 0.45964913\n",
      "Iteration 197, loss = 0.45879972\n",
      "Iteration 198, loss = 0.45795515\n",
      "Iteration 199, loss = 0.45711534\n",
      "Iteration 200, loss = 0.45628025\n",
      "Iteration 201, loss = 0.45544960\n",
      "Iteration 202, loss = 0.45462358\n",
      "Iteration 203, loss = 0.45380149\n",
      "Iteration 204, loss = 0.45298390\n",
      "Iteration 205, loss = 0.45217052\n",
      "Iteration 206, loss = 0.45136139\n",
      "Iteration 207, loss = 0.45055687\n",
      "Iteration 208, loss = 0.44975619\n",
      "Iteration 209, loss = 0.44895961\n",
      "Iteration 210, loss = 0.44816693\n",
      "Iteration 211, loss = 0.44737782\n",
      "Iteration 212, loss = 0.44659254\n",
      "Iteration 213, loss = 0.44581085\n",
      "Iteration 214, loss = 0.44503296\n",
      "Iteration 215, loss = 0.44425892\n",
      "Iteration 216, loss = 0.44348863\n",
      "Iteration 217, loss = 0.44272182\n",
      "Iteration 218, loss = 0.44195861\n",
      "Iteration 219, loss = 0.44119824\n",
      "Iteration 220, loss = 0.44044103\n",
      "Iteration 221, loss = 0.43968714\n",
      "Iteration 222, loss = 0.43893638\n",
      "Iteration 223, loss = 0.43818863\n",
      "Iteration 224, loss = 0.43744373\n",
      "Iteration 225, loss = 0.43670139\n",
      "Iteration 226, loss = 0.43596185\n",
      "Iteration 227, loss = 0.43522551\n",
      "Iteration 228, loss = 0.43449176\n",
      "Iteration 229, loss = 0.43376094\n",
      "Iteration 230, loss = 0.43303396\n",
      "Iteration 231, loss = 0.43230967\n",
      "Iteration 232, loss = 0.43158818\n",
      "Iteration 233, loss = 0.43086926\n",
      "Iteration 234, loss = 0.43015210\n",
      "Iteration 235, loss = 0.42943772\n",
      "Iteration 236, loss = 0.42872549\n",
      "Iteration 237, loss = 0.42801581\n",
      "Iteration 238, loss = 0.42730897\n",
      "Iteration 239, loss = 0.42660474\n",
      "Iteration 240, loss = 0.42590309\n",
      "Iteration 241, loss = 0.42520388\n",
      "Iteration 242, loss = 0.42450708\n",
      "Iteration 243, loss = 0.42381207\n",
      "Iteration 244, loss = 0.42311970\n",
      "Iteration 245, loss = 0.42242953\n",
      "Iteration 246, loss = 0.42174076\n",
      "Iteration 247, loss = 0.42105398\n",
      "Iteration 248, loss = 0.42036629\n",
      "Iteration 249, loss = 0.41967899\n",
      "Iteration 250, loss = 0.41899448\n",
      "Iteration 251, loss = 0.41831194\n",
      "Iteration 252, loss = 0.41763174\n",
      "Iteration 253, loss = 0.41695344\n",
      "Iteration 254, loss = 0.41627726\n",
      "Iteration 255, loss = 0.41560388\n",
      "Iteration 256, loss = 0.41493407\n",
      "Iteration 257, loss = 0.41426711\n",
      "Iteration 258, loss = 0.41360217\n",
      "Iteration 259, loss = 0.41293944\n",
      "Iteration 260, loss = 0.41227893\n",
      "Iteration 261, loss = 0.41162113\n",
      "Iteration 262, loss = 0.41096525\n",
      "Iteration 263, loss = 0.41030691\n",
      "Iteration 264, loss = 0.40964857\n",
      "Iteration 265, loss = 0.40898574\n",
      "Iteration 266, loss = 0.40831942\n",
      "Iteration 267, loss = 0.40764717\n",
      "Iteration 268, loss = 0.40697233\n",
      "Iteration 269, loss = 0.40628932\n",
      "Iteration 270, loss = 0.40560366\n",
      "Iteration 271, loss = 0.40491614\n",
      "Iteration 272, loss = 0.40421801\n",
      "Iteration 273, loss = 0.40351260\n",
      "Iteration 274, loss = 0.40279469\n",
      "Iteration 275, loss = 0.40206688\n",
      "Iteration 276, loss = 0.40132111\n",
      "Iteration 277, loss = 0.40056318\n",
      "Iteration 278, loss = 0.39978670\n",
      "Iteration 279, loss = 0.39898941\n",
      "Iteration 280, loss = 0.39818121\n",
      "Iteration 281, loss = 0.39737298\n",
      "Iteration 282, loss = 0.39656732\n",
      "Iteration 283, loss = 0.39575804\n",
      "Iteration 284, loss = 0.39492105\n",
      "Iteration 285, loss = 0.39409101\n",
      "Iteration 286, loss = 0.39329334\n",
      "Iteration 287, loss = 0.39253069\n",
      "Iteration 288, loss = 0.39178893\n",
      "Iteration 289, loss = 0.39106766\n",
      "Iteration 290, loss = 0.39035648\n",
      "Iteration 291, loss = 0.38966845\n",
      "Iteration 292, loss = 0.38900266\n",
      "Iteration 293, loss = 0.38834650\n",
      "Iteration 294, loss = 0.38769999\n",
      "Iteration 295, loss = 0.38706637\n",
      "Iteration 296, loss = 0.38643886\n",
      "Iteration 297, loss = 0.38582411\n",
      "Iteration 298, loss = 0.38521939\n",
      "Iteration 299, loss = 0.38462117\n",
      "Iteration 300, loss = 0.38402811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 301, loss = 0.38343837\n",
      "Iteration 302, loss = 0.38285490\n",
      "Iteration 303, loss = 0.38228149\n",
      "Iteration 304, loss = 0.38171306\n",
      "Iteration 305, loss = 0.38115054\n",
      "Iteration 306, loss = 0.38059023\n",
      "Iteration 307, loss = 0.38003219\n",
      "Iteration 308, loss = 0.37947686\n",
      "Iteration 309, loss = 0.37892516\n",
      "Iteration 310, loss = 0.37837552\n",
      "Iteration 311, loss = 0.37782806\n",
      "Iteration 312, loss = 0.37728328\n",
      "Iteration 313, loss = 0.37674145\n",
      "Iteration 314, loss = 0.37620124\n",
      "Iteration 315, loss = 0.37566271\n",
      "Iteration 316, loss = 0.37512578\n",
      "Iteration 317, loss = 0.37459014\n",
      "Iteration 318, loss = 0.37405618\n",
      "Iteration 319, loss = 0.37352378\n",
      "Iteration 320, loss = 0.37299292\n",
      "Iteration 321, loss = 0.37246344\n",
      "Iteration 322, loss = 0.37193544\n",
      "Iteration 323, loss = 0.37140898\n",
      "Iteration 324, loss = 0.37088407\n",
      "Iteration 325, loss = 0.37036062\n",
      "Iteration 326, loss = 0.36983858\n",
      "Iteration 327, loss = 0.36931798\n",
      "Iteration 328, loss = 0.36879885\n",
      "Iteration 329, loss = 0.36828113\n",
      "Iteration 330, loss = 0.36776484\n",
      "Iteration 331, loss = 0.36725009\n",
      "Iteration 332, loss = 0.36673673\n",
      "Iteration 333, loss = 0.36622467\n",
      "Iteration 334, loss = 0.36571387\n",
      "Iteration 335, loss = 0.36520481\n",
      "Iteration 336, loss = 0.36469706\n",
      "Iteration 337, loss = 0.36419054\n",
      "Iteration 338, loss = 0.36368531\n",
      "Iteration 339, loss = 0.36318125\n",
      "Iteration 340, loss = 0.36267848\n",
      "Iteration 341, loss = 0.36217709\n",
      "Iteration 342, loss = 0.36167703\n",
      "Iteration 343, loss = 0.36117829\n",
      "Iteration 344, loss = 0.36068079\n",
      "Iteration 345, loss = 0.36018460\n",
      "Iteration 346, loss = 0.35968959\n",
      "Iteration 347, loss = 0.35919586\n",
      "Iteration 348, loss = 0.35870340\n",
      "Iteration 349, loss = 0.35821209\n",
      "Iteration 350, loss = 0.35772184\n",
      "Iteration 351, loss = 0.35723279\n",
      "Iteration 352, loss = 0.35674505\n",
      "Iteration 353, loss = 0.35625858\n",
      "Iteration 354, loss = 0.35577341\n",
      "Iteration 355, loss = 0.35528940\n",
      "Iteration 356, loss = 0.35480656\n",
      "Iteration 357, loss = 0.35432472\n",
      "Iteration 358, loss = 0.35384408\n",
      "Iteration 359, loss = 0.35336456\n",
      "Iteration 360, loss = 0.35288616\n",
      "Iteration 361, loss = 0.35240891\n",
      "Iteration 362, loss = 0.35193286\n",
      "Iteration 363, loss = 0.35145789\n",
      "Iteration 364, loss = 0.35098401\n",
      "Iteration 365, loss = 0.35051122\n",
      "Iteration 366, loss = 0.35003953\n",
      "Iteration 367, loss = 0.34956888\n",
      "Iteration 368, loss = 0.34909924\n",
      "Iteration 369, loss = 0.34863072\n",
      "Iteration 370, loss = 0.34816321\n",
      "Iteration 371, loss = 0.34769690\n",
      "Iteration 372, loss = 0.34723165\n",
      "Iteration 373, loss = 0.34676736\n",
      "Iteration 374, loss = 0.34630414\n",
      "Iteration 375, loss = 0.34584194\n",
      "Iteration 376, loss = 0.34538078\n",
      "Iteration 377, loss = 0.34492064\n",
      "Iteration 378, loss = 0.34446150\n",
      "Iteration 379, loss = 0.34400346\n",
      "Iteration 380, loss = 0.34354634\n",
      "Iteration 381, loss = 0.34309027\n",
      "Iteration 382, loss = 0.34263527\n",
      "Iteration 383, loss = 0.34218118\n",
      "Iteration 384, loss = 0.34172827\n",
      "Iteration 385, loss = 0.34127630\n",
      "Iteration 386, loss = 0.34082553\n",
      "Iteration 387, loss = 0.34037577\n",
      "Iteration 388, loss = 0.33992704\n",
      "Iteration 389, loss = 0.33947943\n",
      "Iteration 390, loss = 0.33903275\n",
      "Iteration 391, loss = 0.33858712\n",
      "Iteration 392, loss = 0.33814254\n",
      "Iteration 393, loss = 0.33769888\n",
      "Iteration 394, loss = 0.33725626\n",
      "Iteration 395, loss = 0.33681463\n",
      "Iteration 396, loss = 0.33637403\n",
      "Iteration 397, loss = 0.33593438\n",
      "Iteration 398, loss = 0.33549569\n",
      "Iteration 399, loss = 0.33505799\n",
      "Iteration 400, loss = 0.33462122\n",
      "Iteration 401, loss = 0.33418537\n",
      "Iteration 402, loss = 0.33375051\n",
      "Iteration 403, loss = 0.33331659\n",
      "Iteration 404, loss = 0.33288367\n",
      "Iteration 405, loss = 0.33245169\n",
      "Iteration 406, loss = 0.33202063\n",
      "Iteration 407, loss = 0.33159050\n",
      "Iteration 408, loss = 0.33116134\n",
      "Iteration 409, loss = 0.33073306\n",
      "Iteration 410, loss = 0.33030570\n",
      "Iteration 411, loss = 0.32987934\n",
      "Iteration 412, loss = 0.32945396\n",
      "Iteration 413, loss = 0.32902942\n",
      "Iteration 414, loss = 0.32860586\n",
      "Iteration 415, loss = 0.32818319\n",
      "Iteration 416, loss = 0.32776147\n",
      "Iteration 417, loss = 0.32734070\n",
      "Iteration 418, loss = 0.32692081\n",
      "Iteration 419, loss = 0.32650180\n",
      "Iteration 420, loss = 0.32608373\n",
      "Iteration 421, loss = 0.32566659\n",
      "Iteration 422, loss = 0.32525040\n",
      "Iteration 423, loss = 0.32483514\n",
      "Iteration 424, loss = 0.32442075\n",
      "Iteration 425, loss = 0.32400722\n",
      "Iteration 426, loss = 0.32359463\n",
      "Iteration 427, loss = 0.32318290\n",
      "Iteration 428, loss = 0.32277204\n",
      "Iteration 429, loss = 0.32236214\n",
      "Iteration 430, loss = 0.32195304\n",
      "Iteration 431, loss = 0.32154481\n",
      "Iteration 432, loss = 0.32113753\n",
      "Iteration 433, loss = 0.32073104\n",
      "Iteration 434, loss = 0.32032540\n",
      "Iteration 435, loss = 0.31992068\n",
      "Iteration 436, loss = 0.31951673\n",
      "Iteration 437, loss = 0.31911372\n",
      "Iteration 438, loss = 0.31871150\n",
      "Iteration 439, loss = 0.31831013\n",
      "Iteration 440, loss = 0.31790971\n",
      "Iteration 441, loss = 0.31751004\n",
      "Iteration 442, loss = 0.31711128\n",
      "Iteration 443, loss = 0.31671343\n",
      "Iteration 444, loss = 0.31631635\n",
      "Iteration 445, loss = 0.31592013\n",
      "Iteration 446, loss = 0.31552477\n",
      "Iteration 447, loss = 0.31513020\n",
      "Iteration 448, loss = 0.31473654\n",
      "Iteration 449, loss = 0.31434360\n",
      "Iteration 450, loss = 0.31395155\n",
      "Iteration 451, loss = 0.31356051\n",
      "Iteration 452, loss = 0.31317030\n",
      "Iteration 453, loss = 0.31278098\n",
      "Iteration 454, loss = 0.31239245\n",
      "Iteration 455, loss = 0.31200482\n",
      "Iteration 456, loss = 0.31161798\n",
      "Iteration 457, loss = 0.31123193\n",
      "Iteration 458, loss = 0.31084677\n",
      "Iteration 459, loss = 0.31046242\n",
      "Iteration 460, loss = 0.31007884\n",
      "Iteration 461, loss = 0.30969605\n",
      "Iteration 462, loss = 0.30931408\n",
      "Iteration 463, loss = 0.30893301\n",
      "Iteration 464, loss = 0.30855274\n",
      "Iteration 465, loss = 0.30817322\n",
      "Iteration 466, loss = 0.30779452\n",
      "Iteration 467, loss = 0.30741691\n",
      "Iteration 468, loss = 0.30704007\n",
      "Iteration 469, loss = 0.30666403\n",
      "Iteration 470, loss = 0.30628878\n",
      "Iteration 471, loss = 0.30591437\n",
      "Iteration 472, loss = 0.30554076\n",
      "Iteration 473, loss = 0.30516791\n",
      "Iteration 474, loss = 0.30479594\n",
      "Iteration 475, loss = 0.30442467\n",
      "Iteration 476, loss = 0.30405426\n",
      "Iteration 477, loss = 0.30368462\n",
      "Iteration 478, loss = 0.30331589\n",
      "Iteration 479, loss = 0.30294788\n",
      "Iteration 480, loss = 0.30258069\n",
      "Iteration 481, loss = 0.30221428\n",
      "Iteration 482, loss = 0.30184865\n",
      "Iteration 483, loss = 0.30148384\n",
      "Iteration 484, loss = 0.30111974\n",
      "Iteration 485, loss = 0.30075649\n",
      "Iteration 486, loss = 0.30039397\n",
      "Iteration 487, loss = 0.30003216\n",
      "Iteration 488, loss = 0.29967119\n",
      "Iteration 489, loss = 0.29931092\n",
      "Iteration 490, loss = 0.29895146\n",
      "Iteration 491, loss = 0.29859271\n",
      "Iteration 492, loss = 0.29823473\n",
      "Iteration 493, loss = 0.29787749\n",
      "Iteration 494, loss = 0.29752100\n",
      "Iteration 495, loss = 0.29716529\n",
      "Iteration 496, loss = 0.29681028\n",
      "Iteration 497, loss = 0.29645605\n",
      "Iteration 498, loss = 0.29610254\n",
      "Iteration 499, loss = 0.29574980\n",
      "Iteration 500, loss = 0.29539780\n",
      "Iteration 501, loss = 0.29504652\n",
      "Iteration 502, loss = 0.29469603\n",
      "Iteration 503, loss = 0.29434630\n",
      "Iteration 504, loss = 0.29399727\n",
      "Iteration 505, loss = 0.29364907\n",
      "Iteration 506, loss = 0.29330153\n",
      "Iteration 507, loss = 0.29295479\n",
      "Iteration 508, loss = 0.29260873\n",
      "Iteration 509, loss = 0.29226340\n",
      "Iteration 510, loss = 0.29191883\n",
      "Iteration 511, loss = 0.29157498\n",
      "Iteration 512, loss = 0.29123181\n",
      "Iteration 513, loss = 0.29088940\n",
      "Iteration 514, loss = 0.29054766\n",
      "Iteration 515, loss = 0.29020672\n",
      "Iteration 516, loss = 0.28986642\n",
      "Iteration 517, loss = 0.28952685\n",
      "Iteration 518, loss = 0.28918802\n",
      "Iteration 519, loss = 0.28884988\n",
      "Iteration 520, loss = 0.28851245\n",
      "Iteration 521, loss = 0.28817577\n",
      "Iteration 522, loss = 0.28783971\n",
      "Iteration 523, loss = 0.28750449\n",
      "Iteration 524, loss = 0.28717009\n",
      "Iteration 525, loss = 0.28683639\n",
      "Iteration 526, loss = 0.28650338\n",
      "Iteration 527, loss = 0.28617114\n",
      "Iteration 528, loss = 0.28583955\n",
      "Iteration 529, loss = 0.28550867\n",
      "Iteration 530, loss = 0.28517844\n",
      "Iteration 531, loss = 0.28484890\n",
      "Iteration 532, loss = 0.28452008\n",
      "Iteration 533, loss = 0.28419211\n",
      "Iteration 534, loss = 0.28386475\n",
      "Iteration 535, loss = 0.28353814\n",
      "Iteration 536, loss = 0.28321226\n",
      "Iteration 537, loss = 0.28288705\n",
      "Iteration 538, loss = 0.28256252\n",
      "Iteration 539, loss = 0.28223871\n",
      "Iteration 540, loss = 0.28191560\n",
      "Iteration 541, loss = 0.28159321\n",
      "Iteration 542, loss = 0.28127152\n",
      "Iteration 543, loss = 0.28095053\n",
      "Iteration 544, loss = 0.28063016\n",
      "Iteration 545, loss = 0.28031054\n",
      "Iteration 546, loss = 0.27999158\n",
      "Iteration 547, loss = 0.27967328\n",
      "Iteration 548, loss = 0.27935565\n",
      "Iteration 549, loss = 0.27903867\n",
      "Iteration 550, loss = 0.27872240\n",
      "Iteration 551, loss = 0.27840680\n",
      "Iteration 552, loss = 0.27809182\n",
      "Iteration 553, loss = 0.27777753\n",
      "Iteration 554, loss = 0.27746389\n",
      "Iteration 555, loss = 0.27715093\n",
      "Iteration 556, loss = 0.27683859\n",
      "Iteration 557, loss = 0.27652696\n",
      "Iteration 558, loss = 0.27621592\n",
      "Iteration 559, loss = 0.27590556\n",
      "Iteration 560, loss = 0.27559585\n",
      "Iteration 561, loss = 0.27528681\n",
      "Iteration 562, loss = 0.27497840\n",
      "Iteration 563, loss = 0.27467065\n",
      "Iteration 564, loss = 0.27436356\n",
      "Iteration 565, loss = 0.27405714\n",
      "Iteration 566, loss = 0.27375134\n",
      "Iteration 567, loss = 0.27344619\n",
      "Iteration 568, loss = 0.27314168\n",
      "Iteration 569, loss = 0.27283784\n",
      "Iteration 570, loss = 0.27253462\n",
      "Iteration 571, loss = 0.27223208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 572, loss = 0.27193018\n",
      "Iteration 573, loss = 0.27162891\n",
      "Iteration 574, loss = 0.27132830\n",
      "Iteration 575, loss = 0.27102827\n",
      "Iteration 576, loss = 0.27072888\n",
      "Iteration 577, loss = 0.27043011\n",
      "Iteration 578, loss = 0.27013199\n",
      "Iteration 579, loss = 0.26983452\n",
      "Iteration 580, loss = 0.26953763\n",
      "Iteration 581, loss = 0.26924143\n",
      "Iteration 582, loss = 0.26894589\n",
      "Iteration 583, loss = 0.26865107\n",
      "Iteration 584, loss = 0.26835695\n",
      "Iteration 585, loss = 0.26806344\n",
      "Iteration 586, loss = 0.26777057\n",
      "Iteration 587, loss = 0.26747834\n",
      "Iteration 588, loss = 0.26718672\n",
      "Iteration 589, loss = 0.26689579\n",
      "Iteration 590, loss = 0.26660552\n",
      "Iteration 591, loss = 0.26631586\n",
      "Iteration 592, loss = 0.26602681\n",
      "Iteration 593, loss = 0.26573843\n",
      "Iteration 594, loss = 0.26545067\n",
      "Iteration 595, loss = 0.26516352\n",
      "Iteration 596, loss = 0.26487700\n",
      "Iteration 597, loss = 0.26459114\n",
      "Iteration 598, loss = 0.26430589\n",
      "Iteration 599, loss = 0.26402127\n",
      "Iteration 600, loss = 0.26373725\n",
      "Iteration 601, loss = 0.26345386\n",
      "Iteration 602, loss = 0.26317105\n",
      "Iteration 603, loss = 0.26288884\n",
      "Iteration 604, loss = 0.26260722\n",
      "Iteration 605, loss = 0.26232626\n",
      "Iteration 606, loss = 0.26204587\n",
      "Iteration 607, loss = 0.26176610\n",
      "Iteration 608, loss = 0.26148691\n",
      "Iteration 609, loss = 0.26120831\n",
      "Iteration 610, loss = 0.26093028\n",
      "Iteration 611, loss = 0.26065285\n",
      "Iteration 612, loss = 0.26037599\n",
      "Iteration 613, loss = 0.26009973\n",
      "Iteration 614, loss = 0.25982407\n",
      "Iteration 615, loss = 0.25954904\n",
      "Iteration 616, loss = 0.25927458\n",
      "Iteration 617, loss = 0.25900073\n",
      "Iteration 618, loss = 0.25872746\n",
      "Iteration 619, loss = 0.25845476\n",
      "Iteration 620, loss = 0.25818263\n",
      "Iteration 621, loss = 0.25791107\n",
      "Iteration 622, loss = 0.25764010\n",
      "Iteration 623, loss = 0.25736966\n",
      "Iteration 624, loss = 0.25709976\n",
      "Iteration 625, loss = 0.25683043\n",
      "Iteration 626, loss = 0.25656164\n",
      "Iteration 627, loss = 0.25629345\n",
      "Iteration 628, loss = 0.25602587\n",
      "Iteration 629, loss = 0.25575888\n",
      "Iteration 630, loss = 0.25549245\n",
      "Iteration 631, loss = 0.25522661\n",
      "Iteration 632, loss = 0.25496136\n",
      "Iteration 633, loss = 0.25469670\n",
      "Iteration 634, loss = 0.25443261\n",
      "Iteration 635, loss = 0.25416912\n",
      "Iteration 636, loss = 0.25390617\n",
      "Iteration 637, loss = 0.25364380\n",
      "Iteration 638, loss = 0.25338202\n",
      "Iteration 639, loss = 0.25312078\n",
      "Iteration 640, loss = 0.25286007\n",
      "Iteration 641, loss = 0.25259990\n",
      "Iteration 642, loss = 0.25234019\n",
      "Iteration 643, loss = 0.25208105\n",
      "Iteration 644, loss = 0.25182238\n",
      "Iteration 645, loss = 0.25156419\n",
      "Iteration 646, loss = 0.25130656\n",
      "Iteration 647, loss = 0.25104944\n",
      "Iteration 648, loss = 0.25079283\n",
      "Iteration 649, loss = 0.25053672\n",
      "Iteration 650, loss = 0.25028112\n",
      "Iteration 651, loss = 0.25002604\n",
      "Iteration 652, loss = 0.24977150\n",
      "Iteration 653, loss = 0.24951746\n",
      "Iteration 654, loss = 0.24926390\n",
      "Iteration 655, loss = 0.24901089\n",
      "Iteration 656, loss = 0.24875834\n",
      "Iteration 657, loss = 0.24850633\n",
      "Iteration 658, loss = 0.24825486\n",
      "Iteration 659, loss = 0.24800390\n",
      "Iteration 660, loss = 0.24775348\n",
      "Iteration 661, loss = 0.24750360\n",
      "Iteration 662, loss = 0.24725423\n",
      "Iteration 663, loss = 0.24700540\n",
      "Iteration 664, loss = 0.24675708\n",
      "Iteration 665, loss = 0.24650921\n",
      "Iteration 666, loss = 0.24626177\n",
      "Iteration 667, loss = 0.24601485\n",
      "Iteration 668, loss = 0.24576844\n",
      "Iteration 669, loss = 0.24552255\n",
      "Iteration 670, loss = 0.24527717\n",
      "Iteration 671, loss = 0.24503231\n",
      "Iteration 672, loss = 0.24478800\n",
      "Iteration 673, loss = 0.24454427\n",
      "Iteration 674, loss = 0.24430102\n",
      "Iteration 675, loss = 0.24405815\n",
      "Iteration 676, loss = 0.24381574\n",
      "Iteration 677, loss = 0.24357378\n",
      "Iteration 678, loss = 0.24333233\n",
      "Iteration 679, loss = 0.24309138\n",
      "Iteration 680, loss = 0.24285094\n",
      "Iteration 681, loss = 0.24261100\n",
      "Iteration 682, loss = 0.24237156\n",
      "Iteration 683, loss = 0.24213261\n",
      "Iteration 684, loss = 0.24189415\n",
      "Iteration 685, loss = 0.24165620\n",
      "Iteration 686, loss = 0.24141865\n",
      "Iteration 687, loss = 0.24118150\n",
      "Iteration 688, loss = 0.24094487\n",
      "Iteration 689, loss = 0.24070869\n",
      "Iteration 690, loss = 0.24047284\n",
      "Iteration 691, loss = 0.24023739\n",
      "Iteration 692, loss = 0.24000241\n",
      "Iteration 693, loss = 0.23976789\n",
      "Iteration 694, loss = 0.23953385\n",
      "Iteration 695, loss = 0.23930028\n",
      "Iteration 696, loss = 0.23906720\n",
      "Iteration 697, loss = 0.23883462\n",
      "Iteration 698, loss = 0.23860251\n",
      "Iteration 699, loss = 0.23837091\n",
      "Iteration 700, loss = 0.23813979\n",
      "Iteration 701, loss = 0.23790916\n",
      "Iteration 702, loss = 0.23767898\n",
      "Iteration 703, loss = 0.23744927\n",
      "Iteration 704, loss = 0.23721997\n",
      "Iteration 705, loss = 0.23699108\n",
      "Iteration 706, loss = 0.23676256\n",
      "Iteration 707, loss = 0.23653451\n",
      "Iteration 708, loss = 0.23630693\n",
      "Iteration 709, loss = 0.23607973\n",
      "Iteration 710, loss = 0.23585294\n",
      "Iteration 711, loss = 0.23562662\n",
      "Iteration 712, loss = 0.23540076\n",
      "Iteration 713, loss = 0.23517537\n",
      "Iteration 714, loss = 0.23495041\n",
      "Iteration 715, loss = 0.23472582\n",
      "Iteration 716, loss = 0.23450140\n",
      "Iteration 717, loss = 0.23427708\n",
      "Iteration 718, loss = 0.23405314\n",
      "Iteration 719, loss = 0.23382962\n",
      "Iteration 720, loss = 0.23360651\n",
      "Iteration 721, loss = 0.23338382\n",
      "Iteration 722, loss = 0.23316149\n",
      "Iteration 723, loss = 0.23293949\n",
      "Iteration 724, loss = 0.23271793\n",
      "Iteration 725, loss = 0.23249680\n",
      "Iteration 726, loss = 0.23227611\n",
      "Iteration 727, loss = 0.23205587\n",
      "Iteration 728, loss = 0.23183601\n",
      "Iteration 729, loss = 0.23161649\n",
      "Iteration 730, loss = 0.23139734\n",
      "Iteration 731, loss = 0.23117853\n",
      "Iteration 732, loss = 0.23096016\n",
      "Iteration 733, loss = 0.23074223\n",
      "Iteration 734, loss = 0.23052475\n",
      "Iteration 735, loss = 0.23030770\n",
      "Iteration 736, loss = 0.23009074\n",
      "Iteration 737, loss = 0.22987412\n",
      "Iteration 738, loss = 0.22965794\n",
      "Iteration 739, loss = 0.22944218\n",
      "Iteration 740, loss = 0.22922685\n",
      "Iteration 741, loss = 0.22901197\n",
      "Iteration 742, loss = 0.22879751\n",
      "Iteration 743, loss = 0.22858330\n",
      "Iteration 744, loss = 0.22836951\n",
      "Iteration 745, loss = 0.22815611\n",
      "Iteration 746, loss = 0.22794315\n",
      "Iteration 747, loss = 0.22773061\n",
      "Iteration 748, loss = 0.22751823\n",
      "Iteration 749, loss = 0.22730626\n",
      "Iteration 750, loss = 0.22709473\n",
      "Iteration 751, loss = 0.22688366\n",
      "Iteration 752, loss = 0.22667302\n",
      "Iteration 753, loss = 0.22646283\n",
      "Iteration 754, loss = 0.22625307\n",
      "Iteration 755, loss = 0.22604375\n",
      "Iteration 756, loss = 0.22583488\n",
      "Iteration 757, loss = 0.22562625\n",
      "Iteration 758, loss = 0.22541796\n",
      "Iteration 759, loss = 0.22521009\n",
      "Iteration 760, loss = 0.22500265\n",
      "Iteration 761, loss = 0.22479554\n",
      "Iteration 762, loss = 0.22458865\n",
      "Iteration 763, loss = 0.22438218\n",
      "Iteration 764, loss = 0.22417612\n",
      "Iteration 765, loss = 0.22397050\n",
      "Iteration 766, loss = 0.22376527\n",
      "Iteration 767, loss = 0.22356048\n",
      "Iteration 768, loss = 0.22335613\n",
      "Iteration 769, loss = 0.22315220\n",
      "Iteration 770, loss = 0.22294870\n",
      "Iteration 771, loss = 0.22274562\n",
      "Iteration 772, loss = 0.22254299\n",
      "Iteration 773, loss = 0.22234078\n",
      "Iteration 774, loss = 0.22213902\n",
      "Iteration 775, loss = 0.22193769\n",
      "Iteration 776, loss = 0.22173680\n",
      "Iteration 777, loss = 0.22153635\n",
      "Iteration 778, loss = 0.22133634\n",
      "Iteration 779, loss = 0.22113678\n",
      "Iteration 780, loss = 0.22093765\n",
      "Iteration 781, loss = 0.22073895\n",
      "Iteration 782, loss = 0.22054070\n",
      "Iteration 783, loss = 0.22034288\n",
      "Iteration 784, loss = 0.22014550\n",
      "Iteration 785, loss = 0.21994853\n",
      "Iteration 786, loss = 0.21975204\n",
      "Iteration 787, loss = 0.21955595\n",
      "Iteration 788, loss = 0.21936030\n",
      "Iteration 789, loss = 0.21916508\n",
      "Iteration 790, loss = 0.21897029\n",
      "Iteration 791, loss = 0.21877595\n",
      "Iteration 792, loss = 0.21858204\n",
      "Iteration 793, loss = 0.21838856\n",
      "Iteration 794, loss = 0.21819551\n",
      "Iteration 795, loss = 0.21800290\n",
      "Iteration 796, loss = 0.21781073\n",
      "Iteration 797, loss = 0.21761899\n",
      "Iteration 798, loss = 0.21742767\n",
      "Iteration 799, loss = 0.21723677\n",
      "Iteration 800, loss = 0.21704630\n",
      "Iteration 801, loss = 0.21685622\n",
      "Iteration 802, loss = 0.21666659\n",
      "Iteration 803, loss = 0.21647736\n",
      "Iteration 804, loss = 0.21628856\n",
      "Iteration 805, loss = 0.21610017\n",
      "Iteration 806, loss = 0.21591222\n",
      "Iteration 807, loss = 0.21572466\n",
      "Iteration 808, loss = 0.21553752\n",
      "Iteration 809, loss = 0.21535076\n",
      "Iteration 810, loss = 0.21516441\n",
      "Iteration 811, loss = 0.21497847\n",
      "Iteration 812, loss = 0.21479297\n",
      "Iteration 813, loss = 0.21460786\n",
      "Iteration 814, loss = 0.21442317\n",
      "Iteration 815, loss = 0.21423888\n",
      "Iteration 816, loss = 0.21405501\n",
      "Iteration 817, loss = 0.21387156\n",
      "Iteration 818, loss = 0.21368850\n",
      "Iteration 819, loss = 0.21350584\n",
      "Iteration 820, loss = 0.21332357\n",
      "Iteration 821, loss = 0.21314170\n",
      "Iteration 822, loss = 0.21296022\n",
      "Iteration 823, loss = 0.21277912\n",
      "Iteration 824, loss = 0.21259843\n",
      "Iteration 825, loss = 0.21241814\n",
      "Iteration 826, loss = 0.21223823\n",
      "Iteration 827, loss = 0.21205873\n",
      "Iteration 828, loss = 0.21187961\n",
      "Iteration 829, loss = 0.21170089\n",
      "Iteration 830, loss = 0.21152258\n",
      "Iteration 831, loss = 0.21134465\n",
      "Iteration 832, loss = 0.21116713\n",
      "Iteration 833, loss = 0.21099000\n",
      "Iteration 834, loss = 0.21081327\n",
      "Iteration 835, loss = 0.21063694\n",
      "Iteration 836, loss = 0.21046098\n",
      "Iteration 837, loss = 0.21028539\n",
      "Iteration 838, loss = 0.21011018\n",
      "Iteration 839, loss = 0.20993536\n",
      "Iteration 840, loss = 0.20976089\n",
      "Iteration 841, loss = 0.20958680\n",
      "Iteration 842, loss = 0.20941307\n",
      "Iteration 843, loss = 0.20923974\n",
      "Iteration 844, loss = 0.20906673\n",
      "Iteration 845, loss = 0.20889409\n",
      "Iteration 846, loss = 0.20872181\n",
      "Iteration 847, loss = 0.20854990\n",
      "Iteration 848, loss = 0.20837837\n",
      "Iteration 849, loss = 0.20820722\n",
      "Iteration 850, loss = 0.20803638\n",
      "Iteration 851, loss = 0.20786590\n",
      "Iteration 852, loss = 0.20769577\n",
      "Iteration 853, loss = 0.20752599\n",
      "Iteration 854, loss = 0.20735657\n",
      "Iteration 855, loss = 0.20718749\n",
      "Iteration 856, loss = 0.20701878\n",
      "Iteration 857, loss = 0.20685043\n",
      "Iteration 858, loss = 0.20668243\n",
      "Iteration 859, loss = 0.20651480\n",
      "Iteration 860, loss = 0.20634755\n",
      "Iteration 861, loss = 0.20618066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 862, loss = 0.20601412\n",
      "Iteration 863, loss = 0.20584795\n",
      "Iteration 864, loss = 0.20568213\n",
      "Iteration 865, loss = 0.20551670\n",
      "Iteration 866, loss = 0.20535157\n",
      "Iteration 867, loss = 0.20518682\n",
      "Iteration 868, loss = 0.20502240\n",
      "Iteration 869, loss = 0.20485834\n",
      "Iteration 870, loss = 0.20469463\n",
      "Iteration 871, loss = 0.20453130\n",
      "Iteration 872, loss = 0.20436830\n",
      "Iteration 873, loss = 0.20420568\n",
      "Iteration 874, loss = 0.20404340\n",
      "Iteration 875, loss = 0.20388148\n",
      "Iteration 876, loss = 0.20371985\n",
      "Iteration 877, loss = 0.20355851\n",
      "Iteration 878, loss = 0.20339752\n",
      "Iteration 879, loss = 0.20323687\n",
      "Iteration 880, loss = 0.20307655\n",
      "Iteration 881, loss = 0.20291659\n",
      "Iteration 882, loss = 0.20275697\n",
      "Iteration 883, loss = 0.20259771\n",
      "Iteration 884, loss = 0.20243875\n",
      "Iteration 885, loss = 0.20228013\n",
      "Iteration 886, loss = 0.20212184\n",
      "Iteration 887, loss = 0.20196388\n",
      "Iteration 888, loss = 0.20180625\n",
      "Iteration 889, loss = 0.20164874\n",
      "Iteration 890, loss = 0.20149154\n",
      "Iteration 891, loss = 0.20133466\n",
      "Iteration 892, loss = 0.20117805\n",
      "Iteration 893, loss = 0.20102173\n",
      "Iteration 894, loss = 0.20086569\n",
      "Iteration 895, loss = 0.20070998\n",
      "Iteration 896, loss = 0.20055458\n",
      "Iteration 897, loss = 0.20039949\n",
      "Iteration 898, loss = 0.20024474\n",
      "Iteration 899, loss = 0.20009029\n",
      "Iteration 900, loss = 0.19993615\n",
      "Iteration 901, loss = 0.19978235\n",
      "Iteration 902, loss = 0.19962888\n",
      "Iteration 903, loss = 0.19947574\n",
      "Iteration 904, loss = 0.19932290\n",
      "Iteration 905, loss = 0.19917040\n",
      "Iteration 906, loss = 0.19901821\n",
      "Iteration 907, loss = 0.19886630\n",
      "Iteration 908, loss = 0.19871472\n",
      "Iteration 909, loss = 0.19856342\n",
      "Iteration 910, loss = 0.19841245\n",
      "Iteration 911, loss = 0.19826179\n",
      "Iteration 912, loss = 0.19811142\n",
      "Iteration 913, loss = 0.19796136\n",
      "Iteration 914, loss = 0.19781162\n",
      "Iteration 915, loss = 0.19766219\n",
      "Iteration 916, loss = 0.19751309\n",
      "Iteration 917, loss = 0.19736430\n",
      "Iteration 918, loss = 0.19721580\n",
      "Iteration 919, loss = 0.19706757\n",
      "Iteration 920, loss = 0.19691964\n",
      "Iteration 921, loss = 0.19677199\n",
      "Iteration 922, loss = 0.19662463\n",
      "Iteration 923, loss = 0.19647757\n",
      "Iteration 924, loss = 0.19633083\n",
      "Iteration 925, loss = 0.19618438\n",
      "Iteration 926, loss = 0.19603819\n",
      "Iteration 927, loss = 0.19589217\n",
      "Iteration 928, loss = 0.19574635\n",
      "Iteration 929, loss = 0.19560081\n",
      "Iteration 930, loss = 0.19545557\n",
      "Iteration 931, loss = 0.19531060\n",
      "Iteration 932, loss = 0.19516586\n",
      "Iteration 933, loss = 0.19502139\n",
      "Iteration 934, loss = 0.19487721\n",
      "Iteration 935, loss = 0.19473331\n",
      "Iteration 936, loss = 0.19458970\n",
      "Iteration 937, loss = 0.19444638\n",
      "Iteration 938, loss = 0.19430335\n",
      "Iteration 939, loss = 0.19416059\n",
      "Iteration 940, loss = 0.19401810\n",
      "Iteration 941, loss = 0.19387573\n",
      "Iteration 942, loss = 0.19373363\n",
      "Iteration 943, loss = 0.19359183\n",
      "Iteration 944, loss = 0.19345030\n",
      "Iteration 945, loss = 0.19330898\n",
      "Iteration 946, loss = 0.19316790\n",
      "Iteration 947, loss = 0.19302712\n",
      "Iteration 948, loss = 0.19288661\n",
      "Iteration 949, loss = 0.19274639\n",
      "Iteration 950, loss = 0.19260645\n",
      "Iteration 951, loss = 0.19246679\n",
      "Iteration 952, loss = 0.19232742\n",
      "Iteration 953, loss = 0.19218834\n",
      "Iteration 954, loss = 0.19204944\n",
      "Iteration 955, loss = 0.19191062\n",
      "Iteration 956, loss = 0.19177204\n",
      "Iteration 957, loss = 0.19163371\n",
      "Iteration 958, loss = 0.19149565\n",
      "Iteration 959, loss = 0.19135787\n",
      "Iteration 960, loss = 0.19122035\n",
      "Iteration 961, loss = 0.19108309\n",
      "Iteration 962, loss = 0.19094611\n",
      "Iteration 963, loss = 0.19080941\n",
      "Iteration 964, loss = 0.19067299\n",
      "Iteration 965, loss = 0.19053685\n",
      "Iteration 966, loss = 0.19040098\n",
      "Iteration 967, loss = 0.19026534\n",
      "Iteration 968, loss = 0.19012996\n",
      "Iteration 969, loss = 0.18999485\n",
      "Iteration 970, loss = 0.18986001\n",
      "Iteration 971, loss = 0.18972545\n",
      "Iteration 972, loss = 0.18959113\n",
      "Iteration 973, loss = 0.18945691\n",
      "Iteration 974, loss = 0.18932291\n",
      "Iteration 975, loss = 0.18918918\n",
      "Iteration 976, loss = 0.18905570\n",
      "Iteration 977, loss = 0.18892249\n",
      "Iteration 978, loss = 0.18878952\n",
      "Iteration 979, loss = 0.18865683\n",
      "Iteration 980, loss = 0.18852429\n",
      "Iteration 981, loss = 0.18839193\n",
      "Iteration 982, loss = 0.18825982\n",
      "Iteration 983, loss = 0.18812796\n",
      "Iteration 984, loss = 0.18799636\n",
      "Iteration 985, loss = 0.18786501\n",
      "Iteration 986, loss = 0.18773379\n",
      "Iteration 987, loss = 0.18760283\n",
      "Iteration 988, loss = 0.18747211\n",
      "Iteration 989, loss = 0.18734166\n",
      "Iteration 990, loss = 0.18721146\n",
      "Iteration 991, loss = 0.18708143\n",
      "Iteration 992, loss = 0.18695164\n",
      "Iteration 993, loss = 0.18682211\n",
      "Iteration 994, loss = 0.18669283\n",
      "Iteration 995, loss = 0.18656381\n",
      "Iteration 996, loss = 0.18643504\n",
      "Iteration 997, loss = 0.18630653\n",
      "Iteration 998, loss = 0.18617828\n",
      "Iteration 999, loss = 0.18605027\n",
      "Iteration 1000, loss = 0.18592242\n",
      "Iteration 1, loss = 1.92297910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y520\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2, loss = 1.54655419\n",
      "Iteration 3, loss = 1.22958849\n",
      "Iteration 4, loss = 1.08789872\n",
      "Iteration 5, loss = 1.10478581\n",
      "Iteration 6, loss = 1.05798564\n",
      "Iteration 7, loss = 0.96373703\n",
      "Iteration 8, loss = 0.88494460\n",
      "Iteration 9, loss = 0.81523066\n",
      "Iteration 10, loss = 0.75556560\n",
      "Iteration 11, loss = 0.70939626\n",
      "Iteration 12, loss = 0.67871216\n",
      "Iteration 13, loss = 0.65093841\n",
      "Iteration 14, loss = 0.62223110\n",
      "Iteration 15, loss = 0.59304100\n",
      "Iteration 16, loss = 0.56577725\n",
      "Iteration 17, loss = 0.54299241\n",
      "Iteration 18, loss = 0.52280194\n",
      "Iteration 19, loss = 0.50447089\n",
      "Iteration 20, loss = 0.48771945\n",
      "Iteration 21, loss = 0.47237390\n",
      "Iteration 22, loss = 0.45826048\n",
      "Iteration 23, loss = 0.44523285\n",
      "Iteration 24, loss = 0.43309978\n",
      "Iteration 25, loss = 0.42186616\n",
      "Iteration 26, loss = 0.41153311\n",
      "Iteration 27, loss = 0.40191926\n",
      "Iteration 28, loss = 0.39287899\n",
      "Iteration 29, loss = 0.38442256\n",
      "Iteration 30, loss = 0.37649335\n",
      "Iteration 31, loss = 0.36901602\n",
      "Iteration 32, loss = 0.36192931\n",
      "Iteration 33, loss = 0.35518569\n",
      "Iteration 34, loss = 0.34874456\n",
      "Iteration 35, loss = 0.34257262\n",
      "Iteration 36, loss = 0.33664932\n",
      "Iteration 37, loss = 0.33095138\n",
      "Iteration 38, loss = 0.32546271\n",
      "Iteration 39, loss = 0.32016946\n",
      "Iteration 40, loss = 0.31505537\n",
      "Iteration 41, loss = 0.31011469\n",
      "Iteration 42, loss = 0.30532398\n",
      "Iteration 43, loss = 0.30067578\n",
      "Iteration 44, loss = 0.29616234\n",
      "Iteration 45, loss = 0.29177672\n",
      "Iteration 46, loss = 0.28751277\n",
      "Iteration 47, loss = 0.28336581\n",
      "Iteration 48, loss = 0.27932987\n",
      "Iteration 49, loss = 0.27539944\n",
      "Iteration 50, loss = 0.27157060\n",
      "Iteration 51, loss = 0.26783992\n",
      "Iteration 52, loss = 0.26420507\n",
      "Iteration 53, loss = 0.26066271\n",
      "Iteration 54, loss = 0.25721097\n",
      "Iteration 55, loss = 0.25384407\n",
      "Iteration 56, loss = 0.25056083\n",
      "Iteration 57, loss = 0.24736021\n",
      "Iteration 58, loss = 0.24423873\n",
      "Iteration 59, loss = 0.24119432\n",
      "Iteration 60, loss = 0.23822480\n",
      "Iteration 61, loss = 0.23532834\n",
      "Iteration 62, loss = 0.23250279\n",
      "Iteration 63, loss = 0.22974676\n",
      "Iteration 64, loss = 0.22705783\n",
      "Iteration 65, loss = 0.22443454\n",
      "Iteration 66, loss = 0.22187497\n",
      "Iteration 67, loss = 0.21937766\n",
      "Iteration 68, loss = 0.21694072\n",
      "Iteration 69, loss = 0.21456274\n",
      "Iteration 70, loss = 0.21224228\n",
      "Iteration 71, loss = 0.20997781\n",
      "Iteration 72, loss = 0.20776739\n",
      "Iteration 73, loss = 0.20560955\n",
      "Iteration 74, loss = 0.20350295\n",
      "Iteration 75, loss = 0.20144633\n",
      "Iteration 76, loss = 0.19943819\n",
      "Iteration 77, loss = 0.19747716\n",
      "Iteration 78, loss = 0.19556196\n",
      "Iteration 79, loss = 0.19369150\n",
      "Iteration 80, loss = 0.19186454\n",
      "Iteration 81, loss = 0.19007970\n",
      "Iteration 82, loss = 0.18833606\n",
      "Iteration 83, loss = 0.18663245\n",
      "Iteration 84, loss = 0.18496768\n",
      "Iteration 85, loss = 0.18334086\n",
      "Iteration 86, loss = 0.18175088\n",
      "Iteration 87, loss = 0.18019651\n",
      "Iteration 88, loss = 0.17867708\n",
      "Iteration 89, loss = 0.17719188\n",
      "Iteration 90, loss = 0.17573967\n",
      "Iteration 91, loss = 0.17431997\n",
      "Iteration 92, loss = 0.17293170\n",
      "Iteration 93, loss = 0.17157366\n",
      "Iteration 94, loss = 0.17024494\n",
      "Iteration 95, loss = 0.16894514\n",
      "Iteration 96, loss = 0.16767456\n",
      "Iteration 97, loss = 0.16643190\n",
      "Iteration 98, loss = 0.16521612\n",
      "Iteration 99, loss = 0.16402600\n",
      "Iteration 100, loss = 0.16286089\n",
      "Iteration 101, loss = 0.16172021\n",
      "Iteration 102, loss = 0.16060334\n",
      "Iteration 103, loss = 0.15950965\n",
      "Iteration 104, loss = 0.15843854\n",
      "Iteration 105, loss = 0.15738924\n",
      "Iteration 106, loss = 0.15636105\n",
      "Iteration 107, loss = 0.15535344\n",
      "Iteration 108, loss = 0.15436612\n",
      "Iteration 109, loss = 0.15339842\n",
      "Iteration 110, loss = 0.15244992\n",
      "Iteration 111, loss = 0.15151994\n",
      "Iteration 112, loss = 0.15060803\n",
      "Iteration 113, loss = 0.14971364\n",
      "Iteration 114, loss = 0.14883642\n",
      "Iteration 115, loss = 0.14797583\n",
      "Iteration 116, loss = 0.14713143\n",
      "Iteration 117, loss = 0.14630289\n",
      "Iteration 118, loss = 0.14548974\n",
      "Iteration 119, loss = 0.14469159\n",
      "Iteration 120, loss = 0.14390801\n",
      "Iteration 121, loss = 0.14313872\n",
      "Iteration 122, loss = 0.14238334\n",
      "Iteration 123, loss = 0.14164157\n",
      "Iteration 124, loss = 0.14091301\n",
      "Iteration 125, loss = 0.14019738\n",
      "Iteration 126, loss = 0.13949433\n",
      "Iteration 127, loss = 0.13880354\n",
      "Iteration 128, loss = 0.13812470\n",
      "Iteration 129, loss = 0.13745754\n",
      "Iteration 130, loss = 0.13680177\n",
      "Iteration 131, loss = 0.13615713\n",
      "Iteration 132, loss = 0.13552341\n",
      "Iteration 133, loss = 0.13490032\n",
      "Iteration 134, loss = 0.13428759\n",
      "Iteration 135, loss = 0.13368503\n",
      "Iteration 136, loss = 0.13309241\n",
      "Iteration 137, loss = 0.13250946\n",
      "Iteration 138, loss = 0.13193597\n",
      "Iteration 139, loss = 0.13137172\n",
      "Iteration 140, loss = 0.13081651\n",
      "Iteration 141, loss = 0.13027013\n",
      "Iteration 142, loss = 0.12973239\n",
      "Iteration 143, loss = 0.12920313\n",
      "Iteration 144, loss = 0.12868214\n",
      "Iteration 145, loss = 0.12816927\n",
      "Iteration 146, loss = 0.12766431\n",
      "Iteration 147, loss = 0.12716709\n",
      "Iteration 148, loss = 0.12667746\n",
      "Iteration 149, loss = 0.12619523\n",
      "Iteration 150, loss = 0.12572027\n",
      "Iteration 151, loss = 0.12525242\n",
      "Iteration 152, loss = 0.12479153\n",
      "Iteration 153, loss = 0.12433749\n",
      "Iteration 154, loss = 0.12389012\n",
      "Iteration 155, loss = 0.12344927\n",
      "Iteration 156, loss = 0.12301483\n",
      "Iteration 157, loss = 0.12258664\n",
      "Iteration 158, loss = 0.12216462\n",
      "Iteration 159, loss = 0.12174862\n",
      "Iteration 160, loss = 0.12133851\n",
      "Iteration 161, loss = 0.12093418\n",
      "Iteration 162, loss = 0.12053551\n",
      "Iteration 163, loss = 0.12014238\n",
      "Iteration 164, loss = 0.11975470\n",
      "Iteration 165, loss = 0.11937234\n",
      "Iteration 166, loss = 0.11899521\n",
      "Iteration 167, loss = 0.11862321\n",
      "Iteration 168, loss = 0.11825624\n",
      "Iteration 169, loss = 0.11789420\n",
      "Iteration 170, loss = 0.11753700\n",
      "Iteration 171, loss = 0.11718454\n",
      "Iteration 172, loss = 0.11683674\n",
      "Iteration 173, loss = 0.11649352\n",
      "Iteration 174, loss = 0.11615480\n",
      "Iteration 175, loss = 0.11582049\n",
      "Iteration 176, loss = 0.11549049\n",
      "Iteration 177, loss = 0.11516474\n",
      "Iteration 178, loss = 0.11484314\n",
      "Iteration 179, loss = 0.11452562\n",
      "Iteration 180, loss = 0.11421212\n",
      "Iteration 181, loss = 0.11390255\n",
      "Iteration 182, loss = 0.11359686\n",
      "Iteration 183, loss = 0.11329502\n",
      "Iteration 184, loss = 0.11299692\n",
      "Iteration 185, loss = 0.11270248\n",
      "Iteration 186, loss = 0.11241165\n",
      "Iteration 187, loss = 0.11212436\n",
      "Iteration 188, loss = 0.11184054\n",
      "Iteration 189, loss = 0.11156013\n",
      "Iteration 190, loss = 0.11128307\n",
      "Iteration 191, loss = 0.11100931\n",
      "Iteration 192, loss = 0.11073879\n",
      "Iteration 193, loss = 0.11047146\n",
      "Iteration 194, loss = 0.11020719\n",
      "Iteration 195, loss = 0.10994599\n",
      "Iteration 196, loss = 0.10968782\n",
      "Iteration 197, loss = 0.10943258\n",
      "Iteration 198, loss = 0.10918024\n",
      "Iteration 199, loss = 0.10893078\n",
      "Iteration 200, loss = 0.10868414\n",
      "Iteration 201, loss = 0.10844025\n",
      "Iteration 202, loss = 0.10819910\n",
      "Iteration 203, loss = 0.10796065\n",
      "Iteration 204, loss = 0.10772483\n",
      "Iteration 205, loss = 0.10749160\n",
      "Iteration 206, loss = 0.10726093\n",
      "Iteration 207, loss = 0.10703280\n",
      "Iteration 208, loss = 0.10680717\n",
      "Iteration 209, loss = 0.10658400\n",
      "Iteration 210, loss = 0.10636325\n",
      "Iteration 211, loss = 0.10614488\n",
      "Iteration 212, loss = 0.10592886\n",
      "Iteration 213, loss = 0.10571514\n",
      "Iteration 214, loss = 0.10550366\n",
      "Iteration 215, loss = 0.10529441\n",
      "Iteration 216, loss = 0.10508735\n",
      "Iteration 217, loss = 0.10488240\n",
      "Iteration 218, loss = 0.10467958\n",
      "Iteration 219, loss = 0.10447884\n",
      "Iteration 220, loss = 0.10428015\n",
      "Iteration 221, loss = 0.10408344\n",
      "Iteration 222, loss = 0.10388872\n",
      "Iteration 223, loss = 0.10369589\n",
      "Iteration 224, loss = 0.10350500\n",
      "Iteration 225, loss = 0.10331601\n",
      "Iteration 226, loss = 0.10312888\n",
      "Iteration 227, loss = 0.10294360\n",
      "Iteration 228, loss = 0.10276014\n",
      "Iteration 229, loss = 0.10257847\n",
      "Iteration 230, loss = 0.10239855\n",
      "Iteration 231, loss = 0.10222039\n",
      "Iteration 232, loss = 0.10204391\n",
      "Iteration 233, loss = 0.10186912\n",
      "Iteration 234, loss = 0.10169603\n",
      "Iteration 235, loss = 0.10152460\n",
      "Iteration 236, loss = 0.10135480\n",
      "Iteration 237, loss = 0.10118658\n",
      "Iteration 238, loss = 0.10101993\n",
      "Iteration 239, loss = 0.10085486\n",
      "Iteration 240, loss = 0.10069134\n",
      "Iteration 241, loss = 0.10052931\n",
      "Iteration 242, loss = 0.10036878\n",
      "Iteration 243, loss = 0.10020971\n",
      "Iteration 244, loss = 0.10005209\n",
      "Iteration 245, loss = 0.09989588\n",
      "Iteration 246, loss = 0.09974109\n",
      "Iteration 247, loss = 0.09958767\n",
      "Iteration 248, loss = 0.09943560\n",
      "Iteration 249, loss = 0.09928488\n",
      "Iteration 250, loss = 0.09913550\n",
      "Iteration 251, loss = 0.09898745\n",
      "Iteration 252, loss = 0.09884072\n",
      "Iteration 253, loss = 0.09869536\n",
      "Iteration 254, loss = 0.09855129\n",
      "Iteration 255, loss = 0.09840851\n",
      "Iteration 256, loss = 0.09826703\n",
      "Iteration 257, loss = 0.09812667\n",
      "Iteration 258, loss = 0.09798749\n",
      "Iteration 259, loss = 0.09784996\n",
      "Iteration 260, loss = 0.09771364\n",
      "Iteration 261, loss = 0.09757851\n",
      "Iteration 262, loss = 0.09744453\n",
      "Iteration 263, loss = 0.09731170\n",
      "Iteration 264, loss = 0.09718000\n",
      "Iteration 265, loss = 0.09704849\n",
      "Iteration 266, loss = 0.09691407\n",
      "Iteration 267, loss = 0.09677970\n",
      "Iteration 268, loss = 0.09664548\n",
      "Iteration 269, loss = 0.09651155\n",
      "Iteration 270, loss = 0.09637785\n",
      "Iteration 271, loss = 0.09624425\n",
      "Iteration 272, loss = 0.09611142\n",
      "Iteration 273, loss = 0.09597943\n",
      "Iteration 274, loss = 0.09585108\n",
      "Iteration 275, loss = 0.09572069\n",
      "Iteration 276, loss = 0.09557650\n",
      "Iteration 277, loss = 0.09542532\n",
      "Iteration 278, loss = 0.09527986\n",
      "Iteration 279, loss = 0.09515244\n",
      "Iteration 280, loss = 0.09502578\n",
      "Iteration 281, loss = 0.09490379\n",
      "Iteration 282, loss = 0.09480090\n",
      "Iteration 283, loss = 0.09469795\n",
      "Iteration 284, loss = 0.09459241\n",
      "Iteration 285, loss = 0.09448457\n",
      "Iteration 286, loss = 0.09437442\n",
      "Iteration 287, loss = 0.09426252\n",
      "Iteration 288, loss = 0.09414915\n",
      "Iteration 289, loss = 0.09403423\n",
      "Iteration 290, loss = 0.09391975\n",
      "Iteration 291, loss = 0.09380835\n",
      "Iteration 292, loss = 0.09370543\n",
      "Iteration 293, loss = 0.09360275\n",
      "Iteration 294, loss = 0.09350027\n",
      "Iteration 295, loss = 0.09339799\n",
      "Iteration 296, loss = 0.09329594\n",
      "Iteration 297, loss = 0.09319415\n",
      "Iteration 298, loss = 0.09309269\n",
      "Iteration 299, loss = 0.09299162\n",
      "Iteration 300, loss = 0.09289091\n",
      "Iteration 301, loss = 0.09279564\n",
      "Iteration 302, loss = 0.09269753\n",
      "Iteration 303, loss = 0.09259832\n",
      "Iteration 304, loss = 0.09250438\n",
      "Iteration 305, loss = 0.09240910\n",
      "Iteration 306, loss = 0.09231400\n",
      "Iteration 307, loss = 0.09221909\n",
      "Iteration 308, loss = 0.09212654\n",
      "Iteration 309, loss = 0.09203292\n",
      "Iteration 310, loss = 0.09194180\n",
      "Iteration 311, loss = 0.09185034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(alpha=0.1, hidden_layer_sizes=40,\n",
       "                                     learning_rate_init=0.01, max_iter=1000,\n",
       "                                     random_state=1, solver='sgd', verbose=10),\n",
       "             param_grid={'hidden_layer_sizes': [20, 40, 60, 80, 100],\n",
       "                         'learning_rate_init': (0.1, 0.01, 0.001),\n",
       "                         'solver': ['adam', 'lbfgs', 'sgd']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokladnosc dla GridSearcha: 98.0%\n",
      "Najlepsze parametry:  {'hidden_layer_sizes': 40, 'learning_rate_init': 0.01, 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dokladnosc dla GridSearcha: \" + str(clf.score(x, y)*100) + \"%\")\n",
    "print(\"Najlepsze parametry:  \" + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron wielowarstwowy dla najlepszych parametrow\n",
    "mlp = MLPClassifier(alpha=0.1, hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'], \n",
    "                    learning_rate_init=clf.best_params_['learning_rate_init'],\n",
    "                    max_iter=1000, random_state=1, solver=clf.best_params_['solver'], verbose=10)\n",
    "mlp.out_activation = 'softmax'# jakos wplywa na wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.57858390\n",
      "Iteration 2, loss = 2.62061654\n",
      "Iteration 3, loss = 2.00769338\n",
      "Iteration 4, loss = 1.86721903\n",
      "Iteration 5, loss = 1.87454809\n",
      "Iteration 6, loss = 1.87131581\n",
      "Iteration 7, loss = 1.83294951\n",
      "Iteration 8, loss = 1.76485990\n",
      "Iteration 9, loss = 1.68979664\n",
      "Iteration 10, loss = 1.61827657\n",
      "Iteration 11, loss = 1.54523991\n",
      "Iteration 12, loss = 1.47299416\n",
      "Iteration 13, loss = 1.40580356\n",
      "Iteration 14, loss = 1.34793736\n",
      "Iteration 15, loss = 1.30133866\n",
      "Iteration 16, loss = 1.26483930\n",
      "Iteration 17, loss = 1.23498791\n",
      "Iteration 18, loss = 1.20864157\n",
      "Iteration 19, loss = 1.18367290\n",
      "Iteration 20, loss = 1.15940446\n",
      "Iteration 21, loss = 1.13640202\n",
      "Iteration 22, loss = 1.11401462\n",
      "Iteration 23, loss = 1.09187821\n",
      "Iteration 24, loss = 1.07018890\n",
      "Iteration 25, loss = 1.04926204\n",
      "Iteration 26, loss = 1.02950475\n",
      "Iteration 27, loss = 1.01130046\n",
      "Iteration 28, loss = 0.99488642\n",
      "Iteration 29, loss = 0.98022649\n",
      "Iteration 30, loss = 0.96712014\n",
      "Iteration 31, loss = 0.95534638\n",
      "Iteration 32, loss = 0.94469208\n",
      "Iteration 33, loss = 0.93495315\n",
      "Iteration 34, loss = 0.92601224\n",
      "Iteration 35, loss = 0.91777102\n",
      "Iteration 36, loss = 0.91012777\n",
      "Iteration 37, loss = 0.90299520\n",
      "Iteration 38, loss = 0.89631264\n",
      "Iteration 39, loss = 0.89002798\n",
      "Iteration 40, loss = 0.88409823\n",
      "Iteration 41, loss = 0.87849117\n",
      "Iteration 42, loss = 0.87317063\n",
      "Iteration 43, loss = 0.86810977\n",
      "Iteration 44, loss = 0.86328126\n",
      "Iteration 45, loss = 0.85866556\n",
      "Iteration 46, loss = 0.85423986\n",
      "Iteration 47, loss = 0.84998730\n",
      "Iteration 48, loss = 0.84589278\n",
      "Iteration 49, loss = 0.84193945\n",
      "Iteration 50, loss = 0.83812082\n",
      "Iteration 51, loss = 0.83442339\n",
      "Iteration 52, loss = 0.83083858\n",
      "Iteration 53, loss = 0.82736177\n",
      "Iteration 54, loss = 0.82398628\n",
      "Iteration 55, loss = 0.82070606\n",
      "Iteration 56, loss = 0.81751480\n",
      "Iteration 57, loss = 0.81440864\n",
      "Iteration 58, loss = 0.81138120\n",
      "Iteration 59, loss = 0.80843573\n",
      "Iteration 60, loss = 0.80556788\n",
      "Iteration 61, loss = 0.80277389\n",
      "Iteration 62, loss = 0.80004609\n",
      "Iteration 63, loss = 0.79738666\n",
      "Iteration 64, loss = 0.79478037\n",
      "Iteration 65, loss = 0.79222234\n",
      "Iteration 66, loss = 0.78971218\n",
      "Iteration 67, loss = 0.78724954\n",
      "Iteration 68, loss = 0.78482351\n",
      "Iteration 69, loss = 0.78242438\n",
      "Iteration 70, loss = 0.78005225\n",
      "Iteration 71, loss = 0.77770300\n",
      "Iteration 72, loss = 0.77537414\n",
      "Iteration 73, loss = 0.77306053\n",
      "Iteration 74, loss = 0.77076232\n",
      "Iteration 75, loss = 0.76848108\n",
      "Iteration 76, loss = 0.76621477\n",
      "Iteration 77, loss = 0.76396163\n",
      "Iteration 78, loss = 0.76172281\n",
      "Iteration 79, loss = 0.75949730\n",
      "Iteration 80, loss = 0.75728244\n",
      "Iteration 81, loss = 0.75507687\n",
      "Iteration 82, loss = 0.75287927\n",
      "Iteration 83, loss = 0.75068903\n",
      "Iteration 84, loss = 0.74850501\n",
      "Iteration 85, loss = 0.74632325\n",
      "Iteration 86, loss = 0.74414168\n",
      "Iteration 87, loss = 0.74195955\n",
      "Iteration 88, loss = 0.73977396\n",
      "Iteration 89, loss = 0.73758161\n",
      "Iteration 90, loss = 0.73538028\n",
      "Iteration 91, loss = 0.73316801\n",
      "Iteration 92, loss = 0.73095556\n",
      "Iteration 93, loss = 0.72877169\n",
      "Iteration 94, loss = 0.72664714\n",
      "Iteration 95, loss = 0.72456394\n",
      "Iteration 96, loss = 0.72249347\n",
      "Iteration 97, loss = 0.72043436\n",
      "Iteration 98, loss = 0.71826884\n",
      "Iteration 99, loss = 0.71601912\n",
      "Iteration 100, loss = 0.71376382\n",
      "Iteration 101, loss = 0.71150885\n",
      "Iteration 102, loss = 0.70924152\n",
      "Iteration 103, loss = 0.70695255\n",
      "Iteration 104, loss = 0.70463437\n",
      "Iteration 105, loss = 0.70229179\n",
      "Iteration 106, loss = 0.69992247\n",
      "Iteration 107, loss = 0.69751903\n",
      "Iteration 108, loss = 0.69508072\n",
      "Iteration 109, loss = 0.69260613\n",
      "Iteration 110, loss = 0.69008511\n",
      "Iteration 111, loss = 0.68751754\n",
      "Iteration 112, loss = 0.68490873\n",
      "Iteration 113, loss = 0.68225441\n",
      "Iteration 114, loss = 0.67955175\n",
      "Iteration 115, loss = 0.67679861\n",
      "Iteration 116, loss = 0.67399319\n",
      "Iteration 117, loss = 0.67113339\n",
      "Iteration 118, loss = 0.66821867\n",
      "Iteration 119, loss = 0.66525090\n",
      "Iteration 120, loss = 0.66228967\n",
      "Iteration 121, loss = 0.65933230\n",
      "Iteration 122, loss = 0.65636042\n",
      "Iteration 123, loss = 0.65334787\n",
      "Iteration 124, loss = 0.65029674\n",
      "Iteration 125, loss = 0.64731782\n",
      "Iteration 126, loss = 0.64434231\n",
      "Iteration 127, loss = 0.64140312\n",
      "Iteration 128, loss = 0.63865779\n",
      "Iteration 129, loss = 0.63605532\n",
      "Iteration 130, loss = 0.63352724\n",
      "Iteration 131, loss = 0.63089388\n",
      "Iteration 132, loss = 0.62816535\n",
      "Iteration 133, loss = 0.62534235\n",
      "Iteration 134, loss = 0.62243209\n",
      "Iteration 135, loss = 0.61944810\n",
      "Iteration 136, loss = 0.61648202\n",
      "Iteration 137, loss = 0.61360636\n",
      "Iteration 138, loss = 0.61075406\n",
      "Iteration 139, loss = 0.60790203\n",
      "Iteration 140, loss = 0.60504730\n",
      "Iteration 141, loss = 0.60216687\n",
      "Iteration 142, loss = 0.59925842\n",
      "Iteration 143, loss = 0.59632267\n",
      "Iteration 144, loss = 0.59336091\n",
      "Iteration 145, loss = 0.59037576\n",
      "Iteration 146, loss = 0.58738725\n",
      "Iteration 147, loss = 0.58438344\n",
      "Iteration 148, loss = 0.58136349\n",
      "Iteration 149, loss = 0.57832739\n",
      "Iteration 150, loss = 0.57527581\n",
      "Iteration 151, loss = 0.57222569\n",
      "Iteration 152, loss = 0.56915633\n",
      "Iteration 153, loss = 0.56606608\n",
      "Iteration 154, loss = 0.56296332\n",
      "Iteration 155, loss = 0.55984425\n",
      "Iteration 156, loss = 0.55671371\n",
      "Iteration 157, loss = 0.55356821\n",
      "Iteration 158, loss = 0.55041336\n",
      "Iteration 159, loss = 0.54724549\n",
      "Iteration 160, loss = 0.54406249\n",
      "Iteration 161, loss = 0.54086619\n",
      "Iteration 162, loss = 0.53765997\n",
      "Iteration 163, loss = 0.53444021\n",
      "Iteration 164, loss = 0.53120854\n",
      "Iteration 165, loss = 0.52796989\n",
      "Iteration 166, loss = 0.52471733\n",
      "Iteration 167, loss = 0.52145535\n",
      "Iteration 168, loss = 0.51818655\n",
      "Iteration 169, loss = 0.51490475\n",
      "Iteration 170, loss = 0.51162194\n",
      "Iteration 171, loss = 0.50832220\n",
      "Iteration 172, loss = 0.50502355\n",
      "Iteration 173, loss = 0.50171380\n",
      "Iteration 174, loss = 0.49840083\n",
      "Iteration 175, loss = 0.49509080\n",
      "Iteration 176, loss = 0.49179284\n",
      "Iteration 177, loss = 0.48850108\n",
      "Iteration 178, loss = 0.48520589\n",
      "Iteration 179, loss = 0.48192161\n",
      "Iteration 180, loss = 0.47869284\n",
      "Iteration 181, loss = 0.47549817\n",
      "Iteration 182, loss = 0.47232628\n",
      "Iteration 183, loss = 0.46916131\n",
      "Iteration 184, loss = 0.46600552\n",
      "Iteration 185, loss = 0.46289634\n",
      "Iteration 186, loss = 0.45981372\n",
      "Iteration 187, loss = 0.45675145\n",
      "Iteration 188, loss = 0.45375681\n",
      "Iteration 189, loss = 0.45079912\n",
      "Iteration 190, loss = 0.44791995\n",
      "Iteration 191, loss = 0.44513614\n",
      "Iteration 192, loss = 0.44235827\n",
      "Iteration 193, loss = 0.43957979\n",
      "Iteration 194, loss = 0.43677644\n",
      "Iteration 195, loss = 0.43396568\n",
      "Iteration 196, loss = 0.43116273\n",
      "Iteration 197, loss = 0.42839744\n",
      "Iteration 198, loss = 0.42566391\n",
      "Iteration 199, loss = 0.42297245\n",
      "Iteration 200, loss = 0.42031541\n",
      "Iteration 201, loss = 0.41769509\n",
      "Iteration 202, loss = 0.41509302\n",
      "Iteration 203, loss = 0.41251053\n",
      "Iteration 204, loss = 0.40994987\n",
      "Iteration 205, loss = 0.40741523\n",
      "Iteration 206, loss = 0.40490281\n",
      "Iteration 207, loss = 0.40241624\n",
      "Iteration 208, loss = 0.39995383\n",
      "Iteration 209, loss = 0.39751995\n",
      "Iteration 210, loss = 0.39510593\n",
      "Iteration 211, loss = 0.39272188\n",
      "Iteration 212, loss = 0.39035817\n",
      "Iteration 213, loss = 0.38802049\n",
      "Iteration 214, loss = 0.38571385\n",
      "Iteration 215, loss = 0.38342700\n",
      "Iteration 216, loss = 0.38116691\n",
      "Iteration 217, loss = 0.37893070\n",
      "Iteration 218, loss = 0.37672251\n",
      "Iteration 219, loss = 0.37453734\n",
      "Iteration 220, loss = 0.37237318\n",
      "Iteration 221, loss = 0.37024013\n",
      "Iteration 222, loss = 0.36812677\n",
      "Iteration 223, loss = 0.36604217\n",
      "Iteration 224, loss = 0.36397736\n",
      "Iteration 225, loss = 0.36194208\n",
      "Iteration 226, loss = 0.35992804\n",
      "Iteration 227, loss = 0.35793808\n",
      "Iteration 228, loss = 0.35597148\n",
      "Iteration 229, loss = 0.35403145\n",
      "Iteration 230, loss = 0.35211325\n",
      "Iteration 231, loss = 0.35022088\n",
      "Iteration 232, loss = 0.34835031\n",
      "Iteration 233, loss = 0.34650170\n",
      "Iteration 234, loss = 0.34467554\n",
      "Iteration 235, loss = 0.34287432\n",
      "Iteration 236, loss = 0.34109503\n",
      "Iteration 237, loss = 0.33933664\n",
      "Iteration 238, loss = 0.33760207\n",
      "Iteration 239, loss = 0.33588907\n",
      "Iteration 240, loss = 0.33419689\n",
      "Iteration 241, loss = 0.33252743\n",
      "Iteration 242, loss = 0.33087836\n",
      "Iteration 243, loss = 0.32925217\n",
      "Iteration 244, loss = 0.32764497\n",
      "Iteration 245, loss = 0.32605944\n",
      "Iteration 246, loss = 0.32449387\n",
      "Iteration 247, loss = 0.32294835\n",
      "Iteration 248, loss = 0.32142337\n",
      "Iteration 249, loss = 0.31991790\n",
      "Iteration 250, loss = 0.31843145\n",
      "Iteration 251, loss = 0.31696508\n",
      "Iteration 252, loss = 0.31551641\n",
      "Iteration 253, loss = 0.31408690\n",
      "Iteration 254, loss = 0.31267593\n",
      "Iteration 255, loss = 0.31128249\n",
      "Iteration 256, loss = 0.30990965\n",
      "Iteration 257, loss = 0.30855123\n",
      "Iteration 258, loss = 0.30721211\n",
      "Iteration 259, loss = 0.30589062\n",
      "Iteration 260, loss = 0.30458489\n",
      "Iteration 261, loss = 0.30329713\n",
      "Iteration 262, loss = 0.30202617\n",
      "Iteration 263, loss = 0.30076978\n",
      "Iteration 264, loss = 0.29953256\n",
      "Iteration 265, loss = 0.29830883\n",
      "Iteration 266, loss = 0.29710427\n",
      "Iteration 267, loss = 0.29591385\n",
      "Iteration 268, loss = 0.29474003\n",
      "Iteration 269, loss = 0.29358038\n",
      "Iteration 270, loss = 0.29243662\n",
      "Iteration 271, loss = 0.29130815\n",
      "Iteration 272, loss = 0.29019298\n",
      "Iteration 273, loss = 0.28909232\n",
      "Iteration 274, loss = 0.28800637\n",
      "Iteration 275, loss = 0.28693448\n",
      "Iteration 276, loss = 0.28587650\n",
      "Iteration 277, loss = 0.28483229\n",
      "Iteration 278, loss = 0.28380067\n",
      "Iteration 279, loss = 0.28278367\n",
      "Iteration 280, loss = 0.28177775\n",
      "Iteration 281, loss = 0.28078488\n",
      "Iteration 282, loss = 0.27980510\n",
      "Iteration 283, loss = 0.27883757\n",
      "Iteration 284, loss = 0.27788187\n",
      "Iteration 285, loss = 0.27693842\n",
      "Iteration 286, loss = 0.27600676\n",
      "Iteration 287, loss = 0.27508697\n",
      "Iteration 288, loss = 0.27417794\n",
      "Iteration 289, loss = 0.27328112\n",
      "Iteration 290, loss = 0.27239427\n",
      "Iteration 291, loss = 0.27151882\n",
      "Iteration 292, loss = 0.27065414\n",
      "Iteration 293, loss = 0.26980009\n",
      "Iteration 294, loss = 0.26895634\n",
      "Iteration 295, loss = 0.26812334\n",
      "Iteration 296, loss = 0.26730012\n",
      "Iteration 297, loss = 0.26648674\n",
      "Iteration 298, loss = 0.26568345\n",
      "Iteration 299, loss = 0.26488988\n",
      "Iteration 300, loss = 0.26410628\n",
      "Iteration 301, loss = 0.26333271\n",
      "Iteration 302, loss = 0.26256725\n",
      "Iteration 303, loss = 0.26181125\n",
      "Iteration 304, loss = 0.26106514\n",
      "Iteration 305, loss = 0.26032658\n",
      "Iteration 306, loss = 0.25959861\n",
      "Iteration 307, loss = 0.25887733\n",
      "Iteration 308, loss = 0.25816576\n",
      "Iteration 309, loss = 0.25746234\n",
      "Iteration 310, loss = 0.25676747\n",
      "Iteration 311, loss = 0.25607998\n",
      "Iteration 312, loss = 0.25540051\n",
      "Iteration 313, loss = 0.25472956\n",
      "Iteration 314, loss = 0.25406538\n",
      "Iteration 315, loss = 0.25340928\n",
      "Iteration 316, loss = 0.25276085\n",
      "Iteration 317, loss = 0.25211933\n",
      "Iteration 318, loss = 0.25148687\n",
      "Iteration 319, loss = 0.25085958\n",
      "Iteration 320, loss = 0.25024092\n",
      "Iteration 321, loss = 0.24962846\n",
      "Iteration 322, loss = 0.24902275\n",
      "Iteration 323, loss = 0.24842521\n",
      "Iteration 324, loss = 0.24783308\n",
      "Iteration 325, loss = 0.24724855\n",
      "Iteration 326, loss = 0.24666946\n",
      "Iteration 327, loss = 0.24609832\n",
      "Iteration 328, loss = 0.24553209\n",
      "Iteration 329, loss = 0.24497323\n",
      "Iteration 330, loss = 0.24442019\n",
      "Iteration 331, loss = 0.24387283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 332, loss = 0.24333281\n",
      "Iteration 333, loss = 0.24279704\n",
      "Iteration 334, loss = 0.24226850\n",
      "Iteration 335, loss = 0.24174503\n",
      "Iteration 336, loss = 0.24122706\n",
      "Iteration 337, loss = 0.24071593\n",
      "Iteration 338, loss = 0.24020867\n",
      "Iteration 339, loss = 0.23970791\n",
      "Iteration 340, loss = 0.23921240\n",
      "Iteration 341, loss = 0.23872164\n",
      "Iteration 342, loss = 0.23823680\n",
      "Iteration 343, loss = 0.23775665\n",
      "Iteration 344, loss = 0.23728130\n",
      "Iteration 345, loss = 0.23681231\n",
      "Iteration 346, loss = 0.23634706\n",
      "Iteration 347, loss = 0.23588643\n",
      "Iteration 348, loss = 0.23543214\n",
      "Iteration 349, loss = 0.23498044\n",
      "Iteration 350, loss = 0.23453532\n",
      "Iteration 351, loss = 0.23409359\n",
      "Iteration 352, loss = 0.23365618\n",
      "Iteration 353, loss = 0.23322433\n",
      "Iteration 354, loss = 0.23279592\n",
      "Iteration 355, loss = 0.23237200\n",
      "Iteration 356, loss = 0.23195365\n",
      "Iteration 357, loss = 0.23153788\n",
      "Iteration 358, loss = 0.23112675\n",
      "Iteration 359, loss = 0.23072067\n",
      "Iteration 360, loss = 0.23031767\n",
      "Iteration 361, loss = 0.22991880\n",
      "Iteration 362, loss = 0.22952390\n",
      "Iteration 363, loss = 0.22913374\n",
      "Iteration 364, loss = 0.22874667\n",
      "Iteration 365, loss = 0.22836320\n",
      "Iteration 366, loss = 0.22798346\n",
      "Iteration 367, loss = 0.22760829\n",
      "Iteration 368, loss = 0.22723602\n",
      "Iteration 369, loss = 0.22686719\n",
      "Iteration 370, loss = 0.22650279\n",
      "Iteration 371, loss = 0.22614093\n",
      "Iteration 372, loss = 0.22578270\n",
      "Iteration 373, loss = 0.22542911\n",
      "Iteration 374, loss = 0.22507838\n",
      "Iteration 375, loss = 0.22473090\n",
      "Iteration 376, loss = 0.22438672\n",
      "Iteration 377, loss = 0.22404694\n",
      "Iteration 378, loss = 0.22370893\n",
      "Iteration 379, loss = 0.22337457\n",
      "Iteration 380, loss = 0.22304321\n",
      "Iteration 381, loss = 0.22271498\n",
      "Iteration 382, loss = 0.22239047\n",
      "Iteration 383, loss = 0.22206844\n",
      "Iteration 384, loss = 0.22174939\n",
      "Iteration 385, loss = 0.22143310\n",
      "Iteration 386, loss = 0.22111969\n",
      "Iteration 387, loss = 0.22080903\n",
      "Iteration 388, loss = 0.22050255\n",
      "Iteration 389, loss = 0.22019686\n",
      "Iteration 390, loss = 0.21989476\n",
      "Iteration 391, loss = 0.21959531\n",
      "Iteration 392, loss = 0.21929896\n",
      "Iteration 393, loss = 0.21900501\n",
      "Iteration 394, loss = 0.21871360\n",
      "Iteration 395, loss = 0.21842468\n",
      "Iteration 396, loss = 0.21813828\n",
      "Iteration 397, loss = 0.21785427\n",
      "Iteration 398, loss = 0.21757266\n",
      "Iteration 399, loss = 0.21729417\n",
      "Iteration 400, loss = 0.21701740\n",
      "Iteration 401, loss = 0.21674326\n",
      "Iteration 402, loss = 0.21647140\n",
      "Iteration 403, loss = 0.21620177\n",
      "Iteration 404, loss = 0.21593437\n",
      "Iteration 405, loss = 0.21566993\n",
      "Iteration 406, loss = 0.21540698\n",
      "Iteration 407, loss = 0.21514643\n",
      "Iteration 408, loss = 0.21488804\n",
      "Iteration 409, loss = 0.21463175\n",
      "Iteration 410, loss = 0.21437757\n",
      "Iteration 411, loss = 0.21412543\n",
      "Iteration 412, loss = 0.21387648\n",
      "Iteration 413, loss = 0.21362793\n",
      "Iteration 414, loss = 0.21338213\n",
      "Iteration 415, loss = 0.21313827\n",
      "Iteration 416, loss = 0.21289634\n",
      "Iteration 417, loss = 0.21265634\n",
      "Iteration 418, loss = 0.21241838\n",
      "Iteration 419, loss = 0.21218266\n",
      "Iteration 420, loss = 0.21194856\n",
      "Iteration 421, loss = 0.21171627\n",
      "Iteration 422, loss = 0.21148581\n",
      "Iteration 423, loss = 0.21125710\n",
      "Iteration 424, loss = 0.21103015\n",
      "Iteration 425, loss = 0.21080520\n",
      "Iteration 426, loss = 0.21058184\n",
      "Iteration 427, loss = 0.21036035\n",
      "Iteration 428, loss = 0.21014163\n",
      "Iteration 429, loss = 0.20992295\n",
      "Iteration 430, loss = 0.20970667\n",
      "Iteration 431, loss = 0.20949200\n",
      "Iteration 432, loss = 0.20927894\n",
      "Iteration 433, loss = 0.20906749\n",
      "Iteration 434, loss = 0.20885762\n",
      "Iteration 435, loss = 0.20864945\n",
      "Iteration 436, loss = 0.20844272\n",
      "Iteration 437, loss = 0.20823843\n",
      "Iteration 438, loss = 0.20803439\n",
      "Iteration 439, loss = 0.20783237\n",
      "Iteration 440, loss = 0.20763193\n",
      "Iteration 441, loss = 0.20743293\n",
      "Iteration 442, loss = 0.20723533\n",
      "Iteration 443, loss = 0.20703923\n",
      "Iteration 444, loss = 0.20684454\n",
      "Iteration 445, loss = 0.20665120\n",
      "Iteration 446, loss = 0.20645933\n",
      "Iteration 447, loss = 0.20626881\n",
      "Iteration 448, loss = 0.20607961\n",
      "Iteration 449, loss = 0.20589186\n",
      "Iteration 450, loss = 0.20570635\n",
      "Iteration 451, loss = 0.20552057\n",
      "Iteration 452, loss = 0.20533693\n",
      "Iteration 453, loss = 0.20515452\n",
      "Iteration 454, loss = 0.20497338\n",
      "Iteration 455, loss = 0.20479350\n",
      "Iteration 456, loss = 0.20461488\n",
      "Iteration 457, loss = 0.20443748\n",
      "Iteration 458, loss = 0.20426131\n",
      "Iteration 459, loss = 0.20408637\n",
      "Iteration 460, loss = 0.20391266\n",
      "Iteration 461, loss = 0.20374006\n",
      "Iteration 462, loss = 0.20356920\n",
      "Iteration 463, loss = 0.20339880\n",
      "Iteration 464, loss = 0.20322988\n",
      "Iteration 465, loss = 0.20306210\n",
      "Iteration 466, loss = 0.20289542\n",
      "Iteration 467, loss = 0.20272986\n",
      "Iteration 468, loss = 0.20256540\n",
      "Iteration 469, loss = 0.20240204\n",
      "Iteration 470, loss = 0.20223979\n",
      "Iteration 471, loss = 0.20207856\n",
      "Iteration 472, loss = 0.20191838\n",
      "Iteration 473, loss = 0.20175930\n",
      "Iteration 474, loss = 0.20160120\n",
      "Iteration 475, loss = 0.20144414\n",
      "Iteration 476, loss = 0.20128814\n",
      "Iteration 477, loss = 0.20113389\n",
      "Iteration 478, loss = 0.20097954\n",
      "Iteration 479, loss = 0.20082669\n",
      "Iteration 480, loss = 0.20067481\n",
      "Iteration 481, loss = 0.20052384\n",
      "Iteration 482, loss = 0.20037390\n",
      "Iteration 483, loss = 0.20022489\n",
      "Iteration 484, loss = 0.20007682\n",
      "Iteration 485, loss = 0.19992967\n",
      "Iteration 486, loss = 0.19978344\n",
      "Iteration 487, loss = 0.19963816\n",
      "Iteration 488, loss = 0.19949375\n",
      "Iteration 489, loss = 0.19935024\n",
      "Iteration 490, loss = 0.19920767\n",
      "Iteration 491, loss = 0.19906592\n",
      "Iteration 492, loss = 0.19892596\n",
      "Iteration 493, loss = 0.19878544\n",
      "Iteration 494, loss = 0.19864637\n",
      "Iteration 495, loss = 0.19850821\n",
      "Iteration 496, loss = 0.19837084\n",
      "Iteration 497, loss = 0.19823433\n",
      "Iteration 498, loss = 0.19809865\n",
      "Iteration 499, loss = 0.19796377\n",
      "Iteration 500, loss = 0.19782968\n",
      "Iteration 501, loss = 0.19769640\n",
      "Iteration 502, loss = 0.19756393\n",
      "Iteration 503, loss = 0.19743222\n",
      "Iteration 504, loss = 0.19730128\n",
      "Iteration 505, loss = 0.19717117\n",
      "Iteration 506, loss = 0.19704177\n",
      "Iteration 507, loss = 0.19691314\n",
      "Iteration 508, loss = 0.19678562\n",
      "Iteration 509, loss = 0.19665843\n",
      "Iteration 510, loss = 0.19653213\n",
      "Iteration 511, loss = 0.19640656\n",
      "Iteration 512, loss = 0.19628172\n",
      "Iteration 513, loss = 0.19615758\n",
      "Iteration 514, loss = 0.19603415\n",
      "Iteration 515, loss = 0.19591142\n",
      "Iteration 516, loss = 0.19578940\n",
      "Iteration 517, loss = 0.19566805\n",
      "Iteration 518, loss = 0.19554739\n",
      "Iteration 519, loss = 0.19542740\n",
      "Iteration 520, loss = 0.19530811\n",
      "Iteration 521, loss = 0.19518945\n",
      "Iteration 522, loss = 0.19507148\n",
      "Iteration 523, loss = 0.19495416\n",
      "Iteration 524, loss = 0.19483748\n",
      "Iteration 525, loss = 0.19472147\n",
      "Iteration 526, loss = 0.19460614\n",
      "Iteration 527, loss = 0.19449146\n",
      "Iteration 528, loss = 0.19437824\n",
      "Iteration 529, loss = 0.19426425\n",
      "Iteration 530, loss = 0.19415151\n",
      "Iteration 531, loss = 0.19403939\n",
      "Iteration 532, loss = 0.19392788\n",
      "Iteration 533, loss = 0.19381695\n",
      "Iteration 534, loss = 0.19370662\n",
      "Iteration 535, loss = 0.19359689\n",
      "Iteration 536, loss = 0.19348773\n",
      "Iteration 537, loss = 0.19337915\n",
      "Iteration 538, loss = 0.19327113\n",
      "Iteration 539, loss = 0.19316372\n",
      "Iteration 540, loss = 0.19305682\n",
      "Iteration 541, loss = 0.19295052\n",
      "Iteration 542, loss = 0.19284477\n",
      "Iteration 543, loss = 0.19273955\n",
      "Iteration 544, loss = 0.19263491\n",
      "Iteration 545, loss = 0.19253078\n",
      "Iteration 546, loss = 0.19242721\n",
      "Iteration 547, loss = 0.19232418\n",
      "Iteration 548, loss = 0.19222166\n",
      "Iteration 549, loss = 0.19211976\n",
      "Iteration 550, loss = 0.19201843\n",
      "Iteration 551, loss = 0.19191752\n",
      "Iteration 552, loss = 0.19181715\n",
      "Iteration 553, loss = 0.19171728\n",
      "Iteration 554, loss = 0.19161790\n",
      "Iteration 555, loss = 0.19151902\n",
      "Iteration 556, loss = 0.19142065\n",
      "Iteration 557, loss = 0.19132274\n",
      "Iteration 558, loss = 0.19122532\n",
      "Iteration 559, loss = 0.19112839\n",
      "Iteration 560, loss = 0.19103192\n",
      "Iteration 561, loss = 0.19093593\n",
      "Iteration 562, loss = 0.19084042\n",
      "Iteration 563, loss = 0.19074536\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.1, hidden_layer_sizes=40, learning_rate_init=0.01,\n",
       "              max_iter=1000, random_state=1, solver='sgd', verbose=10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trening Perceptrona z najlepszymi wartosciami\n",
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokladnosc dla danych treningowych: 97.5%\n",
      "Dokladnosc dla danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Dokladnosc dla danych treningowych: \" + str(mlp.score(x_train, y_train)*100) + \"%\")\n",
    "print(\"Dokladnosc dla danych testowych: \" + str(mlp.score(x_test, y_test)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wykres funkcji celu dla solvera 'sgd'\n",
    "x_label = []\n",
    "for x in range(len(mlp.loss_curve_)):\n",
    "    x_label.append(x)\n",
    "plt.plot(x_label, mlp.loss_curve_)\n",
    "plt.grid(which='both')\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Numerical value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " samples avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Raport klasyfikacji\n",
    "y_true, y_pred = y_test, mlp.predict(x_test)\n",
    "print(classification_report(y_true, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
